[
    {
        "title": "Product-Quantised Image Representation for High-Quality Image Synthesis",
        "summary": "Product quantisation (PQ) is a classical method for scalable vector encoding,\nyet it has seen limited usage for latent representations in high-fidelity image\ngeneration. In this work, we introduce PQGAN, a quantised image autoencoder\nthat integrates PQ into the well-known vector quantisation (VQ) framework of\nVQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in\nterms of reconstruction performance, including both quantisation methods and\ntheir continuous counterparts. We achieve a PSNR score of 37dB, where prior\nwork achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up\nto 96%. Our key to success is a thorough analysis of the interaction between\ncodebook size, embedding dimensionality, and subspace factorisation, with\nvector and scalar quantisation as special cases. We obtain novel findings, such\nthat the performance of VQ and PQ behaves in opposite ways when scaling the\nembedding dimension. Furthermore, our analysis shows performance trends for PQ\nthat help guide optimal hyperparameter selection. Finally, we demonstrate that\nPQGAN can be seamlessly integrated into pre-trained diffusion models. This\nenables either a significantly faster and more compute-efficient generation, or\na doubling of the output resolution at no additional cost, positioning PQ as a\nstrong extension for discrete latent representation in image synthesis.",
        "url": "http://arxiv.org/abs/2510.03191v1",
        "published_date": "2025-10-03T17:17:38+00:00",
        "updated_date": "2025-10-03T17:17:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Denis Zavadski",
            "Nikita Philip Tatsch",
            "Carsten Rother"
        ],
        "tldr": "The paper introduces PQGAN, a product-quantized autoencoder that significantly improves image reconstruction and can be integrated into diffusion models for faster generation or higher resolution at the same cost.",
        "tldr_zh": "该论文介绍了 PQGAN，一种产品量化自编码器，显著提高了图像重建效果，并且可以集成到扩散模型中，以更快的速度生成图像，或者以相同的成本获得更高的分辨率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft",
        "summary": "Autoregressive video diffusion models have proved effective for world\nmodeling and interactive scene generation, with Minecraft gameplay as a\nrepresentative application. To faithfully simulate play, a model must generate\nnatural content while exploring new scenes and preserve spatial consistency\nwhen revisiting explored areas. Under limited computation budgets, it must\ncompress and exploit historical cues within a finite context window, which\nexposes a trade-off: Temporal-only memory lacks long-term spatial consistency,\nwhereas adding spatial memory strengthens consistency but may degrade new scene\ngeneration quality when the model over-relies on insufficient spatial context.\nWe present Memory Forcing, a learning framework that pairs training protocols\nwith a geometry-indexed spatial memory. Hybrid Training exposes distinct\ngameplay regimes, guiding the model to rely on temporal memory during\nexploration and incorporate spatial memory for revisits. Chained Forward\nTraining extends autoregressive training with model rollouts, where chained\npredictions create larger pose variations and encourage reliance on spatial\nmemory for maintaining consistency. Point-to-Frame Retrieval efficiently\nretrieves history by mapping currently visible points to their source frames,\nwhile Incremental 3D Reconstruction maintains and updates an explicit 3D cache.\nExtensive experiments demonstrate that Memory Forcing achieves superior\nlong-term spatial consistency and generative quality across diverse\nenvironments, while maintaining computational efficiency for extended\nsequences.",
        "url": "http://arxiv.org/abs/2510.03198v1",
        "published_date": "2025-10-03T17:35:16+00:00",
        "updated_date": "2025-10-03T17:35:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junchao Huang",
            "Xinting Hu",
            "Boyao Han",
            "Shaoshuai Shi",
            "Zhuotao Tian",
            "Tianyu He",
            "Li Jiang"
        ],
        "tldr": "This paper introduces Memory Forcing, a framework that combines temporal and spatial memory with specific training protocols and a geometry-indexed spatial memory to improve long-term spatial consistency and generative quality in autoregressive video diffusion models for Minecraft scene generation.",
        "tldr_zh": "本文介绍了一种名为Memory Forcing的框架，它结合了时间记忆和空间记忆，以及特定的训练协议和几何索引空间记忆，以提高Minecraft场景生成中自回归视频扩散模型的长期空间一致性和生成质量。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Mask2IV: Interaction-Centric Video Generation via Mask Trajectories",
        "summary": "Generating interaction-centric videos, such as those depicting humans or\nrobots interacting with objects, is crucial for embodied intelligence, as they\nprovide rich and diverse visual priors for robot learning, manipulation policy\ntraining, and affordance reasoning. However, existing methods often struggle to\nmodel such complex and dynamic interactions. While recent studies show that\nmasks can serve as effective control signals and enhance generation quality,\nobtaining dense and precise mask annotations remains a major challenge for\nreal-world use. To overcome this limitation, we introduce Mask2IV, a novel\nframework specifically designed for interaction-centric video generation. It\nadopts a decoupled two-stage pipeline that first predicts plausible motion\ntrajectories for both actor and object, then generates a video conditioned on\nthese trajectories. This design eliminates the need for dense mask inputs from\nusers while preserving the flexibility to manipulate the interaction process.\nFurthermore, Mask2IV supports versatile and intuitive control, allowing users\nto specify the target object of interaction and guide the motion trajectory\nthrough action descriptions or spatial position cues. To support systematic\ntraining and evaluation, we curate two benchmarks covering diverse action and\nobject categories across both human-object interaction and robotic manipulation\nscenarios. Extensive experiments demonstrate that our method achieves superior\nvisual realism and controllability compared to existing baselines.",
        "url": "http://arxiv.org/abs/2510.03135v1",
        "published_date": "2025-10-03T16:04:33+00:00",
        "updated_date": "2025-10-03T16:04:33+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Gen Li",
            "Bo Zhao",
            "Jianfei Yang",
            "Laura Sevilla-Lara"
        ],
        "tldr": "The paper introduces Mask2IV, a two-stage framework for interaction-centric video generation that predicts actor and object motion trajectories and then generates the video, enabling flexible control without dense mask annotations.",
        "tldr_zh": "该论文介绍了一种名为Mask2IV的交互中心视频生成框架，它通过预测演员和物体的运动轨迹，然后生成视频，从而实现灵活的控制，而无需密集的掩码标注。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction",
        "summary": "This study focuses on a challenging yet promising task,\nText-to-Sounding-Video (T2SV) generation, which aims to generate a video with\nsynchronized audio from text conditions, meanwhile ensuring both modalities are\naligned with text. Despite progress in joint audio-video training, two critical\nchallenges still remain unaddressed: (1) a single, shared text caption where\nthe text for video is equal to the text for audio often creates modal\ninterference, confusing the pretrained backbones, and (2) the optimal mechanism\nfor cross-modal feature interaction remains unclear. To address these\nchallenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)\nframework that generates pairs of disentangled captions, a video caption, and\nan audio caption, eliminating interference at the conditioning stage. Based on\nHVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,\nwhich employs a Dual CrossAttention (DCA) mechanism that acts as a robust\n``bridge\" to enable a symmetric, bidirectional exchange of information,\nachieving both semantic and temporal synchronization. Extensive experiments on\nthree benchmark datasets, supported by human evaluations, demonstrate that our\nmethod achieves state-of-the-art results on most metrics. Comprehensive\nablation studies further validate the effectiveness of our contributions,\noffering key insights for the future T2SV task. All the codes and checkpoints\nwill be publicly released.",
        "url": "http://arxiv.org/abs/2510.03117v1",
        "published_date": "2025-10-03T15:43:56+00:00",
        "updated_date": "2025-10-03T15:43:56+00:00",
        "categories": [
            "cs.CV",
            "cs.SD"
        ],
        "authors": [
            "Kaisi Guan",
            "Xihua Wang",
            "Zhengfeng Lai",
            "Xin Cheng",
            "Peng Zhang",
            "XiaoJiang Liu",
            "Ruihua Song",
            "Meng Cao"
        ],
        "tldr": "The paper introduces HVGC and BridgeDiT, a novel approach to text-to-sounding video generation, addressing modal interference and improving cross-modal interaction through disentangled captions and a dual diffusion transformer with Dual CrossAttention.",
        "tldr_zh": "本文提出了一种新的文本到声音视频生成方法HVGC和BridgeDiT，通过解耦字幕和带有双重交叉注意力的双扩散转换器，解决了模态干扰并改进了跨模态交互。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "What Drives Compositional Generalization in Visual Generative Models?",
        "summary": "Compositional generalization, the ability to generate novel combinations of\nknown concepts, is a key ingredient for visual generative models. Yet, not all\nmechanisms that enable or inhibit it are fully understood. In this work, we\nconduct a systematic study of how various design choices influence\ncompositional generalization in image and video generation in a positive or\nnegative way. Through controlled experiments, we identify two key factors: (i)\nwhether the training objective operates on a discrete or continuous\ndistribution, and (ii) to what extent conditioning provides information about\nthe constituent concepts during training. Building on these insights, we show\nthat relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based\nobjective can improve compositional performance in discrete models like\nMaskGIT.",
        "url": "http://arxiv.org/abs/2510.03075v1",
        "published_date": "2025-10-03T15:02:27+00:00",
        "updated_date": "2025-10-03T15:02:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Karim Farid",
            "Rajat Sahay",
            "Yumna Ali Alnaggar",
            "Simon Schrodi",
            "Volker Fischer",
            "Cordelia Schmid",
            "Thomas Brox"
        ],
        "tldr": "This paper investigates factors influencing compositional generalization in visual generative models, identifying the importance of discrete/continuous training objectives and the informativeness of conditioning. They improve a discrete model using a continuous objective.",
        "tldr_zh": "本文研究了影响视觉生成模型中组合泛化的因素，确定了离散/连续训练目标和条件信息的重要性。他们使用连续目标改进了一种离散模型。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "When and Where do Events Switch in Multi-Event Video Generation?",
        "summary": "Text-to-video (T2V) generation has surged in response to challenging\nquestions, especially when a long video must depict multiple sequential events\nwith temporal coherence and controllable content. Existing methods that extend\nto multi-event generation omit an inspection of the intrinsic factor in event\nshifting. The paper aims to answer the central question: When and where\nmulti-event prompts control event transition during T2V generation. This work\nintroduces MEve, a self-curated prompt suite for evaluating multi-event\ntext-to-video (T2V) generation, and conducts a systematic study of two\nrepresentative model families, i.e., OpenSora and CogVideoX. Extensive\nexperiments demonstrate the importance of early intervention in denoising steps\nand block-wise model layers, revealing the essential factor for multi-event\nvideo generation and highlighting the possibilities for multi-event\nconditioning in future models.",
        "url": "http://arxiv.org/abs/2510.03049v1",
        "published_date": "2025-10-03T14:31:56+00:00",
        "updated_date": "2025-10-03T14:31:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ruotong Liao",
            "Guowen Huang",
            "Qing Cheng",
            "Thomas Seidl",
            "Daniel Cremers",
            "Volker Tresp"
        ],
        "tldr": "This paper introduces MEve, a benchmark suite for multi-event text-to-video generation, and analyzes event transition control in existing models, revealing the importance of early intervention during video denoising.",
        "tldr_zh": "该论文介绍了一个用于多事件文本到视频生成的基准测试套件MEve，并分析了现有模型中的事件转换控制，揭示了在视频去噪过程中早期干预的重要性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency",
        "summary": "With the rapid advancement of large multimodal models (LMMs), recent\ntext-to-image (T2I) models can generate high-quality images and demonstrate\ngreat alignment to short prompts. However, they still struggle to effectively\nunderstand and follow long and detailed prompts, displaying inconsistent\ngeneration. To address this challenge, we introduce LPG-Bench, a comprehensive\nbenchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench\nfeatures 200 meticulously crafted prompts with an average length of over 250\nwords, approaching the input capacity of several leading commercial models.\nUsing these prompts, we generate 2,600 images from 13 state-of-the-art models\nand further perform comprehensive human-ranked annotations. Based on LPG-Bench,\nwe observe that state-of-the-art T2I alignment evaluation metrics exhibit poor\nconsistency with human preferences on long-prompt-based image generation. To\naddress the gap, we introduce a novel zero-shot metric based on\ntext-to-image-to-text consistency, termed TIT, for evaluating\nlong-prompt-generated images. The core concept of TIT is to quantify T2I\nalignment by directly comparing the consistency between the raw prompt and the\nLMM-produced description on the generated image, which includes an efficient\nscore-based instantiation TIT-Score and a large-language-model (LLM) based\ninstantiation TIT-Score-LLM. Extensive experiments demonstrate that our\nframework achieves superior alignment with human judgment compared to\nCLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute\nimprovement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT\nmethods together offer a deeper perspective to benchmark and foster the\ndevelopment of T2I models. All resources will be made publicly available.",
        "url": "http://arxiv.org/abs/2510.02987v1",
        "published_date": "2025-10-03T13:25:16+00:00",
        "updated_date": "2025-10-03T13:25:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Juntong Wang",
            "Huiyu Duan",
            "Jiarui Wang",
            "Ziheng Jia",
            "Guangtao Zhai",
            "Xiongkuo Min"
        ],
        "tldr": "This paper introduces LPG-Bench, a benchmark for evaluating text-to-image generation with long prompts, and TIT-Score, a novel metric for assessing text-image alignment based on text-to-image-to-text consistency, showing improved correlation with human judgment.",
        "tldr_zh": "该论文介绍了LPG-Bench，一个用于评估长文本提示下的文本到图像生成的基准测试，以及TIT-Score，一种基于文本到图像到文本一致性的新型文本-图像对齐评估指标，结果表明其与人类判断的相关性有所提高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis",
        "summary": "Cross-modal medical image synthesis research focuses on reconstructing\nmissing imaging modalities from available ones to support clinical diagnosis.\nDriven by clinical necessities for flexible modality reconstruction, we explore\nK to N medical generation, where three critical challenges emerge: How can we\nmodel the heterogeneous contributions of different modalities to various target\ntasks? How can we ensure fusion quality control to prevent degradation from\nnoisy information? How can we maintain modality identity consistency in\nmulti-output generation? Driven by these clinical necessities, and drawing\ninspiration from SAM2's sequential frame paradigm and clinicians' progressive\nworkflow of incrementally adding and selectively integrating multi-modal\ninformation, we treat multi-modal medical data as sequential frames with\nquality-driven selection mechanisms. Our key idea is to \"learn\" adaptive\nweights for each modality-task pair and \"memorize\" beneficial fusion patterns\nthrough progressive enhancement. To achieve this, we design three collaborative\nmodules: PreWeightNet for global contribution assessment, ThresholdNet for\nadaptive filtering, and EffiWeightNet for effective weight computation.\nMeanwhile, to maintain modality identity consistency, we propose the Causal\nModality Identity Module (CMIM) that establishes causal constraints between\ngenerated images and target modality descriptions using vision-language\nmodeling. Extensive experimental results demonstrate that our proposed Med-K2N\noutperforms state-of-the-art methods by significant margins on multiple\nbenchmarks. Source code is available.",
        "url": "http://arxiv.org/abs/2510.02815v1",
        "published_date": "2025-10-03T08:47:17+00:00",
        "updated_date": "2025-10-03T08:47:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feng Yuan",
            "Yifan Gao",
            "Yuehua Ye",
            "Haoyue Li",
            "Xin Gao"
        ],
        "tldr": "The paper introduces Med-K2N, a novel method for flexible K-to-N modality translation in medical image synthesis, addressing challenges in heterogeneous modality contributions, fusion quality control, and modality identity consistency. It incorporates several modules, including PreWeightNet, ThresholdNet, EffiWeightNet, and CMIM, and demonstrates state-of-the-art performance on multiple benchmarks.",
        "tldr_zh": "该论文提出了Med-K2N，一种用于医学图像合成中灵活的K到N模态转换的新方法，解决了异构模态贡献、融合质量控制和模态身份一致性方面的挑战。它包含多个模块，包括PreWeightNet、ThresholdNet、EffiWeightNet和CMIM，并在多个基准测试中展示了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context",
        "summary": "Existing text-driven motion generation methods often treat synthesis as a\nbidirectional mapping between language and motion, but remain limited in\ncapturing the causal logic of action execution and the human intentions that\ndrive behavior. The absence of visual grounding further restricts precision and\npersonalization, as language alone cannot specify fine-grained spatiotemporal\ndetails. We propose MoGIC, a unified framework that integrates intention\nmodeling and visual priors into multimodal motion synthesis. By jointly\noptimizing multimodal-conditioned motion generation and intention prediction,\nMoGIC uncovers latent human goals, leverages visual priors to enhance\ngeneration, and exhibits versatile multimodal generative capability. We further\nintroduce a mixture-of-attention mechanism with adaptive scope to enable\neffective local alignment between conditional tokens and motion subsequences.\nTo support this paradigm, we curate Mo440H, a 440-hour benchmark from 21\nhigh-quality motion datasets. Experiments show that after finetuning, MoGIC\nreduces FID by 38.6\\% on HumanML3D and 34.6\\% on Mo440H, surpasses LLM-based\nmethods in motion captioning with a lightweight text head, and further enables\nintention prediction and vision-conditioned generation, advancing controllable\nmotion synthesis and intention understanding. The code is available at\nhttps://github.com/JunyuShi02/MoGIC",
        "url": "http://arxiv.org/abs/2510.02722v1",
        "published_date": "2025-10-03T04:54:39+00:00",
        "updated_date": "2025-10-03T04:54:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junyu Shi",
            "Yong Sun",
            "Zhiyuan Zhang",
            "Lijiang Liu",
            "Zhengjie Zhang",
            "Yuxin He",
            "Qiang Nie"
        ],
        "tldr": "MoGIC is a framework that integrates intention modeling and visual priors into multimodal motion generation, achieving state-of-the-art results on HumanML3D and a newly curated dataset (Mo440H). It advances controllable motion synthesis and intention understanding.",
        "tldr_zh": "MoGIC是一个将意图建模和视觉先验融入多模态运动生成的框架，在HumanML3D和新策划的数据集(Mo440H)上取得了最先进的结果。 该方法推进了可控运动合成和意图理解。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation",
        "summary": "Diffusion models can synthesize realistic co-speech video from audio for\nvarious applications, such as video creation and virtual agents. However,\nexisting diffusion-based methods are slow due to numerous denoising steps and\ncostly attention mechanisms, preventing real-time deployment. In this work, we\ndistill a many-step diffusion video model into a few-step student model.\nUnfortunately, directly applying recent diffusion distillation methods degrades\nvideo quality and falls short of real-time performance. To address these\nissues, our new video distillation method leverages input human pose\nconditioning for both attention and loss functions. We first propose using\naccurate correspondence between input human pose keypoints to guide attention\nto relevant regions, such as the speaker's face, hands, and upper body. This\ninput-aware sparse attention reduces redundant computations and strengthens\ntemporal correspondences of body parts, improving inference efficiency and\nmotion coherence. To further enhance visual quality, we introduce an\ninput-aware distillation loss that improves lip synchronization and hand motion\nrealism. By integrating our input-aware sparse attention and distillation loss,\nour method achieves real-time performance with improved visual quality compared\nto recent audio-driven and input-driven methods. We also conduct extensive\nexperiments showing the effectiveness of our algorithmic design choices.",
        "url": "http://arxiv.org/abs/2510.02617v1",
        "published_date": "2025-10-02T23:35:52+00:00",
        "updated_date": "2025-10-02T23:35:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Beijia Lu",
            "Ziyi Chen",
            "Jing Xiao",
            "Jun-Yan Zhu"
        ],
        "tldr": "This paper introduces a novel input-aware sparse attention and distillation loss for real-time co-speech video generation using diffusion models, significantly improving inference speed and video quality compared to existing methods.",
        "tldr_zh": "本文介绍了一种新颖的输入感知稀疏注意力和蒸馏损失，用于使用扩散模型进行实时口头视频生成，与现有方法相比，显著提高了推理速度和视频质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting",
        "summary": "Driving scene manipulation with sensor data is emerging as a promising\nalternative to traditional virtual driving simulators. However, existing\nframeworks struggle to generate realistic scenarios efficiently due to limited\nediting capabilities. To address these challenges, we present SIMSplat, a\npredictive driving scene editor with language-aligned Gaussian splatting. As a\nlanguage-controlled editor, SIMSplat enables intuitive manipulation using\nnatural language prompts. By aligning language with Gaussian-reconstructed\nscenes, it further supports direct querying of road objects, allowing precise\nand flexible editing. Our method provides detailed object-level editing,\nincluding adding new objects and modifying the trajectories of both vehicles\nand pedestrians, while also incorporating predictive path refinement through\nmulti-agent motion prediction to generate realistic interactions among all\nagents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's\nextensive editing capabilities and adaptability across a wide range of\nscenarios. Project page: https://sungyeonparkk.github.io/simsplat/",
        "url": "http://arxiv.org/abs/2510.02469v1",
        "published_date": "2025-10-02T18:22:03+00:00",
        "updated_date": "2025-10-02T18:22:03+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Sung-Yeon Park",
            "Adam Lee",
            "Juanwu Lu",
            "Can Cui",
            "Luyang Jiang",
            "Rohit Gupta",
            "Kyungtae Han",
            "Ahmadreza Moradipari",
            "Ziran Wang"
        ],
        "tldr": "SIMSplat introduces a language-controlled driving scene editor based on 4D Gaussian splatting for realistic scenario generation, enabling intuitive object manipulation and trajectory editing using natural language.",
        "tldr_zh": "SIMSplat 提出了一种基于 4D Gaussian Splatting 的语言控制驾驶场景编辑器，用于生成真实的场景，通过自然语言实现直观的对象操控和轨迹编辑。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Scalable and Consistent 3D Editing",
        "summary": "3D editing - the task of locally modifying the geometry or appearance of a 3D\nasset - has wide applications in immersive content creation, digital\nentertainment, and AR/VR. However, unlike 2D editing, it remains challenging\ndue to the need for cross-view consistency, structural fidelity, and\nfine-grained controllability. Existing approaches are often slow, prone to\ngeometric distortions, or dependent on manual and accurate 3D masks that are\nerror-prone and impractical. To address these challenges, we advance both the\ndata and model fronts. On the data side, we introduce 3DEditVerse, the largest\npaired 3D editing benchmark to date, comprising 116,309 high-quality training\npairs and 1,500 curated test pairs. Built through complementary pipelines of\npose-driven geometric edits and foundation model-guided appearance edits,\n3DEditVerse ensures edit locality, multi-view consistency, and semantic\nalignment. On the model side, we propose 3DEditFormer, a\n3D-structure-preserving conditional transformer. By enhancing image-to-3D\ngeneration with dual-guidance attention and time-adaptive gating, 3DEditFormer\ndisentangles editable regions from preserved structure, enabling precise and\nconsistent edits without requiring auxiliary 3D masks. Extensive experiments\ndemonstrate that our framework outperforms state-of-the-art baselines both\nquantitatively and qualitatively, establishing a new standard for practical and\nscalable 3D editing. Dataset and code will be released. Project:\nhttps://www.lv-lab.org/3DEditFormer/",
        "url": "http://arxiv.org/abs/2510.02994v1",
        "published_date": "2025-10-03T13:34:55+00:00",
        "updated_date": "2025-10-03T13:34:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruihao Xia",
            "Yang Tang",
            "Pan Zhou"
        ],
        "tldr": "This paper introduces 3DEditVerse, a large 3D editing dataset, and 3DEditFormer, a 3D-structure-preserving transformer, for scalable and consistent 3D editing without manual masks, demonstrating state-of-the-art performance.",
        "tldr_zh": "本文介绍了3DEditVerse，一个大型的3D编辑数据集，以及3DEditFormer，一个用于可扩展和一致的3D编辑的3D结构保持型Transformer，无需手动掩码，并展示了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Deep Generative Continual Learning using Functional LoRA: FunLoRA",
        "summary": "Continual adaptation of deep generative models holds tremendous potential and\ncritical importance, given their rapid and expanding usage in text and vision\nbased applications. Incremental training, however, remains highly challenging\ndue to catastrophic forgetting phenomenon, which makes it difficult for neural\nnetworks to effectively incorporate new knowledge. A common strategy consists\nin retraining the generative model on its own synthetic data in order to\nmitigate forgetting. Yet, such an approach faces two major limitations: (i) the\ncontinually increasing training time eventually becomes intractable, and (ii)\nreliance on synthetic data inevitably leads to long-term performance\ndegradation, since synthetic samples lack the richness of real training data.\nIn this paper, we attenuate these issues by designing a novel and more\nexpressive conditioning mechanism for generative models based on low rank\nadaptation (LoRA), that exclusively employs rank 1 matrices, whose\nreparametrized matrix rank is functionally increased using carefully selected\nfunctions -- and dubbed functional LoRA: FunLoRA. Using this dynamic\nconditioning, the generative model is guaranteed to avoid catastrophic\nforgetting and needs only to be trained on data from the current task.\nExtensive experiments using flow-matching based models trained from scratch,\nshowcase that our proposed parameter-efficient fine-tuning (PEFT) method\nsurpasses prior state-of-the-art results based on diffusion models, reaching\nhigher classification accuracy scores, while only requiring a fraction of the\nmemory cost and sampling time.",
        "url": "http://arxiv.org/abs/2510.02631v1",
        "published_date": "2025-10-03T00:18:05+00:00",
        "updated_date": "2025-10-03T00:18:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Victor Enescu",
            "Hichem Sahbi"
        ],
        "tldr": "The paper introduces FunLoRA, a novel parameter-efficient fine-tuning method for deep generative models that mitigates catastrophic forgetting in continual learning by using a dynamic conditioning mechanism based on low-rank adaptation, achieving state-of-the-art results with less memory and faster sampling time.",
        "tldr_zh": "该论文介绍了FunLoRA， 一种用于深度生成模型的新型参数高效微调方法，它通过使用基于低秩自适应的动态条件机制来减轻持续学习中的灾难性遗忘，以更少的内存和更快的采样时间实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Dale meets Langevin: A Multiplicative Denoising Diffusion Model",
        "summary": "Gradient descent has proven to be a powerful and effective technique for\noptimization in numerous machine learning applications. Recent advances in\ncomputational neuroscience have shown that learning in standard gradient\ndescent optimization formulation is not consistent with learning in biological\nsystems. This has opened up interesting avenues for building biologically\ninspired learning techniques. One such approach is inspired by Dale's law,\nwhich states that inhibitory and excitatory synapses do not swap roles during\nthe course of learning. The resulting exponential gradient descent optimization\nscheme leads to log-normally distributed synaptic weights. Interestingly, the\ndensity that satisfies the Fokker-Planck equation corresponding to the\nstochastic differential equation (SDE) with geometric Brownian motion (GBM) is\nthe log-normal density. Leveraging this connection, we start with the SDE\ngoverning geometric Brownian motion, and show that discretizing the\ncorresponding reverse-time SDE yields a multiplicative update rule, which\nsurprisingly, coincides with the sampling equivalent of the exponential\ngradient descent update founded on Dale's law. Furthermore, we propose a new\nformalism for multiplicative denoising score-matching, subsuming the loss\nfunction proposed by Hyvaerinen for non-negative data. Indeed, log-normally\ndistributed data is positive and the proposed score-matching formalism turns\nout to be a natural fit. This allows for training of score-based models for\nimage data and results in a novel multiplicative update scheme for sample\ngeneration starting from a log-normal density. Experimental results on MNIST,\nFashion MNIST, and Kuzushiji datasets demonstrate generative capability of the\nnew scheme. To the best of our knowledge, this is the first instance of a\nbiologically inspired generative model employing multiplicative updates,\nfounded on geometric Brownian motion.",
        "url": "http://arxiv.org/abs/2510.02730v1",
        "published_date": "2025-10-03T05:23:33+00:00",
        "updated_date": "2025-10-03T05:23:33+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Nishanth Shetty",
            "Madhava Prasath",
            "Chandra Sekhar Seelamantula"
        ],
        "tldr": "This paper introduces a novel multiplicative denoising diffusion model inspired by Dale's law and geometric Brownian motion, demonstrating generative capabilities on image datasets. It is the first biologically inspired approach to multiplicative sample generation.",
        "tldr_zh": "本文介绍了一种新型的乘性去噪扩散模型，该模型受到Dale定律和几何布朗运动的启发，并在图像数据集上展示了生成能力。这是第一个受生物学启发的乘性样本生成方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Image Enhancement Based on Pigment Representation",
        "summary": "This paper presents a novel and efficient image enhancement method based on\npigment representation. Unlike conventional methods where the color\ntransformation is restricted to pre-defined color spaces like RGB, our method\ndynamically adapts to input content by transforming RGB colors into a\nhigh-dimensional feature space referred to as \\textit{pigments}. The proposed\npigment representation offers adaptability and expressiveness, achieving\nsuperior image enhancement performance. The proposed method involves\ntransforming input RGB colors into high-dimensional pigments, which are then\nreprojected individually and blended to refine and aggregate the information of\nthe colors in pigment spaces. Those pigments are then transformed back into RGB\ncolors to generate an enhanced output image. The transformation and\nreprojection parameters are derived from the visual encoder which adaptively\nestimates such parameters based on the content in the input image. Extensive\nexperimental results demonstrate the superior performance of the proposed\nmethod over state-of-the-art methods in image enhancement tasks, including\nimage retouching and tone mapping, while maintaining relatively low\ncomputational complexity and small model size.",
        "url": "http://arxiv.org/abs/2510.02713v1",
        "published_date": "2025-10-03T04:28:44+00:00",
        "updated_date": "2025-10-03T04:28:44+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Se-Ho Lee",
            "Keunsoo Ko",
            "Seung-Wook Kim"
        ],
        "tldr": "The paper introduces a novel image enhancement method using a dynamically adapting, high-dimensional \"pigment\" representation of RGB colors, achieving superior performance with low computational complexity.",
        "tldr_zh": "该论文介绍了一种新颖的图像增强方法，该方法使用动态适应的高维“颜料”表示RGB颜色，实现了卓越的性能，且计算复杂度较低。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]