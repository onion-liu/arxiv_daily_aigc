[
    {
        "title": "Universal Few-Shot Spatial Control for Diffusion Models",
        "summary": "Spatial conditioning in pretrained text-to-image diffusion models has\nsignificantly improved fine-grained control over the structure of generated\nimages. However, existing control adapters exhibit limited adaptability and\nincur high training costs when encountering novel spatial control conditions\nthat differ substantially from the training tasks. To address this limitation,\nwe propose Universal Few-Shot Control (UFC), a versatile few-shot control\nadapter capable of generalizing to novel spatial conditions. Given a few\nimage-condition pairs of an unseen task and a query condition, UFC leverages\nthe analogy between query and support conditions to construct task-specific\ncontrol features, instantiated by a matching mechanism and an update on a small\nset of task-specific parameters. Experiments on six novel spatial control tasks\nshow that UFC, fine-tuned with only 30 annotated examples of novel tasks,\nachieves fine-grained control consistent with the spatial conditions. Notably,\nwhen fine-tuned with 0.1% of the full training data, UFC achieves competitive\nperformance with the fully supervised baselines in various control tasks. We\nalso show that UFC is applicable agnostically to various diffusion backbones\nand demonstrate its effectiveness on both UNet and DiT architectures. Code is\navailable at https://github.com/kietngt00/UFC.",
        "url": "http://arxiv.org/abs/2509.07530v1",
        "published_date": "2025-09-09T09:08:07+00:00",
        "updated_date": "2025-09-09T09:08:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kiet T. Nguyen",
            "Chanhuyk Lee",
            "Donggyun Kim",
            "Dong Hoon Lee",
            "Seunghoon Hong"
        ],
        "tldr": "The paper introduces Universal Few-Shot Control (UFC), a novel adapter for diffusion models that enables fine-grained spatial control with only a few training examples and generalizes well to new spatial conditions across different diffusion architectures.",
        "tldr_zh": "本文介绍了通用小样本控制(UFC)，一种新型扩散模型适配器，仅需少量训练样本即可实现细粒度的空间控制，并且能够很好地推广到不同扩散架构上的新空间条件。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LINR Bridge: Vector Graphic Animation via Neural Implicits and Video Diffusion Priors",
        "summary": "Vector graphics, known for their scalability and user-friendliness, provide a\nunique approach to visual content compared to traditional pixel-based images.\nAnimation of these graphics, driven by the motion of their elements, offers\nenhanced comprehensibility and controllability but often requires substantial\nmanual effort. To automate this process, we propose a novel method that\nintegrates implicit neural representations with text-to-video diffusion models\nfor vector graphic animation. Our approach employs layered implicit neural\nrepresentations to reconstruct vector graphics, preserving their inherent\nproperties such as infinite resolution and precise color and shape constraints,\nwhich effectively bridges the large domain gap between vector graphics and\ndiffusion models. The neural representations are then optimized using video\nscore distillation sampling, which leverages motion priors from pretrained\ntext-to-video diffusion models. Finally, the vector graphics are warped to\nmatch the representations resulting in smooth animation. Experimental results\nvalidate the effectiveness of our method in generating vivid and natural vector\ngraphic animations, demonstrating significant improvement over existing\ntechniques that suffer from limitations in flexibility and animation quality.",
        "url": "http://arxiv.org/abs/2509.07484v1",
        "published_date": "2025-09-09T08:04:36+00:00",
        "updated_date": "2025-09-09T08:04:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenshuo Gao",
            "Xicheng Lan",
            "Luyao Zhang",
            "Shuai Yang"
        ],
        "tldr": "This paper introduces a novel method for animating vector graphics by combining implicit neural representations with text-to-video diffusion models, overcoming limitations of existing techniques and achieving smooth and natural animations.",
        "tldr_zh": "本文提出了一种新的矢量图形动画方法，该方法结合了隐式神经表示与文本到视频扩散模型，克服了现有技术的局限性，并实现了平滑自然的动画。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ANYPORTAL: Zero-Shot Consistent Video Background Replacement",
        "summary": "Despite the rapid advancements in video generation technology, creating\nhigh-quality videos that precisely align with user intentions remains a\nsignificant challenge. Existing methods often fail to achieve fine-grained\ncontrol over video details, limiting their practical applicability. We\nintroduce ANYPORTAL, a novel zero-shot framework for video background\nreplacement that leverages pre-trained diffusion models. Our framework\ncollaboratively integrates the temporal prior of video diffusion models with\nthe relighting capabilities of image diffusion models in a zero-shot setting.\nTo address the critical challenge of foreground consistency, we propose a\nRefinement Projection Algorithm, which enables pixel-level detail manipulation\nto ensure precise foreground preservation. ANYPORTAL is training-free and\novercomes the challenges of achieving foreground consistency and temporally\ncoherent relighting. Experimental results demonstrate that ANYPORTAL achieves\nhigh-quality results on consumer-grade GPUs, offering a practical and efficient\nsolution for video content creation and editing.",
        "url": "http://arxiv.org/abs/2509.07472v1",
        "published_date": "2025-09-09T07:50:53+00:00",
        "updated_date": "2025-09-09T07:50:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenshuo Gao",
            "Xicheng Lan",
            "Shuai Yang"
        ],
        "tldr": "ANYPORTAL is a zero-shot video background replacement framework that uses diffusion models and a novel Refinement Projection Algorithm to achieve foreground consistency and temporally coherent relighting on consumer GPUs.",
        "tldr_zh": "ANYPORTAL是一个零样本视频背景替换框架，它使用扩散模型和一个新的细化投影算法，从而在消费级GPU上实现前景一致性和时间连贯的重照明。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reconstruction Alignment Improves Unified Multimodal Models",
        "summary": "Unified multimodal models (UMMs) unify visual understanding and generation\nwithin a single architecture. However, conventional training relies on\nimage-text pairs (or sequences) whose captions are typically sparse and miss\nfine-grained visual details--even when they use hundreds of words to describe a\nsimple image. We introduce Reconstruction Alignment (RecA), a\nresource-efficient post-training method that leverages visual understanding\nencoder embeddings as dense \"text prompts,\" providing rich supervision without\ncaptions. Concretely, RecA conditions a UMM on its own visual understanding\nembeddings and optimizes it to reconstruct the input image with a\nself-supervised reconstruction loss, thereby realigning understanding and\ngeneration. Despite its simplicity, RecA is broadly applicable: across\nautoregressive, masked-autoregressive, and diffusion-based UMMs, it\nconsistently improves generation and editing fidelity. With only 27 GPU-hours,\npost-training with RecA substantially improves image generation performance on\nGenEval (0.73$\\rightarrow$0.90) and DPGBench (80.93$\\rightarrow$88.15), while\nalso boosting editing benchmarks (ImgEdit 3.38$\\rightarrow$3.75, GEdit\n6.94$\\rightarrow$7.25). Notably, RecA surpasses much larger open-source models\nand applies broadly across diverse UMM architectures, establishing it as an\nefficient and general post-training alignment strategy for UMMs",
        "url": "http://arxiv.org/abs/2509.07295v1",
        "published_date": "2025-09-08T23:59:32+00:00",
        "updated_date": "2025-09-08T23:59:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ji Xie",
            "Trevor Darrell",
            "Luke Zettlemoyer",
            "XuDong Wang"
        ],
        "tldr": "The paper introduces Reconstruction Alignment (RecA), a resource-efficient post-training method for Unified Multimodal Models (UMMs) that leverages visual understanding embeddings as dense \"text prompts\" for improved image generation and editing fidelity.",
        "tldr_zh": "该论文介绍了一种名为重建对齐（RecA）的资源高效的统一多模态模型（UMM）后训练方法，该方法利用视觉理解嵌入作为密集的“文本提示”，从而提高图像生成和编辑的逼真度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SVGauge: Towards Human-Aligned Evaluation for SVG Generation",
        "summary": "Generated Scalable Vector Graphics (SVG) images demand evaluation criteria\ntuned to their symbolic and vectorial nature: criteria that existing metrics\nsuch as FID, LPIPS, or CLIPScore fail to satisfy. In this paper, we introduce\nSVGauge, the first human-aligned, reference based metric for text-to-SVG\ngeneration. SVGauge jointly measures (i) visual fidelity, obtained by\nextracting SigLIP image embeddings and refining them with PCA and whitening for\ndomain alignment, and (ii) semantic consistency, captured by comparing\nBLIP-2-generated captions of the SVGs against the original prompts in the\ncombined space of SBERT and TF-IDF. Evaluation on the proposed SHE benchmark\nshows that SVGauge attains the highest correlation with human judgments and\nreproduces system-level rankings of eight zero-shot LLM-based generators more\nfaithfully than existing metrics. Our results highlight the necessity of\nvector-specific evaluation and provide a practical tool for benchmarking future\ntext-to-SVG generation models.",
        "url": "http://arxiv.org/abs/2509.07127v1",
        "published_date": "2025-09-08T18:28:31+00:00",
        "updated_date": "2025-09-08T18:28:31+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Leonardo Zini",
            "Elia Frigieri",
            "Sebastiano Aloscari",
            "Marcello Generali",
            "Lorenzo Dodi",
            "Robert Dosen",
            "Lorenzo Baraldi"
        ],
        "tldr": "The paper introduces SVGauge, a novel metric for evaluating text-to-SVG generation that aligns with human perception by considering both visual fidelity and semantic consistency.",
        "tldr_zh": "该论文介绍了SVGauge，一种用于评估文本到SVG生成的新型指标，通过考虑视觉保真度和语义一致性，从而与人类感知对齐。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting",
        "summary": "3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D\nscene representations from sets of multi-view images. However, inpainting\nmissing regions, whether due to occlusion or scene editing, remains a\nchallenging task, often leading to blurry details, artifacts, and inconsistent\ngeometry. In this work, we introduce SplatFill, a novel depth-guided approach\nfor 3DGS scene inpainting that achieves state-of-the-art perceptual quality and\nimproved efficiency. Our method combines two key ideas: (1) joint depth-based\nand object-based supervision to ensure inpainted Gaussians are accurately\nplaced in 3D space and aligned with surrounding geometry, and (2) we propose a\nconsistency-aware refinement scheme that selectively identifies and corrects\ninconsistent regions without disrupting the rest of the scene. Evaluations on\nthe SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing\nNeRF-based and 3DGS-based inpainting methods in visual fidelity but also\nreduces training time by 24.5%. Qualitative results show our method delivers\nsharper details, fewer artifacts, and greater coherence across challenging\nviewpoints.",
        "url": "http://arxiv.org/abs/2509.07809v1",
        "published_date": "2025-09-09T14:47:47+00:00",
        "updated_date": "2025-09-09T14:47:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mahtab Dahaghin",
            "Milind G. Padalkar",
            "Matteo Toso",
            "Alessio Del Bue"
        ],
        "tldr": "The paper introduces SplatFill, a depth-guided Gaussian Splatting approach for 3D scene inpainting, achieving state-of-the-art perceptual quality and improved efficiency compared to existing NeRF-based and 3DGS-based methods.",
        "tldr_zh": "该论文介绍了SplatFill，一种深度引导的高斯溅射方法，用于3D场景修复，与现有的基于NeRF和3DGS的方法相比，它实现了最先进的感知质量和更高的效率。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]