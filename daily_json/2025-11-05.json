[
    {
        "title": "CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning",
        "summary": "In human cognition, there exist numerous thought processes that are tacit and\nbeyond verbal expression, enabling us to understand and interact with the world\nin multiple ways. However, contemporary Vision-Language Models (VLMs) remain\nconstrained to reasoning within the discrete and rigid space of linguistic\ntokens, thereby bottlenecking the rich, high-dimensional nature of visual\nperception. To bridge this gap, we propose CoCoVa (Chain of Continuous\nVision-Language Thought), a novel framework for vision-language model that\nleverages continuous cross-modal reasoning for diverse vision-language tasks.\nThe core of CoCoVa is an iterative reasoning cycle, where a novel Latent\nQ-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a\nchain of latent thought vectors through cross-modal fusion. To focus this\nprocess, a token selection mechanism dynamically identifies salient visual\nregions, mimicking attentional focus. To ensure these latent thoughts remain\ngrounded, we train the model with a multi-task objective that combines\ncontrastive learning and diffusion-based reconstruction, enforcing alignment\nbetween latent representations and both visual and textual modalities.\nEvaluations show CoCoVa improves accuracy and token efficiency over strong\nbaselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B\nmodels on almost all benchmarks. When scaled to 7B LLM backbones, it remains\ncompetitive with state-of-the-art models. Qualitative analysis validates that\nlearned latent space captures interpretable and structured reasoning patterns,\nhighlighting the potential of CoCoVa to bridge the representational gap between\ndiscrete language processing and the continuous nature of visual understanding.",
        "url": "http://arxiv.org/abs/2511.02360v1",
        "published_date": "2025-11-04T08:28:46+00:00",
        "updated_date": "2025-11-04T08:28:46+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Jizheng Ma",
            "Xiaofei Zhou",
            "Yanlong Song",
            "Han Yan"
        ],
        "tldr": "CoCoVa introduces a novel vision-language framework using a latent space reasoning approach with iterative refinement and attention mechanisms, achieving competitive performance with larger models. This approach aims to bridge the gap between discrete language and continuous visual understanding.",
        "tldr_zh": "CoCoVa 引入了一种新的视觉语言框架，该框架采用具有迭代细化和注意力机制的潜在空间推理方法，与更大的模型相比，实现了具有竞争力的性能。该方法旨在弥合离散语言和连续视觉理解之间的差距。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Language-Enhanced Generative Modeling for PET Synthesis from MRI and Blood Biomarkers",
        "summary": "Background: Alzheimer's disease (AD) diagnosis heavily relies on amyloid-beta\npositron emission tomography (Abeta-PET), which is limited by high cost and\nlimited accessibility. This study explores whether Abeta-PET spatial patterns\ncan be predicted from blood-based biomarkers (BBMs) and MRI scans. Methods: We\ncollected Abeta-PET images, T1-weighted MRI scans, and BBMs from 566\nparticipants. A language-enhanced generative model, driven by a large language\nmodel (LLM) and multimodal information fusion, was developed to synthesize PET\nimages. Synthesized images were evaluated for image quality, diagnostic\nconsistency, and clinical applicability within a fully automated diagnostic\npipeline. Findings: The synthetic PET images closely resemble real PET scans in\nboth structural details (SSIM = 0.920 +/- 0.003) and regional patterns\n(Pearson's r = 0.955 +/- 0.007). Diagnostic outcomes using synthetic PET show\nhigh agreement with real PET-based diagnoses (accuracy = 0.80). Using synthetic\nPET, we developed a fully automatic AD diagnostic pipeline integrating PET\nsynthesis and classification. The synthetic PET-based model (AUC = 0.78)\noutperforms T1-based (AUC = 0.68) and BBM-based (AUC = 0.73) models, while\ncombining synthetic PET and BBMs further improved performance (AUC = 0.79).\nAblation analysis supports the advantages of LLM integration and prompt\nengineering. Interpretation: Our language-enhanced generative model synthesizes\nrealistic PET images, enhancing the utility of MRI and BBMs for Abeta spatial\npattern assessment and improving the diagnostic workflow for Alzheimer's\ndisease.",
        "url": "http://arxiv.org/abs/2511.02206v1",
        "published_date": "2025-11-04T02:53:25+00:00",
        "updated_date": "2025-11-04T02:53:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhengjie Zhang",
            "Xiaoxie Mao",
            "Qihao Guo",
            "Shaoting Zhang",
            "Qi Huang",
            "Mu Zhou",
            "Fang Xie",
            "Mianxin Liu"
        ],
        "tldr": "This paper introduces a language-enhanced generative model using LLMs to synthesize amyloid-beta PET images from MRI and blood biomarkers, achieving high fidelity and improving Alzheimer's disease diagnosis.",
        "tldr_zh": "本文介绍了一种利用大型语言模型（LLM）的语言增强生成模型，通过MRI和血液生物标志物合成淀粉样蛋白-β PET图像，实现了高保真度并改善了阿尔茨海默病的诊断。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "ESA: Energy-Based Shot Assembly Optimization for Automatic Video Editing",
        "summary": "Shot assembly is a crucial step in film production and video editing,\ninvolving the sequencing and arrangement of shots to construct a narrative,\nconvey information, or evoke emotions. Traditionally, this process has been\nmanually executed by experienced editors. While current intelligent video\nediting technologies can handle some automated video editing tasks, they often\nfail to capture the creator's unique artistic expression in shot assembly. To\naddress this challenge, we propose an energy-based optimization method for\nvideo shot assembly. Specifically, we first perform visual-semantic matching\nbetween the script generated by a large language model and a video library to\nobtain subsets of candidate shots aligned with the script semantics. Next, we\nsegment and label the shots from reference videos, extracting attributes such\nas shot size, camera motion, and semantics. We then employ energy-based models\nto learn from these attributes, scoring candidate shot sequences based on their\nalignment with reference styles. Finally, we achieve shot assembly optimization\nby combining multiple syntax rules, producing videos that align with the\nassembly style of the reference videos. Our method not only automates the\narrangement and combination of independent shots according to specific logic,\nnarrative requirements, or artistic styles but also learns the assembly style\nof reference videos, creating a coherent visual sequence or holistic visual\nexpression. With our system, even users with no prior video editing experience\ncan create visually compelling videos. Project page:\nhttps://sobeymil.github.io/esa.com",
        "url": "http://arxiv.org/abs/2511.02505v2",
        "published_date": "2025-11-04T11:48:22+00:00",
        "updated_date": "2025-11-05T04:30:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaosen Chen",
            "Wei Wang",
            "Tianheng Zheng",
            "Xuming Wen",
            "Han Yang",
            "Yanru Zhang"
        ],
        "tldr": "The paper introduces an energy-based optimization method for automatic video editing by learning from reference video styles and aligning with script semantics, enabling users without expertise to create compelling videos.",
        "tldr_zh": "该论文提出了一种基于能量优化的自动视频编辑方法，通过学习参考视频风格并与脚本语义对齐，使没有专业知识的用户也能创作引人注目的视频。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]