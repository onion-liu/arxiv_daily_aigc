[
    {
        "title": "Seeing What You Say: Expressive Image Generation from Speech",
        "summary": "This paper proposes VoxStudio, the first unified and end-to-end\nspeech-to-image model that generates expressive images directly from spoken\ndescriptions by jointly aligning linguistic and paralinguistic information. At\nits core is a speech information bottleneck (SIB) module, which compresses raw\nspeech into compact semantic tokens, preserving prosody and emotional nuance.\nBy operating directly on these tokens, VoxStudio eliminates the need for an\nadditional speech-to-text system, which often ignores the hidden details beyond\ntext, e.g., tone or emotion. We also release VoxEmoset, a large-scale paired\nemotional speech-image dataset built via an advanced TTS engine to affordably\ngenerate richly expressive utterances. Comprehensive experiments on the\nSpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility\nof our method and highlight key challenges, including emotional consistency and\nlinguistic ambiguity, paving the way for future research.",
        "url": "http://arxiv.org/abs/2511.03423v1",
        "published_date": "2025-11-05T12:40:28+00:00",
        "updated_date": "2025-11-05T12:40:28+00:00",
        "categories": [
            "eess.AS",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Jiyoung Lee",
            "Song Park",
            "Sanghyuk Chun",
            "Soo-Whan Chung"
        ],
        "tldr": "The paper introduces VoxStudio, a novel end-to-end speech-to-image model that generates expressive images from speech using a speech information bottleneck to capture prosody and emotion, along with a new paired emotional speech-image dataset, VoxEmoset.",
        "tldr_zh": "该论文介绍了 VoxStudio，一种新型的端到端语音到图像模型，它使用语音信息瓶颈从语音生成富有表现力的图像，以捕获韵律和情感，并发布了一个新的配对情感语音-图像数据集 VoxEmoset。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions",
        "summary": "Due to the lack of effective cross-modal modeling, existing open-source\naudio-video generation methods often exhibit compromised lip synchronization\nand insufficient semantic consistency. To mitigate these drawbacks, we propose\nUniAVGen, a unified framework for joint audio and video generation. UniAVGen is\nanchored in a dual-branch joint synthesis architecture, incorporating two\nparallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent\nspace. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which\nenables bidirectional, temporally aligned cross-attention, thus ensuring\nprecise spatiotemporal synchronization and semantic consistency. Furthermore,\nthis cross-modal interaction is augmented by a Face-Aware Modulation module,\nwhich dynamically prioritizes salient regions in the interaction process. To\nenhance generative fidelity during inference, we additionally introduce\nModality-Aware Classifier-Free Guidance, a novel strategy that explicitly\namplifies cross-modal correlation signals. Notably, UniAVGen's robust joint\nsynthesis design enables seamless unification of pivotal audio-video tasks\nwithin a single model, such as joint audio-video generation and continuation,\nvideo-to-audio dubbing, and audio-driven video synthesis. Comprehensive\nexperiments validate that, with far fewer training samples (1.3M vs. 30.1M),\nUniAVGen delivers overall advantages in audio-video synchronization, timbre\nconsistency, and emotion consistency.",
        "url": "http://arxiv.org/abs/2511.03334v1",
        "published_date": "2025-11-05T10:06:51+00:00",
        "updated_date": "2025-11-05T10:06:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guozhen Zhang",
            "Zixiang Zhou",
            "Teng Hu",
            "Ziqiao Peng",
            "Youliang Zhang",
            "Yi Chen",
            "Yuan Zhou",
            "Qinglin Lu",
            "Limin Wang"
        ],
        "tldr": "UniAVGen is a unified audio-video generation framework using Diffusion Transformers and asymmetric cross-modal interaction to improve lip synchronization and semantic consistency while reducing training data requirements. It tackles multiple audio-video tasks within a single model.",
        "tldr_zh": "UniAVGen是一个统一的音视频生成框架，它利用扩散变换器和非对称跨模态交互来提高口型同步和语义一致性，同时减少了训练数据需求。它在一个模型中解决了多个音视频任务。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Finetuning-Free Personalization of Text to Image Generation via Hypernetworks",
        "summary": "Personalizing text-to-image diffusion models has traditionally relied on\nsubject-specific fine-tuning approaches such as\nDreamBooth~\\cite{ruiz2023dreambooth}, which are computationally expensive and\nslow at inference. Recent adapter- and encoder-based methods attempt to reduce\nthis overhead but still depend on additional fine-tuning or large backbone\nmodels for satisfactory results. In this work, we revisit an orthogonal\ndirection: fine-tuning-free personalization via Hypernetworks that predict\nLoRA-adapted weights directly from subject images. Prior hypernetwork-based\napproaches, however, suffer from costly data generation or unstable attempts to\nmimic base model optimization trajectories. We address these limitations with\nan end-to-end training objective, stabilized by a simple output regularization,\nyielding reliable and effective hypernetworks. Our method removes the need for\nper-subject optimization at test time while preserving both subject fidelity\nand prompt alignment. To further enhance compositional generalization at\ninference time, we introduce Hybrid-Model Classifier-Free Guidance (HM-CFG),\nwhich combines the compositional strengths of the base diffusion model with the\nsubject fidelity of personalized models during sampling. Extensive experiments\non CelebA-HQ, AFHQ-v2, and DreamBench demonstrate that our approach achieves\nstrong personalization performance and highlights the promise of hypernetworks\nas a scalable and effective direction for open-category personalization.",
        "url": "http://arxiv.org/abs/2511.03156v1",
        "published_date": "2025-11-05T03:31:33+00:00",
        "updated_date": "2025-11-05T03:31:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sagar Shrestha",
            "Gopal Sharma",
            "Luowei Zhou",
            "Suren Kumar"
        ],
        "tldr": "This paper introduces a fine-tuning-free approach for personalized text-to-image generation using hypernetworks with a stabilized training objective and Hybrid-Model Classifier-Free Guidance to improve compositional generalization.",
        "tldr_zh": "这篇论文介绍了一种无需微调的个性化文本到图像生成方法，它使用超网络，通过稳定的训练目标和混合模型无分类器指导来提高组合泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology",
        "summary": "We introduce ProM3E, a probabilistic masked multimodal embedding model for\nany-to-any generation of multimodal representations for ecology. ProM3E is\nbased on masked modality reconstruction in the embedding space, learning to\ninfer missing modalities given a few context modalities. By design, our model\nsupports modality inversion in the embedding space. The probabilistic nature of\nour model allows us to analyse the feasibility of fusing various modalities for\ngiven downstream tasks, essentially learning what to fuse. Using these features\nof our model, we propose a novel cross-modal retrieval approach that mixes\ninter-modal and intra-modal similarities to achieve superior performance across\nall retrieval tasks. We further leverage the hidden representation from our\nmodel to perform linear probing tasks and demonstrate the superior\nrepresentation learning capability of our model. All our code, datasets and\nmodel will be released at https://vishu26.github.io/prom3e.",
        "url": "http://arxiv.org/abs/2511.02946v1",
        "published_date": "2025-11-04T19:47:22+00:00",
        "updated_date": "2025-11-04T19:47:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Srikumar Sastry",
            "Subash Khanal",
            "Aayush Dhakal",
            "Jiayu Lin",
            "Dan Cher",
            "Phoenix Jarosz",
            "Nathan Jacobs"
        ],
        "tldr": "The paper introduces ProM3E, a probabilistic multimodal embedding model for ecological data, enabling any-to-any modality generation and cross-modal retrieval through masked modality reconstruction and probabilistic fusion analysis.",
        "tldr_zh": "本文介绍了一种名为 ProM3E 的概率多模态嵌入模型，用于生态数据，通过屏蔽模态重建和概率融合分析，实现任何模态之间的生成和跨模态检索。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]