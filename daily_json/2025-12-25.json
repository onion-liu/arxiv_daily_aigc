[
    {
        "title": "SemanticGen: Video Generation in Semantic Space",
        "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.",
        "url": "http://arxiv.org/abs/2512.20619v2",
        "published_date": "2025-12-23T18:59:56+00:00",
        "updated_date": "2025-12-24T11:39:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianhong Bai",
            "Xiaoshi Wu",
            "Xintao Wang",
            "Xiao Fu",
            "Yuanxing Zhang",
            "Qinghe Wang",
            "Xiaoyu Shi",
            "Menghan Xia",
            "Zuozhu Liu",
            "Haoji Hu",
            "Pengfei Wan",
            "Kun Gai"
        ],
        "tldr": "SemanticGen introduces a two-stage video generation approach using diffusion models in a semantic space for efficient and high-quality video generation, outperforming state-of-the-art methods.",
        "tldr_zh": "SemanticGen 提出了一种两阶段视频生成方法，该方法使用语义空间中的扩散模型来实现高效、高质量的视频生成，并优于最先进的方法。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation",
        "summary": "Modern deep learning methods typically treat image sequences as large tensors of sequentially stacked frames. However, is this straightforward representation ideal given the current state-of-the-art (SoTA)? In this work, we address this question in the context of generative models and aim to devise a more effective way of modeling image sequence data. Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution. We train a generative model solely on grid images comprising subsampled frames. Yet, we learn to generate image sequences, using the strong self-attention mechanism of the Diffusion Transformer (DiT) to capture correlations between frames. In effect, our formulation extends a 2D image generator to operate as a low-resolution 3D image-sequence generator without introducing any architectural modifications. Subsequently, we super-resolve each frame individually to add the sequence-independent high-resolution details. This approach offers several advantages and can overcome key limitations of the SoTA in this domain. Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences. It also delivers high-fidelity generation of arbitrary-length sequences and increased efficiency in inference time and training data usage. Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context. Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.",
        "url": "http://arxiv.org/abs/2512.21276v1",
        "published_date": "2025-12-24T16:46:04+00:00",
        "updated_date": "2025-12-24T16:46:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Snehal Singh Tomar",
            "Alexandros Graikos",
            "Arjun Krishna",
            "Dimitris Samaras",
            "Klaus Mueller"
        ],
        "tldr": "The paper introduces GriDiT, a factorized grid-based diffusion model for efficient long image sequence generation, achieving improved synthesis quality, coherence, and speed compared to state-of-the-art methods.",
        "tldr_zh": "本文介绍了一种名为 GriDiT 的分解网格扩散模型，用于高效生成长图像序列，与最先进的方法相比，该模型在合成质量、连贯性和速度方面都有所提高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision",
        "summary": "Controllability is a fundamental requirement in video synthesis, where accurate alignment with conditioning signals is essential. Existing classifier-free guidance methods typically achieve conditioning indirectly by modeling the joint distribution of data and conditions, which often results in limited controllability over the specified conditions. Classifier-based guidance enforces conditions through an external classifier, but the model may exploit this mechanism to raise the classifier score without genuinely satisfying the intended condition, resulting in adversarial artifacts and limited effective controllability. In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision. By aligning the model's attention maps with external control signals, ACD achieves better controllability. To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration. Extensive experiments on benchmark video generation datasets demonstrate that ACD delivers superior alignment with conditioning inputs while preserving temporal coherence and visual fidelity, establishing an effective paradigm for conditional video synthesis.",
        "url": "http://arxiv.org/abs/2512.21268v1",
        "published_date": "2025-12-24T16:24:18+00:00",
        "updated_date": "2025-12-24T16:24:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weiqi Li",
            "Zehao Zhang",
            "Liang Lin",
            "Guangrun Wang"
        ],
        "tldr": "The paper introduces Attention-Conditional Diffusion (ACD), a new framework for video diffusion models that uses attention supervision to improve conditional control, particularly with sparse 3D-aware object layouts. It achieves better alignment with conditioning inputs while maintaining coherence and fidelity.",
        "tldr_zh": "该论文介绍了Attention-Conditional Diffusion (ACD)，一种新的视频扩散模型框架，它使用注意力监督来改善条件控制，特别是使用稀疏的3D感知对象布局。它在保持连贯性和保真度的同时，更好地与条件输入对齐。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation",
        "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.",
        "url": "http://arxiv.org/abs/2512.21252v1",
        "published_date": "2025-12-24T16:00:15+00:00",
        "updated_date": "2025-12-24T16:00:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiawei Liu",
            "Junqiao Li",
            "Jiangfan Deng",
            "Gen Li",
            "Siyu Zhou",
            "Zetao Fang",
            "Shanshan Lao",
            "Zengde Deng",
            "Jianing Zhu",
            "Tingting Ma",
            "Jiayi Li",
            "Yunqiu Wang",
            "Qian He",
            "Xinglong Wu"
        ],
        "tldr": "DreaMontage is a framework for generating seamless one-shot videos from frame-guided inputs, utilizing techniques like intermediate-conditioning DiT, Visual Expression SFT, Tailored DPO, and a Segment-wise Auto-Regressive inference strategy.",
        "tldr_zh": "DreaMontage是一个框架，用于从帧引导的输入生成无缝的单镜头视频，利用中间条件DiT、视觉表达SFT、定制DPO和分段自回归推理策略等技术。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Turn Toward Better Alignment: Few-Shot Generative Adaptation with Equivariant Feature Rotation",
        "summary": "Few-shot image generation aims to effectively adapt a source generative model to a target domain using very few training images. Most existing approaches introduce consistency constraints-typically through instance-level or distribution-level loss functions-to directly align the distribution patterns of source and target domains within their respective latent spaces. However, these strategies often fall short: overly strict constraints can amplify the negative effects of the domain gap, leading to distorted or uninformative content, while overly relaxed constraints may fail to leverage the source domain effectively. This limitation primarily stems from the inherent discrepancy in the underlying distribution structures of the source and target domains. The scarcity of target samples further compounds this issue by hindering accurate estimation of the target domain's distribution. To overcome these limitations, we propose Equivariant Feature Rotation (EFR), a novel adaptation strategy that aligns source and target domains at two complementary levels within a self-rotated proxy feature space. Specifically, we perform adaptive rotations within a parameterized Lie Group to transform both source and target features into an equivariant proxy space, where alignment is conducted. These learnable rotation matrices serve to bridge the domain gap by preserving intra-domain structural information without distortion, while the alignment optimization facilitates effective knowledge transfer from the source to the target domain. Comprehensive experiments on a variety of commonly used datasets demonstrate that our method significantly enhances the generative performance within the targeted domain.",
        "url": "http://arxiv.org/abs/2512.21174v1",
        "published_date": "2025-12-24T13:48:22+00:00",
        "updated_date": "2025-12-24T13:48:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenghao Xu",
            "Qi Liu",
            "Jiexi Yan",
            "Muli Yang",
            "Cheng Deng"
        ],
        "tldr": "This paper proposes Equivariant Feature Rotation (EFR), a novel adaptation strategy for few-shot image generation that aligns source and target domains in a self-rotated proxy feature space via learnable rotation matrices.",
        "tldr_zh": "本文提出了一种新颖的少样本图像生成自适应策略，即等变特征旋转（EFR），它通过可学习的旋转矩阵，在自旋转代理特征空间中对齐源域和目标域。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "FreeInpaint: Tuning-free Prompt Alignment and Visual Rationality Enhancement in Image Inpainting",
        "summary": "Text-guided image inpainting endeavors to generate new content within specified regions of images using textual prompts from users. The primary challenge is to accurately align the inpainted areas with the user-provided prompts while maintaining a high degree of visual fidelity. While existing inpainting methods have produced visually convincing results by leveraging the pre-trained text-to-image diffusion models, they still struggle to uphold both prompt alignment and visual rationality simultaneously. In this work, we introduce FreeInpaint, a plug-and-play tuning-free approach that directly optimizes the diffusion latents on the fly during inference to improve the faithfulness of the generated images. Technically, we introduce a prior-guided noise optimization method that steers model attention towards valid inpainting regions by optimizing the initial noise. Furthermore, we meticulously design a composite guidance objective tailored specifically for the inpainting task. This objective efficiently directs the denoising process, enhancing prompt alignment and visual rationality by optimizing intermediate latents at each step. Through extensive experiments involving various inpainting diffusion models and evaluation metrics, we demonstrate the effectiveness and robustness of our proposed FreeInpaint.",
        "url": "http://arxiv.org/abs/2512.21104v1",
        "published_date": "2025-12-24T11:06:26+00:00",
        "updated_date": "2025-12-24T11:06:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chao Gong",
            "Dong Li",
            "Yingwei Pan",
            "Jingjing Chen",
            "Ting Yao",
            "Tao Mei"
        ],
        "tldr": "FreeInpaint presents a tuning-free method for improving text-guided image inpainting by optimizing diffusion latents during inference to enhance prompt alignment and visual rationality.",
        "tldr_zh": "FreeInpaint 提出了一种无需微调的方法，通过在推理过程中优化扩散潜在空间来改善文本引导的图像修复，从而增强提示对齐和视觉合理性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation",
        "summary": "Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.",
        "url": "http://arxiv.org/abs/2512.21094v1",
        "published_date": "2025-12-24T10:30:35+00:00",
        "updated_date": "2025-12-24T10:30:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhe Cao",
            "Tao Wang",
            "Jiaming Wang",
            "Yanghai Wang",
            "Yuanxing Zhang",
            "Jialu Chen",
            "Miao Deng",
            "Jiahao Wang",
            "Yubin Guo",
            "Chenxi Liao",
            "Yize Zhang",
            "Zhaoxiang Zhang",
            "Jiaheng Liu"
        ],
        "tldr": "The paper introduces T2AV-Compass, a unified benchmark for Text-to-Audio-Video generation, addressing limitations in current evaluation methods by incorporating diverse prompts and a dual-level evaluation framework using both objective metrics and subjective MLLM-as-a-Judge assessment.",
        "tldr_zh": "该论文介绍了 T2AV-Compass，这是一个用于文本到音频视频生成的统一基准，它通过整合多样化的提示词和一个双层评估框架（结合客观指标和主观 MLLM 评估）来解决当前评估方法的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control",
        "summary": "In computational pathology, understanding and generation have evolved along disparate paths: advanced understanding models already exhibit diagnostic-level competence, whereas generative models largely simulate pixels. Progress remains hindered by three coupled factors: the scarcity of large, high-quality image-text corpora; the lack of precise, fine-grained semantic control, which forces reliance on non-semantic cues; and terminological heterogeneity, where diverse phrasings for the same diagnostic concept impede reliable text conditioning. We introduce UniPath, a semantics-driven pathology image generation framework that leverages mature diagnostic understanding to enable controllable generation. UniPath implements Multi-Stream Control: a Raw-Text stream; a High-Level Semantics stream that uses learnable queries to a frozen pathology MLLM to distill paraphrase-robust Diagnostic Semantic Tokens and to expand prompts into diagnosis-aware attribute bundles; and a Prototype stream that affords component-level morphological control via a prototype bank. On the data front, we curate a 2.65M image-text corpus and a finely annotated, high-quality 68K subset to alleviate data scarcity. For a comprehensive assessment, we establish a four-tier evaluation hierarchy tailored to pathology. Extensive experiments demonstrate UniPath's SOTA performance, including a Patho-FID of 80.9 (51% better than the second-best) and fine-grained semantic control achieving 98.7% of the real-image. The meticulously curated datasets, complete source code, and pre-trained model weights developed in this study will be made openly accessible to the public.",
        "url": "http://arxiv.org/abs/2512.21058v1",
        "published_date": "2025-12-24T08:52:08+00:00",
        "updated_date": "2025-12-24T08:52:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minghao Han",
            "YiChen Liu",
            "Yizhou Liu",
            "Zizhi Chen",
            "Jingqun Tang",
            "Xuecheng Wu",
            "Dingkang Yang",
            "Lihua Zhang"
        ],
        "tldr": "The paper introduces UniPath, a semantics-driven framework for pathology image generation leveraging diagnostic understanding and multi-stream control, along with a large curated dataset and a detailed evaluation hierarchy, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了 UniPath，一个语义驱动的病理图像生成框架，它利用成熟的诊断理解和多流控制，以及一个大型的整理数据集和一个详细的评估体系，实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FluencyVE: Marrying Temporal-Aware Mamba with Bypass Attention for Video Editing",
        "summary": "Large-scale text-to-image diffusion models have achieved unprecedented success in image generation and editing. However, extending this success to video editing remains challenging. Recent video editing efforts have adapted pretrained text-to-image models by adding temporal attention mechanisms to handle video tasks. Unfortunately, these methods continue to suffer from temporal inconsistency issues and high computational overheads. In this study, we propose FluencyVE, which is a simple yet effective one-shot video editing approach. FluencyVE integrates the linear time-series module, Mamba, into a video editing model based on pretrained Stable Diffusion models, replacing the temporal attention layer. This enables global frame-level attention while reducing the computational costs. In addition, we employ low-rank approximation matrices to replace the query and key weight matrices in the causal attention, and use a weighted averaging technique during training to update the attention scores. This approach significantly preserves the generative power of the text-to-image model while effectively reducing the computational burden. Experiments and analyses demonstrate promising results in editing various attributes, subjects, and locations in real-world videos.",
        "url": "http://arxiv.org/abs/2512.21015v1",
        "published_date": "2025-12-24T07:21:59+00:00",
        "updated_date": "2025-12-24T07:21:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingshu Cai",
            "Yixuan Li",
            "Osamu Yoshie",
            "Yuya Ieiri"
        ],
        "tldr": "FluencyVE replaces temporal attention in video editing diffusion models with Mamba and a low-rank approximation of causal attention to improve temporal consistency and reduce computational cost, demonstrating promising real-world video editing results.",
        "tldr_zh": "FluencyVE用Mamba和因果注意力的低秩近似代替视频编辑扩散模型中的时间注意力，以提高时间一致性并降低计算成本，展示了有希望的真实世界视频编辑结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations",
        "summary": "Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.",
        "url": "http://arxiv.org/abs/2512.21004v1",
        "published_date": "2025-12-24T07:07:08+00:00",
        "updated_date": "2025-12-24T07:07:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinghan Li",
            "Yang Jin",
            "Hao Jiang",
            "Yadong Mu",
            "Yang Song",
            "Kun Xu"
        ],
        "tldr": "The paper introduces NExT-Vid, a novel autoregressive video pretraining framework using masked next-frame prediction with context-isolated autoregressive predictor and conditioned flow-matching decoder, demonstrating improved performance in downstream video classification tasks.",
        "tldr_zh": "该论文介绍了NExT-Vid，一种新颖的自回归视频预训练框架，它使用带有上下文隔离自回归预测器和条件流匹配解码器的掩码下一帧预测，并在下游视频分类任务中表现出改进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
        "summary": "Diffusion models excel at generating high-quality, diverse samples, yet they risk memorizing training data when overfit to the training objective. We analyze the distinctions between memorization and generalization in diffusion models through the lens of representation learning. By investigating a two-layer ReLU denoising autoencoder (DAE), we prove that (i) memorization corresponds to the model storing raw training samples in the learned weights for encoding and decoding, yielding localized \"spiky\" representations, whereas (ii) generalization arises when the model captures local data statistics, producing \"balanced\" representations. Furthermore, we validate these theoretical findings on real-world unconditional and text-to-image diffusion models, demonstrating that the same representation structures emerge in deep generative models with significant practical implications. Building on these insights, we propose a representation-based method for detecting memorization and a training-free editing technique that allows precise control via representation steering. Together, our results highlight that learning good representations is central to novel and meaningful generative modeling.",
        "url": "http://arxiv.org/abs/2512.20963v1",
        "published_date": "2025-12-24T05:40:40+00:00",
        "updated_date": "2025-12-24T05:40:40+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Zekai Zhang",
            "Xiao Li",
            "Xiang Li",
            "Lianghe Shi",
            "Meng Wu",
            "Molei Tao",
            "Qing Qu"
        ],
        "tldr": "This paper analyzes memorization vs. generalization in diffusion models through representation learning, proposes a method for detecting memorization, and introduces a training-free editing technique via representation steering.",
        "tldr_zh": "本文通过表征学习分析了扩散模型中的记忆与泛化问题，提出了一种检测记忆的方法，并介绍了一种通过表征控制的免训练编辑技术。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]