[
    {
        "title": "SD3.5-Flash: Distribution-Guided Distillation of Generative Flows",
        "summary": "We present SD3.5-Flash, an efficient few-step distillation framework that\nbrings high-quality image generation to accessible consumer devices. Our\napproach distills computationally prohibitive rectified flow models through a\nreformulated distribution matching objective tailored specifically for few-step\ngeneration. We introduce two key innovations: \"timestep sharing\" to reduce\ngradient noise and \"split-timestep fine-tuning\" to improve prompt alignment.\nCombined with comprehensive pipeline optimizations like text encoder\nrestructuring and specialized quantization, our system enables both rapid\ngeneration and memory-efficient deployment across different hardware\nconfigurations. This democratizes access across the full spectrum of devices,\nfrom mobile phones to desktop computers. Through extensive evaluation including\nlarge-scale user studies, we demonstrate that SD3.5-Flash consistently\noutperforms existing few-step methods, making advanced generative AI truly\naccessible for practical deployment.",
        "url": "http://arxiv.org/abs/2509.21318v1",
        "published_date": "2025-09-25T16:07:38+00:00",
        "updated_date": "2025-09-25T16:07:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hmrishav Bandyopadhyay",
            "Rahim Entezari",
            "Jim Scott",
            "Reshinth Adithyan",
            "Yi-Zhe Song",
            "Varun Jampani"
        ],
        "tldr": "SD3.5-Flash is a distillation framework for rectified flow models, enabling high-quality image generation on resource-constrained devices via timestep sharing and split-timestep fine-tuning.",
        "tldr_zh": "SD3.5-Flash是一个用于校正流模型的蒸馏框架，通过时间步共享和分割时间步微调，在资源有限的设备上实现高质量的图像生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics",
        "summary": "A primary bottleneck in large-scale text-to-video generation today is\nphysical consistency and controllability. Despite recent advances,\nstate-of-the-art models often produce unrealistic motions, such as objects\nfalling upward, or abrupt changes in velocity and direction. Moreover, these\nmodels lack precise parameter control, struggling to generate physically\nconsistent dynamics under different initial conditions. We argue that this\nfundamental limitation stems from current models learning motion distributions\nsolely from appearance, while lacking an understanding of the underlying\ndynamics. In this work, we propose NewtonGen, a framework that integrates\ndata-driven synthesis with learnable physical principles. At its core lies\ntrainable Neural Newtonian Dynamics (NND), which can model and predict a\nvariety of Newtonian motions, thereby injecting latent dynamical constraints\ninto the video generation process. By jointly leveraging data priors and\ndynamical guidance, NewtonGen enables physically consistent video synthesis\nwith precise parameter control.",
        "url": "http://arxiv.org/abs/2509.21309v1",
        "published_date": "2025-09-25T15:25:33+00:00",
        "updated_date": "2025-09-25T15:25:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Yuan",
            "Xijun Wang",
            "Tharindu Wickremasinghe",
            "Zeeshan Nadir",
            "Bole Ma",
            "Stanley H. Chan"
        ],
        "tldr": "NewtonGen addresses the problem of physically inconsistent motions in text-to-video generation by integrating learnable Neural Newtonian Dynamics (NND) for improved control and realism.",
        "tldr_zh": "NewtonGen通过整合可学习的神经牛顿动力学（NND），解决了文本到视频生成中物理不一致运动的问题，从而提高了控制性和真实感。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Does FLUX Already Know How to Perform Physically Plausible Image Composition?",
        "summary": "Image composition aims to seamlessly insert a user-specified object into a\nnew scene, but existing models struggle with complex lighting (e.g., accurate\nshadows, water reflections) and diverse, high-resolution inputs. Modern\ntext-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential\nphysical and resolution priors, yet lack a framework to unleash them without\nresorting to latent inversion, which often locks object poses into contextually\ninappropriate orientations, or brittle attention surgery. We propose SHINE, a\ntraining-free framework for Seamless, High-fidelity Insertion with Neutralized\nErrors. SHINE introduces manifold-steered anchor loss, leveraging pretrained\ncustomization adapters (e.g., IP-Adapter) to guide latents for faithful subject\nrepresentation while preserving background integrity. Degradation-suppression\nguidance and adaptive background blending are proposed to further eliminate\nlow-quality outputs and visible seams. To address the lack of rigorous\nbenchmarks, we introduce ComplexCompo, featuring diverse resolutions and\nchallenging conditions such as low lighting, strong illumination, intricate\nshadows, and reflective surfaces. Experiments on ComplexCompo and\nDreamEditBench show state-of-the-art performance on standard metrics (e.g.,\nDINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward).\nCode and benchmark will be publicly available upon publication.",
        "url": "http://arxiv.org/abs/2509.21278v1",
        "published_date": "2025-09-25T15:01:49+00:00",
        "updated_date": "2025-09-25T15:01:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Shilin Lu",
            "Zhuming Lian",
            "Zihan Zhou",
            "Shaocong Zhang",
            "Chen Zhao",
            "Adams Wai-Kin Kong"
        ],
        "tldr": "The paper introduces SHINE, a training-free framework for high-fidelity image composition using diffusion models, which addresses limitations of existing methods in handling complex lighting and high-resolution inputs. It also introduces a new benchmark, ComplexCompo, for evaluating image composition.",
        "tldr_zh": "该论文介绍了一种名为SHINE的无训练框架，用于使用扩散模型进行高保真图像合成，解决了现有方法在处理复杂光照和高分辨率输入方面的局限性。它还引入了一个新的基准测试，ComplexCompo，用于评估图像合成。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation",
        "summary": "Generating videos guided by camera trajectories poses significant challenges\nin achieving consistency and generalizability, particularly when both camera\nand object motions are present. Existing approaches often attempt to learn\nthese motions separately, which may lead to confusion regarding the relative\nmotion between the camera and the objects. To address this challenge, we\npropose a novel approach that integrates both camera and object motions by\nconverting them into the motion of corresponding pixels. Utilizing a stable\ndiffusion network, we effectively learn reference motion maps in relation to\nthe specified camera trajectory. These maps, along with an extracted semantic\nobject prior, are then fed into an image-to-video network to generate the\ndesired video that can accurately follow the designated camera trajectory while\nmaintaining consistent object motions. Extensive experiments verify that our\nmodel outperforms SOTA methods by a large margin.",
        "url": "http://arxiv.org/abs/2509.21119v1",
        "published_date": "2025-09-25T13:06:12+00:00",
        "updated_date": "2025-09-25T13:06:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guojun Lei",
            "Chi Wang",
            "Yikai Wang",
            "Hong Li",
            "Ying Song",
            "Weiwei Xu"
        ],
        "tldr": "The paper introduces MotionFlow, a method for generating videos with complex camera trajectories by learning implicit motion flow maps that integrate camera and object movements, outperforming SOTA approaches.",
        "tldr_zh": "该论文介绍了MotionFlow，一种通过学习整合相机和物体运动的隐式运动流图来生成具有复杂相机轨迹的视频的方法，并且优于现有技术。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition",
        "summary": "We propose a novel architecture UniTransfer, which introduces both spatial\nand diffusion timestep decomposition in a progressive paradigm, achieving\nprecise and controllable video concept transfer. Specifically, in terms of\nspatial decomposition, we decouple videos into three key components: the\nforeground subject, the background, and the motion flow. Building upon this\ndecomposed formulation, we further introduce a dual-to-single-stream DiT-based\narchitecture for supporting fine-grained control over different components in\nthe videos. We also introduce a self-supervised pretraining strategy based on\nrandom masking to enhance the decomposed representation learning from\nlarge-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning\nparadigm, we further revisit the denoising diffusion process and propose a\nChain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We\ndecompose the denoising process into three stages of different granularity and\nleverage large language models (LLMs) for stage-specific instructions to guide\nthe generation progressively. We also curate an animal-centric video dataset\ncalled OpenAnimal to facilitate the advancement and benchmarking of research in\nvideo concept transfer. Extensive experiments demonstrate that our method\nachieves high-quality and controllable video concept transfer across diverse\nreference images and scenes, surpassing existing baselines in both visual\nfidelity and editability. Web Page:\nhttps://yu-shaonian.github.io/UniTransfer-Web/",
        "url": "http://arxiv.org/abs/2509.21086v1",
        "published_date": "2025-09-25T12:39:06+00:00",
        "updated_date": "2025-09-25T12:39:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guojun Lei",
            "Rong Zhang",
            "Chi Wang",
            "Tianhang Liu",
            "Hong Li",
            "Zhiyuan Ma",
            "Weiwei Xu"
        ],
        "tldr": "UniTransfer achieves precise video concept transfer by spatial and temporal decomposition, using a dual-stream DiT architecture, self-supervised pretraining, and a Chain-of-Prompt mechanism guided by LLMs. They also introduce a new dataset, OpenAnimal.",
        "tldr_zh": "UniTransfer通过空间和时间分解实现精确的视频概念转换，利用双流DiT架构、自监督预训练以及LLM引导的Chain-of-Prompt机制。他们还提出了一个新的数据集OpenAnimal。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FreeInsert: Personalized Object Insertion with Geometric and Style Control",
        "summary": "Text-to-image diffusion models have made significant progress in image\ngeneration, allowing for effortless customized generation. However, existing\nimage editing methods still face certain limitations when dealing with\npersonalized image composition tasks. First, there is the issue of lack of\ngeometric control over the inserted objects. Current methods are confined to 2D\nspace and typically rely on textual instructions, making it challenging to\nmaintain precise geometric control over the objects. Second, there is the\nchallenge of style consistency. Existing methods often overlook the style\nconsistency between the inserted object and the background, resulting in a lack\nof realism. In addition, the challenge of inserting objects into images without\nextensive training remains significant. To address these issues, we propose\n\\textit{FreeInsert}, a novel training-free framework that customizes object\ninsertion into arbitrary scenes by leveraging 3D geometric information.\nBenefiting from the advances in existing 3D generation models, we first convert\nthe 2D object into 3D, perform interactive editing at the 3D level, and then\nre-render it into a 2D image from a specified view. This process introduces\ngeometric controls such as shape or view. The rendered image, serving as\ngeometric control, is combined with style and content control achieved through\ndiffusion adapters, ultimately producing geometrically controlled,\nstyle-consistent edited images via the diffusion model.",
        "url": "http://arxiv.org/abs/2509.20756v1",
        "published_date": "2025-09-25T05:26:10+00:00",
        "updated_date": "2025-09-25T05:26:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhong Zhang",
            "Han Wang",
            "Yiwen Wang",
            "Rong Xie",
            "Li Song"
        ],
        "tldr": "The paper introduces FreeInsert, a training-free framework for personalized object insertion into images, offering geometric and style control by leveraging 3D geometric information and diffusion adapters.",
        "tldr_zh": "该论文介绍了FreeInsert，一个无需训练的图像个性化对象插入框架，通过利用3D几何信息和扩散适配器来实现几何和风格控制。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection",
        "summary": "Maritime object detection is essential for navigation safety, surveillance,\nand autonomous operations, yet constrained by two key challenges: the scarcity\nof annotated maritime data and poor generalization across various maritime\nattributes (e.g., object category, viewpoint, location, and imaging\nenvironment). % In particular, models trained on existing datasets often\nunderperform in underrepresented scenarios such as open-sea environments. To\naddress these challenges, we propose Neptune-X, a data-centric\ngenerative-selection framework that enhances training effectiveness by\nleveraging synthetic data generation with task-aware sample selection. From the\ngeneration perspective, we develop X-to-Maritime, a multi-modality-conditioned\ngenerative model that synthesizes diverse and realistic maritime scenes. A key\ncomponent is the Bidirectional Object-Water Attention module, which captures\nboundary interactions between objects and their aquatic surroundings to improve\nvisual fidelity. To further improve downstream tasking performance, we propose\nAttribute-correlated Active Sampling, which dynamically selects synthetic\nsamples based on their task relevance. To support robust benchmarking, we\nconstruct the Maritime Generation Dataset, the first dataset tailored for\ngenerative maritime learning, encompassing a wide range of semantic conditions.\nExtensive experiments demonstrate that our approach sets a new benchmark in\nmaritime scene synthesis, significantly improving detection accuracy,\nparticularly in challenging and previously underrepresented settings.The code\nis available at https://github.com/gy65896/Neptune-X.",
        "url": "http://arxiv.org/abs/2509.20745v1",
        "published_date": "2025-09-25T04:59:02+00:00",
        "updated_date": "2025-09-25T04:59:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Guo",
            "Shengfeng He",
            "Yuxu Lu",
            "Haonan An",
            "Yihang Tao",
            "Huilin Zhu",
            "Jingxian Liu",
            "Yuguang Fang"
        ],
        "tldr": "The paper introduces Neptune-X, a data-centric generative-selection framework with a multi-modality-conditioned generative model for maritime scene synthesis, and demonstrates improved object detection accuracy, especially in underrepresented scenarios.",
        "tldr_zh": "该论文介绍了Neptune-X，一个以数据为中心的生成-选择框架，它采用多模态条件生成模型进行海事场景合成，并展示了目标检测精度的提高，尤其是在代表性不足的场景中。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets",
        "summary": "Recent advances in 3D-native generative models have accelerated asset\ncreation for games, film, and design. However, most methods still rely\nprimarily on image or text conditioning and lack fine-grained, cross-modal\ncontrols, which limits controllability and practical adoption. To address this\ngap, we present Hunyuan3D-Omni, a unified framework for fine-grained,\ncontrollable 3D asset generation built on Hunyuan3D 2.1. In addition to images,\nHunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose\npriors as conditioning signals, enabling precise control over geometry,\ntopology, and pose. Instead of separate heads for each modality, our model\nunifies all signals in a single cross-modal architecture. We train with a\nprogressive, difficulty-aware sampling strategy that selects one control\nmodality per example and biases sampling toward harder signals (e.g., skeletal\npose) while downweighting easier ones (e.g., point clouds), encouraging robust\nmulti-modal fusion and graceful handling of missing inputs. Experiments show\nthat these additional controls improve generation accuracy, enable\ngeometry-aware transformations, and increase robustness for production\nworkflows.",
        "url": "http://arxiv.org/abs/2509.21245v1",
        "published_date": "2025-09-25T14:39:17+00:00",
        "updated_date": "2025-09-25T14:39:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Team Hunyuan3D",
            ":",
            "Bowen Zhang",
            "Chunchao Guo",
            "Haolin Liu",
            "Hongyu Yan",
            "Huiwen Shi",
            "Jingwei Huang",
            "Junlin Yu",
            "Kunhong Li",
            "Linus",
            "Penghao Wang",
            "Qingxiang Lin",
            "Sicong Liu",
            "Xianghui Yang",
            "Yixuan Tang",
            "Yunfei Zhao",
            "Zeqiang Lai",
            "Zhihao Liang",
            "Zibo Zhao"
        ],
        "tldr": "The paper introduces Hunyuan3D-Omni, a unified framework for controllable 3D asset generation using multiple conditioning signals (images, point clouds, voxels, bounding boxes, skeletal pose) with a focus on improving control and robustness in production workflows.",
        "tldr_zh": "该论文介绍了Hunyuan3D-Omni，一个统一的框架，用于通过多个条件信号（图像、点云、体素、边界框、骨骼姿势）来可控地生成3D资产，重点是提高生产工作流程中的控制和鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]