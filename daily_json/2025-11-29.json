[
    {
        "title": "Markovian Scale Prediction: A New Era of Visual Autoregressive Generation",
        "summary": "Visual AutoRegressive modeling (VAR) based on next-scale prediction has revitalized autoregressive visual generation. Although its full-context dependency, i.e., modeling all previous scales for next-scale prediction, facilitates more stable and comprehensive representation learning by leveraging complete information flow, the resulting computational inefficiency and substantial overhead severely hinder VAR's practicality and scalability. This motivates us to develop a new VAR model with better performance and efficiency without full-context dependency. To address this, we reformulate VAR as a non-full-context Markov process, proposing Markov-VAR. It is achieved via Markovian Scale Prediction: we treat each scale as a Markov state and introduce a sliding window that compresses certain previous scales into a compact history vector to compensate for historical information loss owing to non-full-context dependency. Integrating the history vector with the Markov state yields a representative dynamic state that evolves under a Markov process. Extensive experiments demonstrate that Markov-VAR is extremely simple yet highly effective: Compared to VAR on ImageNet, Markov-VAR reduces FID by 10.5% (256 $\\times$ 256) and decreases peak memory consumption by 83.8% (1024 $\\times$ 1024). We believe that Markov-VAR can serve as a foundation for future research on visual autoregressive generation and other downstream tasks.",
        "url": "http://arxiv.org/abs/2511.23334v1",
        "published_date": "2025-11-28T16:42:18+00:00",
        "updated_date": "2025-11-28T16:42:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Zhang",
            "Jingyi Liu",
            "Yiwei Shi",
            "Qi Zhang",
            "Duoqian Miao",
            "Changwei Wang",
            "Longbing Cao"
        ],
        "tldr": "The paper introduces Markov-VAR, a more efficient Visual AutoRegressive model that uses Markovian Scale Prediction to reduce computational overhead while maintaining competitive performance by foregoing full-context dependency.",
        "tldr_zh": "本文介绍了一种更高效的视觉自回归模型Markov-VAR，它使用马尔可夫尺度预测来减少计算开销，同时通过放弃全上下文依赖性来保持竞争性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Captain Safari: A World Engine",
        "summary": "World engines aim to synthesize long, 3D-consistent videos that support interactive exploration of a scene under user-controlled camera motion. However, existing systems struggle under aggressive 6-DoF trajectories and complex outdoor layouts: they lose long-range geometric coherence, deviate from the target path, or collapse into overly conservative motion. To this end, we introduce Captain Safari, a pose-conditioned world engine that generates videos by retrieving from a persistent world memory. Given a camera path, our method maintains a dynamic local memory and uses a retriever to fetch pose-aligned world tokens, which then condition video generation along the trajectory. This design enables the model to maintain stable 3D structure while accurately executing challenging camera maneuvers. To evaluate this setting, we curate OpenSafari, a new in-the-wild FPV dataset containing high-dynamic drone videos with verified camera trajectories, constructed through a multi-stage geometric and kinematic validation pipeline. Across video quality, 3D consistency, and trajectory following, Captain Safari substantially outperforms state-of-the-art camera-controlled generators. It reduces MEt3R from 0.3703 to 0.3690, improves AUC@30 from 0.181 to 0.200, and yields substantially lower FVD than all camera-controlled baselines. More importantly, in a 50-participant, 5-way human study where annotators select the best result among five anonymized models, 67.6% of preferences favor our method across all axes. Our results demonstrate that pose-conditioned world memory is a powerful mechanism for long-horizon, controllable video generation and provide OpenSafari as a challenging new benchmark for future world-engine research.",
        "url": "http://arxiv.org/abs/2511.22815v1",
        "published_date": "2025-11-28T00:27:46+00:00",
        "updated_date": "2025-11-28T00:27:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu-Cheng Chou",
            "Xingrui Wang",
            "Yitong Li",
            "Jiahao Wang",
            "Hanting Liu",
            "Cihang Xie",
            "Alan Yuille",
            "Junfei Xiao"
        ],
        "tldr": "Captain Safari introduces a pose-conditioned world engine that leverages a persistent world memory for generating 3D-consistent, controllable videos, outperforming existing methods and establishing a new benchmark dataset, OpenSafari.",
        "tldr_zh": "Captain Safari 提出了一种姿势条件的世界引擎，利用持久的世界记忆来生成 3D 一致且可控的视频，优于现有方法，并建立了一个新的基准数据集 OpenSafari。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
        "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.",
        "url": "http://arxiv.org/abs/2511.22699v1",
        "published_date": "2025-11-27T18:52:07+00:00",
        "updated_date": "2025-11-27T18:52:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Z-Image Team",
            "Huanqia Cai",
            "Sihan Cao",
            "Ruoyi Du",
            "Peng Gao",
            "Steven Hoi",
            "Shijie Huang",
            "Zhaohui Hou",
            "Dengyang Jiang",
            "Xin Jin",
            "Liangchen Li",
            "Zhen Li",
            "Zhong-Yu Li",
            "David Liu",
            "Dongyang Liu",
            "Junhan Shi",
            "Qilong Wu",
            "Feng Yu",
            "Chi Zhang",
            "Shifeng Zhang",
            "Shilin Zhou"
        ],
        "tldr": "Z-Image is a 6B-parameter image generation model that achieves performance comparable to larger proprietary models with significantly reduced computational cost by optimizing the model lifecycle and using a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture, with additional models for turbo inference and editing.",
        "tldr_zh": "Z-Image是一个60亿参数的图像生成模型，通过优化模型生命周期和使用可扩展的单流扩散Transformer（S3-DiT）架构，实现了与大型专有模型相当的性能，同时显著降低了计算成本，还有用于加速推理和编辑的额外模型。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Flow Straighter and Faster: Efficient One-Step Generative Modeling via MeanFlow on Rectified Trajectories",
        "summary": "Flow-based generative models have recently demonstrated strong performance, yet sampling typically relies on expensive numerical integration of ordinary differential equations (ODEs). Rectified Flow enables one-step sampling by learning nearly straight probability paths, but achieving such straightness requires multiple computationally intensive reflow iterations. MeanFlow achieves one-step generation by directly modeling the average velocity over time; however, when trained on highly curved flows, it suffers from slow convergence and noisy supervision. To address these limitations, we propose Rectified MeanFlow, a framework that models the mean velocity field along the rectified trajectory using only a single reflow step. This eliminates the need for perfectly straightened trajectories while enabling efficient training. Furthermore, we introduce a simple yet effective truncation heuristic that aims to reduce residual curvature and further improve performance. Extensive experiments on ImageNet at 64, 256, and 512 resolutions show that Re-MeanFlow consistently outperforms prior one-step flow distillation and Rectified Flow methods in both sample quality and training efficiency. Code is available at https://github.com/Xinxi-Zhang/Re-MeanFlow.",
        "url": "http://arxiv.org/abs/2511.23342v1",
        "published_date": "2025-11-28T16:50:08+00:00",
        "updated_date": "2025-11-28T16:50:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinxi Zhang",
            "Shiwei Tan",
            "Quang Nguyen",
            "Quan Dao",
            "Ligong Han",
            "Xiaoxiao He",
            "Tunyu Zhang",
            "Alen Mrdovic",
            "Dimitris Metaxas"
        ],
        "tldr": "The paper introduces Rectified MeanFlow, a method for efficient one-step image generation using flow-based models by modeling the mean velocity field along a rectified trajectory, outperforming previous methods in sample quality and training efficiency.",
        "tldr_zh": "该论文介绍了 Rectified MeanFlow，这是一种高效的单步图像生成方法，它使用基于流的模型，通过对矫正轨迹上的平均速度场进行建模，在样本质量和训练效率方面优于以前的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "GeoWorld: Unlocking the Potential of Geometry Models to Facilitate High-Fidelity 3D Scene Generation",
        "summary": "Previous works leveraging video models for image-to-3D scene generation tend to suffer from geometric distortions and blurry content. In this paper, we renovate the pipeline of image-to-3D scene generation by unlocking the potential of geometry models and present our GeoWorld. Instead of exploiting geometric information obtained from a single-frame input, we propose to first generate consecutive video frames and then take advantage of the geometry model to provide full-frame geometry features, which contain richer information than single-frame depth maps or camera embeddings used in previous methods, and use these geometry features as geometrical conditions to aid the video generation model. To enhance the consistency of geometric structures, we further propose a geometry alignment loss to provide the model with real-world geometric constraints and a geometry adaptation module to ensure the effective utilization of geometry features. Extensive experiments show that our GeoWorld can generate high-fidelity 3D scenes from a single image and a given camera trajectory, outperforming prior methods both qualitatively and quantitatively. Project Page: https://peaes.github.io/GeoWorld/.",
        "url": "http://arxiv.org/abs/2511.23191v1",
        "published_date": "2025-11-28T13:55:45+00:00",
        "updated_date": "2025-11-28T13:55:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhao Wan",
            "Lijuan Liu",
            "Jingzhi Zhou",
            "Zihan Zhou",
            "Xuying Zhang",
            "Dongbo Zhang",
            "Shaohui Jiao",
            "Qibin Hou",
            "Ming-Ming Cheng"
        ],
        "tldr": "GeoWorld improves image-to-3D scene generation by using geometry models with video frames to provide richer geometric features and novel losses/modules, resulting in improved fidelity and consistency.",
        "tldr_zh": "GeoWorld 通过使用视频帧的几何模型来提供更丰富的几何特征以及新颖的损失/模块，改进了图像到 3D 场景的生成，从而提高了保真度和一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fast Multi-view Consistent 3D Editing with Video Priors",
        "summary": "Text-driven 3D editing enables user-friendly 3D object or scene editing with text instructions. Due to the lack of multi-view consistency priors, existing methods typically resort to employing 2D generation or editing models to process each view individually, followed by iterative 2D-3D-2D updating. However, these methods are not only time-consuming but also prone to over-smoothed results because the different editing signals gathered from different views are averaged during the iterative process. In this paper, we propose generative Video Prior based 3D Editing (ViP3DE) to employ the temporal consistency priors from pre-trained video generation models for multi-view consistent 3D editing in a single forward pass. Our key insight is to condition the video generation model on a single edited view to generate other consistent edited views for 3D updating directly, thereby bypassing the iterative editing paradigm. Since 3D updating requires edited views to be paired with specific camera poses, we propose motion-preserved noise blending for the video model to generate edited views at predefined camera poses. In addition, we introduce geometry-aware denoising to further enhance multi-view consistency by integrating 3D geometric priors into video models. Extensive experiments demonstrate that our proposed ViP3DE can achieve high-quality 3D editing results even within a single forward pass, significantly outperforming existing methods in both editing quality and speed.",
        "url": "http://arxiv.org/abs/2511.23172v1",
        "published_date": "2025-11-28T13:31:10+00:00",
        "updated_date": "2025-11-28T13:31:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liyi Chen",
            "Ruihuang Li",
            "Guowen Zhang",
            "Pengfei Wang",
            "Lei Zhang"
        ],
        "tldr": "This paper presents a method (ViP3DE) for fast and multi-view consistent 3D editing from text, using video priors from a pre-trained video generation model to bypass iterative 2D-3D-2D updating.",
        "tldr_zh": "本文提出了一种名为 ViP3DE 的方法，它利用预训练视频生成模型的视频先验，实现快速且多视角一致的文本驱动 3D 编辑，从而避免了迭代的 2D-3D-2D 更新过程。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InstanceV: Instance-Level Video Generation",
        "summary": "Recent advances in text-to-video diffusion models have enabled the generation of high-quality videos conditioned on textual descriptions. However, most existing text-to-video models rely solely on textual conditions, lacking general fine-grained controllability over video generation. To address this challenge, we propose InstanceV, a video generation framework that enables i) instance-level control and ii) global semantic consistency. Specifically, with the aid of proposed Instance-aware Masked Cross-Attention mechanism, InstanceV maximizes the utilization of additional instance-level grounding information to generate correctly attributed instances at designated spatial locations. To improve overall consistency, We introduce the Shared Timestep-Adaptive Prompt Enhancement module, which connects local instances with global semantics in a parameter-efficient manner. Furthermore, we incorporate Spatially-Aware Unconditional Guidance during both training and inference to alleviate the disappearance of small instances. Finally, we propose a new benchmark, named InstanceBench, which combines general video quality metrics with instance-aware metrics for more comprehensive evaluation on instance-level video generation. Extensive experiments demonstrate that InstanceV not only achieves remarkable instance-level controllability in video generation, but also outperforms existing state-of-the-art models in both general quality and instance-aware metrics across qualitative and quantitative evaluations.",
        "url": "http://arxiv.org/abs/2511.23146v1",
        "published_date": "2025-11-28T12:52:37+00:00",
        "updated_date": "2025-11-28T12:52:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuheng Chen",
            "Teng Hu",
            "Jiangning Zhang",
            "Zhucun Xue",
            "Ran Yi",
            "Lizhuang Ma"
        ],
        "tldr": "InstanceV introduces a novel video generation framework with instance-level control and global semantic consistency, utilizing instance-aware attention, timestep-adaptive prompt enhancement, and spatially-aware unconditional guidance.",
        "tldr_zh": "InstanceV 提出了一个新颖的视频生成框架，具有实例级别的控制和全局语义一致性，它利用了实例感知注意力、时间步自适应提示增强和空间感知无条件指导。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation",
        "summary": "This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation. Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness. DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences. To harmonize these two modalities, we further propose the Semantic Guided Mutual Alignment (SIGMA) mechanism, which performs RGB-depth fusion in a semantics-guided and mutually reinforced manner. These designs collectively enable DualCamCtrl to better disentangle appearance and geometry modeling, generating videos that more faithfully adhere to the specified camera trajectories. Additionally, we analyze and reveal the distinct influence of depth and camera poses across denoising stages and further demonstrate that early and late stages play complementary roles in forming global structure and refining local details. Extensive experiments demonstrate that DualCamCtrl achieves more consistent camera-controlled video generation, with over 40\\% reduction in camera motion errors compared with prior methods. Our project page: https://soyouthinkyoucantell.github.io/dualcamctrl\\-page/",
        "url": "http://arxiv.org/abs/2511.23127v1",
        "published_date": "2025-11-28T12:19:57+00:00",
        "updated_date": "2025-11-28T12:19:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongfei Zhang",
            "Kanghao Chen",
            "Zixin Zhang",
            "Harold Haodong Chen",
            "Yuanhuiyi Lyu",
            "Yuqi Zhang",
            "Shuai Yang",
            "Kun Zhou",
            "Yingcong Chen"
        ],
        "tldr": "DualCamCtrl is a diffusion model for geometry-aware, camera-controlled video generation that uses a dual-branch framework to generate camera-consistent RGB and depth sequences, achieving lower camera motion errors than prior methods.",
        "tldr_zh": "DualCamCtrl是一个用于几何感知、相机控制的视频生成的扩散模型，它使用一个双分支框架来生成相机一致的RGB和深度序列，与之前的方法相比，实现了更低的相机运动误差。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NumeriKontrol: Adding Numeric Control to Diffusion Transformers for Instruction-based Image Editing",
        "summary": "Instruction-based image editing enables intuitive manipulation through natural language commands. However, text instructions alone often lack the precision required for fine-grained control over edit intensity. We introduce NumeriKontrol, a framework that allows users to precisely adjust image attributes using continuous scalar values with common units. NumeriKontrol encodes numeric editing scales via an effective Numeric Adapter and injects them into diffusion models in a plug-and-play manner. Thanks to a task-separated design, our approach supports zero-shot multi-condition editing, allowing users to specify multiple instructions in any order. To provide high-quality supervision, we synthesize precise training data from reliable sources, including high-fidelity rendering engines and DSLR cameras. Our Common Attribute Transform (CAT) dataset covers diverse attribute manipulations with accurate ground-truth scales, enabling NumeriKontrol to function as a simple yet powerful interactive editing studio. Extensive experiments show that NumeriKontrol delivers accurate, continuous, and stable scale control across a wide range of attribute editing scenarios. These contributions advance instruction-based image editing by enabling precise, scalable, and user-controllable image manipulation.",
        "url": "http://arxiv.org/abs/2511.23105v1",
        "published_date": "2025-11-28T11:43:52+00:00",
        "updated_date": "2025-11-28T11:43:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenyu Xu",
            "Xiaoqi Shen",
            "Haotian Nan",
            "Xinyu Zhang"
        ],
        "tldr": "The paper introduces NumeriKontrol, a framework that enhances instruction-based image editing by allowing precise control over image attributes using continuous numeric scales within diffusion models, trained on a synthesized dataset with accurate ground-truth scales.",
        "tldr_zh": "该论文介绍了NumeriKontrol，一个通过在扩散模型中使用连续数值尺度精确控制图像属性来增强基于指令的图像编辑的框架，该框架基于具有精确地面真实尺度的合成数据集进行训练。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "JarvisEvo: Towards a Self-Evolving Photo Editing Agent with Synergistic Editor-Evaluator Optimization",
        "summary": "Agent-based editing models have substantially advanced interactive experiences, processing quality, and creative flexibility. However, two critical challenges persist: (1) instruction hallucination, text-only chain-of-thought (CoT) reasoning cannot fully prevent factual errors due to inherent information bottlenecks; (2) reward hacking, dynamic policy optimization against static reward models allows agents to exploit flaws in reward functions. To address these issues, we propose JarvisEvo, a unified image editing agent that emulates an expert human designer by iteratively editing, selecting appropriate tools, evaluating results, and reflecting on its own decisions to refine outcomes. JarvisEvo offers three key advantages: (1) an interleaved multimodal chain-of-thought (iMCoT) reasoning mechanism that enhances instruction following and editing quality; (2) a synergistic editor-evaluator policy optimization (SEPO) framework that enables self-improvement without external rewards, effectively mitigating reward hacking; and (3) support for both global and local fine-grained editing through seamless integration of Adobe Lightroom. On ArtEdit-Bench, JarvisEvo outperforms Nano-Banana by an average of 18.95% on preservative editing metrics, including a substantial 44.96% improvement in pixel-level content fidelity.",
        "url": "http://arxiv.org/abs/2511.23002v1",
        "published_date": "2025-11-28T09:04:51+00:00",
        "updated_date": "2025-11-28T09:04:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunlong Lin",
            "Linqing Wang",
            "Kunjie Lin",
            "Zixu Lin",
            "Kaixiong Gong",
            "Wenbo Li",
            "Bin Lin",
            "Zhenxi Li",
            "Shiyi Zhang",
            "Yuyang Peng",
            "Wenxun Dai",
            "Xinghao Ding",
            "Chunyu Wang",
            "Qinglin Lu"
        ],
        "tldr": "JarvisEvo is a self-evolving photo editing agent that uses interleaved multimodal chain-of-thought reasoning and synergistic editor-evaluator policy optimization to improve instruction following and editing quality without external rewards, outperforming existing methods on preservation metrics.",
        "tldr_zh": "JarvisEvo 是一种自进化的照片编辑代理，它利用交错的多模态思维链推理和协同编辑-评估器策略优化来提高指令遵循和编辑质量，无需外部奖励，并在保持指标上优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Guiding Visual Autoregressive Models through Spectrum Weakening",
        "summary": "Classifier-free guidance (CFG) has become a widely adopted and practical approach for enhancing generation quality and improving condition alignment. Recent studies have explored guidance mechanisms for unconditional generation, yet these approaches remain fundamentally tied to assumptions specific to diffusion models. In this work, we propose a spectrum-weakening framework for visual autoregressive (AR) models. This method works without the need for re-training, specific conditions, or any architectural modifications. It achieves this by constructing a controllable weak model in the spectral domain. We theoretically show that invertible spectral transformations preserve information, while selectively retaining only a subset of spectrum introduces controlled information reduction. Based on this insight, we perform spectrum selection along the channel dimension of internal representations, which avoids the structural constraints imposed by diffusion models. We further introduce two spectrum renormalization strategies that ensures numerical stability during the weakening process. Extensive experiments were conducted on both discrete and continuous AR models, with text or class conditioning. The results demonstrate that our method enables high-quality unconditional generation while maintaining strong prompt alignment for conditional generation.",
        "url": "http://arxiv.org/abs/2511.22991v1",
        "published_date": "2025-11-28T08:52:50+00:00",
        "updated_date": "2025-11-28T08:52:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chaoyang Wang",
            "Tianmeng Yang",
            "Jingdong Wang",
            "Yunhai Tong"
        ],
        "tldr": "This paper introduces a spectrum-weakening framework for visual autoregressive models that enhances generation quality and condition alignment without retraining or architectural modifications, applicable to both discrete and continuous AR models.",
        "tldr_zh": "本文介绍了一种用于视觉自回归模型的频谱弱化框架，该框架无需重新训练或修改架构即可提高生成质量和条件对齐，适用于离散和连续的自回归模型。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MultiBanana: A Challenging Benchmark for Multi-Reference Text-to-Image Generation",
        "summary": "Recent text-to-image generation models have acquired the ability of multi-reference generation and editing; the ability to inherit the appearance of subjects from multiple reference images and re-render them under new contexts. However, the existing benchmark datasets often focus on the generation with single or a few reference images, which prevents us from measuring the progress on how model performance advances or pointing out their weaknesses, under different multi-reference conditions. In addition, their task definitions are still vague, typically limited to axes such as \"what to edit\" or \"how many references are given\", and therefore fail to capture the intrinsic difficulty of multi-reference settings. To address this gap, we introduce $\\textbf{MultiBanana}$, which is carefully designed to assesses the edge of model capabilities by widely covering multi-reference-specific problems at scale: (1) varying the number of references, (2) domain mismatch among references (e.g., photo vs. anime), (3) scale mismatch between reference and target scenes, (4) references containing rare concepts (e.g., a red banana), and (5) multilingual textual references for rendering. Our analysis among a variety of text-to-image models reveals their superior performances, typical failure modes, and areas for improvement. MultiBanana will be released as an open benchmark to push the boundaries and establish a standardized basis for fair comparison in multi-reference image generation. Our data and code are available at https://github.com/matsuolab/multibanana .",
        "url": "http://arxiv.org/abs/2511.22989v1",
        "published_date": "2025-11-28T08:49:55+00:00",
        "updated_date": "2025-11-28T08:49:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuta Oshima",
            "Daiki Miyake",
            "Kohsei Matsutani",
            "Yusuke Iwasawa",
            "Masahiro Suzuki",
            "Yutaka Matsuo",
            "Hiroki Furuta"
        ],
        "tldr": "The paper introduces MultiBanana, a new benchmark dataset for evaluating multi-reference text-to-image generation models, addressing the limitations of existing datasets in capturing the complexities of diverse multi-reference scenarios.",
        "tldr_zh": "该论文介绍了MultiBanana，一个新的基准数据集，用于评估多参考文本到图像生成模型，解决了现有数据集在捕捉多参考场景多样性方面存在的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Ovis-Image Technical Report",
        "summary": "We introduce $\\textbf{Ovis-Image}$, a 7B text-to-image model specifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates a diffusion-based visual decoder with the stronger Ovis 2.5 multimodal backbone, leveraging a text-centric training pipeline that combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such as Qwen-Image and approaches closed-source systems like Seedream and GPT4o. Crucially, the model remains deployable on a single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining a strong multimodal backbone with a carefully designed, text-focused training recipe is sufficient to achieve reliable bilingual text rendering without resorting to oversized or proprietary models.",
        "url": "http://arxiv.org/abs/2511.22982v1",
        "published_date": "2025-11-28T08:42:31+00:00",
        "updated_date": "2025-11-28T08:42:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Guo-Hua Wang",
            "Liangfu Cao",
            "Tianyu Cui",
            "Minghao Fu",
            "Xiaohao Chen",
            "Pengxin Zhan",
            "Jianshan Zhao",
            "Lan Li",
            "Bowen Fu",
            "Jiaqi Liu",
            "Qing-Guo Chen"
        ],
        "tldr": "Ovis-Image is a compact 7B text-to-image model optimized for high-quality text rendering, achieving performance comparable to larger models while being deployable on a single GPU.",
        "tldr_zh": "Ovis-Image是一个紧凑的7B文本到图像模型，针对高质量文本渲染进行了优化，在单个GPU上即可部署，并且性能可与更大的模型相媲美。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning",
        "summary": "Text-to-video (T2V) generation has achieved remarkable progress in producing high-quality videos aligned with textual prompts. However, aligning synthesized videos with nuanced human preference remains challenging due to the subjective and multifaceted nature of human judgment. Existing video preference alignment methods rely on costly human annotations or utilize proxy metrics to predict preference, which lacks the understanding of human preference logic. Moreover, they usually directly align T2V models with the overall preference distribution, ignoring potential conflict dimensions like motion dynamics and visual quality, which may bias models towards low-motion content. To address these issues, we present Motion-corrective alignment with Self-critic hierarchical Reasoning (McSc), a three-stage reinforcement learning framework for robust preference modeling and alignment. Firstly, Self-critic Dimensional Reasoning (ScDR) trains a generative reward model (RM) to decompose preferences into per-dimension assessments, using self-critic reasoning chains for reliable learning. Secondly, to achieve holistic video comparison, we introduce Hierarchical Comparative Reasoning (HCR) for structural multi-dimensional reasoning with hierarchical reward supervision. Finally, using RM-preferred videos, we propose Motion-corrective Direct Preference Optimization (McDPO) to optimize T2V models, while dynamically re-weighting alignment objective to mitigate bias towards low-motion content. Experiments show that McSc achieves superior performance in human preference alignment and generates videos with high-motion dynamic.",
        "url": "http://arxiv.org/abs/2511.22974v1",
        "published_date": "2025-11-28T08:27:53+00:00",
        "updated_date": "2025-11-28T08:27:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiushi Yang",
            "Yingjie Chen",
            "Yuan Yao",
            "Yifang Men",
            "Huaizhuo Liu",
            "Miaomiao Cui"
        ],
        "tldr": "The paper introduces McSc, a reinforcement learning framework with self-critic hierarchical reasoning, to improve text-to-video generation by better aligning with human preferences, particularly regarding motion dynamics and visual quality.",
        "tldr_zh": "该论文介绍了McSc，一个具有自批评分层推理的强化学习框架，旨在通过更好地与人类偏好对齐，特别是关于运动动力学和视觉质量，来改进文本到视频的生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation",
        "summary": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.",
        "url": "http://arxiv.org/abs/2511.22973v1",
        "published_date": "2025-11-28T08:25:59+00:00",
        "updated_date": "2025-11-28T08:25:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeyu Zhang",
            "Shuning Chang",
            "Yuanyu He",
            "Yizeng Han",
            "Jiasheng Tang",
            "Fan Wang",
            "Bohan Zhuang"
        ],
        "tldr": "The paper introduces BlockVid, a novel block diffusion framework for generating high-quality, coherent minute-long videos, addressing challenges in long-horizon error accumulation and the lack of suitable benchmarks. It also includes a new benchmark, LV-Bench, specifically tailored for evaluating long-range coherence.",
        "tldr_zh": "该论文提出了 BlockVid，一种新颖的块扩散框架，用于生成高质量、连贯的分钟级视频，解决了长时序误差累积和缺乏合适基准的问题。它还包括一个新的基准 LV-Bench，专门用于评估长程连贯性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Puns from Idioms: An Iterative LLM-T2IM-MLLM Framework",
        "summary": "We study idiom-based visual puns--images that align an idiom's literal and figurative meanings--and present an iterative framework that coordinates a large language model (LLM), a text-to-image model (T2IM), and a multimodal LLM (MLLM) for automatic generation and evaluation. Given an idiom, the system iteratively (i) generates detailed visual prompts, (ii) synthesizes an image, (iii) infers the idiom from the image, and (iv) refines the prompt until recognition succeeds or a step limit is reached. Using 1,000 idioms as inputs, we synthesize a corresponding dataset of visual pun images with paired prompts, enabling benchmarking of both generation and understanding. Experiments across 10 LLMs, 10 MLLMs, and one T2IM (Qwen-Image) show that MLLM choice is the primary performance driver: GPT achieves the highest accuracies, Gemini follows, and the best open-source MLLM (Gemma) is competitive with some closed models. On the LLM side, Claude attains the strongest average performance for prompt generation.",
        "url": "http://arxiv.org/abs/2511.22943v1",
        "published_date": "2025-11-28T07:30:58+00:00",
        "updated_date": "2025-11-28T07:30:58+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Kelaiti Xiao",
            "Liang Yang",
            "Dongyu Zhang",
            "Paerhati Tulajiang",
            "Hongfei Lin"
        ],
        "tldr": "The paper introduces an iterative LLM-T2IM-MLLM framework for generating idiom-based visual puns, creating a dataset for benchmarking generation and understanding of such puns, and evaluates various LLMs and MLLMs for this task.",
        "tldr_zh": "该论文介绍了一个迭代的LLM-T2IM-MLLM框架，用于生成基于成语的视觉双关语，创建了一个数据集以评估此类双关语的生成和理解，并评估了各种LLM和MLLM在这个任务上的表现。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Scalable Diffusion Transformer for Conditional 4D fMRI Synthesis",
        "summary": "Generating whole-brain 4D fMRI sequences conditioned on cognitive tasks remains challenging due to the high-dimensional, heterogeneous BOLD dynamics across subjects/acquisitions and the lack of neuroscience-grounded validation. We introduce the first diffusion transformer for voxelwise 4D fMRI conditional generation, combining 3D VQ-GAN latent compression with a CNN-Transformer backbone and strong task conditioning via AdaLN-Zero and cross-attention. On HCP task fMRI, our model reproduces task-evoked activation maps, preserves the inter-task representational structure observed in real data (RSA), achieves perfect condition specificity, and aligns ROI time-courses with canonical hemodynamic responses. Performance improves predictably with scale, reaching task-evoked map correlation of 0.83 and RSA of 0.98, consistently surpassing a U-Net baseline on all metrics. By coupling latent diffusion with a scalable backbone and strong conditioning, this work establishes a practical path to conditional 4D fMRI synthesis, paving the way for future applications such as virtual experiments, cross-site harmonization, and principled augmentation for downstream neuroimaging models.",
        "url": "http://arxiv.org/abs/2511.22870v1",
        "published_date": "2025-11-28T04:18:11+00:00",
        "updated_date": "2025-11-28T04:18:11+00:00",
        "categories": [
            "cs.CV",
            "q-bio.NC"
        ],
        "authors": [
            "Jungwoo Seo",
            "David Keetae Park",
            "Shinjae Yoo",
            "Jiook Cha"
        ],
        "tldr": "This paper introduces a scalable diffusion transformer for generating 4D fMRI sequences conditioned on cognitive tasks, demonstrating improved performance over U-Net baselines and enabling applications like virtual experiments and data augmentation.",
        "tldr_zh": "本文提出了一种可扩展的扩散Transformer，用于生成以认知任务为条件的4D fMRI序列，性能优于U-Net基线，并为虚拟实验和数据增强等应用铺平了道路。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Ar2Can: An Architect and an Artist Leveraging a Canvas for Multi-Human Generation",
        "summary": "Despite recent advances in text-to-image generation, existing models consistently fail to produce reliable multi-human scenes, often duplicating faces, merging identities, or miscounting individuals. We present Ar2Can, a novel two-stage framework that disentangles spatial planning from identity rendering for multi-human generation. The Architect module predicts structured layouts, specifying where each person should appear. The Artist module then synthesizes photorealistic images, guided by a spatially-grounded face matching reward that combines Hungarian spatial alignment with ArcFace identity similarity. This approach ensures faces are rendered at correct locations and faithfully preserve reference identities. We develop two Architect variants, seamlessly integrated with our diffusion-based Artist model and optimized via Group Relative Policy Optimization (GRPO) using compositional rewards for count accuracy, image quality, and identity matching. Evaluated on the MultiHuman-Testbench, Ar2Can achieves substantial improvements in both count accuracy and identity preservation, while maintaining high perceptual quality. Notably, our method achieves these results using primarily synthetic data, without requiring real multi-human images.",
        "url": "http://arxiv.org/abs/2511.22690v1",
        "published_date": "2025-11-27T18:45:23+00:00",
        "updated_date": "2025-11-27T18:45:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shubhankar Borse",
            "Phuc Pham",
            "Farzad Farhadzadeh",
            "Seokeon Choi",
            "Phong Ha Nguyen",
            "Anh Tuan Tran",
            "Sungrack Yun",
            "Munawar Hayat",
            "Fatih Porikli"
        ],
        "tldr": "The paper introduces Ar2Can, a two-stage framework for multi-human image generation that separates spatial planning from identity rendering, achieving improved accuracy and identity preservation using synthetic data.",
        "tldr_zh": "该论文介绍了一种名为Ar2Can的两阶段框架，用于生成多人图像，它将空间规划与身份渲染分离，使用合成数据实现了更高的准确性和身份保持。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield",
        "summary": "Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.",
        "url": "http://arxiv.org/abs/2511.22677v1",
        "published_date": "2025-11-27T18:24:28+00:00",
        "updated_date": "2025-11-27T18:24:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dongyang Liu",
            "Peng Gao",
            "David Liu",
            "Ruoyi Du",
            "Zhen Li",
            "Qilong Wu",
            "Xin Jin",
            "Sihan Cao",
            "Shifeng Zhang",
            "Hongsheng Li",
            "Steven Hoi"
        ],
        "tldr": "This paper challenges the conventional understanding of Distribution Matching Distillation (DMD) in text-to-image generation. It identifies CFG Augmentation as the primary driver of few-step distillation and proposes modifications to the distillation process for performance gains, validated by its adoption in a top-tier image generation model.",
        "tldr_zh": "本文挑战了文本到图像生成中分布匹配蒸馏（DMD）的传统理解。它认为CFG增强是少步蒸馏的主要驱动力，并提出了对蒸馏过程的修改以提高性能，并通过其在顶级图像生成模型中的采用来验证其有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
        "summary": "Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.",
        "url": "http://arxiv.org/abs/2511.23386v1",
        "published_date": "2025-11-28T17:26:34+00:00",
        "updated_date": "2025-11-28T17:26:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sinan Du",
            "Jiahao Guo",
            "Bo Li",
            "Shuhao Cui",
            "Zhengzhuo Xu",
            "Yifu Luo",
            "Yongxian Wei",
            "Kun Gai",
            "Xinggang Wang",
            "Kai Wu",
            "Chun Yuan"
        ],
        "tldr": "The paper introduces VQRAE, a Vector Quantization Representation Autoencoder, which aims to unify multimodal understanding, generation, and reconstruction into a single tokenizer, utilizing a high-dimensional semantic VQ codebook within a Vit-based architecture.",
        "tldr_zh": "本文介绍了VQRAE，一种向量量化表示自编码器，旨在将多模态理解、生成和重建统一到一个单一的tokenizer中，并在基于Vit的架构中使用高维语义VQ码本。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "db-SP: Accelerating Sparse Attention for Visual Generative Models with Dual-Balanced Sequence Parallelism",
        "summary": "Scaling Diffusion Transformer (DiT) inference via sequence parallelism is critical for reducing latency in visual generation, but is severely hampered by workload imbalance when applied to models employing block-wise sparse attention. The imbalance stems from the inherent variation in sparsity across attention heads and the irregular distribution of dense blocks within the sparse mask, when sequence parallelism is applied along the head dimension (as in Ulysses) or the block dimension (as in Ring Attention). In this paper, we formalize a sparse imbalance ratio to quantify the imbalance, and propose db-SP, a sparsity-aware sequence parallelism technique that tackles the challenge. db-SP contains a dual-level partitioning approach that achieves near-perfect workload balance at both the head and block levels with negligible overhead. Furthermore, to handle the evolving sparsity patterns across denoising steps and layers, db-SP dynamically determines the parallel degrees for the head and block dimensions at runtime. Experimental results demonstrate that db-SP delivers an end-to-end speedup of 1.25x and an attention-specific speedup of 1.40x over state-of-the-art sequence parallel methods on average. Code is available at https://github.com/thu-nics/db-SP.",
        "url": "http://arxiv.org/abs/2511.23113v1",
        "published_date": "2025-11-28T11:55:46+00:00",
        "updated_date": "2025-11-28T11:55:46+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Siqi Chen",
            "Ke Hong",
            "Tianchen Zhao",
            "Ruiqi Xie",
            "Zhenhua Zhu",
            "Xudong Zhang",
            "Yu Wang"
        ],
        "tldr": "This paper introduces db-SP, a sparsity-aware sequence parallelism technique to accelerate sparse attention in Diffusion Transformer models, achieving speedups in visual generation by addressing workload imbalance issues.",
        "tldr_zh": "本文介绍了db-SP，一种稀疏感知序列并行技术，通过解决工作负载不平衡问题来加速扩散Transformer模型中的稀疏注意力，从而在视觉生成方面实现加速。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MammoRGB: Dual-View Mammogram Synthesis Using Denoising Diffusion Probabilistic Models",
        "summary": "Purpose: This study aims to develop and evaluate a three channel denoising diffusion probabilistic model (DDPM) for synthesizing single breast dual view mammograms and to assess the impact of channel representations on image fidelity and cross view consistency. Materials and Methods: A pretrained three channel DDPM, sourced from Hugging Face, was fine tuned on a private dataset of 11020 screening mammograms to generate paired craniocaudal (CC) and mediolateral oblique (MLO) views. Three third channel encodings of the CC and MLO views were evaluated: sum, absolute difference, and zero channel. Each model produced 500 synthetic image pairs. Quantitative assessment involved breast mask segmentation using Intersection over Union (IoU) and Dice Similarity Coefficient (DSC), with distributional comparisons against 2500 real pairs using Earth Movers Distance (EMD) and Kolmogorov Smirnov (KS) tests. Qualitative evaluation included a visual Turing test by a non expert radiologist to assess cross view consistency and artifacts. Results: Synthetic mammograms showed IoU and DSC distributions comparable to real images, with EMD and KS values (0.020 and 0.077 respectively). Models using sum or absolute difference encodings outperformed others in IoU and DSC (p < 0.001), though distributions remained broadly similar. Generated CC and MLO views maintained cross view consistency, with 6 to 8 percent of synthetic images exhibiting artifacts consistent with those in the training data. Conclusion: Three channel DDPMs can generate realistic and anatomically consistent dual view mammograms with promising applications in dataset augmentation.",
        "url": "http://arxiv.org/abs/2511.22759v1",
        "published_date": "2025-11-27T21:10:36+00:00",
        "updated_date": "2025-11-27T21:10:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jorge Alberto Garza-Abdala",
            "Gerardo A. Fumagal-González",
            "Daly Avendano",
            "Servando Cardona",
            "Sadam Hussain",
            "Eduardo de Avila-Armenta",
            "Jasiel H. Toscano-Martínez",
            "Diana S. M. Rosales Gurmendi",
            "Alma A. Pedro-Pérez",
            "Jose Gerardo Tamez-Pena"
        ],
        "tldr": "This paper explores using a three-channel DDPM fine-tuned on mammograms to generate realistic dual-view (CC and MLO) mammogram images for dataset augmentation, achieving good image fidelity and cross-view consistency.",
        "tldr_zh": "本文探索使用在乳房X光片上微调的三通道DDPM来生成逼真的双视图（CC和MLO）乳房X光片图像，用于数据集增强，实现了良好的图像保真度和交叉视图一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]