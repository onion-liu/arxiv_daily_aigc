[
    {
        "title": "OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation",
        "summary": "This paper introduces OmniMotion-X, a versatile multimodal framework for\nwhole-body human motion generation, leveraging an autoregressive diffusion\ntransformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently\nsupports diverse multimodal tasks, including text-to-motion, music-to-dance,\nspeech-to-gesture, and global spatial-temporal control scenarios (e.g., motion\nprediction, in-betweening, completion, and joint/trajectory-guided synthesis),\nas well as flexible combinations of these tasks. Specifically, we propose the\nuse of reference motion as a novel conditioning signal, substantially enhancing\nthe consistency of generated content, style, and temporal dynamics crucial for\nrealistic animations. To handle multimodal conflicts, we introduce a\nprogressive weak-to-strong mixed-condition training strategy. To enable\nhigh-quality multimodal training, we construct OmniMoCap-X, the largest unified\nmultimodal motion dataset to date, integrating 28 publicly available MoCap\nsources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps.\nTo ensure detailed and consistent annotations, we render sequences into videos\nand use GPT-4o to automatically generate structured and hierarchical captions,\ncapturing both low-level actions and high-level semantics. Extensive\nexperimental evaluations confirm that OmniMotion-X significantly surpasses\nexisting methods, demonstrating state-of-the-art performance across multiple\nmultimodal tasks and enabling the interactive generation of realistic,\ncoherent, and controllable long-duration motions.",
        "url": "http://arxiv.org/abs/2510.19789v1",
        "published_date": "2025-10-22T17:25:33+00:00",
        "updated_date": "2025-10-22T17:25:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guowei Xu",
            "Yuxuan Bian",
            "Ailing Zeng",
            "Mingyi Shi",
            "Shaoli Huang",
            "Wen Li",
            "Lixin Duan",
            "Qiang Xu"
        ],
        "tldr": "OmniMotion-X introduces a versatile framework for multimodal whole-body motion generation using a diffusion transformer and a large, newly constructed dataset, achieving state-of-the-art performance across various tasks.",
        "tldr_zh": "OmniMotion-X 提出了一种通用的多模态全身运动生成框架，该框架使用扩散 Transformer 和一个新构建的大型数据集，在各种任务中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing",
        "summary": "Recent advances in multimodal models have demonstrated remarkable text-guided\nimage editing capabilities, with systems like GPT-4o and Nano-Banana setting\nnew benchmarks. However, the research community's progress remains constrained\nby the absence of large-scale, high-quality, and openly accessible datasets\nbuilt from real images. We introduce Pico-Banana-400K, a comprehensive\n400K-image dataset for instruction-based image editing. Our dataset is\nconstructed by leveraging Nano-Banana to generate diverse edit pairs from real\nphotographs in the OpenImages collection. What distinguishes Pico-Banana-400K\nfrom previous synthetic datasets is our systematic approach to quality and\ndiversity. We employ a fine-grained image editing taxonomy to ensure\ncomprehensive coverage of edit types while maintaining precise content\npreservation and instruction faithfulness through MLLM-based quality scoring\nand careful curation. Beyond single turn editing, Pico-Banana-400K enables\nresearch into complex editing scenarios. The dataset includes three specialized\nsubsets: (1) a 72K-example multi-turn collection for studying sequential\nediting, reasoning, and planning across consecutive modifications; (2) a\n56K-example preference subset for alignment research and reward model training;\nand (3) paired long-short editing instructions for developing instruction\nrewriting and summarization capabilities. By providing this large-scale,\nhigh-quality, and task-rich resource, Pico-Banana-400K establishes a robust\nfoundation for training and benchmarking the next generation of text-guided\nimage editing models.",
        "url": "http://arxiv.org/abs/2510.19808v1",
        "published_date": "2025-10-22T17:43:15+00:00",
        "updated_date": "2025-10-22T17:43:15+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Yusu Qian",
            "Eli Bocek-Rivele",
            "Liangchen Song",
            "Jialing Tong",
            "Yinfei Yang",
            "Jiasen Lu",
            "Wenze Hu",
            "Zhe Gan"
        ],
        "tldr": "The paper introduces Pico-Banana-400K, a large-scale, high-quality dataset for text-guided image editing, featuring diverse edit types and specialized subsets for multi-turn editing, preference learning, and instruction rewriting/summarization.",
        "tldr_zh": "该论文介绍了Pico-Banana-400K，这是一个大规模、高质量的文本引导图像编辑数据集，具有多种编辑类型和专门的子集，用于多轮编辑、偏好学习和指令重写/摘要。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation",
        "summary": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
        "url": "http://arxiv.org/abs/2510.19755v1",
        "published_date": "2025-10-22T16:46:05+00:00",
        "updated_date": "2025-10-22T16:46:05+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jiacheng Liu",
            "Xinyu Wang",
            "Yuqi Lin",
            "Zhikai Wang",
            "Peiru Wang",
            "Peiliang Cai",
            "Qinming Zhou",
            "Zhengan Yan",
            "Zexuan Yan",
            "Zhengyi Shi",
            "Chang Zou",
            "Yue Ma",
            "Linfeng Zhang"
        ],
        "tldr": "This survey paper reviews Diffusion Caching, a training-free and architecture-agnostic method for accelerating diffusion models by reusing computations. It analyzes the evolution of Diffusion Caching and its potential for efficient multimodal and interactive applications.",
        "tldr_zh": "该综述论文回顾了扩散缓存（Diffusion Caching），这是一种无需训练且与架构无关的方法，通过重用计算来加速扩散模型。它分析了扩散缓存的演变及其在高效多模态和交互式应用中的潜力。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "The Intricate Dance of Prompt Complexity, Quality, Diversity, and Consistency in T2I Models",
        "summary": "Text-to-image (T2I) models offer great potential for creating virtually\nlimitless synthetic data, a valuable resource compared to fixed and finite real\ndatasets. Previous works evaluate the utility of synthetic data from T2I models\non three key desiderata: quality, diversity, and consistency. While prompt\nengineering is the primary means of interacting with T2I models, the systematic\nimpact of prompt complexity on these critical utility axes remains\nunderexplored. In this paper, we first conduct synthetic experiments to\nmotivate the difficulty of generalization w.r.t. prompt complexity and explain\nthe observed difficulty with theoretical derivations. Then, we introduce a new\nevaluation framework that can compare the utility of real data and synthetic\ndata, and present a comprehensive analysis of how prompt complexity influences\nthe utility of synthetic data generated by commonly used T2I models. We conduct\nour study across diverse datasets, including CC12M, ImageNet-1k, and DCI, and\nevaluate different inference-time intervention methods. Our synthetic\nexperiments show that generalizing to more general conditions is harder than\nthe other way round, since the former needs an estimated likelihood that is not\nlearned by diffusion models. Our large-scale empirical experiments reveal that\nincreasing prompt complexity results in lower conditional diversity and prompt\nconsistency, while reducing the synthetic-to-real distribution shift, which\naligns with the synthetic experiments. Moreover, current inference-time\ninterventions can augment the diversity of the generations at the expense of\nmoving outside the support of real data. Among those interventions, prompt\nexpansion, by deliberately using a pre-trained language model as a likelihood\nestimator, consistently achieves the highest performance in both image\ndiversity and aesthetics, even higher than that of real data.",
        "url": "http://arxiv.org/abs/2510.19557v1",
        "published_date": "2025-10-22T13:13:27+00:00",
        "updated_date": "2025-10-22T13:13:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaofeng Zhang",
            "Aaron Courville",
            "Michal Drozdzal",
            "Adriana Romero-Soriano"
        ],
        "tldr": "This paper investigates the impact of prompt complexity on the quality, diversity, and consistency of synthetic data generated by text-to-image models and introduces an evaluation framework comparing real and synthetic data utility.",
        "tldr_zh": "本文研究了提示词复杂度对文本到图像模型生成的合成数据质量、多样性和一致性的影响，并提出了一个评估框架，用于比较真实数据和合成数据的效用。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Reinforcement and Imitation Learning for Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) have achieved remarkable progress, yet their\nlarge scale often renders them impractical for resource-constrained\nenvironments. This paper introduces Unified Reinforcement and Imitation\nLearning (RIL), a novel and efficient training algorithm designed to create\npowerful, lightweight VLMs. RIL distinctively combines the strengths of\nreinforcement learning with adversarial imitation learning. This enables\nsmaller student VLMs not only to mimic the sophisticated text generation of\nlarge teacher models but also to systematically improve their generative\ncapabilities through reinforcement signals. Key to our imitation framework is\nan LLM-based discriminator that adeptly distinguishes between student and\nteacher outputs, complemented by guidance from multiple large teacher VLMs to\nensure diverse learning. This unified learning strategy, leveraging both\nreinforcement and imitation, empowers student models to achieve significant\nperformance gains, making them competitive with leading closed-source VLMs.\nExtensive experiments on diverse vision-language benchmarks demonstrate that\nRIL significantly narrows the performance gap with state-of-the-art open- and\nclosed-source VLMs and, in several instances, surpasses them.",
        "url": "http://arxiv.org/abs/2510.19307v1",
        "published_date": "2025-10-22T07:12:14+00:00",
        "updated_date": "2025-10-22T07:12:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Byung-Kwan Lee",
            "Ryo Hachiuma",
            "Yong Man Ro",
            "Yu-Chiang Frank Wang",
            "Yueh-Hua Wu"
        ],
        "tldr": "This paper introduces a Unified Reinforcement and Imitation Learning (RIL) method to train smaller, more efficient Vision-Language Models (VLMs) that can compete with large closed-source models by combining reinforcement and imitation learning strategies.",
        "tldr_zh": "本文介绍了一种统一强化和模仿学习（RIL）方法，用于训练更小、更高效的视觉-语言模型（VLMs），通过结合强化学习和模仿学习策略，使其能够与大型闭源模型竞争。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models",
        "summary": "Text-to-video diffusion models have enabled high-quality video synthesis, yet\noften fail to generate temporally coherent and physically plausible motion. A\nkey reason is the models' insufficient understanding of complex motions that\nnatural videos often entail. Recent works tackle this problem by aligning\ndiffusion model features with those from pretrained video encoders. However,\nthese encoders mix video appearance and dynamics into entangled features,\nlimiting the benefit of such alignment. In this paper, we propose a\nmotion-centric alignment framework that learns a disentangled motion subspace\nfrom a pretrained video encoder. This subspace is optimized to predict\nground-truth optical flow, ensuring it captures true motion dynamics. We then\nalign the latent features of a text-to-video diffusion model to this new\nsubspace, enabling the generative model to internalize motion knowledge and\ngenerate more plausible videos. Our method improves the physical commonsense in\na state-of-the-art video diffusion model, while preserving adherence to textual\nprompts, as evidenced by empirical evaluations on VideoPhy, VideoPhy2, VBench,\nand VBench-2.0, along with a user study.",
        "url": "http://arxiv.org/abs/2510.19022v1",
        "published_date": "2025-10-21T19:05:23+00:00",
        "updated_date": "2025-10-21T19:05:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aritra Bhowmik",
            "Denis Korzhenkov",
            "Cees G. M. Snoek",
            "Amirhossein Habibian",
            "Mohsen Ghafoorian"
        ],
        "tldr": "The paper introduces MoAlign, a motion-centric alignment framework that learns a disentangled motion subspace to improve temporal coherence and physical plausibility in text-to-video diffusion models by aligning latent features with this subspace.",
        "tldr_zh": "该论文介绍了MoAlign，一种以运动为中心的对齐框架，该框架学习一种解耦的运动子空间，通过将潜在特征与该子空间对齐，来提高文本到视频扩散模型的时间连贯性和物理合理性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Advances in 4D Representation: Geometry, Motion, and Interaction",
        "summary": "We present a survey on 4D generation and reconstruction, a fast-evolving\nsubfield of computer graphics whose developments have been propelled by recent\nadvances in neural fields, geometric and motion deep learning, as well 3D\ngenerative artificial intelligence (GenAI). While our survey is not the first\nof its kind, we build our coverage of the domain from a unique and distinctive\nperspective of 4D representations\\/}, to model 3D geometry evolving over time\nwhile exhibiting motion and interaction. Specifically, instead of offering an\nexhaustive enumeration of many works, we take a more selective approach by\nfocusing on representative works to highlight both the desirable properties and\nensuing challenges of each representation under different computation,\napplication, and data scenarios. The main take-away message we aim to convey to\nthe readers is on how to select and then customize the appropriate 4D\nrepresentations for their tasks. Organizationally, we separate the 4D\nrepresentations based on three key pillars: geometry, motion, and interaction.\nOur discourse will not only encompass the most popular representations of\ntoday, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS),\nbut also bring attention to relatively under-explored representations in the 4D\ncontext, such as structured models and long-range motions. Throughout our\nsurvey, we will reprise the role of large language models (LLMs) and video\nfoundational models (VFMs) in a variety of 4D applications, while steering our\ndiscussion towards their current limitations and how they can be addressed. We\nalso provide a dedicated coverage on what 4D datasets are currently available,\nas well as what is lacking, in driving the subfield forward. Project\npage:https://mingrui-zhao.github.io/4DRep-GMI/",
        "url": "http://arxiv.org/abs/2510.19255v1",
        "published_date": "2025-10-22T05:22:20+00:00",
        "updated_date": "2025-10-22T05:22:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingrui Zhao",
            "Sauradip Nag",
            "Kai Wang",
            "Aditya Vora",
            "Guangda Ji",
            "Peter Chun",
            "Ali Mahdavi-Amiri",
            "Hao Zhang"
        ],
        "tldr": "This survey paper explores 4D generation and reconstruction techniques, focusing on geometry, motion, and interaction, and discusses the role of LLMs and VFMs while highlighting current limitations and available datasets.",
        "tldr_zh": "这篇综述论文探讨了4D生成和重建技术，重点关注几何、运动和交互，并讨论了LLM和VFM的作用，同时强调了当前的局限性和可用的数据集。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]