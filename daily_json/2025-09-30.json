[
    {
        "title": "OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing",
        "summary": "The performance of unified multimodal models for image generation and editing\nis fundamentally constrained by the quality and comprehensiveness of their\ntraining data. While existing datasets have covered basic tasks like style\ntransfer and simple object manipulation, they often lack the systematic\nstructure and challenging scenarios required for real-world applications. To\naddress this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset\nconstructed using a novel methodology that combines hierarchical task taxonomy\nwith automated data generation. Our taxonomy not only includes fundamental\ncapabilities such as text rendering and style control but also introduces\nhighly practical yet challenging categories like scientific imagery for\nchemistry illustrations and complex instruction editing requiring simultaneous\nexecution of multiple operations. Through an automated pipeline leveraging\nstructured resource pools and GPT-4o, we generate 80k high-quality\ninstruction-image pairs with controlled diversity, covering 11 major domains\nand 51 subtasks. Extensive experiments show that fine-tuning leading models on\nour dataset achieves significant performance gains across multiple benchmarks,\nwith improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench)\nand 13% on generation tasks (Harmon on GenEval). Our work demonstrates that\nsystematic data construction is key to advancing multimodal AI capabilities.",
        "url": "http://arxiv.org/abs/2509.24900v1",
        "published_date": "2025-09-29T15:11:09+00:00",
        "updated_date": "2025-09-29T15:11:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhihong Chen",
            "Xuehai Bai",
            "Yang Shi",
            "Chaoyou Fu",
            "Huanyu Zhang",
            "Haotian Wang",
            "Xiaoyan Sun",
            "Zhang Zhang",
            "Liang Wang",
            "Yuanxing Zhang",
            "Pengfei Wan",
            "Yi-Fan Zhang"
        ],
        "tldr": "The paper introduces OpenGPT-4o-Image, a large-scale dataset for training multimodal models in image generation and editing, constructed using a novel hierarchical taxonomy and automated data generation pipeline, demonstrating significant performance gains when fine-tuning leading models.",
        "tldr_zh": "该论文介绍了OpenGPT-4o-Image，一个用于训练多模态图像生成和编辑模型的大规模数据集。该数据集通过一种新的层级分类法和自动数据生成流程构建，实验表明在微调领先模型时能显著提高性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer",
        "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
        "url": "http://arxiv.org/abs/2509.24695v1",
        "published_date": "2025-09-29T12:28:09+00:00",
        "updated_date": "2025-09-29T12:28:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Junsong Chen",
            "Yuyang Zhao",
            "Jincheng Yu",
            "Ruihang Chu",
            "Junyu Chen",
            "Shuai Yang",
            "Xianbang Wang",
            "Yicheng Pan",
            "Daquan Zhou",
            "Huan Ling",
            "Haozhe Liu",
            "Hongwei Yi",
            "Hao Zhang",
            "Muyang Li",
            "Yukang Chen",
            "Han Cai",
            "Sanja Fidler",
            "Ping Luo",
            "Song Han",
            "Enze Xie"
        ],
        "tldr": "SANA-Video is a small diffusion model that efficiently generates high-resolution, minute-long videos with strong text-video alignment, leveraging linear attention and a constant-memory KV cache, and demonstrating significant speedups and reduced training costs compared to existing models.",
        "tldr_zh": "SANA-Video是一个小型扩散模型，它利用线性注意力和恒定内存KV缓存，高效生成具有强文本-视频对齐的高分辨率、长达一分钟的视频。该模型与现有模型相比，显著提高了速度并降低了训练成本。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Learning Object-Centric Representations Based on Slots in Real World Scenarios",
        "summary": "A central goal in AI is to represent scenes as compositions of discrete\nobjects, enabling fine-grained, controllable image and video generation. Yet\nleading diffusion models treat images holistically and rely on text\nconditioning, creating a mismatch for object-level editing. This thesis\nintroduces a framework that adapts powerful pretrained diffusion models for\nobject-centric synthesis while retaining their generative capacity.\n  We identify a core challenge: balancing global scene coherence with\ndisentangled object control. Our method integrates lightweight, slot-based\nconditioning into pretrained models, preserving their visual priors while\nproviding object-specific manipulation. For images, SlotAdapt augments\ndiffusion models with a register token for background/style and\nslot-conditioned modules for objects, reducing text-conditioning bias and\nachieving state-of-the-art results in object discovery, segmentation,\ncompositional editing, and controllable image generation.\n  We further extend the framework to video. Using Invariant Slot Attention\n(ISA) to separate object identity from pose and a Transformer-based temporal\naggregator, our approach maintains consistent object representations and\ndynamics across frames. This yields new benchmarks in unsupervised video object\nsegmentation and reconstruction, and supports advanced editing tasks such as\nobject removal, replacement, and insertion without explicit supervision.\n  Overall, this work establishes a general and scalable approach to\nobject-centric generative modeling for images and videos. By bridging human\nobject-based perception and machine learning, it expands the design space for\ninteractive, structured, and user-driven generative tools in creative,\nscientific, and practical domains.",
        "url": "http://arxiv.org/abs/2509.24652v1",
        "published_date": "2025-09-29T12:01:49+00:00",
        "updated_date": "2025-09-29T12:01:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Adil Kaan Akan"
        ],
        "tldr": "This paper presents a framework for object-centric image and video generation by integrating slot-based conditioning into pretrained diffusion models, achieving state-of-the-art results in object discovery, segmentation, and editing tasks.",
        "tldr_zh": "本文提出了一种基于预训练扩散模型的以对象为中心的图像和视频生成框架，通过将基于槽的条件控制集成到模型中，在对象发现、分割和编辑任务中取得了最先进的结果。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models",
        "summary": "Flow map models such as Consistency Models (CM) and Mean Flow (MF) enable\nfew-step generation by learning the long jump of the ODE solution of diffusion\nmodels, yet training remains unstable, sensitive to hyperparameters, and\ncostly. Initializing from a pre-trained diffusion model helps, but still\nrequires converting infinitesimal steps into a long-jump map, leaving\ninstability unresolved. We introduce mid-training, the first concept and\npractical method that inserts a lightweight intermediate stage between the\n(diffusion) pre-training and the final flow map training (i.e., post-training)\nfor vision generation. Concretely, Consistency Mid-Training (CMT) is a compact\nand principled stage that trains a model to map points along a solver\ntrajectory from a pre-trained model, starting from a prior sample, directly to\nthe solver-generated clean sample. It yields a trajectory-consistent and stable\ninitialization. This initializer outperforms random and diffusion-based\nbaselines and enables fast, robust convergence without heuristics. Initializing\npost-training with CMT weights further simplifies flow map learning.\nEmpirically, CMT achieves state of the art two step FIDs: 1.97 on CIFAR-10,\n1.32 on ImageNet 64x64, and 1.84 on ImageNet 512x512, while using up to 98%\nless training data and GPU time, compared to CMs. On ImageNet 256x256, CMT\nreaches 1-step FID 3.34 while cutting total training time by about 50% compared\nto MF from scratch (FID 3.43). This establishes CMT as a principled, efficient,\nand general framework for training flow map models.",
        "url": "http://arxiv.org/abs/2509.24526v1",
        "published_date": "2025-09-29T09:42:08+00:00",
        "updated_date": "2025-09-29T09:42:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Zheyuan Hu",
            "Chieh-Hsin Lai",
            "Yuki Mitsufuji",
            "Stefano Ermon"
        ],
        "tldr": "This paper introduces Consistency Mid-Training (CMT), a novel intermediate training stage for flow map models used in image generation. It significantly improves training efficiency and stability, achieving state-of-the-art FID scores with reduced training costs.",
        "tldr_zh": "该论文介绍了一致性中期训练 (CMT)，这是一种用于图像生成中流映射模型的新型中间训练阶段。它显著提高了训练效率和稳定性，并以更低的训练成本实现了最先进的 FID 分数。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark",
        "summary": "Generative diffusion models are developing rapidly and attracting increasing\nattention due to their wide range of applications. Image-to-Video (I2V)\ngeneration has become a major focus in the field of video synthesis. However,\nexisting evaluation benchmarks primarily focus on aspects such as video quality\nand temporal consistency, while largely overlooking the model's ability to\nunderstand the semantics of specific subjects in the input image or to ensure\nthat the generated video aligns with physical laws and human commonsense. To\naddress this gap, we propose UI2V-Bench, a novel benchmark for evaluating I2V\nmodels with a focus on semantic understanding and reasoning. It introduces four\nprimary evaluation dimensions: spatial understanding, attribute binding,\ncategory understanding, and reasoning. To assess these dimensions, we design\ntwo evaluation methods based on Multimodal Large Language Models (MLLMs): an\ninstance-level pipeline for fine-grained semantic understanding, and a\nfeedback-based reasoning pipeline that enables step-by-step causal assessment\nfor more accurate evaluation. UI2V-Bench includes approximately 500 carefully\nconstructed text-image pairs and evaluates a range of both open source and\nclosed-source I2V models across all defined dimensions. We further incorporate\nhuman evaluations, which show strong alignment with the proposed MLLM-based\nmetrics. Overall, UI2V-Bench fills a critical gap in I2V evaluation by\nemphasizing semantic comprehension and reasoning ability, offering a robust\nframework and dataset to support future research and model development in the\nfield.",
        "url": "http://arxiv.org/abs/2509.24427v1",
        "published_date": "2025-09-29T08:14:26+00:00",
        "updated_date": "2025-09-29T08:14:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ailing Zhang",
            "Lina Lei",
            "Dehong Kong",
            "Zhixin Wang",
            "Jiaqi Xu",
            "Fenglong Song",
            "Chun-Le Guo",
            "Chang Liu",
            "Fan Li",
            "Jie Chen"
        ],
        "tldr": "The paper introduces UI2V-Bench, a new benchmark for Image-to-Video (I2V) generation models that focuses on evaluating semantic understanding and reasoning, addressing a gap in existing benchmarks.",
        "tldr_zh": "该论文介绍了UI2V-Bench，一个新的图像到视频（I2V）生成模型的基准，重点评估语义理解和推理能力，解决了现有基准测试中的一个空白。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "NeRV-Diffusion: Diffuse Implicit Neural Representations for Video Synthesis",
        "summary": "We present NeRV-Diffusion, an implicit latent video diffusion model that\nsynthesizes videos via generating neural network weights. The generated weights\ncan be rearranged as the parameters of a convolutional neural network, which\nforms an implicit neural representation (INR), and decodes into videos with\nframe indices as the input. Our framework consists of two stages: 1) A\nhypernetworkbased tokenizer that encodes raw videos from pixel space to neural\nparameter space, where the bottleneck latent serves as INR weights to decode.\n2) An implicit diffusion transformer that denoises on the latent INR weights.\nIn contrast to traditional video tokenizers that encode videos into frame-wise\nfeature maps, NeRV-Diffusion compresses and generates a video holistically as a\nunified neural network. This enables efficient and high-quality video synthesis\nvia obviating temporal cross-frame attentions in the denoiser and decoding\nvideo latent with dedicated decoders. To achieve Gaussian-distributed INR\nweights with high expressiveness, we reuse the bottleneck latent across all\nNeRV layers, as well as reform its weight assignment, upsampling connection and\ninput coordinates. We also introduce SNR-adaptive loss weighting and scheduled\nsampling for effective training of the implicit diffusion model. NeRV-Diffusion\nreaches superior video generation quality over previous INR-based models and\ncomparable performance to most recent state-of-the-art non-implicit models on\nreal-world video benchmarks including UCF-101 and Kinetics-600. It also brings\na smooth INR weight space that facilitates seamless interpolations between\nframes or videos.",
        "url": "http://arxiv.org/abs/2509.24353v1",
        "published_date": "2025-09-29T06:53:08+00:00",
        "updated_date": "2025-09-29T06:53:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yixuan Ren",
            "Hanyu Wang",
            "Hao Chen",
            "Bo He",
            "Abhinav Shrivastava"
        ],
        "tldr": "NeRV-Diffusion is a video synthesis method that uses a diffusion model operating on implicit neural representation (INR) weights, achieving high-quality video generation with efficient encoding and decoding.",
        "tldr_zh": "NeRV-Diffusion 是一种视频合成方法，它使用在隐式神经表示（INR）权重上运行的扩散模型，通过高效的编码和解码实现高质量的视频生成。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 9
    },
    {
        "title": "Hyperspherical Latents Improve Continuous-Token Autoregressive Generation",
        "summary": "Autoregressive (AR) models are promising for image generation, yet\ncontinuous-token AR variants often trail latent diffusion and masked-generation\nmodels. The core issue is heterogeneous variance in VAE latents, which is\namplified during AR decoding, especially under classifier-free guidance (CFG),\nand can cause variance collapse. We propose SphereAR to address this issue. Its\ncore design is to constrain all AR inputs and outputs -- including after CFG --\nto lie on a fixed-radius hypersphere (constant $\\ell_2$ norm), leveraging\nhyperspherical VAEs. Our theoretical analysis shows that hyperspherical\nconstraint removes the scale component (the primary cause of variance\ncollapse), thereby stabilizing AR decoding. Empirically, on ImageNet\ngeneration, SphereAR-H (943M) sets a new state of the art for AR models,\nachieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54\nand SphereAR-B (208M) reaches 1.92, matching or surpassing much larger\nbaselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge,\nthis is the first time a pure next-token AR image generator with raster order\nsurpasses diffusion and masked-generation models at comparable parameter\nscales.",
        "url": "http://arxiv.org/abs/2509.24335v1",
        "published_date": "2025-09-29T06:34:24+00:00",
        "updated_date": "2025-09-29T06:34:24+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Guolin Ke",
            "Hui Xue"
        ],
        "tldr": "The paper proposes SphereAR, a continuous-token autoregressive model for image generation that utilizes hyperspherical VAEs to address variance collapse, achieving state-of-the-art FID scores compared to other AR, diffusion, and masked-generation models.",
        "tldr_zh": "该论文提出了SphereAR，一种用于图像生成的连续token自回归模型，它利用超球面VAE来解决方差崩溃问题，与其他AR、扩散和掩码生成模型相比，实现了最先进的FID分数。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "UniVid: The Open-Source Unified Video Model",
        "summary": "Unified video modeling that combines generation and understanding\ncapabilities is increasingly important but faces two key challenges:\nmaintaining semantic faithfulness during flow-based generation due to\ntext-visual token imbalance and the limitations of uniform cross-modal\nattention across the flow trajectory, and efficiently extending image-centric\nMLLMs to video without costly retraining. We present UniVid, a unified\narchitecture that couples an MLLM with a diffusion decoder through a\nlightweight adapter, enabling both video understanding and generation. We\nintroduce Temperature Modality Alignment to improve prompt adherence and\nPyramid Reflection for efficient temporal reasoning via dynamic keyframe\nselection. Extensive experiments on standard benchmarks demonstrate\nstate-of-the-art performance, achieving a 2.2% improvement on VBench-Long total\nscore compared to EasyAnimateV5.1, and 1.0% and 3.3% accuracy gains on MSVD-QA\nand ActivityNet-QA, respectively, compared with the best prior 7B baselines.",
        "url": "http://arxiv.org/abs/2509.24200v1",
        "published_date": "2025-09-29T02:31:36+00:00",
        "updated_date": "2025-09-29T02:31:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiabin Luo",
            "Junhui Lin",
            "Zeyu Zhang",
            "Biao Wu",
            "Meng Fang",
            "Ling Chen",
            "Hao Tang"
        ],
        "tldr": "UniVid is a unified video model combining generation and understanding via MLLM and diffusion, improving performance on video benchmarks with techniques like Temperature Modality Alignment and Pyramid Reflection.",
        "tldr_zh": "UniVid是一个统一的视频模型，通过MLLM和扩散结合了生成和理解能力，并通过温度模态对齐和金字塔反射等技术提高了视频基准测试的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Score Distillation of Flow Matching Models",
        "summary": "Diffusion models achieve high-quality image generation but are limited by\nslow iterative sampling. Distillation methods alleviate this by enabling one-\nor few-step generation. Flow matching, originally introduced as a distinct\nframework, has since been shown to be theoretically equivalent to diffusion\nunder Gaussian assumptions, raising the question of whether distillation\ntechniques such as score distillation transfer directly. We provide a simple\nderivation -- based on Bayes' rule and conditional expectations -- that unifies\nGaussian diffusion and flow matching without relying on ODE/SDE formulations.\nBuilding on this view, we extend Score identity Distillation (SiD) to\npretrained text-to-image flow-matching models, including SANA, SD3-Medium,\nSD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show\nthat, with only modest flow-matching- and DiT-specific adjustments, SiD works\nout of the box across these models, in both data-free and data-aided settings,\nwithout requiring teacher finetuning or architectural changes. This provides\nthe first systematic evidence that score distillation applies broadly to\ntext-to-image flow matching models, resolving prior concerns about stability\nand soundness and unifying acceleration techniques across diffusion- and\nflow-based generators. We will make the PyTorch implementation publicly\navailable.",
        "url": "http://arxiv.org/abs/2509.25127v1",
        "published_date": "2025-09-29T17:45:48+00:00",
        "updated_date": "2025-09-29T17:45:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Mingyuan Zhou",
            "Yi Gu",
            "Huangjie Zheng",
            "Liangchen Song",
            "Guande He",
            "Yizhe Zhang",
            "Wenze Hu",
            "Yinfei Yang"
        ],
        "tldr": "This paper extends score distillation techniques, specifically Score identity Distillation (SiD), to various pre-trained text-to-image flow-matching models, demonstrating its broad applicability without significant model-specific adjustments. This unifies acceleration techniques across diffusion- and flow-based generative models.",
        "tldr_zh": "本文将分数蒸馏技术，特别是分数身份蒸馏（SiD），扩展到各种预训练的文本到图像的流动匹配模型，证明了它广泛的适用性，无需进行重大的模型特定调整。这统一了基于扩散和基于流的生成模型的加速技术。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "STAGE: Stable and Generalizable GRPO for Autoregressive Image Generation",
        "summary": "Reinforcement learning has recently been explored to improve text-to-image\ngeneration, yet applying existing GRPO algorithms to autoregressive (AR) image\nmodels remains challenging. The instability of the training process easily\ndisrupts the pretrained model capability during long runs, resulting in\nmarginal gains, degraded image quality, and poor generalization. In this work,\nwe revisit GRPO for AR image generation and identify two key issues:\ncontradictory gradients from unnecessary tokens and unstable policy entropy\ndynamics. To address these, we introduce STAGE, a stable and generalizable\nframework that leverages two targeted solutions: 1) Advantage/KL reweighting.\nSimilarity-aware reweighting to alleviate conflicting updates; and 2) Entropy\nreward. An entropy-based reward corresponding to reference model to stabilize\nlearning. With the help of alleviating conflicts between tokens and an entropy\nreward for stabilizing training, we reduce disruption of the pretrained\ndistribution and mitigate reward hacking, which in turn improves generalization\nand transfer better to other benchmarks. Experiments across multiple benchmarks\nshow that STAGE consistently improves visual quality, stability, and cross-task\ngeneralization compared to baseline GRPO.",
        "url": "http://arxiv.org/abs/2509.25027v1",
        "published_date": "2025-09-29T16:50:21+00:00",
        "updated_date": "2025-09-29T16:50:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoxiao Ma",
            "Haibo Qiu",
            "Guohui Zhang",
            "Zhixiong Zeng",
            "Siqi Yang",
            "Lin Ma",
            "Feng Zhao"
        ],
        "tldr": "The paper introduces STAGE, a stable and generalizable GRPO framework for autoregressive image generation that addresses contradictory gradients and unstable policy entropy dynamics using advantage/KL reweighting and an entropy reward.",
        "tldr_zh": "本文介绍了一种用于自回归图像生成的稳定且可泛化的GRPO框架STAGE，该框架通过优势/KL重加权和熵奖励来解决矛盾梯度和不稳定的策略熵动态。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion",
        "summary": "Generating a complete and explorable 360-degree visual world enables a wide\nrange of downstream applications. While prior works have advanced the field,\nthey remain constrained by either narrow field-of-view limitations, which\nhinder the synthesis of continuous and holistic scenes, or insufficient camera\ncontrollability that restricts free exploration by users or autonomous agents.\nTo address this, we propose PanoWorld-X, a novel framework for high-fidelity\nand controllable panoramic video generation with diverse camera trajectories.\nSpecifically, we first construct a large-scale dataset of panoramic\nvideo-exploration route pairs by simulating camera trajectories in virtual 3D\nenvironments via Unreal Engine. As the spherical geometry of panoramic data\nmisaligns with the inductive priors from conventional video diffusion, we then\nintroduce a Sphere-Aware Diffusion Transformer architecture that reprojects\nequirectangular features onto the spherical surface to model geometric\nadjacency in latent space, significantly enhancing visual fidelity and\nspatiotemporal continuity. Extensive experiments demonstrate that our\nPanoWorld-X achieves superior performance in various aspects, including motion\nrange, control precision, and visual quality, underscoring its potential for\nreal-world applications.",
        "url": "http://arxiv.org/abs/2509.24997v1",
        "published_date": "2025-09-29T16:22:00+00:00",
        "updated_date": "2025-09-29T16:22:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuyang Yin",
            "HaoXiang Guo",
            "Fangfu Liu",
            "Mengyu Wang",
            "Hanwen Liang",
            "Eric Li",
            "Yikai Wang",
            "Xiaojie Jin",
            "Yao Zhao",
            "Yunchao Wei"
        ],
        "tldr": "PanoWorld-X introduces a sphere-aware diffusion transformer for generating high-fidelity, controllable panoramic videos with diverse camera trajectories, addressing limitations in existing panoramic video generation methods.",
        "tldr_zh": "PanoWorld-X 提出了一种球体感知扩散变换器，用于生成具有多样相机轨迹的高保真、可控全景视频，解决了现有全景视频生成方法的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel",
        "summary": "RGBA video generation, which includes an alpha channel to represent\ntransparency, is gaining increasing attention across a wide range of\napplications. However, existing methods often neglect visual quality, limiting\ntheir practical usability. In this paper, we propose \\textit{Wan-Alpha}, a new\nframework that generates transparent videos by learning both RGB and alpha\nchannels jointly. We design an effective variational autoencoder (VAE) that\nencodes the alpha channel into the RGB latent space. Then, to support the\ntraining of our diffusion transformer, we construct a high-quality and diverse\nRGBA video dataset. Compared with state-of-the-art methods, our model\ndemonstrates superior performance in visual quality, motion realism, and\ntransparency rendering. Notably, our model can generate a wide variety of\nsemi-transparent objects, glowing effects, and fine-grained details such as\nhair strands. The released model is available on our website:\n\\href{https://donghaotian123.github.io/Wan-Alpha/}{https://donghaotian123.github.io/Wan-Alpha/}.",
        "url": "http://arxiv.org/abs/2509.24979v1",
        "published_date": "2025-09-29T16:08:21+00:00",
        "updated_date": "2025-09-29T16:08:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haotian Dong",
            "Wenjing Wang",
            "Chen Li",
            "Di Lin"
        ],
        "tldr": "The paper introduces Wan-Alpha, a new text-to-RGBA video generation framework with a focus on high visual quality and realistic transparency rendering, achieved through a VAE-enhanced diffusion transformer trained on a newly constructed high-quality dataset.",
        "tldr_zh": "该论文介绍了Wan-Alpha，一个新的文本到RGBA视频生成框架，专注于高质量的视觉效果和逼真的透明度渲染，通过VAE增强的扩散transformer，并使用新构建的高质量数据集进行训练。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scalable GANs with Transformers",
        "summary": "Scalability has driven recent advances in generative modeling, yet its\nprinciples remain underexplored for adversarial learning. We investigate the\nscalability of Generative Adversarial Networks (GANs) through two design\nchoices that have proven to be effective in other types of generative models:\ntraining in a compact Variational Autoencoder latent space and adopting purely\ntransformer-based generators and discriminators. Training in latent space\nenables efficient computation while preserving perceptual fidelity, and this\nefficiency pairs naturally with plain transformers, whose performance scales\nwith computational budget. Building on these choices, we analyze failure modes\nthat emerge when naively scaling GANs. Specifically, we find issues as\nunderutilization of early layers in the generator and optimization instability\nas the network scales. Accordingly, we provide simple and scale-friendly\nsolutions as lightweight intermediate supervision and width-aware learning-rate\nadjustment. Our experiments show that GAT, a purely transformer-based and\nlatent-space GANs, can be easily trained reliably across a wide range of\ncapacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art\nsingle-step, class-conditional generation performance (FID of 2.96) on\nImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.",
        "url": "http://arxiv.org/abs/2509.24935v1",
        "published_date": "2025-09-29T15:36:15+00:00",
        "updated_date": "2025-09-29T15:36:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Sangeek Hyun",
            "MinKyu Lee",
            "Jae-Pil Heo"
        ],
        "tldr": "This paper introduces a scalable GAN architecture (GAT) using transformers in a VAE latent space, addressing scaling issues and achieving state-of-the-art results on ImageNet-256 with improved training efficiency.",
        "tldr_zh": "本文介绍了一种可扩展的GAN架构（GAT），它在VAE潜在空间中使用Transformer，解决了缩放问题，并在ImageNet-256上实现了最先进的结果，并提高了训练效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer",
        "summary": "Transformer-based video diffusion models (VDMs) deliver state-of-the-art\nvideo generation quality but are constrained by the quadratic cost of\nself-attention, making long sequences and high resolutions computationally\nexpensive. While linear attention offers sub-quadratic complexity, prior\nattempts fail to match the expressiveness of softmax attention without costly\nretraining. We introduce \\textit{Attention Surgery}, an efficient framework for\n\\textit{linearizing} or \\textit{hybridizing} attention in pretrained VDMs\nwithout training from scratch. Inspired by recent advances in language models,\nour method combines a novel hybrid attention mechanism-mixing softmax and\nlinear tokens-with a lightweight distillation and fine-tuning pipeline\nrequiring only a few GPU-days. Additionally, we incorporate a cost-aware\nblock-rate strategy to balance expressiveness and efficiency across layers.\nApplied to Wan2.1 1.3B, a state-of-the-art DiT-based VDM, Attention Surgery\nachieves the first competitive sub-quadratic attention video diffusion models,\nreducing attention cost by up to 40\\% in terms of FLOPs, while maintaining\ngeneration quality as measured on the standard VBench and VBench-2.0\nbenchmarks.",
        "url": "http://arxiv.org/abs/2509.24899v1",
        "published_date": "2025-09-29T15:09:51+00:00",
        "updated_date": "2025-09-29T15:09:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohsen Ghafoorian",
            "Denis Korzhenkov",
            "Amirhossein Habibian"
        ],
        "tldr": "The paper introduces \"Attention Surgery,\" a method to efficiently linearize attention mechanisms in video diffusion models without significant retraining, achieving a 40% reduction in FLOPs while maintaining generation quality.",
        "tldr_zh": "本文介绍了一种名为“注意力手术”的方法，可以有效地线性化视频扩散模型中的注意力机制，而无需大量的重新训练，实现了40%的FLOPs减少，同时保持了生成质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Environment-Aware Satellite Image Generation with Diffusion Models",
        "summary": "Diffusion-based foundation models have recently garnered much attention in\nthe field of generative modeling due to their ability to generate images of\nhigh quality and fidelity. Although not straightforward, their recent\napplication to the field of remote sensing signaled the first successful trials\ntowards harnessing the large volume of publicly available datasets containing\nmultimodal information. Despite their success, existing methods face\nconsiderable limitations: they rely on limited environmental context, struggle\nwith missing or corrupted data, and often fail to reliably reflect user\nintentions in generated outputs. In this work, we propose a novel diffusion\nmodel conditioned on environmental context, that is able to generate satellite\nimages by conditioning from any combination of three different control signals:\na) text, b) metadata, and c) visual data. In contrast to previous works, the\nproposed method is i) to our knowledge, the first of its kind to condition\nsatellite image generation on dynamic environmental conditions as part of its\ncontrol signals, and ii) incorporating a metadata fusion strategy that models\nattribute embedding interactions to account for partially corrupt and/or\nmissing observations. Our method outperforms previous methods both\nqualitatively (robustness to missing metadata, higher responsiveness to control\ninputs) and quantitatively (higher fidelity, accuracy, and quality of\ngenerations measured using 6 different metrics) in the trials of single-image\nand temporal generation. The reported results support our hypothesis that\nconditioning on environmental context can improve the performance of foundation\nmodels for satellite imagery, and render our model a promising candidate for\nusage in downstream tasks. The collected 3-modal dataset is to our knowledge,\nthe first publicly-available dataset to combine data from these three different\nmediums.",
        "url": "http://arxiv.org/abs/2509.24875v1",
        "published_date": "2025-09-29T14:54:53+00:00",
        "updated_date": "2025-09-29T14:54:53+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nikos Kostagiolas",
            "Pantelis Georgiades",
            "Yannis Panagakis",
            "Mihalis A. Nicolaou"
        ],
        "tldr": "This paper introduces a novel diffusion model for satellite image generation conditioned on environmental context using text, metadata, and visual data, outperforming existing methods in robustness and generation quality.",
        "tldr_zh": "本文介绍了一种新的扩散模型，用于生成以环境上下文为条件的卫星图像，利用文本、元数据和视觉数据，在鲁棒性和生成质量方面优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation",
        "summary": "We present Causal-Adapter, a modular framework that adapts frozen\ntext-to-image diffusion backbones for counterfactual image generation. Our\nmethod enables causal interventions on target attributes, consistently\npropagating their effects to causal dependents without altering the core\nidentity of the image. In contrast to prior approaches that rely on prompt\nengineering without explicit causal structure, Causal-Adapter leverages\nstructural causal modeling augmented with two attribute regularization\nstrategies: prompt-aligned injection, which aligns causal attributes with\ntextual embeddings for precise semantic control, and a conditioned token\ncontrastive loss to disentangle attribute factors and reduce spurious\ncorrelations. Causal-Adapter achieves state-of-the-art performance on both\nsynthetic and real-world datasets, with up to 91\\% MAE reduction on Pendulum\nfor accurate attribute control and 87\\% FID reduction on ADNI for high-fidelity\nMRI image generation. These results show that our approach enables robust,\ngeneralizable counterfactual editing with faithful attribute modification and\nstrong identity preservation.",
        "url": "http://arxiv.org/abs/2509.24798v1",
        "published_date": "2025-09-29T13:49:28+00:00",
        "updated_date": "2025-09-29T13:49:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lei Tong",
            "Zhihua Liu",
            "Chaochao Lu",
            "Dino Oglic",
            "Tom Diethe",
            "Philip Teare",
            "Sotirios A. Tsaftaris",
            "Chen Jin"
        ],
        "tldr": "The paper introduces Causal-Adapter, a novel framework for counterfactual image generation using text-to-image diffusion models, achieving state-of-the-art performance by leveraging structural causal modeling and attribute regularization strategies.",
        "tldr_zh": "该论文介绍了Causal-Adapter，一种用于反事实图像生成的新框架，它使用文本到图像的扩散模型，通过利用结构因果建模和属性正则化策略实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning",
        "summary": "Video-conditioned sound and speech generation, encompassing video-to-sound\n(V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed\nas separate tasks, with limited exploration to unify them within a signle\nframework. Recent attempts to unify V2S and VisualTTS face challenges in\nhandling distinct condition types (e.g., heterogeneous video and transcript\nconditions) and require complex training stages. Unifying these two tasks\nremains an open problem. To bridge this gap, we present VSSFlow, which\nseamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching\nframework. VSSFlow uses a novel condition aggregation mechanism to handle\ndistinct input signals. We find that cross-attention and self-attention layer\nexhibit different inductive biases in the process of introducing condition.\nTherefore, VSSFlow leverages these inductive biases to effectively handle\ndifferent representations: cross-attention for ambiguous video conditions and\nself-attention for more deterministic speech transcripts. Furthermore, contrary\nto the prevailing belief that joint training on the two tasks requires complex\ntraining strategies and may degrade performance, we find that VSSFlow benefits\nfrom the end-to-end joint learning process for sound and speech generation\nwithout extra designs on training stages. Detailed analysis attributes it to\nthe learned general audio prior shared between tasks, which accelerates\nconvergence, enhances conditional generation, and stabilizes the\nclassifier-free guidance process. Extensive experiments demonstrate that\nVSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S\nand VisualTTS benchmarks, underscoring the critical potential of unified\ngenerative models.",
        "url": "http://arxiv.org/abs/2509.24773v1",
        "published_date": "2025-09-29T13:38:24+00:00",
        "updated_date": "2025-09-29T13:38:24+00:00",
        "categories": [
            "eess.AS",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.SD"
        ],
        "authors": [
            "Xin Cheng",
            "Yuyue Wang",
            "Xihua Wang",
            "Yihan Wu",
            "Kaisi Guan",
            "Yijing Chen",
            "Peng Zhang",
            "Xiaojiang Liu",
            "Meng Cao",
            "Ruihua Song"
        ],
        "tldr": "The paper introduces VSSFlow, a unified flow-matching framework for video-conditioned sound and speech generation tasks (V2S and VisualTTS), demonstrating improved performance through joint learning and a novel condition aggregation mechanism.",
        "tldr_zh": "该论文介绍了VSSFlow，一个统一的流匹配框架，用于视频条件下的声音和语音生成任务（V2S和VisualTTS），通过联合学习和一种新颖的条件聚合机制，展示了性能的提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Physical Plausibility in Video Generation by Reasoning the Implausibility",
        "summary": "Diffusion models can generate realistic videos, but existing methods rely on\nimplicitly learning physical reasoning from large-scale text-video datasets,\nwhich is costly, difficult to scale, and still prone to producing implausible\nmotions that violate fundamental physical laws. We introduce a training-free\nframework that improves physical plausibility at inference time by explicitly\nreasoning about implausibility and guiding the generation away from it.\nSpecifically, we employ a lightweight physics-aware reasoning pipeline to\nconstruct counterfactual prompts that deliberately encode physics-violating\nbehaviors. Then, we propose a novel Synchronized Decoupled Guidance (SDG)\nstrategy, which leverages these prompts through synchronized directional\nnormalization to counteract lagged suppression and trajectory-decoupled\ndenoising to mitigate cumulative trajectory bias, ensuring that implausible\ncontent is suppressed immediately and consistently throughout denoising.\nExperiments across different physical domains show that our approach\nsubstantially enhances physical fidelity while maintaining photorealism,\ndespite requiring no additional training. Ablation studies confirm the\ncomplementary effectiveness of both the physics-aware reasoning component and\nSDG. In particular, the aforementioned two designs of SDG are also individually\nvalidated to contribute critically to the suppression of implausible content\nand the overall gains in physical plausibility. This establishes a new and\nplug-and-play physics-aware paradigm for video generation.",
        "url": "http://arxiv.org/abs/2509.24702v1",
        "published_date": "2025-09-29T12:32:54+00:00",
        "updated_date": "2025-09-29T12:32:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yutong Hao",
            "Chen Chen",
            "Ajmal Saeed Mian",
            "Chang Xu",
            "Daochang Liu"
        ],
        "tldr": "This paper introduces a training-free method that improves the physical plausibility of generated videos by explicitly reasoning about and suppressing implausible motions using physics-aware reasoning and a novel Synchronized Decoupled Guidance strategy.",
        "tldr_zh": "本文介绍了一种无需训练的方法，通过显式推理和抑制不合理的运动，利用物理感知推理和一种新的同步解耦指导策略来提高生成视频的物理合理性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Instruction Guided Multi Object Image Editing with Quantity and Layout Consistency",
        "summary": "Instruction driven image editing with standard CLIP text encoders often fails\nin complex scenes with many objects. We present QL-Adapter, a framework for\nmultiple object editing that tackles two challenges: enforcing object counts\nand spatial layouts, and accommodating diverse categories. QL-Adapter consists\nof two core modules: the Image-Layout Fusion Module (ILFM) and the Cross-Modal\nAugmentation Module (CMAM). ILFM fuses layout priors with ViT patch tokens from\nthe CLIP image encoder to strengthen spatial structure understanding. CMAM\ninjects image features into the text branch to enrich textual embeddings and\nimprove instruction following. We further build QL-Dataset, a benchmark that\nspans broad category, layout, and count variations, and define the task of\nquantity and layout consistent image editing (QL-Edit). Extensive experiments\nshow that QL-Adapter achieves state of the art performance on QL-Edit and\nsignificantly outperforms existing models.",
        "url": "http://arxiv.org/abs/2509.24514v1",
        "published_date": "2025-09-29T09:33:51+00:00",
        "updated_date": "2025-09-29T09:33:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaqi Tan",
            "Fangyu Li",
            "Yang Liu"
        ],
        "tldr": "The paper introduces QL-Adapter, a novel framework for instruction-guided multi-object image editing that addresses challenges in enforcing object counts and spatial layouts. It outperforms existing models on a new benchmark dataset, QL-Edit.",
        "tldr_zh": "该论文介绍了QL-Adapter，一种用于指令引导的多对象图像编辑的新框架，解决了在执行对象计数和空间布局方面的挑战。它在名为QL-Edit的新的基准数据集上优于现有模型。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs",
        "summary": "Multimodal Large Language Models (MLLMs) have significantly improved the\nperformance of various tasks, but continue to suffer from visual\nhallucinations, a critical issue where generated responses contradict visual\nevidence. While Direct Preference Optimization(DPO) is widely used for\nalignment, its application to MLLMs often fails to capture fine-grained\nsemantic differences and encourages shortcut learning. To address these\nchallenges, we propose Semantic Curriculum Preference Optimization (SCPO), a\nnovel framework for MLLM alignment. SCPO employs a progressive, easy-to-hard\ncurriculum built upon our Semantic Curriculum Preference Pairs dataset, which\nprovides fine-grained semantic contrasts sorted by difficulty. This curriculum\nis trained with a dynamic reference model and a novel symmetric, bidirectional\nobjective to facilitate simultaneous learning from both textual and visual\npreferences. To our knowledge, SCPO is the first framework to unify semantics,\nsymmetry, and curriculum for MLLMs alignment, effectively mitigating visual\nhallucinations. Extensive experiments on LLaVA models across various scales and\nversions validate that SCPO demonstrates superior performance compared to\nbaseline models on multiple hallucination benchmarks, reducing the\nhallucination rate by up to 62.9%. Moreover, evaluations on generalized\nbenchmarks show that SCPO improves factuality while preserving general\ncapabilities, with its performance remaining stable across general\nvision-language benchmarks.",
        "url": "http://arxiv.org/abs/2509.24491v1",
        "published_date": "2025-09-29T09:03:36+00:00",
        "updated_date": "2025-09-29T09:03:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuanshuai Li",
            "Yuping Yan",
            "Junfeng Tang",
            "Yunxuan Li",
            "Zeqi Zheng",
            "Yaochu Jin"
        ],
        "tldr": "This paper introduces Semantic Curriculum Preference Optimization (SCPO), a novel framework for mitigating visual hallucinations in Multimodal Large Language Models (MLLMs) using a curriculum learning approach based on semantic preference pairs. It achieves significant reduction in hallucination rates while preserving general capabilities.",
        "tldr_zh": "本文介绍了一种名为语义课程偏好优化 (SCPO) 的新型框架，该框架通过基于语义偏好对的课程学习方法来减轻多模态大型语言模型 (MLLM) 中的视觉幻觉。它在保持一般能力的同时，显著降低了幻觉率。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NeoWorld: Neural Simulation of Explorable Virtual Worlds via Progressive 3D Unfolding",
        "summary": "We introduce NeoWorld, a deep learning framework for generating interactive\n3D virtual worlds from a single input image. Inspired by the on-demand\nworldbuilding concept in the science fiction novel Simulacron-3 (1964), our\nsystem constructs expansive environments where only the regions actively\nexplored by the user are rendered with high visual realism through\nobject-centric 3D representations. Unlike previous approaches that rely on\nglobal world generation or 2D hallucination, NeoWorld models key foreground\nobjects in full 3D, while synthesizing backgrounds and non-interacted regions\nin 2D to ensure efficiency. This hybrid scene structure, implemented with\ncutting-edge representation learning and object-to-3D techniques, enables\nflexible viewpoint manipulation and physically plausible scene animation,\nallowing users to control object appearance and dynamics using natural language\ncommands. As users interact with the environment, the virtual world\nprogressively unfolds with increasing 3D detail, delivering a dynamic,\nimmersive, and visually coherent exploration experience. NeoWorld significantly\noutperforms existing 2D and depth-layered 2.5D methods on the WorldScore\nbenchmark.",
        "url": "http://arxiv.org/abs/2509.24441v1",
        "published_date": "2025-09-29T08:24:28+00:00",
        "updated_date": "2025-09-29T08:24:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanpeng Zhao",
            "Shanyan Guan",
            "Yunbo Wang",
            "Yanhao Ge",
            "Wei Li",
            "Xiaokang Yang"
        ],
        "tldr": "NeoWorld generates interactive 3D virtual worlds from a single image, using a hybrid 2D/3D approach for efficiency and progressive unfolding as the user explores. It outperforms existing methods on the WorldScore benchmark.",
        "tldr_zh": "NeoWorld从单张图像生成交互式3D虚拟世界，采用混合2D/3D方法提高效率，并随着用户探索逐步展开。它在WorldScore基准测试中优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers",
        "summary": "Visual generation quality has been greatly promoted with the rapid advances\nin diffusion transformers (DiTs), which is attributed to the scaling of model\nsize and complexity. However, these attributions also hinder the practical\ndeployment of DiTs on edge devices, limiting their development and application.\nServe as an efficient model compression technique, model post-training\nquantization (PTQ) can reduce the memory consumption and speed up the\ninference, with inevitable performance degradation. To alleviate the\ndegradation, we propose CLQ, a cross-layer guided orthogonal-based quantization\nmethod for DiTs. To be specific, CLQ consists of three key designs. First, we\nobserve that the calibration data used by most of the PTQ methods can not\nhonestly represent the distribution of the activations. Therefore, we propose\ncross-block calibration (CBC) to obtain accurate calibration data, with which\nthe quantization can be better guided. Second, we propose orthogonal-based\nsmoothing (OBS), which quantifies the outlier score of each channel and\nleverages block Hadamard matrix to smooth the outliers with negligible\noverhead. Third, we propose cross-layer parameter searching (CLPS) to search.\nWe evaluate CLQ with both image generation and video generation models and\nsuccessfully compress the model into W4A4 with negligible degradation in visual\nquality and metrics. CLQ achieves 3.98x memory saving and 3.95x speedup. Our\ncode is available at\n\\hyperlink{https://github.com/Kai-Liu001/CLQ}{https://github.com/Kai-Liu001/CLQ}.",
        "url": "http://arxiv.org/abs/2509.24416v1",
        "published_date": "2025-09-29T08:06:42+00:00",
        "updated_date": "2025-09-29T08:06:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kai Liu",
            "Shaoqiu Zhang",
            "Linghe Kong",
            "Yulun Zhang"
        ],
        "tldr": "The paper proposes CLQ, a post-training quantization method for diffusion transformers that uses cross-layer guidance and orthogonal-based techniques to achieve high compression (W4A4) with minimal performance degradation, resulting in significant memory savings and speedup.",
        "tldr_zh": "该论文提出了一种用于扩散Transformer的后训练量化方法CLQ，该方法利用跨层指导和基于正交的技术，以实现高性能压缩（W4A4），并以最小的性能降降低 memory 占用和提高推理速度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RapidMV: Leveraging Spatio-Angular Representations for Efficient and Consistent Text-to-Multi-View Synthesis",
        "summary": "Generating synthetic multi-view images from a text prompt is an essential\nbridge to generating synthetic 3D assets. In this work, we introduce RapidMV, a\nnovel text-to-multi-view generative model that can produce 32 multi-view\nsynthetic images in just around 5 seconds. In essence, we propose a novel\nspatio-angular latent space, encoding both the spatial appearance and angular\nviewpoint deviations into a single latent for improved efficiency and\nmulti-view consistency. We achieve effective training of RapidMV by\nstrategically decomposing our training process into multiple steps. We\ndemonstrate that RapidMV outperforms existing methods in terms of consistency\nand latency, with competitive quality and text-image alignment.",
        "url": "http://arxiv.org/abs/2509.24410v1",
        "published_date": "2025-09-29T07:57:11+00:00",
        "updated_date": "2025-09-29T07:57:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seungwook Kim",
            "Yichun Shi",
            "Kejie Li",
            "Minsu Cho",
            "Peng Wang"
        ],
        "tldr": "RapidMV is a new text-to-multi-view image generation model that leverages a spatio-angular latent space to generate 32 consistent multi-view images in approximately 5 seconds, outperforming existing methods in consistency and latency.",
        "tldr_zh": "RapidMV是一种新的文本到多视角图像生成模型，它利用时空角潜在空间在约5秒内生成32个一致的多视角图像，在一致性和延迟方面优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models",
        "summary": "Unified Multimodal Models (UMMs) built on shared autoregressive (AR)\ntransformers are attractive for their architectural simplicity. However, we\nidentify a critical limitation: when trained on multimodal inputs,\nmodality-shared transformers suffer from severe gradient conflicts between\nvision and text, particularly in shallow and deep layers. We trace this issue\nto the fundamentally different low-level statistical properties of images and\ntext, while noting that conflicts diminish in middle layers where\nrepresentations become more abstract and semantically aligned. To overcome this\nchallenge, we propose Uni-X, a two-end-separated, middle-shared architecture.\nUni-X dedicates its initial and final layers to modality-specific processing,\nwhile maintaining shared parameters in the middle layers for high-level\nsemantic fusion. This X-shaped design not only eliminates gradient conflicts at\nboth ends but also further alleviates residual conflicts in the shared layers.\nExtensive experiments validate the effectiveness of Uni-X. Under identical\ntraining conditions, Uni-X achieves superior training efficiency compared to\nstrong baselines. When scaled to 3B parameters with larger training data, Uni-X\nmatches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for\nimage generation alongside strong performance in text and vision understanding\ntasks. These results establish Uni-X as a parameter-efficient and scalable\nfoundation for future unified multimodal modeling. Our code is available at\nhttps://github.com/CURRENTF/Uni-X",
        "url": "http://arxiv.org/abs/2509.24365v1",
        "published_date": "2025-09-29T07:05:10+00:00",
        "updated_date": "2025-09-29T07:05:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jitai Hao",
            "Hao Liu",
            "Xinyan Xiao",
            "Qiang Huang",
            "Jun Yu"
        ],
        "tldr": "The paper introduces Uni-X, a two-end-separated architecture for unified multimodal modeling, addressing gradient conflicts between vision and text modalities, achieving better training efficiency and performance compared to existing models, especially in image generation.",
        "tldr_zh": "该论文介绍了Uni-X，一种用于统一多模态建模的两端分离架构，解决了视觉和文本模态之间的梯度冲突，与现有模型相比，实现了更好的训练效率和性能，尤其是在图像生成方面。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semantic Editing with Coupled Stochastic Differential Equations",
        "summary": "Editing the content of an image with a pretrained text-to-image model remains\nchallenging. Existing methods often distort fine details or introduce\nunintended artifacts. We propose using coupled stochastic differential\nequations (coupled SDEs) to guide the sampling process of any pre-trained\ngenerative model that can be sampled by solving an SDE, including diffusion and\nrectified flow models. By driving both the source image and the edited image\nwith the same correlated noise, our approach steers new samples toward the\ndesired semantics while preserving visual similarity to the source. The method\nworks out-of-the-box-without retraining or auxiliary networks-and achieves high\nprompt fidelity along with near-pixel-level consistency. These results position\ncoupled SDEs as a simple yet powerful tool for controlled generative AI.",
        "url": "http://arxiv.org/abs/2509.24223v1",
        "published_date": "2025-09-29T03:05:16+00:00",
        "updated_date": "2025-09-29T03:05:16+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Jianxin Zhang",
            "Clayton Scott"
        ],
        "tldr": "This paper presents a novel approach using coupled Stochastic Differential Equations (SDEs) to improve semantic image editing with pre-trained text-to-image models, achieving high prompt fidelity and pixel-level consistency without retraining.",
        "tldr_zh": "本文提出了一种使用耦合随机微分方程（SDE）的新方法，以改进使用预训练文本到图像模型的语义图像编辑，无需重新训练即可实现高prompt保真度和像素级一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Multi-Modal Interactive & Reactive 3D Motion Generation via Rectified Flow",
        "summary": "Generating realistic, context-aware two-person motion conditioned on diverse\nmodalities remains a central challenge in computer graphics, animation, and\nhuman-computer interaction. We introduce DualFlow, a unified and efficient\nframework for multi-modal two-person motion generation. DualFlow conditions 3D\nmotion synthesis on diverse inputs, including text, music, and prior motion\nsequences. Leveraging rectified flow, it achieves deterministic straight-line\nsampling paths between noise and data, reducing inference time and mitigating\nerror accumulation common in diffusion-based models. To enhance semantic\ngrounding, DualFlow employs a Retrieval-Augmented Generation (RAG) module that\nretrieves motion exemplars using music features and LLM-based text\ndecompositions of spatial relations, body movements, and rhythmic patterns. We\nuse contrastive objective that further strengthens alignment with conditioning\nsignals and introduce synchronization loss that improves inter-person\ncoordination. Extensive evaluations across text-to-motion, music-to-motion, and\nmulti-modal interactive benchmarks show consistent gains in motion quality,\nresponsiveness, and efficiency. DualFlow produces temporally coherent and\nrhythmically synchronized motions, setting state-of-the-art in multi-modal\nhuman motion generation.",
        "url": "http://arxiv.org/abs/2509.24099v1",
        "published_date": "2025-09-28T22:36:18+00:00",
        "updated_date": "2025-09-28T22:36:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Prerit Gupta",
            "Shourya Verma",
            "Ananth Grama",
            "Aniket Bera"
        ],
        "tldr": "The paper introduces DualFlow, a rectified flow-based framework for generating realistic two-person motion conditioned on text, music, and prior motion, using RAG and synchronization loss for enhanced quality and efficiency.",
        "tldr_zh": "该论文介绍了DualFlow，一个基于校正流的框架，用于生成逼真的双人运动，以文本、音乐和先前的运动为条件，使用RAG和同步损失来提高质量和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Autoregressive Video Generation beyond Next Frames Prediction",
        "summary": "Autoregressive models for video generation typically operate frame-by-frame,\nextending next-token prediction from language to video's temporal dimension. We\nquestion that unlike word as token is universally agreed in language if frame\nis a appropriate prediction unit? To address this, we present VideoAR, a\nunified framework that supports a spectrum of prediction units including full\nframes, key-detail frames, multiscale refinements, and spatiotemporal cubes.\nAmong these designs, we find model video generation using\n\\textit{spatiotemporal} cubes as prediction units, which allows autoregressive\nmodels to operate across both spatial and temporal dimensions simultaneously.\nThis approach eliminates the assumption that frames are the natural atomic\nunits for video autoregression. We evaluate VideoAR across diverse prediction\nstrategies, finding that cube-based prediction consistently delivers superior\nquality, speed, and temporal coherence. By removing the frame-by-frame\nconstraint, our video generator surpasses state-of-the-art baselines on VBench\nwhile achieving faster inference and enabling seamless scaling to minute-long\nsequences. We hope this work will motivate rethinking sequence decomposition in\nvideo and other spatiotemporal domains.",
        "url": "http://arxiv.org/abs/2509.24081v1",
        "published_date": "2025-09-28T21:37:53+00:00",
        "updated_date": "2025-09-28T21:37:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sucheng Ren",
            "Chen Chen",
            "Zhenbang Wang",
            "Liangchen Song",
            "Xiangxin Zhu",
            "Alan Yuille",
            "Yinfei Yang",
            "Jiasen Lu"
        ],
        "tldr": "The paper introduces VideoAR, a video generation framework that moves beyond frame-by-frame prediction by using spatiotemporal cubes as prediction units, achieving improved quality, speed, and temporal coherence compared to existing methods.",
        "tldr_zh": "该论文介绍了VideoAR，一种视频生成框架，它通过使用时空立方体作为预测单元，超越了逐帧预测，与现有方法相比，实现了更高的质量、速度和时间连贯性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation",
        "summary": "High-fidelity 3D asset generation is crucial for various industries. While\nrecent 3D pretrained models show strong capability in producing realistic\ncontent, most are built upon diffusion models and follow a two-stage pipeline\nthat first generates geometry and then synthesizes appearance. Such a decoupled\ndesign tends to produce geometry-texture misalignment and non-negligible cost.\nIn this paper, we propose UniLat3D, a unified framework that encodes geometry\nand appearance in a single latent space, enabling direct single-stage\ngeneration. Our key contribution is a geometry-appearance Unified VAE, which\ncompresses high-resolution sparse features into a compact latent representation\n-- UniLat. UniLat integrates structural and visual information into a dense\nlow-resolution latent, which can be efficiently decoded into diverse 3D\nformats, e.g., 3D Gaussians and meshes. Based on this unified representation,\nwe train a single flow-matching model to map Gaussian noise directly into\nUniLat, eliminating redundant stages. Trained solely on public datasets,\nUniLat3D produces high-quality 3D assets in seconds from a single image,\nachieving superior appearance fidelity and geometric quality. More demos \\&\ncode are available at https://unilat3d.github.io/",
        "url": "http://arxiv.org/abs/2509.25079v1",
        "published_date": "2025-09-29T17:21:23+00:00",
        "updated_date": "2025-09-29T17:21:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.GR"
        ],
        "authors": [
            "Guanjun Wu",
            "Jiemin Fang",
            "Chen Yang",
            "Sikuang Li",
            "Taoran Yi",
            "Jia Lu",
            "Zanwei Zhou",
            "Jiazhong Cen",
            "Lingxi Xie",
            "Xiaopeng Zhang",
            "Wei Wei",
            "Wenyu Liu",
            "Xinggang Wang",
            "Qi Tian"
        ],
        "tldr": "UniLat3D proposes a single-stage 3D asset generation framework that unifies geometry and appearance into a single latent space using a novel VAE and flow-matching model, achieving high-quality results from a single image.",
        "tldr_zh": "UniLat3D 提出了一个单阶段3D资产生成框架，该框架使用一种新的VAE和流匹配模型将几何和外观统一到一个潜在空间中，从而从单个图像中获得高质量的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CharGen: Fast and Fluent Portrait Modification",
        "summary": "Interactive editing of character images with diffusion models remains\nchallenging due to the inherent trade-off between fine-grained control,\ngeneration speed, and visual fidelity. We introduce CharGen, a\ncharacter-focused editor that combines attribute-specific Concept Sliders,\ntrained to isolate and manipulate attributes such as facial feature size,\nexpression, and decoration with the StreamDiffusion sampling pipeline for more\ninteractive performance. To counteract the loss of detail that often\naccompanies accelerated sampling, we propose a lightweight Repair Step that\nreinstates fine textures without compromising structural consistency.\nThroughout extensive ablation studies and in comparison to open-source\nInstructPix2Pix and closed-source Google Gemini, and a comprehensive user\nstudy, CharGen achieves two-to-four-fold faster edit turnaround with precise\nediting control and identity-consistent results. Project page:\nhttps://chargen.jdihlmann.com/",
        "url": "http://arxiv.org/abs/2509.25058v1",
        "published_date": "2025-09-29T17:09:30+00:00",
        "updated_date": "2025-09-29T17:09:30+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Jan-Niklas Dihlmann",
            "Arnela Killguss",
            "Hendrik P. A. Lensch"
        ],
        "tldr": "CharGen is a character image editor combining attribute-specific Concept Sliders with StreamDiffusion for faster, more controlled edits, and a Repair Step to maintain fine details. It outperforms existing methods regarding speed, control, and identity consistency.",
        "tldr_zh": "CharGen 是一款人物图像编辑器，结合了特定属性的概念滑块和 StreamDiffusion 以实现更快、更可控的编辑，并使用修复步骤来保持精细细节。在速度、控制和身份一致性方面，它优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Toward a Vision-Language Foundation Model for Medical Data: Multimodal Dataset and Benchmarks for Vietnamese PET/CT Report Generation",
        "summary": "Vision-Language Foundation Models (VLMs), trained on large-scale multimodal\ndatasets, have driven significant advances in Artificial Intelligence by\nenabling rich cross-modal reasoning. Despite their success in general domains,\napplying these models to medical imaging remains challenging due to the limited\navailability of diverse imaging modalities and multilingual clinical data. Most\nexisting medical VLMs are trained on a subset of imaging modalities and focus\nprimarily on high-resource languages, thus limiting their generalizability and\nclinical utility. To address these limitations, we introduce a novel\nVietnamese-language multimodal medical dataset comprising 1,567,062 paired\nCT-PET images and corresponding 2,757 full-length clinical reports. This\ndataset is designed to fill two pressing gaps in medical AI development: (1)\nthe lack of PET/CT imaging data in existing VLMs training corpora, which\nhinders the development of models capable of handling functional imaging tasks;\nand (2) the underrepresentation of low-resource languages, particularly the\nVietnamese language, in medical vision-language research. To the best of our\nknowledge, this is the first dataset to provide comprehensive PET/CT-report\npairs in Vietnamese. We further introduce a training framework to enhance VLMs'\nlearning, including data augmentation and expert-validated test sets. We\nconduct comprehensive experiments benchmarking state-of-the-art VLMs on\ndownstream tasks, including medical report generation and visual question\nanswering. The experimental results show that incorporating our dataset\nsignificantly improves the performance of existing VLMs. We believe this\ndataset and benchmark will serve as a pivotal step in advancing the development\nof more robust VLMs for medical imaging, particularly in low-resource\nlanguages, and improving their clinical relevance in Vietnamese healthcare.",
        "url": "http://arxiv.org/abs/2509.24739v1",
        "published_date": "2025-09-29T13:03:57+00:00",
        "updated_date": "2025-09-29T13:03:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huu Tien Nguyen",
            "Dac Thai Nguyen",
            "The Minh Duc Nguyen",
            "Trung Thanh Nguyen",
            "Thao Nguyen Truong",
            "Huy Hieu Pham",
            "Johan Barthelemy",
            "Minh Quan Tran",
            "Thanh Tam Nguyen",
            "Quoc Viet Hung Nguyen",
            "Quynh Anh Chau",
            "Hong Son Mai",
            "Thanh Trung Nguyen",
            "Phi Le Nguyen"
        ],
        "tldr": "This paper introduces a new Vietnamese-language PET/CT imaging dataset paired with clinical reports for training vision-language foundation models, addressing the lack of such data and low-resource language representation in medical AI.",
        "tldr_zh": "本文介绍了一个新的越南语 PET/CT 图像数据集，其中包含临床报告，用于训练视觉语言基础模型，解决了医学人工智能中此类数据和低资源语言表示的缺失问题。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "GANji: A Framework for Introductory AI Image Generation",
        "summary": "The comparative study of generative models often requires significant\ncomputational resources, creating a barrier for researchers and practitioners.\nThis paper introduces GANji, a lightweight framework for benchmarking\nfoundational AI image generation techniques using a dataset of 10,314 Japanese\nKanji characters. It systematically compares the performance of a Variational\nAutoencoder (VAE), a Generative Adversarial Network (GAN), and a Denoising\nDiffusion Probabilistic Model (DDPM). The results demonstrate that while the\nDDPM achieves the highest image fidelity, with a Fr\\'echet Inception Distance\n(FID) score of 26.2, its sampling time is over 2,000 times slower than the\nother models. The GANji framework is an effective and accessible tool for\nrevealing the fundamental trade-offs between model architecture, computational\ncost, and visual quality, making it ideal for both educational and research\npurposes.",
        "url": "http://arxiv.org/abs/2509.24128v1",
        "published_date": "2025-09-28T23:54:59+00:00",
        "updated_date": "2025-09-28T23:54:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chandon Hamel",
            "Mike Busch"
        ],
        "tldr": "The paper introduces GANji, a lightweight benchmarking framework for VAEs, GANs, and DDPMs on a Kanji character dataset, highlighting the trade-offs between image quality and computational cost. It is suitable for educational and research purposes.",
        "tldr_zh": "该论文介绍了GANji，一个轻量级的基准测试框架，用于在汉字数据集上评估VAE、GAN和DDPM，突出了图像质量和计算成本之间的权衡。它适用于教育和研究目的。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LayerD: Decomposing Raster Graphic Designs into Layers",
        "summary": "Designers craft and edit graphic designs in a layer representation, but\nlayer-based editing becomes impossible once composited into a raster image. In\nthis work, we propose LayerD, a method to decompose raster graphic designs into\nlayers for re-editable creative workflow. LayerD addresses the decomposition\ntask by iteratively extracting unoccluded foreground layers. We propose a\nsimple yet effective refinement approach taking advantage of the assumption\nthat layers often exhibit uniform appearance in graphic designs. As\ndecomposition is ill-posed and the ground-truth layer structure may not be\nreliable, we develop a quality metric that addresses the difficulty. In\nexperiments, we show that LayerD successfully achieves high-quality\ndecomposition and outperforms baselines. We also demonstrate the use of LayerD\nwith state-of-the-art image generators and layer-based editing.",
        "url": "http://arxiv.org/abs/2509.25134v1",
        "published_date": "2025-09-29T17:50:12+00:00",
        "updated_date": "2025-09-29T17:50:12+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Tomoyuki Suzuki",
            "Kang-Jun Liu",
            "Naoto Inoue",
            "Kota Yamaguchi"
        ],
        "tldr": "LayerD is a method to decompose raster images into editable layers, facilitating creative workflows by enabling layer-based editing on rasterized designs.",
        "tldr_zh": "LayerD是一种将栅格图像分解为可编辑图层的方法，通过对栅格化设计启用基于图层的编辑，从而促进创意工作流程。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers",
        "summary": "Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable\noff-the-shelf video representation by predicting masked regions in latent space\nwith an exponential moving average (EMA)-updated teacher. While EMA prevents\nrepresentation collapse, it complicates scalable model selection and couples\nteacher and student architectures. We revisit masked-latent prediction and show\nthat a frozen teacher suffices. Concretely, we (i) train a target encoder with\na simple pixel-reconstruction objective under V-JEPA masking, then (ii) freeze\nit and train a student to predict the teacher's latents on masked regions. This\nleads to a two-stage, unregularized scheme that we refer to as SALT\n(Static-teacher Asymmetric Latent Training). SALT decouples optimization into\npixel reconstruction (teacher) and masked latent prediction (student),\nincreasing transparency, efficiency, and scalability while preserving the\nability of representation to generalize under frozen evaluation. Empirically,\nour student models outperform recently proposed V-JEPA 2 encoders under frozen\nbackbone evaluation across diverse benchmarks. They are also more\ncompute-optimal: at matched pretraining FLOPs, our method achieves higher\nprobing accuracy, and its scaling curves dominate V-JEPA's accuracy-FLOPs\nPareto frontier. Finally, we find that student quality is remarkably robust to\nteacher quality: high-performing students emerge even with small, sub-optimal\nteachers. This points to a compute budget allocation that should overwhelmingly\nfavor the student. These results position SALT as a simple, scalable, and\ncompute-efficient alternative to EMA-based self-distillation for video\nrepresentation learning.",
        "url": "http://arxiv.org/abs/2509.24317v1",
        "published_date": "2025-09-29T05:55:17+00:00",
        "updated_date": "2025-09-29T05:55:17+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Xianhang Li",
            "Chen Huang",
            "Chun-Liang Li",
            "Eran Malach",
            "Josh Susskind",
            "Vimal Thilak",
            "Etai Littwin"
        ],
        "tldr": "The paper introduces SALT, a compute-efficient video SSL method using a frozen teacher network to improve video representation learning, outperforming V-JEPA with better scalability and robustness to teacher quality.",
        "tldr_zh": "本文介绍了一种名为SALT的计算高效视频自监督学习方法，它使用冻结的教师网络来改进视频表示学习，在可扩展性和对教师质量的鲁棒性方面优于V-JEPA。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]