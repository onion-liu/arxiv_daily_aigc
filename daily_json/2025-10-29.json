[
    {
        "title": "Uniform Discrete Diffusion with Metric Path for Video Generation",
        "summary": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA",
        "url": "http://arxiv.org/abs/2510.24717v1",
        "published_date": "2025-10-28T17:59:57+00:00",
        "updated_date": "2025-10-28T17:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoge Deng",
            "Ting Pan",
            "Fan Zhang",
            "Yang Liu",
            "Zhuoyan Luo",
            "Yufeng Cui",
            "Wenxuan Wang",
            "Chunhua Shen",
            "Shiguang Shan",
            "Zhaoxiang Zhang",
            "Xinlong Wang"
        ],
        "tldr": "The paper introduces URSA, a novel discrete diffusion framework for video generation that addresses the limitations of previous discrete methods by using a Linearized Metric Path and Resolution-dependent Timestep Shifting, achieving comparable performance to continuous diffusion models.",
        "tldr_zh": "该论文介绍了一种名为URSA的新型离散扩散视频生成框架，通过线性化度量路径和分辨率相关的步长调整来解决先前离散方法的局限性，实现了与连续扩散模型相媲美的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling",
        "summary": "Denoising generative models, such as diffusion and flow-based models, produce\nhigh-quality samples but require many denoising steps due to discretization\nerror. Flow maps, which estimate the average velocity between timesteps,\nmitigate this error and enable faster sampling. However, their training\ntypically demands architectural changes that limit compatibility with\npretrained flow models. We introduce Decoupled MeanFlow, a simple decoding\nstrategy that converts flow models into flow map models without architectural\nmodifications. Our method conditions the final blocks of diffusion transformers\non the subsequent timestep, allowing pretrained flow models to be directly\nrepurposed as flow maps. Combined with enhanced training techniques, this\ndesign enables high-quality generation in as few as 1 to 4 steps. Notably, we\nfind that training flow models and subsequently converting them is more\nefficient and effective than training flow maps from scratch. On ImageNet\n256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,\nrespectively, surpassing prior art by a large margin. Furthermore, we achieve\nFID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the\nperformance of flow models while delivering over 100x faster inference.",
        "url": "http://arxiv.org/abs/2510.24474v1",
        "published_date": "2025-10-28T14:43:48+00:00",
        "updated_date": "2025-10-28T14:43:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kyungmin Lee",
            "Sihyun Yu",
            "Jinwoo Shin"
        ],
        "tldr": "The paper introduces Decoupled MeanFlow, a method to convert pretrained flow models into flow maps for accelerated sampling without architectural changes, achieving state-of-the-art FID scores with significantly faster inference, especially on ImageNet.",
        "tldr_zh": "该论文介绍了 Decoupled MeanFlow，一种将预训练的流模型转换为流映射的方法，用于加速采样，无需架构修改，并在 ImageNet 上实现了最先进的 FID 分数，且具备显著更快的推理速度。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs",
        "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.",
        "url": "http://arxiv.org/abs/2510.24514v1",
        "published_date": "2025-10-28T15:26:20+00:00",
        "updated_date": "2025-10-28T15:26:20+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Huanyu Zhang",
            "Wenshan Wu",
            "Chengzu Li",
            "Ning Shang",
            "Yan Xia",
            "Yangyu Huang",
            "Yifan Zhang",
            "Li Dong",
            "Zhang Zhang",
            "Liang Wang",
            "Tieniu Tan",
            "Furu Wei"
        ],
        "tldr": "The paper introduces Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad to improve visual planning and reasoning by interleaving textual reasoning with visual latent generation, resulting in comparable or superior performance on various MLLMs.",
        "tldr_zh": "该论文介绍了Latent Sketchpad，一个为MLLMs配备内部视觉草稿本的框架，通过将文本推理与视觉潜在生成交织在一起，来提高视觉规划和推理能力，从而在各种MLLMs上获得相当甚至更优越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
        "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.",
        "url": "http://arxiv.org/abs/2510.24448v1",
        "published_date": "2025-10-28T14:12:11+00:00",
        "updated_date": "2025-10-28T14:12:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68T07, 68T45, 68T20",
            "I.2.10; I.4.8; I.5.1; I.2.6"
        ],
        "authors": [
            "Pablo Acuaviva",
            "Aram Davtyan",
            "Mariam Hassan",
            "Sebastian Stapf",
            "Ahmad Rahimi",
            "Alexandre Alahi",
            "Paolo Favaro"
        ],
        "tldr": "The paper investigates Video Diffusion Models (VDMs) pretraining as a way to improve visual intelligence, showing they outperform LLMs in data efficiency across various visual tasks due to stronger inductive biases. This suggests video pretraining can help build better visual foundation models.",
        "tldr_zh": "该论文研究了视频扩散模型（VDM）预训练，旨在提升视觉智能。结果表明，由于更强的归纳偏置，VDM在各种视觉任务中的数据效率优于LLM。这表明视频预训练有助于构建更好的视觉基础模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration",
        "summary": "While autoregressive (AR) modeling has recently emerged as a new paradigm in\nvisual generation, its practical adoption is severely constrained by the slow\ninference speed of per-token generation, which often requires thousands of\nsteps to produce a single sample. To address this challenge, we propose MC-SJD,\na training-free, lossless parallel decoding framework designed to accelerate AR\nvisual generation by extending the recently introduced Speculative Jacobi\nDecoding (SJD). Although SJD shows strong potential for accelerating AR\ngeneration, we demonstrate that token instability across iterations\nsignificantly reduces the acceptance rate, a limitation that primarily arises\nfrom the independent sampling process used during draft token generation. To\novercome this, we introduce MC-SJD, an information-theoretic approach based on\ncoupling, which substantially accelerates standard SJD by maximizing the\nprobability of sampling identical draft tokens across consecutive iterations,\nall while preserving its lossless property. Remarkably, this method requires\nonly a single-line modification to the existing algorithm, yet achieves\nsubstantial performance gains, delivering up to a ~4.2x acceleration in image\ngeneration and ~13.3x acceleration in video generation compared to standard AR\ndecoding, without any degradation in output quality.",
        "url": "http://arxiv.org/abs/2510.24211v1",
        "published_date": "2025-10-28T09:26:27+00:00",
        "updated_date": "2025-10-28T09:26:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junhyuk So",
            "Hyunho Kook",
            "Chaeyeon Jang",
            "Eunhyeok Park"
        ],
        "tldr": "The paper introduces MC-SJD, an improved, training-free, lossless parallel decoding framework for accelerating autoregressive visual generation by maximizing the probability of sampling identical draft tokens across iterations within Speculative Jacobi Decoding, achieving up to 4.2x and 13.3x acceleration in image and video generation, respectively.",
        "tldr_zh": "该论文介绍了 MC-SJD，一种改进的、无需训练的、无损并行解码框架，通过最大化 Speculative Jacobi 解码中迭代采样相同草稿 tokens 的概率来加速自回归视觉生成，在图像和视频生成方面分别实现了高达 4.2 倍和 13.3 倍的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VC4VG: Optimizing Video Captions for Text-to-Video Generation",
        "summary": "Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels.We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/qyr0403/VC4VG to support further\nresearch.",
        "url": "http://arxiv.org/abs/2510.24134v1",
        "published_date": "2025-10-28T07:19:01+00:00",
        "updated_date": "2025-10-28T07:19:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Yang Du",
            "Zhuoran Lin",
            "Kaiqiang Song",
            "Biao Wang",
            "Zhicheng Zheng",
            "Tiezheng Ge",
            "Bo Zheng",
            "Qin Jin"
        ],
        "tldr": "The paper introduces VC4VG, a framework for optimizing video captions specifically to improve text-to-video generation, along with a new benchmark for evaluation.",
        "tldr_zh": "该论文介绍了VC4VG，一个专门用于优化视频字幕以改进文本到视频生成的框架，并提出了一个新的评估基准。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ETC: training-free diffusion models acceleration with Error-aware Trend Consistency",
        "summary": "Diffusion models have achieved remarkable generative quality but remain\nbottlenecked by costly iterative sampling. Recent training-free methods\naccelerate diffusion process by reusing model outputs. However, these methods\nignore denoising trends and lack error control for model-specific tolerance,\nleading to trajectory deviations under multi-step reuse and exacerbating\ninconsistencies in the generated results. To address these issues, we introduce\nError-aware Trend Consistency (ETC), a framework that (1) introduces a\nconsistent trend predictor that leverages the smooth continuity of diffusion\ntrajectories, projecting historical denoising patterns into stable future\ndirections and progressively distributing them across multiple approximation\nsteps to achieve acceleration without deviating; (2) proposes a model-specific\nerror tolerance search mechanism that derives corrective thresholds by\nidentifying transition points from volatile semantic planning to stable quality\nrefinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX\nwith negligible (-0.074 SSIM score) degradation of consistency.",
        "url": "http://arxiv.org/abs/2510.24129v1",
        "published_date": "2025-10-28T07:08:09+00:00",
        "updated_date": "2025-10-28T07:08:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiajian Xie",
            "Hubery Yin",
            "Chen Li",
            "Zhou Zhao",
            "Shengyu Zhang"
        ],
        "tldr": "This paper introduces ETC, a training-free method to accelerate diffusion models by predicting denoising trends and adaptively controlling error tolerance, achieving significant speedups with minimal quality degradation.",
        "tldr_zh": "本文介绍了一种名为ETC的无需训练的方法，通过预测去噪趋势和自适应地控制误差容限来加速扩散模型，从而在质量几乎没有下降的情况下实现显著的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification",
        "summary": "Text-to-image (T2I) models are increasingly used for synthetic dataset\ngeneration, but generating effective synthetic training data for classification\nremains challenging. Fine-tuning a T2I model with a few real examples can help\nimprove the quality of synthetic training data; however, it may also cause\noverfitting and reduce diversity in the generated samples. We propose a\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\nfine-grained classification. Given a small set of real examples, we first\nextract class-agnostic attributes such as scene background and object pose. We\nthen explicitly condition on these attributes during fine-tuning of the T2I\nmodel and marginalize them out during generation. This design mitigates\noverfitting, preserves the T2I model's generative prior, reduces estimation\nerrors, and further minimizes unintended inter-class associations. Extensive\nexperiments across multiple T2I models, backbones, and datasets show that our\nmethod achieves state-of-the-art performance in low-shot fine-grained\nclassification when augmented with synthetic data. Concretely, BOB outperforms\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\na CLIP classifier with five real images augmented with 100 synthetic images).\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\nimages augmented with BOB achieves better performance than fine-tuning with 10\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\nsettings, with 2+% accuracy improvements in 14 of these settings.",
        "url": "http://arxiv.org/abs/2510.24078v1",
        "published_date": "2025-10-28T05:40:14+00:00",
        "updated_date": "2025-10-28T05:40:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "William Yang",
            "Xindi Wu",
            "Zhiwei Deng",
            "Esin Tureci",
            "Olga Russakovsky"
        ],
        "tldr": "The paper introduces BOB, a fine-tuning strategy for text-to-image models, to generate high-quality synthetic data for fine-grained classification by conditioning on class-agnostic attributes, achieving state-of-the-art performance in low-shot settings.",
        "tldr_zh": "该论文介绍了一种名为BOB的微调策略，用于文本到图像模型，通过对与类别无关的属性进行条件约束，生成高质量的合成数据进行细粒度分类，并在小样本设置中实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Neural USD: An object-centric framework for iterative editing and control",
        "summary": "Amazing progress has been made in controllable generative modeling,\nespecially over the last few years. However, some challenges remain. One of\nthem is precise and iterative object editing. In many of the current methods,\ntrying to edit the generated image (for example, changing the color of a\nparticular object in the scene or changing the background while keeping other\nelements unchanged) by changing the conditioning signals often leads to\nunintended global changes in the scene. In this work, we take the first steps\nto address the above challenges. Taking inspiration from the Universal Scene\nDescriptor (USD) standard developed in the computer graphics community, we\nintroduce the \"Neural Universal Scene Descriptor\" or Neural USD. In this\nframework, we represent scenes and objects in a structured, hierarchical\nmanner. This accommodates diverse signals, minimizes model-specific\nconstraints, and enables per-object control over appearance, geometry, and\npose. We further apply a fine-tuning approach which ensures that the above\ncontrol signals are disentangled from one another. We evaluate several design\nconsiderations for our framework, demonstrating how Neural USD enables\niterative and incremental workflows. More information at:\nhttps://escontrela.me/neural_usd .",
        "url": "http://arxiv.org/abs/2510.23956v1",
        "published_date": "2025-10-28T00:19:42+00:00",
        "updated_date": "2025-10-28T00:19:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Alejandro Escontrela",
            "Shrinu Kushagra",
            "Sjoerd van Steenkiste",
            "Yulia Rubanova",
            "Aleksander Holynski",
            "Kelsey Allen",
            "Kevin Murphy",
            "Thomas Kipf"
        ],
        "tldr": "The paper introduces \"Neural USD,\" a framework for controllable generative modeling that represents scenes hierarchically, enabling iterative object editing with disentangled control over appearance, geometry, and pose. It addresses the problem of unintended global changes during local editing.",
        "tldr_zh": "该论文介绍了“神经USD”，一个用于可控生成建模的框架，它以分层的方式表示场景，从而能够对外观、几何形状和姿态进行解耦控制，实现迭代的对象编辑。它解决了局部编辑期间意外全局变化的问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Group Relative Attention Guidance for Image Editing",
        "summary": "Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.",
        "url": "http://arxiv.org/abs/2510.24657v1",
        "published_date": "2025-10-28T17:22:44+00:00",
        "updated_date": "2025-10-28T17:22:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuanpu Zhang",
            "Xuesong Niu",
            "Ruidong Chen",
            "Dan Song",
            "Jianhao Zeng",
            "Penghui Du",
            "Haoxiang Cao",
            "Kai Wu",
            "An-an Liu"
        ],
        "tldr": "The paper introduces Group Relative Attention Guidance (GRAG), a method to improve control over the intensity of image editing in Diffusion-in-Transformer models by reweighting attention deltas. It claims enhanced editing quality with minimal code integration and smoother control compared to Classifier-Free Guidance.",
        "tldr_zh": "该论文介绍了组相对注意力引导（GRAG），一种通过重新加权注意力增量来提高Diffusion-in-Transformer模型中图像编辑强度控制的方法。该方法声称以最少的代码集成增强编辑质量，并提供比无分类器引导更平滑的控制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TRELLISWorld: Training-Free World Generation from Object Generators",
        "summary": "Text-driven 3D scene generation holds promise for a wide range of\napplications, from virtual prototyping to AR/VR and simulation. However,\nexisting methods are often constrained to single-object generation, require\ndomain-specific training, or lack support for full 360-degree viewability. In\nthis work, we present a training-free approach to 3D scene synthesis by\nrepurposing general-purpose text-to-3D object diffusion models as modular tile\ngenerators. We reformulate scene generation as a multi-tile denoising problem,\nwhere overlapping 3D regions are independently generated and seamlessly blended\nvia weighted averaging. This enables scalable synthesis of large, coherent\nscenes while preserving local semantic control. Our method eliminates the need\nfor scene-level datasets or retraining, relies on minimal heuristics, and\ninherits the generalization capabilities of object-level priors. We demonstrate\nthat our approach supports diverse scene layouts, efficient generation, and\nflexible editing, establishing a simple yet powerful foundation for\ngeneral-purpose, language-driven 3D scene construction.",
        "url": "http://arxiv.org/abs/2510.23880v1",
        "published_date": "2025-10-27T21:40:31+00:00",
        "updated_date": "2025-10-27T21:40:31+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Hanke Chen",
            "Yuan Liu",
            "Minchen Li"
        ],
        "tldr": "The paper introduces TRELLISWorld, a training-free approach for 3D scene generation using text-to-3D object diffusion models as modular tile generators, enabling scalable synthesis of large, coherent scenes with semantic control.",
        "tldr_zh": "该论文介绍了一种名为 TRELLISWorld 的无需训练的 3D 场景生成方法，该方法利用文本到 3D 对象扩散模型作为模块化瓦片生成器，从而能够以语义控制的方式扩展合成大型、连贯的场景。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]