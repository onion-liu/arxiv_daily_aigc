[
    {
        "title": "BitDance: Scaling Autoregressive Generative Models with Binary Tokens",
        "summary": "We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to $2^{256}$ states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.",
        "url": "http://arxiv.org/abs/2602.14041v1",
        "published_date": "2026-02-15T08:09:05+00:00",
        "updated_date": "2026-02-15T08:09:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuang Ai",
            "Jiaming Han",
            "Shaobin Zhuang",
            "Weijia Mao",
            "Xuefeng Hu",
            "Ziyan Yang",
            "Zhenheng Yang",
            "Huaibo Huang",
            "Xiangyu Yue",
            "Hao Chen"
        ],
        "tldr": "BitDance is a scalable autoregressive image generator using binary tokens and a diffusion head for efficient sampling and parallel decoding, achieving state-of-the-art results on ImageNet and significant speedups in high-resolution image generation.",
        "tldr_zh": "BitDance是一个可扩展的自回归图像生成器，它使用二进制标记和扩散头来实现高效的采样和并行解码，在ImageNet上实现了最先进的结果，并在高分辨率图像生成方面实现了显著的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "AbracADDbra: Touch-Guided Object Addition by Decoupling Placement and Editing Subtasks",
        "summary": "Instruction-based object addition is often hindered by the ambiguity of text-only prompts or the tedious nature of mask-based inputs. To address this usability gap, we introduce AbracADDbra, a user-friendly framework that leverages intuitive touch priors to spatially ground succinct instructions for precise placement. Our efficient, decoupled architecture uses a vision-language transformer for touch-guided placement, followed by a diffusion model that jointly generates the object and an instance mask for high-fidelity blending. To facilitate standardized evaluation, we contribute the Touch2Add benchmark for this interactive task. Our extensive evaluations, where our placement model significantly outperforms both random placement and general-purpose VLM baselines, confirm the framework's ability to produce high-fidelity edits. Furthermore, our analysis reveals a strong correlation between initial placement accuracy and final edit quality, validating our decoupled approach. This work thus paves the way for more accessible and efficient creative tools.",
        "url": "http://arxiv.org/abs/2602.14237v1",
        "published_date": "2026-02-15T17:11:20+00:00",
        "updated_date": "2026-02-15T17:11:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kunal Swami",
            "Raghu Chittersu",
            "Yuvraj Rathore",
            "Rajeev Irny",
            "Shashavali Doodekula",
            "Alok Shukla"
        ],
        "tldr": "The paper introduces AbracADDbra, a framework for touch-guided object addition into images using a decoupled architecture of a vision-language transformer for placement and a diffusion model for blending, along with a new benchmark for evaluation.",
        "tldr_zh": "该论文介绍了一个名为AbracADDbra的框架，用于通过触摸引导将物体添加到图像中，采用解耦的架构，使用视觉-语言transformer进行放置，扩散模型进行融合，并提供了一个新的评估基准。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "UniRef-Image-Edit: Towards Scalable and Consistent Multi-Reference Image Editing",
        "summary": "We present UniRef-Image-Edit, a high-performance multi-modal generation system that unifies single-image editing and multi-image composition within a single framework. Existing diffusion-based editing methods often struggle to maintain consistency across multiple conditions due to limited interaction between reference inputs. To address this, we introduce Sequence-Extended Latent Fusion (SELF), a unified input representation that dynamically serializes multiple reference images into a coherent latent sequence. During a dedicated training stage, all reference images are jointly constrained to fit within a fixed-length sequence under a global pixel-budget constraint. Building upon SELF, we propose a two-stage training framework comprising supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we jointly train on single-image editing and multi-image composition tasks to establish a robust generative prior. We adopt a progressive sequence length training strategy, in which all input images are initially resized to a total pixel budget of $1024^2$, and are then gradually increased to $1536^2$ and $2048^2$ to improve visual fidelity and cross-reference consistency. This gradual relaxation of compression enables the model to incrementally capture finer visual details while maintaining stable alignment across references. For the RL stage, we introduce Multi-Source GRPO (MSGRPO), to our knowledge the first reinforcement learning framework tailored for multi-reference image generation. MSGRPO optimizes the model to reconcile conflicting visual constraints, significantly enhancing compositional consistency. We will open-source the code, models, training data, and reward data for community research purposes.",
        "url": "http://arxiv.org/abs/2602.14186v1",
        "published_date": "2026-02-15T15:24:03+00:00",
        "updated_date": "2026-02-15T15:24:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongyang Wei",
            "Bin Wen",
            "Yancheng Long",
            "Yankai Yang",
            "Yuhang Hu",
            "Tianke Zhang",
            "Wei Chen",
            "Haonan Fan",
            "Kaiyu Jiang",
            "Jiankang Chen",
            "Changyi Liu",
            "Kaiyu Tang",
            "Haojie Ding",
            "Xiao Yang",
            "Jia Sun",
            "Huaiqing Wang",
            "Zhenyu Yang",
            "Xinyu Wei",
            "Xianglong He",
            "Yangguang Li",
            "Fan Yang",
            "Tingting Gao",
            "Lei Zhang",
            "Guorui Zhou",
            "Han Li"
        ],
        "tldr": "The paper introduces UniRef-Image-Edit, a multi-modal framework for single and multi-image editing using Sequence-Extended Latent Fusion (SELF) and Multi-Source GRPO (MSGRPO), aiming to improve consistency and fidelity in multi-reference image generation and offering open-source resources.",
        "tldr_zh": "该论文介绍了 UniRef-Image-Edit，一个多模态框架，使用序列扩展潜在融合 (SELF) 和多源 GRPO (MSGRPO) 进行单图像和多图像编辑，旨在提高多参考图像生成中的一致性和保真度，并提供开源资源。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniWeTok: An Unified Binary Tokenizer with Codebook Size $\\mathit{2^{128}}$ for Unified Multimodal Large Language Model",
        "summary": "Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook ($\\mathit{2^{128}}$). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.",
        "url": "http://arxiv.org/abs/2602.14178v1",
        "published_date": "2026-02-15T15:07:19+00:00",
        "updated_date": "2026-02-15T15:07:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shaobin Zhuang",
            "Yuang Ai",
            "Jiaming Han",
            "Weijia Mao",
            "Xiaohui Li",
            "Fangyikang Wang",
            "Xiao Wang",
            "Yan Li",
            "Shanchuan Lin",
            "Kun Xu",
            "Zhenheng Yang",
            "Huaibo Huang",
            "Xiangyu Yue",
            "Hao Chen",
            "Yali Wang"
        ],
        "tldr": "The paper introduces UniWeTok, a unified discrete tokenizer with a massive binary codebook for multimodal large language models, achieving state-of-the-art image generation performance with remarkably low training compute and competitive performance across a range of multimodal tasks.",
        "tldr_zh": "该论文介绍了UniWeTok，一种用于多模态大型语言模型的统一离散tokenizer，其拥有巨大的二元码本。它在图像生成方面实现了最先进的性能，同时训练算力需求极低，并在各种多模态任务中表现出强大的竞争力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "When Test-Time Guidance Is Enough: Fast Image and Video Editing with Diffusion Guidance",
        "summary": "Text-driven image and video editing can be naturally cast as inpainting problems, where masked regions are reconstructed to remain consistent with both the observed content and the editing prompt. Recent advances in test-time guidance for diffusion and flow models provide a principled framework for this task; however, existing methods rely on costly vector--Jacobian product (VJP) computations to approximate the intractable guidance term, limiting their practical applicability. Building upon the recent work of Moufad et al. (2025), we provide theoretical insights into their VJP-free approximation and substantially extend their empirical evaluation to large-scale image and video editing benchmarks. Our results demonstrate that test-time guidance alone can achieve performance comparable to, and in some cases surpass, training-based methods.",
        "url": "http://arxiv.org/abs/2602.14157v1",
        "published_date": "2026-02-15T14:13:25+00:00",
        "updated_date": "2026-02-15T14:13:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ahmed Ghorbel",
            "Badr Moufad",
            "Navid Bagheri Shouraki",
            "Alain Oliviero Durmus",
            "Thomas Hirtz",
            "Eric Moulines",
            "Jimmy Olsson",
            "Yazid Janati"
        ],
        "tldr": "This paper presents a fast, VJP-free test-time guidance method for text-driven image and video editing using diffusion models, achieving comparable or better performance than training-based methods. It builds upon and extends the work of Moufad et al. (2025).",
        "tldr_zh": "本文提出了一种快速、无VJP的测试时引导方法，用于基于扩散模型的文本驱动图像和视频编辑，其性能与基于训练的方法相当甚至更好。该方法建立并扩展了 Moufad et al. (2025) 的工作。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models",
        "summary": "Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing.",
        "url": "http://arxiv.org/abs/2602.14147v1",
        "published_date": "2026-02-15T13:52:45+00:00",
        "updated_date": "2026-02-15T13:52:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shufan Li",
            "Yuchen Zhu",
            "Jiuxiang Gu",
            "Kangning Liu",
            "Zhe Lin",
            "Yongxin Chen",
            "Molei Tao",
            "Aditya Grover",
            "Jason Kuen"
        ],
        "tldr": "LaViDa-R1 is a novel multimodal reasoning diffusion language model trained with a unified post-training framework incorporating supervised finetuning and multi-task reinforcement learning, demonstrating strong performance across various multimodal tasks.",
        "tldr_zh": "LaViDa-R1是一种新型的多模态推理扩散语言模型，它采用统一的后训练框架，结合了监督式微调和多任务强化学习，并在多种多模态任务中表现出强大的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Train Short, Inference Long: Training-free Horizon Extension for Autoregressive Video Generation",
        "summary": "Autoregressive video diffusion models have emerged as a scalable paradigm for long video generation. However, they often suffer from severe extrapolation failure, where rapid error accumulation leads to significant temporal degradation when extending beyond training horizons. We identify that this failure primarily stems from the \\textit{spectral bias} of 3D positional embeddings and the lack of \\textit{dynamic priors} in noise sampling. To address these issues, we propose \\textbf{FLEX} (\\textbf{F}requency-aware \\textbf{L}ength \\textbf{EX}tension), a training-free inference-time framework that bridges the gap between short-term training and long-term inference. FLEX introduces Frequency-aware RoPE Modulation to adaptively interpolate under-trained low-frequency components while extrapolating high-frequency ones to preserve multi-scale temporal discriminability. This is integrated with Antiphase Noise Sampling (ANS) to inject high-frequency dynamic priors and Inference-only Attention Sink to anchor global structure. Extensive evaluations on VBench demonstrate that FLEX significantly outperforms state-of-the-art models at $6\\times$ extrapolation (30s duration) and matches the performance of long-video fine-tuned baselines at $12\\times$ scale (60s duration). As a plug-and-play augmentation, FLEX seamlessly integrates into existing inference pipelines for horizon extension. It effectively pushes the generation limits of models such as LongLive, supporting consistent and dynamic video synthesis at a 4-minute scale. Project page is available at \\href{https://ga-lee.github.io/FLEX_demo}{https://ga-lee.github.io/FLEX}.",
        "url": "http://arxiv.org/abs/2602.14027v1",
        "published_date": "2026-02-15T07:14:47+00:00",
        "updated_date": "2026-02-15T07:14:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jia Li",
            "Xiaomeng Fu",
            "Xurui Peng",
            "Weifeng Chen",
            "Youwei Zheng",
            "Tianyu Zhao",
            "Jiexi Wang",
            "Fangmin Chen",
            "Xing Wang",
            "Hayden Kwok-Hay So"
        ],
        "tldr": "The paper introduces FLEX, a training-free inference-time framework for extending the horizon of autoregressive video generation models, addressing the temporal degradation issue caused by spectral bias and lack of dynamic priors.",
        "tldr_zh": "该论文介绍了FLEX，一个无需训练的推理期框架，用于扩展自回归视频生成模型的视野，解决了由频谱偏差和缺乏动态先验引起的时序退化问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Elastic Diffusion Transformer",
        "summary": "Diffusion Transformers (DiT) have demonstrated remarkable generative capabilities but remain highly computationally expensive. Previous acceleration methods, such as pruning and distillation, typically rely on a fixed computational capacity, leading to insufficient acceleration and degraded generation quality. To address this limitation, we propose \\textbf{Elastic Diffusion Transformer (E-DiT)}, an adaptive acceleration framework for DiT that effectively improves efficiency while maintaining generation quality. Specifically, we observe that the generative process of DiT exhibits substantial sparsity (i.e., some computations can be skipped with minimal impact on quality), and this sparsity varies significantly across samples. Motivated by this observation, E-DiT equips each DiT block with a lightweight router that dynamically identifies sample-dependent sparsity from the input latent. Each router adaptively determines whether the corresponding block can be skipped. If the block is not skipped, the router then predicts the optimal MLP width reduction ratio within the block. During inference, we further introduce a block-level feature caching mechanism that leverages router predictions to eliminate redundant computations in a training-free manner. Extensive experiments across 2D image (Qwen-Image and FLUX) and 3D asset (Hunyuan3D-3.0) demonstrate the effectiveness of E-DiT, achieving up to $\\sim$2$\\times$ speedup with negligible loss in generation quality. Code will be available at https://github.com/wangjiangshan0725/Elastic-DiT.",
        "url": "http://arxiv.org/abs/2602.13993v1",
        "published_date": "2026-02-15T05:19:17+00:00",
        "updated_date": "2026-02-15T05:19:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiangshan Wang",
            "Zeqiang Lai",
            "Jiarui Chen",
            "Jiayi Guo",
            "Hang Guo",
            "Xiu Li",
            "Xiangyu Yue",
            "Chunchao Guo"
        ],
        "tldr": "The paper introduces Elastic Diffusion Transformer (E-DiT), an adaptive acceleration framework for Diffusion Transformers (DiT) that dynamically skips blocks and reduces MLP width based on input, achieving up to 2x speedup with minimal quality loss.",
        "tldr_zh": "该论文介绍了弹性扩散Transformer（E-DiT），一种用于扩散Transformer（DiT）的自适应加速框架，它基于输入动态跳过块并减少MLP宽度，从而在质量损失最小的情况下实现高达2倍的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]