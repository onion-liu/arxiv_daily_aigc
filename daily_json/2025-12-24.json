[
    {
        "title": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation",
        "summary": "Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.\n  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.\n  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.",
        "url": "http://arxiv.org/abs/2512.20362v1",
        "published_date": "2025-12-23T13:44:41+00:00",
        "updated_date": "2025-12-23T13:44:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "V. Kovalev",
            "A. Kuvshinov",
            "A. Buzovkin",
            "D. Pokidov",
            "D. Timonin"
        ],
        "tldr": "The paper introduces CRAFT, a training-free framework for multimodal text-to-image generation that uses structured reasoning and agent-based feedback to refine generated images based on constraint verification, significantly improving accuracy and quality with minimal overhead.",
        "tldr_zh": "该论文介绍了CRAFT，一个无需训练的框架，用于多模态文本到图像生成，它使用结构化推理和基于代理的反馈，基于约束验证来细化生成的图像，从而显著提高准确性和质量，同时开销极小。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs",
        "summary": "Generating realistic and diverse LiDAR point clouds is crucial for autonomous driving simulation. Although previous methods achieve LiDAR point cloud generation from user inputs, they struggle to attain high-quality results while enabling versatile controllability, due to the imbalance between the complex distribution of LiDAR point clouds and the simple control signals. To address the limitation, we propose LiDARDraft, which utilizes the 3D layout to build a bridge between versatile conditional signals and LiDAR point clouds. The 3D layout can be trivially generated from various user inputs such as textual descriptions and images. Specifically, we represent text, images, and point clouds as unified 3D layouts, which are further transformed into semantic and depth control signals. Then, we employ a rangemap-based ControlNet to guide LiDAR point cloud generation. This pixel-level alignment approach demonstrates excellent performance in controllable LiDAR point clouds generation, enabling \"simulation from scratch\", allowing self-driving environments to be created from arbitrary textual descriptions, images and sketches.",
        "url": "http://arxiv.org/abs/2512.20105v1",
        "published_date": "2025-12-23T07:03:31+00:00",
        "updated_date": "2025-12-23T07:03:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haiyun Wei",
            "Fan Lu",
            "Yunwei Zhu",
            "Zehan Zheng",
            "Weiyi Xue",
            "Lin Shao",
            "Xudong Zhang",
            "Ya Wu",
            "Rong Fu",
            "Guang Chen"
        ],
        "tldr": "LiDARDraft generates LiDAR point clouds from versatile inputs (text, images) by using a 3D layout as an intermediate representation and a rangemap-based ControlNet for generation. This allows for controllable generation of LiDAR data from scratch.",
        "tldr_zh": "LiDARDraft通过使用3D布局作为中间表示和基于范围地图的ControlNet，从多种输入（文本，图像）生成LiDAR点云。这使得从头开始可控地生成LiDAR数据。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs",
        "summary": "We present FlashLips, a two-stage, mask-free lip-sync system that decouples lips control from rendering and achieves real-time performance running at over 100 FPS on a single GPU, while matching the visual quality of larger state-of-the-art models. Stage 1 is a compact, one-step latent-space editor that reconstructs an image using a reference identity, a masked target frame, and a low-dimensional lips-pose vector, trained purely with reconstruction losses - no GANs or diffusion. To remove explicit masks at inference, we use self-supervision: we generate mouth-altered variants of the target image, that serve as pseudo ground truth for fine-tuning, teaching the network to localize edits to the lips while preserving the rest. Stage 2 is an audio-to-pose transformer trained with a flow-matching objective to predict lips-poses vectors from speech. Together, these stages form a simple and stable pipeline that combines deterministic reconstruction with robust audio control, delivering high perceptual quality and faster-than-real-time speed.",
        "url": "http://arxiv.org/abs/2512.20033v1",
        "published_date": "2025-12-23T03:54:48+00:00",
        "updated_date": "2025-12-23T03:54:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Andreas Zinonos",
            "Michał Stypułkowski",
            "Antoni Bigata",
            "Stavros Petridis",
            "Maja Pantic",
            "Nikita Drobyshev"
        ],
        "tldr": "FlashLips is a novel lip-sync system achieving real-time performance (100+ FPS) via a two-stage approach using reconstruction and self-supervision, avoiding GANs or diffusion models.",
        "tldr_zh": "FlashLips 是一种新型唇形同步系统，通过使用重建和自监督的两阶段方法，实现实时性能（100+ FPS），避免了 GAN 或扩散模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SE360: Semantic Edit in 360$^\\circ$ Panoramas via Hierarchical Data Construction",
        "summary": "While instruction-based image editing is emerging, extending it to 360$^\\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.",
        "url": "http://arxiv.org/abs/2512.19943v1",
        "published_date": "2025-12-23T00:24:46+00:00",
        "updated_date": "2025-12-23T00:24:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoyi Zhong",
            "Fang-Lue Zhang",
            "Andrew Chalmers",
            "Taehyun Rhee"
        ],
        "tldr": "The paper introduces SE360, a novel framework for instruction-based object editing in 360° panoramas, using an autonomous data generation pipeline and a Transformer-based diffusion model. It outperforms existing methods in visual quality and semantic accuracy.",
        "tldr_zh": "本文介绍了SE360，一个用于360°全景图像中基于指令的对象编辑的新框架，它使用自主数据生成流程和基于Transformer的扩散模型。该方法在视觉质量和语义准确性方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models",
        "summary": "Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \\textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.",
        "url": "http://arxiv.org/abs/2512.19686v1",
        "published_date": "2025-12-22T18:59:03+00:00",
        "updated_date": "2025-12-22T18:59:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixuan Ye",
            "Quande Liu",
            "Cong Wei",
            "Yuanxing Zhang",
            "Xintao Wang",
            "Pengfei Wan",
            "Kun Gai",
            "Wenhan Luo"
        ],
        "tldr": "This paper introduces a visual-aware Chain-of-Thought (CoT) method for unified models to improve visual context consistency in multi-modal generation by using adaptive visual planning and iterative visual correction.",
        "tldr_zh": "本文提出了一种视觉感知的思维链 (CoT) 方法，通过自适应视觉规划和迭代视觉校正，改进统一模型中多模态生成时的视觉上下文一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VA-$π$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
        "summary": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-$π$, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-$π$ formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-$π$ introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-$π$ enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.",
        "url": "http://arxiv.org/abs/2512.19680v1",
        "published_date": "2025-12-22T18:54:30+00:00",
        "updated_date": "2025-12-22T18:54:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyao Liao",
            "Qiyuan He",
            "Kai Xu",
            "Xiaoye Qu",
            "Yicong Li",
            "Wei Wei",
            "Angela Yao"
        ],
        "tldr": "The paper introduces VA-$π$, a post-training framework leveraging variational optimization and reinforcement learning to align autoregressive image generators with pixel-space objectives, resulting in improved image quality and consistency.",
        "tldr_zh": "该论文介绍了VA-$π$，一个利用变分优化和强化学习的后训练框架，旨在将自回归图像生成器与像素空间目标对齐，从而提高图像质量和一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
        "summary": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.",
        "url": "http://arxiv.org/abs/2512.19678v1",
        "published_date": "2025-12-22T18:53:50+00:00",
        "updated_date": "2025-12-22T18:53:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hanyang Kong",
            "Xingyi Yang",
            "Xiaoxu Zheng",
            "Xinchao Wang"
        ],
        "tldr": "WorldWarp introduces a framework for geometrically consistent video generation by coupling a 3D geometric cache with a 2D diffusion model that uses a novel spatio-temporal noise schedule for efficient refinement and generation.",
        "tldr_zh": "WorldWarp 提出了一个几何一致的视频生成框架，该框架将 3D 几何缓存与 2D 扩散模型相结合，使用一种新颖的时空噪声调度方法来实现高效的细化和生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Over++: Generative Video Compositing for Layer Interaction Effects",
        "summary": "In professional video compositing workflows, artists must manually create environmental interactions-such as shadows, reflections, dust, and splashes-between foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, a new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, a video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct a paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation.",
        "url": "http://arxiv.org/abs/2512.19661v1",
        "published_date": "2025-12-22T18:39:58+00:00",
        "updated_date": "2025-12-22T18:39:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Luchao Qi",
            "Jiaye Wu",
            "Jun Myeong Choi",
            "Cary Phillips",
            "Roni Sengupta",
            "Dan B Goldman"
        ],
        "tldr": "The paper introduces Over++, a video effect generation framework for compositing realistic environmental interactions between video layers based on text prompts, without requiring camera pose or depth information, and demonstrates its effectiveness on a newly constructed dataset.",
        "tldr_zh": "该论文介绍了Over++，一个视频特效生成框架，用于基于文本提示在视频图层之间合成真实的环境交互，无需相机姿态或深度信息，并在一个新建的数据集上展示了其有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Generative diffusion models for agricultural AI: plant image generation, indoor-to-outdoor translation, and expert preference alignment",
        "summary": "The success of agricultural artificial intelligence depends heavily on large, diverse, and high-quality plant image datasets, yet collecting such data in real field conditions is costly, labor intensive, and seasonally constrained. This paper investigates diffusion-based generative modeling to address these challenges through plant image synthesis, indoor-to-outdoor translation, and expert preference aligned fine tuning. First, a Stable Diffusion model is fine tuned on captioned indoor and outdoor plant imagery to generate realistic, text conditioned images of canola and soybean. Evaluation using Inception Score, Frechet Inception Distance, and downstream phenotype classification shows that synthetic images effectively augment training data and improve accuracy. Second, we bridge the gap between high resolution indoor datasets and limited outdoor imagery using DreamBooth-based text inversion and image guided diffusion, generating translated images that enhance weed detection and classification with YOLOv8. Finally, a preference guided fine tuning framework trains a reward model on expert scores and applies reward weighted updates to produce more stable and expert aligned outputs. Together, these components demonstrate a practical pathway toward data efficient generative pipelines for agricultural AI.",
        "url": "http://arxiv.org/abs/2512.19632v1",
        "published_date": "2025-12-22T18:07:08+00:00",
        "updated_date": "2025-12-22T18:07:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Da Tan",
            "Michael Beck",
            "Christopher P. Bidinosti",
            "Robert H. Gulden",
            "Christopher J. Henry"
        ],
        "tldr": "This paper explores using diffusion models for agricultural AI, specifically plant image generation, domain translation (indoor to outdoor), and expert preference alignment, demonstrating improved performance in downstream tasks.",
        "tldr_zh": "本文探讨了使用扩散模型进行农业人工智能，特别是植物图像生成、领域转换（室内到室外）和专家偏好对齐，并展示了在下游任务中性能的提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images",
        "summary": "AI-assisted graphic design has emerged as a powerful tool for automating the creation and editing of design elements such as posters, banners, and advertisements. While diffusion-based text-to-image models have demonstrated strong capabilities in visual content generation, their text rendering performance, particularly for small-scale typography and non-Latin scripts, remains limited. In this paper, we propose UTDesign, a unified framework for high-precision stylized text editing and conditional text generation in design images, supporting both English and Chinese scripts. Our framework introduces a novel DiT-based text style transfer model trained from scratch on a synthetic dataset, capable of generating transparent RGBA text foregrounds that preserve the style of reference glyphs. We further extend this model into a conditional text generation framework by training a multi-modal condition encoder on a curated dataset with detailed text annotations, enabling accurate, style-consistent text synthesis conditioned on background images, prompts, and layout specifications. Finally, we integrate our approach into a fully automated text-to-design (T2D) pipeline by incorporating pre-trained text-to-image (T2I) models and an MLLM-based layout planner. Extensive experiments demonstrate that UTDesign achieves state-of-the-art performance among open-source methods in terms of stylistic consistency and text accuracy, and also exhibits unique advantages compared to proprietary commercial approaches. Code and data for this paper are available at https://github.com/ZYM-PKU/UTDesign.",
        "url": "http://arxiv.org/abs/2512.20479v1",
        "published_date": "2025-12-23T16:13:55+00:00",
        "updated_date": "2025-12-23T16:13:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiming Zhao",
            "Yuanpeng Gao",
            "Yuxuan Luo",
            "Jiwei Duan",
            "Shisong Lin",
            "Longfei Xiong",
            "Zhouhui Lian"
        ],
        "tldr": "The paper introduces UTDesign, a unified framework for stylized text editing and conditional text generation in graphic design images, particularly improving text rendering for small-scale typography and non-Latin scripts, potentially useful in multimodal generation scenarios.",
        "tldr_zh": "该论文介绍了UTDesign，一个统一的框架，用于在图形设计图像中进行风格化文本编辑和条件文本生成，特别是在小型排版和非拉丁文字的文本渲染方面有所改进，可能在多模态生成场景中很有用。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
        "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2512.19693v1",
        "published_date": "2025-12-22T18:59:57+00:00",
        "updated_date": "2025-12-22T18:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weichen Fan",
            "Haiwen Diao",
            "Quan Wang",
            "Dahua Lin",
            "Ziwei Liu"
        ],
        "tldr": "This paper proposes the \"Prism Hypothesis\" to unify semantic and pixel representations using a novel Unified Autoencoding (UAE) model with a frequency-band modulator, achieving SOTA performance on ImageNet and MS-COCO.",
        "tldr_zh": "这篇论文提出了“棱镜假设”，通过一个创新的统一自动编码（UAE）模型，利用频率带调制器来统一语义和像素表示，并在ImageNet和MS-COCO上实现了SOTA性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models",
        "summary": "Generating realistic human-human interactions is a challenging task that requires not only high-quality individual body and hand motions, but also coherent coordination among all interactants. Due to limitations in available data and increased learning complexity, previous methods tend to ignore hand motions, limiting the realism and expressivity of the interactions. Additionally, current diffusion-based approaches generate entire motion sequences simultaneously, limiting their ability to capture the reactive and adaptive nature of human interactions. To address these limitations, we introduce Interact2Ar, the first end-to-end text-conditioned autoregressive diffusion model for generating full-body, human-human interactions. Interact2Ar incorporates detailed hand kinematics through dedicated parallel branches, enabling high-fidelity full-body generation. Furthermore, we introduce an autoregressive pipeline coupled with a novel memory technique that facilitates adaptation to the inherent variability of human interactions using efficient large context windows. The adaptability of our model enables a series of downstream applications, including temporal motion composition, real-time adaptation to disturbances, and extension beyond dyadic to multi-person scenarios. To validate the generated motions, we introduce a set of robust evaluators and extended metrics designed specifically for assessing full-body interactions. Through quantitative and qualitative experiments, we demonstrate the state-of-the-art performance of Interact2Ar.",
        "url": "http://arxiv.org/abs/2512.19692v1",
        "published_date": "2025-12-22T18:59:50+00:00",
        "updated_date": "2025-12-22T18:59:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pablo Ruiz-Ponce",
            "Sergio Escalera",
            "José García-Rodríguez",
            "Jiankang Deng",
            "Rolandos Alexandros Potamias"
        ],
        "tldr": "The paper introduces Interact2Ar, an autoregressive diffusion model for generating realistic full-body human-human interactions, incorporating detailed hand motions and adapting to interaction variability using a novel memory technique.",
        "tldr_zh": "该论文介绍了Interact2Ar，一种用于生成逼真全身人际互动的自回归扩散模型，该模型结合了详细的手部动作，并使用一种新的记忆技术来适应互动中的可变性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]