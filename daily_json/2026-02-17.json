[
    {
        "title": "Controlling Your Image via Simplified Vector Graphics",
        "summary": "Recent advances in image generation have achieved remarkable visual quality, while a fundamental challenge remains: Can image generation be controlled at the element level, enabling intuitive modifications such as adjusting shapes, altering colors, or adding and removing objects? In this work, we address this challenge by introducing layer-wise controllable generation through simplified vector graphics (VGs). Our approach first efficiently parses images into hierarchical VG representations that are semantic-aligned and structurally coherent. Building on this representation, we design a novel image synthesis framework guided by VGs, allowing users to freely modify elements and seamlessly translate these edits into photorealistic outputs. By leveraging the structural and semantic features of VGs in conjunction with noise prediction, our method provides precise control over geometry, color, and object semantics. Extensive experiments demonstrate the effectiveness of our approach in diverse applications, including image editing, object-level manipulation, and fine-grained content creation, establishing a new paradigm for controllable image generation. Project page: https://guolanqing.github.io/Vec2Pix/",
        "url": "http://arxiv.org/abs/2602.14443v1",
        "published_date": "2026-02-16T03:56:42+00:00",
        "updated_date": "2026-02-16T03:56:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lanqing Guo",
            "Xi Liu",
            "Yufei Wang",
            "Zhihao Li",
            "Siyu Huang"
        ],
        "tldr": "This paper introduces a method for layer-wise controllable image generation by parsing images into simplified vector graphics, allowing for intuitive element-level image editing and manipulation.",
        "tldr_zh": "本文介绍了一种通过将图像解析为简化的矢量图形来实现分层控制的图像生成方法，从而实现直观的元素级图像编辑和操作。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories",
        "summary": "Maintaining spatial world consistency over long horizons remains a central challenge for camera-controllable video generation. Existing memory-based approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing a global 3D scene from multiple views inevitably introduces cross-view misalignment, as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through a multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval.",
        "url": "http://arxiv.org/abs/2602.14941v1",
        "published_date": "2026-02-16T17:23:08+00:00",
        "updated_date": "2026-02-16T17:23:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zun Wang",
            "Han Lin",
            "Jaehong Yoon",
            "Jaemin Cho",
            "Yue Zhang",
            "Mohit Bansal"
        ],
        "tldr": "AnchorWeave improves long-term consistency in camera-controllable video generation by using multiple local geometric memories instead of a single, misaligned global memory, reconciling cross-view inconsistencies during the generation process.",
        "tldr_zh": "AnchorWeave通过使用多个局部几何记忆代替单个错位的全局记忆来改善相机可控视频生成中的长期一致性，并在生成过程中协调跨视点的不一致。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SketchingReality: From Freehand Scene Sketches To Photorealistic Images",
        "summary": "Recent years have witnessed remarkable progress in generative AI, with natural language emerging as the most common conditioning input. As underlying models grow more powerful, researchers are exploring increasingly diverse conditioning signals, such as depth maps, edge maps, camera parameters, and reference images, to give users finer control over generation. Among different modalities, sketches are a natural and long-standing form of human communication, enabling rapid expression of visual concepts. Previous literature has largely focused on edge maps, often misnamed 'sketches', yet algorithms that effectively handle true freehand sketches, with their inherent abstraction and distortions, remain underexplored. We pursue the challenging goal of balancing photorealism with sketch adherence when generating images from freehand input. A key obstacle is the absence of ground-truth, pixel-aligned images: by their nature, freehand sketches do not have a single correct alignment. To address this, we propose a modulation-based approach that prioritizes semantic interpretation of the sketch over strict adherence to individual edge positions. We further introduce a novel loss that enables training on freehand sketches without requiring ground-truth pixel-aligned images. We show that our method outperforms existing approaches in both semantic alignment with freehand sketch inputs and in the realism and overall quality of the generated images.",
        "url": "http://arxiv.org/abs/2602.14648v1",
        "published_date": "2026-02-16T11:13:34+00:00",
        "updated_date": "2026-02-16T11:13:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ahmed Bourouis",
            "Mikhail Bessmeltsev",
            "Yulia Gryaditskaya"
        ],
        "tldr": "This paper presents a novel approach to generate photorealistic images from freehand sketches using a modulation-based method and a new loss function that doesn't require pixel-aligned ground truth images, achieving superior semantic alignment and image quality compared to existing methods.",
        "tldr_zh": "本文提出了一种新的方法，通过基于调制的模型和一个不需要像素对齐的真值图像的新损失函数，从手绘草图生成照片级真实感的图像。该方法在语义对齐和图像质量方面优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation",
        "summary": "Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards. Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and text-motion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-of-Motion (CoM), a test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets, MoUnd-CoT-140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over state-of-the-art baselines. Code: https://github.com/AIGeeksGroup/MoRL. Website: https://aigeeksgroup.github.io/MoRL.",
        "url": "http://arxiv.org/abs/2602.14534v1",
        "published_date": "2026-02-16T07:42:45+00:00",
        "updated_date": "2026-02-16T07:42:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongpeng Wang",
            "Zeyu Zhang",
            "Wenhao Li",
            "Hao Tang"
        ],
        "tldr": "The paper introduces MoRL, a unified multimodal motion model trained with reinforcement learning and Chain-of-Motion for improved human motion understanding and generation, showing gains on standard datasets.",
        "tldr_zh": "该论文介绍了MoRL，一个统一的多模态运动模型，通过强化学习和Chain-of-Motion进行训练，以提高人体运动的理解和生成，并在标准数据集上取得了进展。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MedVAR: Towards Scalable and Efficient Medical Image Generation via Next-scale Autoregressive Prediction",
        "summary": "Medical image generation is pivotal in applications like data augmentation for low-resource clinical tasks and privacy-preserving data sharing. However, developing a scalable generative backbone for medical imaging requires architectural efficiency, sufficient multi-organ data, and principled evaluation, yet current approaches leave these aspects unresolved. Therefore, we introduce MedVAR, the first autoregressive-based foundation model that adopts the next-scale prediction paradigm to enable fast and scale-up-friendly medical image synthesis. MedVAR generates images in a coarse-to-fine manner and produces structured multi-scale representations suitable for downstream use. To support hierarchical generation, we curate a harmonized dataset of around 440,000 CT and MRI images spanning six anatomical regions. Comprehensive experiments across fidelity, diversity, and scalability show that MedVAR achieves state-of-the-art generative performance and offers a promising architectural direction for future medical generative foundation models.",
        "url": "http://arxiv.org/abs/2602.14512v1",
        "published_date": "2026-02-16T06:48:48+00:00",
        "updated_date": "2026-02-16T06:48:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhicheng He",
            "Yunpeng Zhao",
            "Junde Wu",
            "Ziwei Niu",
            "Zijun Li",
            "Lanfen Lin",
            "Yueming Jin"
        ],
        "tldr": "MedVAR is a novel autoregressive foundation model for scalable and efficient medical image generation, utilizing next-scale prediction and a large multi-organ dataset to achieve state-of-the-art performance.",
        "tldr_zh": "MedVAR是一种新颖的自回归基础模型，用于可扩展和高效的医学图像生成，它利用下一尺度的预测和一个大型多器官数据集来实现最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoCoDiff: Correspondence-Consistent Diffusion Model for Fine-grained Style Transfer",
        "summary": "Transferring visual style between images while preserving semantic correspondence between similar objects remains a central challenge in computer vision. While existing methods have made great strides, most of them operate at global level but overlook region-wise and even pixel-wise semantic correspondence. To address this, we propose CoCoDiff, a novel training-free and low-cost style transfer framework that leverages pretrained latent diffusion models to achieve fine-grained, semantically consistent stylization. We identify that correspondence cues within generative diffusion models are under-explored and that content consistency across semantically matched regions is often neglected. CoCoDiff introduces a pixel-wise semantic correspondence module that mines intermediate diffusion features to construct a dense alignment map between content and style images. Furthermore, a cycle-consistency module then enforces structural and perceptual alignment across iterations, yielding object and region level stylization that preserves geometry and detail. Despite requiring no additional training or supervision, CoCoDiff delivers state-of-the-art visual quality and strong quantitative results, outperforming methods that rely on extra training or annotations.",
        "url": "http://arxiv.org/abs/2602.14464v1",
        "published_date": "2026-02-16T04:52:29+00:00",
        "updated_date": "2026-02-16T04:52:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wenbo Nie",
            "Zixiang Li",
            "Renshuai Tao",
            "Bin Wu",
            "Yunchao Wei",
            "Yao Zhao"
        ],
        "tldr": "CoCoDiff is a training-free style transfer method that leverages pretrained diffusion models and pixel-wise semantic correspondence to achieve fine-grained and semantically consistent stylization, outperforming existing methods.",
        "tldr_zh": "CoCoDiff 是一种无需训练的风格迁移方法，它利用预训练的扩散模型和像素级的语义对应关系来实现细粒度和语义一致的风格化，优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Exposing Diversity Bias in Deep Generative Models: Statistical Origins and Correction of Diversity Error",
        "summary": "Deep generative models have achieved great success in producing high-quality samples, making them a central tool across machine learning applications. Beyond sample quality, an important yet less systematically studied question is whether trained generative models faithfully capture the diversity of the underlying data distribution. In this work, we address this question by directly comparing the diversity of samples generated by state-of-the-art models with that of test samples drawn from the target data distribution, using recently proposed reference-free entropy-based diversity scores, Vendi and RKE. Across multiple benchmark datasets, we find that test data consistently attains substantially higher Vendi and RKE diversity scores than the generated samples, suggesting a systematic downward diversity bias in modern generative models. To understand the origin of this bias, we analyze the finite-sample behavior of entropy-based diversity scores and show that their expected values increase with sample size, implying that diversity estimated from finite training sets could inherently underestimate the diversity of the true distribution. As a result, optimizing the generators to minimize divergence to empirical data distributions would induce a loss of diversity. Finally, we discuss potential diversity-aware regularization and guidance strategies based on Vendi and RKE as principled directions for mitigating this bias, and provide empirical evidence suggesting their potential to improve the results.",
        "url": "http://arxiv.org/abs/2602.14682v1",
        "published_date": "2026-02-16T12:15:34+00:00",
        "updated_date": "2026-02-16T12:15:34+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "math.OC"
        ],
        "authors": [
            "Farzan Farnia",
            "Mohammad Jalali",
            "Azim Ospanov"
        ],
        "tldr": "This paper identifies and analyzes a systematic downward diversity bias in deep generative models, proposing potential correction strategies using entropy-based diversity scores.",
        "tldr_zh": "该论文识别并分析了深度生成模型中存在的系统性向下多样性偏差，并提出了利用基于熵的多样性评分来纠正此偏差的潜在策略。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]