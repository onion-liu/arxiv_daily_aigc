[
    {
        "title": "Diffusion Transformers with Representation Autoencoders",
        "summary": "Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for\nDiffusion Transformers (DiT); however, the autoencoder component has barely\nevolved. Most DiTs continue to rely on the original VAE encoder, which\nintroduces several limitations: outdated backbones that compromise\narchitectural simplicity, low-dimensional latent spaces that restrict\ninformation capacity, and weak representations that result from purely\nreconstruction-based training and ultimately limit generative quality. In this\nwork, we explore replacing the VAE with pretrained representation encoders\n(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term\nRepresentation Autoencoders (RAEs). These models provide both high-quality\nreconstructions and semantically rich latent spaces, while allowing for a\nscalable transformer-based architecture. Since these latent spaces are\ntypically high-dimensional, a key challenge is enabling diffusion transformers\nto operate effectively within them. We analyze the sources of this difficulty,\npropose theoretically motivated solutions, and validate them empirically. Our\napproach achieves faster convergence without auxiliary representation alignment\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\nachieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\nclear advantages and should be the new default for diffusion transformer\ntraining.",
        "url": "http://arxiv.org/abs/2510.11690v1",
        "published_date": "2025-10-13T17:51:39+00:00",
        "updated_date": "2025-10-13T17:51:39+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Boyang Zheng",
            "Nanye Ma",
            "Shengbang Tong",
            "Saining Xie"
        ],
        "tldr": "This paper introduces Representation Autoencoders (RAEs) for Diffusion Transformers, replacing traditional VAEs with pretrained representation encoders to improve image generation quality and achieve state-of-the-art results on ImageNet.",
        "tldr_zh": "本文介绍了用于扩散Transformer的表征自编码器 (RAEs)，用预训练的表征编码器取代传统的VAE，以提高图像生成质量，并在ImageNet上实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
        "summary": "Generating realistic and controllable 3D human avatars is a long-standing\nchallenge, particularly when covering broad attribute ranges such as ethnicity,\nage, clothing styles, and detailed body shapes. Capturing and annotating\nlarge-scale human datasets for training generative models is prohibitively\nexpensive and limited in scale and diversity. The central question we address\nin this paper is: Can existing foundation models be distilled to generate\ntheoretically unbounded, richly annotated 3D human data? We introduce\nInfiniHuman, a framework that synergistically distills these models to produce\nrichly annotated human data at minimal cost and with theoretically unlimited\nscalability. We propose InfiniHumanData, a fully automatic pipeline that\nleverages vision-language and image generation models to create a large-scale\nmulti-modal dataset. User study shows our automatically generated identities\nare undistinguishable from scan renderings. InfiniHumanData contains 111K\nidentities spanning unprecedented diversity. Each identity is annotated with\nmulti-granularity text descriptions, multi-view RGB images, detailed clothing\nimages, and SMPL body-shape parameters. Building on this dataset, we propose\nInfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body\nshape, and clothing assets. InfiniHumanGen enables fast, realistic, and\nprecisely controllable avatar generation. Extensive experiments demonstrate\nsignificant improvements over state-of-the-art methods in visual quality,\ngeneration speed, and controllability. Our approach enables high-quality avatar\ngeneration with fine-grained control at effectively unbounded scale through a\npractical and affordable solution. We will publicly release the automatic data\ngeneration pipeline, the comprehensive InfiniHumanData dataset, and the\nInfiniHumanGen models at https://yuxuan-xue.com/infini-human.",
        "url": "http://arxiv.org/abs/2510.11650v1",
        "published_date": "2025-10-13T17:29:55+00:00",
        "updated_date": "2025-10-13T17:29:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxuan Xue",
            "Xianghui Xie",
            "Margaret Kostyrko",
            "Gerard Pons-Moll"
        ],
        "tldr": "The paper introduces InfiniHuman, a framework for generating diverse and controllable 3D human avatars by distilling existing foundation models to create a large-scale, richly annotated dataset and a diffusion-based generative pipeline, achieving state-of-the-art results in quality, speed, and controllability.",
        "tldr_zh": "该论文介绍了 InfiniHuman，它通过提炼现有的基础模型来生成多样且可控的 3D 人体化身，从而创建一个大规模、带有丰富注释的数据集和一个基于扩散的生成管道，并在质量、速度和可控性方面取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Scaling Language-Centric Omnimodal Representation Learning",
        "summary": "Recent multimodal embedding approaches leveraging multimodal large language\nmodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promising\nresults, yet the underlying reasons behind their superiority remain\nunderexplored. This work argues that a crucial advantage of MLLM-based\napproaches stems from implicit cross-modal alignment achieved during generative\npretraining, where the language decoder learns to exploit multimodal signals\nwithin a shared representation space for generating unimodal outputs. Through\nanalysis of anisotropy and kernel similarity structure, we empirically confirm\nthat latent alignment emerges within MLLM representations, allowing CL to serve\nas a lightweight refinement stage. Leveraging this insight, we propose a\nLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive\nexperiments across diverse backbones and benchmarks demonstrate its\neffectiveness, achieving state-of-the-art performance across modalities.\nFurthermore, we identify a Generation-Representation Scaling Law (GRSL),\nshowing that the representational capabilities gained through contrastive\nrefinement scales positively with the MLLM's generative capabilities. This\nsuggests that improving generative abilities evolves as an effective paradigm\nfor enhancing representation quality. We provide a theoretical explanation of\nGRSL, which formally links the MLLM's generative quality to the upper bound on\nits representation performance, and validate it on a challenging, low-resource\nvisual-document retrieval task, showing that continual generative pretraining\nbefore CL can further enhance the potential of a model's embedding\ncapabilities. Codes, models, and resources are available at\nhttps://github.com/LCO-Embedding/LCO-Embedding.",
        "url": "http://arxiv.org/abs/2510.11693v1",
        "published_date": "2025-10-13T17:53:52+00:00",
        "updated_date": "2025-10-13T17:53:52+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Chenghao Xiao",
            "Hou Pong Chan",
            "Hao Zhang",
            "Weiwen Xu",
            "Mahani Aljunied",
            "Yu Rong"
        ],
        "tldr": "The paper introduces LCO-Emb, a language-centric omnimodal embedding framework, and demonstrates that generative pretraining in MLLMs leads to better cross-modal alignment and representation quality, formalized by a Generation-Representation Scaling Law (GRSL). Further generative pretraining before contrastive learning enhances embedding capabilities, as validated on a low-resource visual-document retrieval task.",
        "tldr_zh": "该论文介绍了 LCO-Emb，一种以语言为中心的通用模态嵌入框架，并证明了 MLLM 中的生成式预训练可以带来更好的跨模态对齐和表征质量，并用生成-表征缩放定律 (GRSL) 进行了形式化。在对比学习之前进行进一步的生成式预训练可以增强嵌入能力，这已在一个低资源视觉-文档检索任务中得到验证。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Massive Activations are the Key to Local Detail Synthesis in Diffusion Transformers",
        "summary": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone\nfor visual generation. Recent observations reveal \\emph{Massive Activations}\n(MAs) in their internal feature maps, yet their function remains poorly\nunderstood. In this work, we systematically investigate these activations to\nelucidate their role in visual generation. We found that these massive\nactivations occur across all spatial tokens, and their distribution is\nmodulated by the input timestep embeddings. Importantly, our investigations\nfurther demonstrate that these massive activations play a key role in local\ndetail synthesis, while having minimal impact on the overall semantic content\nof output. Building on these insights, we propose \\textbf{D}etail\n\\textbf{G}uidance (\\textbf{DG}), a MAs-driven, training-free self-guidance\nstrategy to explicitly enhance local detail fidelity for DiTs. Specifically, DG\nconstructs a degraded ``detail-deficient'' model by disrupting MAs and\nleverages it to guide the original network toward higher-quality detail\nsynthesis. Our DG can seamlessly integrate with Classifier-Free Guidance (CFG),\nenabling further refinements of fine-grained details. Extensive experiments\ndemonstrate that our DG consistently improves fine-grained detail quality\nacross various pre-trained DiTs (\\eg, SD3, SD3.5, and Flux).",
        "url": "http://arxiv.org/abs/2510.11538v1",
        "published_date": "2025-10-13T15:39:13+00:00",
        "updated_date": "2025-10-13T15:39:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chaofan Gan",
            "Zicheng Zhao",
            "Yuanpeng Tu",
            "Xi Chen",
            "Ziran Qin",
            "Tieyuan Chen",
            "Mehrtash Harandi",
            "Weiyao Lin"
        ],
        "tldr": "This paper investigates massive activations within Diffusion Transformers (DiTs), revealing their crucial role in synthesizing local details, and proposes a training-free guidance strategy, Detail Guidance (DG), to enhance fine-grained detail quality in generated images.",
        "tldr_zh": "本文研究了扩散Transformer（DiT）中的大规模激活，揭示了它们在合成局部细节中的关键作用，并提出了一种无需训练的引导策略Detail Guidance (DG)，以提高生成图像中精细细节的质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models",
        "summary": "General SVG modeling remains challenging due to fragmented datasets, limited\ntransferability of methods across tasks, and the difficulty of handling\nstructural complexity. In response, we leverage the strong transfer and\ngeneralization capabilities of multimodal large language models (MLLMs) to\nachieve unified modeling for SVG understanding, editing, and generation. We\npresent the InternSVG family, an integrated data-benchmark-model suite. At its\ncore is SAgoge, the largest and most comprehensive multimodal dataset for SVG\ntasks, encompassing both static graphics and dynamic animations. It covers\nicons, long-sequence illustrations, scientific diagrams, and dynamic\nanimations, supporting tasks of varied difficulty levels and providing deeper\nhierarchies with richer attributes compared to previous datasets. Based on this\nresource, we introduce SArena, a companion benchmark with comprehensive task\ndefinitions and standardized evaluation that aligns with the domains and\ndifficulty spectrum covered by SAgoge. Building on these foundations, we\npropose InternSVG, a unified MLLM for SVG understanding, editing, and\ngeneration with SVG-specific special tokens, subword-based embedding\ninitialization, and a two-stage training strategy that progresses from short\nstatic SVGs to long-sequence illustrations and complex animations. This unified\nformulation induces positive transfer and improves overall performance.\nExperiments on SArena and prior benchmark confirm that InternSVG achieves\nsubstantial gains and consistently outperforms leading open and proprietary\ncounterparts.",
        "url": "http://arxiv.org/abs/2510.11341v1",
        "published_date": "2025-10-13T12:38:04+00:00",
        "updated_date": "2025-10-13T12:38:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haomin Wang",
            "Jinhui Yin",
            "Qi Wei",
            "Wenguang Zeng",
            "Lixin Gu",
            "Shenglong Ye",
            "Zhangwei Gao",
            "Yaohui Wang",
            "Yanting Zhang",
            "Yuanqi Li",
            "Yanwen Guo",
            "Wenhai Wang",
            "Kai Chen",
            "Yu Qiao",
            "Hongjie Zhang"
        ],
        "tldr": "The paper introduces InternSVG, a multimodal large language model for unified SVG understanding, editing, and generation, along with a new dataset (SAgoge) and benchmark (SArena). Results show it outperforms existing methods.",
        "tldr_zh": "该论文介绍了 InternSVG，一个用于统一 SVG 理解、编辑和生成的的多模态大型语言模型，以及一个新的数据集 (SAgoge) 和基准 (SArena)。结果表明它优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Demystifying Numerosity in Diffusion Models -- Limitations and Remedies",
        "summary": "Numerosity remains a challenge for state-of-the-art text-to-image generation\nmodels like FLUX and GPT-4o, which often fail to accurately follow counting\ninstructions in text prompts. In this paper, we aim to study a fundamental yet\noften overlooked question: Can diffusion models inherently generate the correct\nnumber of objects specified by a textual prompt simply by scaling up the\ndataset and model size? To enable rigorous and reproducible evaluation, we\nconstruct a clean synthetic numerosity benchmark comprising two complementary\ndatasets: GrayCount250 for controlled scaling studies, and NaturalCount6\nfeaturing complex naturalistic scenes. Second, we empirically show that the\nscaling hypothesis does not hold: larger models and datasets alone fail to\nimprove counting accuracy on our benchmark. Our analysis identifies a key\nreason: diffusion models tend to rely heavily on the noise initialization\nrather than the explicit numerosity specified in the prompt. We observe that\nnoise priors exhibit biases toward specific object counts. In addition, we\npropose an effective strategy for controlling numerosity by injecting\ncount-aware layout information into the noise prior. Our method achieves\nsignificant gains, improving accuracy on GrayCount250 from 20.0\\% to 85.3\\% and\non NaturalCount6 from 74.8\\% to 86.3\\%, demonstrating effective generalization\nacross settings.",
        "url": "http://arxiv.org/abs/2510.11117v1",
        "published_date": "2025-10-13T08:07:24+00:00",
        "updated_date": "2025-10-13T08:07:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaqi Zhao",
            "Xiaochen Wang",
            "Li Dong",
            "Wentao Zhang",
            "Yuhui Yuan"
        ],
        "tldr": "This paper investigates the limitations of diffusion models in accurately generating a specified number of objects and proposes a count-aware layout injection method to improve numerosity control, demonstrating significant gains in counting accuracy.",
        "tldr_zh": "本文研究了扩散模型在准确生成指定数量对象方面的局限性，并提出了一种计数感知布局注入方法来提高数量控制，实验结果表明计数准确率显著提高。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps",
        "summary": "This paper addresses the challenge of learning semantically and functionally\nmeaningful 3D motion priors from real-world videos, in order to enable\nprediction of future 3D scene motion from a single input image. We propose a\nnovel pixel-aligned Motion Map (MoMap) representation for 3D scene motion,\nwhich can be generated from existing generative image models to facilitate\nefficient and effective motion prediction. To learn meaningful distributions\nover motion, we create a large-scale database of MoMaps from over 50,000 real\nvideos and train a diffusion model on these representations. Our motion\ngeneration not only synthesizes trajectories in 3D but also suggests a new\npipeline for 2D video synthesis: first generate a MoMap, then warp an image\naccordingly and complete the warped point-based renderings. Experimental\nresults demonstrate that our approach generates plausible and semantically\nconsistent 3D scene motion.",
        "url": "http://arxiv.org/abs/2510.11107v1",
        "published_date": "2025-10-13T07:56:19+00:00",
        "updated_date": "2025-10-13T07:56:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahui Lei",
            "Kyle Genova",
            "George Kopanas",
            "Noah Snavely",
            "Leonidas Guibas"
        ],
        "tldr": "The paper proposes a Motion Map (MoMap) representation and a diffusion model trained on a large dataset of MoMaps to generate semantically consistent 3D scene motion from a single image, offering a novel approach to 2D video synthesis.",
        "tldr_zh": "该论文提出了一种运动地图 (MoMap) 表示，并训练了一个基于大规模 MoMap 数据集的扩散模型，以从单张图像生成语义一致的 3D 场景运动，为 2D 视频合成提供了一种新方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-shot Face Editing via ID-Attribute Decoupled Inversion",
        "summary": "Recent advancements in text-guided diffusion models have shown promise for\ngeneral image editing via inversion techniques, but often struggle to maintain\nID and structural consistency in real face editing tasks. To address this\nlimitation, we propose a zero-shot face editing method based on ID-Attribute\nDecoupled Inversion. Specifically, we decompose the face representation into ID\nand attribute features, using them as joint conditions to guide both the\ninversion and the reverse diffusion processes. This allows independent control\nover ID and attributes, ensuring strong ID preservation and structural\nconsistency while enabling precise facial attribute manipulation. Our method\nsupports a wide range of complex multi-attribute face editing tasks using only\ntext prompts, without requiring region-specific input, and operates at a speed\ncomparable to DDIM inversion. Comprehensive experiments demonstrate its\npracticality and effectiveness.",
        "url": "http://arxiv.org/abs/2510.11050v1",
        "published_date": "2025-10-13T06:34:40+00:00",
        "updated_date": "2025-10-13T06:34:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Hou",
            "Minggu Wang",
            "Jianjun Zhao"
        ],
        "tldr": "This paper introduces a zero-shot face editing method using ID-attribute decoupled inversion to improve ID preservation and structural consistency during text-guided face editing, operating at comparable speed to DDIM inversion.",
        "tldr_zh": "本文提出了一种基于ID属性解耦反演的零样本面部编辑方法，通过改进ID保持和结构一致性来增强文本引导的面部编辑效果，并且运算速度与DDIM反演相当。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
        "summary": "Unified multimodal models integrate the reasoning capacity of large language\nmodels with both image understanding and generation, showing great promise for\nadvanced multimodal intelligence. However, the community still lacks a rigorous\nreasoning-centric benchmark to systematically evaluate the alignment between\nunderstanding and generation, and their generalization potential in complex\nvisual tasks. To this end, we introduce \\textbf{GIR-Bench}, a comprehensive\nbenchmark that evaluates unified models across three complementary\nperspectives. Firstly, we investigate understanding-generation consistency\n(GIR-Bench-UGC), asking whether models can consistently leverage the same\nknowledge in both understanding and generation tasks. Secondly, we investigate\nwhether models can perform reasoning-centric text-to-image generation that\nrequires applying logical constraints and implicit knowledge to generate\nfaithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models\ncan handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset,\nwe carefully design different task-specific evaluation pipelines tailored for\neach task. This enables fine-grained and interpretable evaluation while\nmitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive\nablations over various unified models and generation-only systems have shown\nthat: Although unified models are more capable of reasoning-driven visual\ntasks, they still exhibit a persistent gap between understanding and\ngeneration. The data and code for GIR-Bench are available at\n\\href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}.",
        "url": "http://arxiv.org/abs/2510.11026v1",
        "published_date": "2025-10-13T05:50:44+00:00",
        "updated_date": "2025-10-13T05:50:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongxiang Li",
            "Yaowei Li",
            "Bin Lin",
            "Yuwei Niu",
            "Yuhang Yang",
            "Xiaoshuang Huang",
            "Jiayin Cai",
            "Xiaolong Jiang",
            "Yao Hu",
            "Long Chen"
        ],
        "tldr": "The paper introduces GIR-Bench, a new benchmark for evaluating the reasoning capabilities of unified multimodal models in image generation tasks, focusing on understanding-generation consistency, text-to-image generation with reasoning, and multi-step reasoning in editing.",
        "tldr_zh": "该论文介绍了GIR-Bench，一个用于评估统一多模态模型在图像生成任务中推理能力的新基准，重点关注理解-生成一致性、具有推理的文本到图像生成以及编辑中的多步骤推理。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation",
        "summary": "Multi-instance image generation (MIG) remains a significant challenge for\nmodern diffusion models due to key limitations in achieving precise control\nover object layout and preserving the identity of multiple distinct subjects.\nTo address these limitations, we introduce ContextGen, a novel Diffusion\nTransformer framework for multi-instance generation that is guided by both\nlayout and reference images. Our approach integrates two key technical\ncontributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates\nthe composite layout image into the generation context to robustly anchor the\nobjects in their desired positions, and Identity Consistency Attention (ICA),\nan innovative attention mechanism that leverages contextual reference images to\nensure the identity consistency of multiple instances. Recognizing the lack of\nlarge-scale, hierarchically-structured datasets for this task, we introduce\nIMIG-100K, the first dataset with detailed layout and identity annotations.\nExtensive experiments demonstrate that ContextGen sets a new state-of-the-art,\noutperforming existing methods in control precision, identity fidelity, and\noverall visual quality.",
        "url": "http://arxiv.org/abs/2510.11000v1",
        "published_date": "2025-10-13T04:21:19+00:00",
        "updated_date": "2025-10-13T04:21:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruihang Xu",
            "Dewei Zhou",
            "Fan Ma",
            "Yi Yang"
        ],
        "tldr": "The paper introduces ContextGen, a Diffusion Transformer framework for multi-instance image generation that uses Contextual Layout Anchoring (CLA) and Identity Consistency Attention (ICA) to improve control precision and identity fidelity. They also introduce a new dataset, IMIG-100K.",
        "tldr_zh": "该论文介绍了 ContextGen，一个用于多实例图像生成的 Diffusion Transformer 框架，它使用上下文布局锚定 (CLA) 和身份一致性注意 (ICA) 来提高控制精度和身份保真度。他们还推出了一个名为 IMIG-100K 的新数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation",
        "summary": "Existing vision language models (VLMs), including GPT-4 and DALL-E, often\nstruggle to preserve logic, object identity, and style in multimodal image-text\ngeneration. This limitation significantly hinders the generalization capability\nof VLMs in complex image-text input-output scenarios. To address this issue, we\npropose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which\nenhances existing interleaved VLMs through explicit structured reasoning,\nthereby mitigating context drift in logic, entity identity, and style. The\nproposed framework operates in two stages. (1) A dynamic IUT-Plug extraction\nmodule parses visual scenes into hierarchical symbolic structures. (2) A\ncoordinated narrative-flow and image synthesis mechanism ensures cross-modal\nconsistency. To evaluate our approach, we construct a novel benchmark based on\n3,000 real human-generated question-answer pairs over fine-tuned large models,\nintroducing a dynamic evaluation protocol for quantifying context drift in\ninterleaved VLMs. Experimental results demonstrate that IUT-Plug not only\nimproves accuracy on established benchmarks but also effectively alleviates the\nthree critical forms of context drift across diverse multimodal question\nanswering (QA) scenarios.",
        "url": "http://arxiv.org/abs/2510.10969v1",
        "published_date": "2025-10-13T03:19:45+00:00",
        "updated_date": "2025-10-13T03:19:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeteng Lin",
            "Xingxing Li",
            "Wen You",
            "Xiaoyang Li",
            "Zehan Lu",
            "Yujun Cai",
            "Jing Tang"
        ],
        "tldr": "The paper introduces IUT-Plug, a module that leverages an Image Understanding Tree (IUT) to improve logic, object identity, and style consistency in interleaved image-text generation within VLMs. It also introduces a new benchmark and evaluation protocol.",
        "tldr_zh": "该论文介绍了IUT-Plug，一个利用图像理解树（IUT）的模块，用于提高视觉语言模型（VLMs）中交错图像-文本生成的逻辑、对象身份和风格一致性。还引入了一个新的基准和评估协议。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DreamMakeup: Face Makeup Customization using Latent Diffusion Models",
        "summary": "The exponential growth of the global makeup market has paralleled\nadvancements in virtual makeup simulation technology. Despite the progress led\nby GANs, their application still encounters significant challenges, including\ntraining instability and limited customization capabilities. Addressing these\nchallenges, we introduce DreamMakup - a novel training-free Diffusion model\nbased Makeup Customization method, leveraging the inherent advantages of\ndiffusion models for superior controllability and precise real-image editing.\nDreamMakeup employs early-stopped DDIM inversion to preserve the facial\nstructure and identity while enabling extensive customization through various\nconditioning inputs such as reference images, specific RGB colors, and textual\ndescriptions. Our model demonstrates notable improvements over existing\nGAN-based and recent diffusion-based frameworks - improved customization,\ncolor-matching capabilities, identity preservation and compatibility with\ntextual descriptions or LLMs with affordable computational costs.",
        "url": "http://arxiv.org/abs/2510.10918v1",
        "published_date": "2025-10-13T02:29:23+00:00",
        "updated_date": "2025-10-13T02:29:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Geon Yeong Park",
            "Inhwa Han",
            "Serin Yang",
            "Yeobin Hong",
            "Seongmin Jeong",
            "Heechan Jeon",
            "Myeongjin Goh",
            "Sung Won Yi",
            "Jin Nam",
            "Jong Chul Ye"
        ],
        "tldr": "DreamMakeup introduces a training-free diffusion-based method for face makeup customization, offering improved controllability, precision, and compatibility with textual descriptions compared to GAN-based approaches.",
        "tldr_zh": "DreamMakeup 提出了一种无需训练的基于扩散模型的面部化妆定制方法，与基于 GAN 的方法相比，该方法具有更好的可控性、精度和与文本描述的兼容性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training",
        "summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid\ntraining on perspective and panoramic data for panoramic image generation. For\nthe issues of maintaining geometric fidelity and photorealism in generation\nquality, we attribute the main reason to the lack of large-scale, high-quality,\nreal-world panoramic data, where such a data-centric view differs from prior\nmethods that focus on model design. Basically, DiT360 has several key modules\nfor inter-domain transformation and intra-domain augmentation, applied at both\nthe pre-VAE image level and the post-VAE token level. At the image level, we\nincorporate cross-domain knowledge through perspective image guidance and\npanoramic refinement, which enhance perceptual quality while regularizing\ndiversity and photorealism. At the token level, hybrid supervision is applied\nacross multiple modules, which include circular padding for boundary\ncontinuity, yaw loss for rotational robustness, and cube loss for distortion\nawareness. Extensive experiments on text-to-panorama, inpainting, and\noutpainting tasks demonstrate that our method achieves better boundary\nconsistency and image fidelity across eleven quantitative metrics. Our code is\navailable at https://github.com/Insta360-Research-Team/DiT360.",
        "url": "http://arxiv.org/abs/2510.11712v1",
        "published_date": "2025-10-13T17:59:15+00:00",
        "updated_date": "2025-10-13T17:59:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoran Feng",
            "Dizhe Zhang",
            "Xiangtai Li",
            "Bo Du",
            "Lu Qi"
        ],
        "tldr": "DiT360 is a DiT-based framework using hybrid training on perspective and panoramic data to generate high-fidelity panoramic images, addressing geometric fidelity and photorealism through inter-domain transformation and intra-domain augmentation.",
        "tldr_zh": "DiT360是一个基于DiT的框架，通过对透视和全景数据进行混合训练来生成高保真全景图像，通过域间转换和域内增强来解决几何保真度和照片真实感的问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation",
        "summary": "Generative Models are a valuable tool for the controlled creation of\nhigh-quality image data. Controlled diffusion models like the ControlNet have\nallowed the creation of labeled distributions. Such synthetic datasets can\naugment the original training distribution when discriminative models, like\nsemantic segmentation, are trained. However, this augmentation effect is\nlimited since ControlNets tend to reproduce the original training distribution.\n  This work introduces a method to utilize data from unlabeled domains to train\nControlNets by introducing the concept of uncertainty into the control\nmechanism. The uncertainty indicates that a given image was not part of the\ntraining distribution of a downstream task, e.g., segmentation. Thus, two types\nof control are engaged in the final network: an uncertainty control from an\nunlabeled dataset and a semantic control from the labeled dataset. The\nresulting ControlNet allows us to create annotated data with high uncertainty\nfrom the target domain, i.e., synthetic data from the unlabeled distribution\nwith labels. In our scenario, we consider retinal OCTs, where typically\nhigh-quality Spectralis images are available with given ground truth\nsegmentations, enabling the training of segmentation networks. The recent\ndevelopment in Home-OCT devices, however, yields retinal OCTs with lower\nquality and a large domain shift, such that out-of-the-pocket segmentation\nnetworks cannot be applied for this type of data. Synthesizing annotated images\nfrom the Home-OCT domain using the proposed approach closes this gap and leads\nto significantly improved segmentation results without adding any further\nsupervision. The advantage of uncertainty-guidance becomes obvious when\ncompared to style transfer: it enables arbitrary domain shifts without any\nstrict learning of an image style. This is also demonstrated in a traffic scene\nexperiment.",
        "url": "http://arxiv.org/abs/2510.11346v1",
        "published_date": "2025-10-13T12:41:28+00:00",
        "updated_date": "2025-10-13T12:41:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Joshua Niemeijer",
            "Jan Ehrhardt",
            "Heinz Handels",
            "Hristina Uzunova"
        ],
        "tldr": "This paper introduces Uncertainty-Aware ControlNet, a method that leverages uncertainty from unlabeled data domains to train ControlNets for synthetic data generation, improving performance on downstream tasks like segmentation in target domains where domain shift is a significant issue. They demonstrate this on retinal OCT images and traffic scenes.",
        "tldr_zh": "本文介绍了一种名为“不确定性感知ControlNet”的方法，该方法利用来自未标记数据域的不确定性来训练ControlNet，以生成合成数据，从而提高目标领域（领域转移是主要问题）中分割等下游任务的性能。他们在视网膜OCT图像和交通场景中演示了这一点。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model",
        "summary": "With the rapid development of diffusion models, style transfer has made\nremarkable progress. However, flexible and localized style editing for scene\ntext remains an unsolved challenge. Although existing scene text editing\nmethods have achieved text region editing, they are typically limited to\ncontent replacement and simple styles, which lack the ability of free-style\ntransfer. In this paper, we introduce SceneTextStylizer, a novel training-free\ndiffusion-based framework for flexible and high-fidelity style transfer of text\nin scene images. Unlike prior approaches that either perform global style\ntransfer or focus solely on textual content modification, our method enables\nprompt-guided style transformation specifically for text regions, while\npreserving both text readability and stylistic consistency. To achieve this, we\ndesign a feature injection module that leverages diffusion model inversion and\nself-attention to transfer style features effectively. Additionally, a region\ncontrol mechanism is introduced by applying a distance-based changing mask at\neach denoising step, enabling precise spatial control. To further enhance\nvisual quality, we incorporate a style enhancement module based on the Fourier\ntransform to reinforce stylistic richness. Extensive experiments demonstrate\nthat our method achieves superior performance in scene text style\ntransformation, outperforming existing state-of-the-art methods in both visual\nfidelity and text preservation.",
        "url": "http://arxiv.org/abs/2510.10910v1",
        "published_date": "2025-10-13T02:11:57+00:00",
        "updated_date": "2025-10-13T02:11:57+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Honghui Yuan",
            "Keiji Yanai"
        ],
        "tldr": "The paper introduces SceneTextStylizer, a training-free diffusion-based framework for style transfer of text in scene images, enabling prompt-guided style transformation specifically for text regions with high fidelity and readability.",
        "tldr_zh": "该论文介绍了SceneTextStylizer，一个无需训练的，基于扩散模型的场景文本风格迁移框架，能够根据提示对文本区域进行风格转换，实现高保真和可读性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic Underwater Image Generation",
        "summary": "In this paper, we propose a novel framework, Disentangled Style-Content GAN\n(DISC-GAN), which integrates style-content disentanglement with a\ncluster-specific training strategy towards photorealistic underwater image\nsynthesis. The quality of synthetic underwater images is challenged by optical\ndue to phenomena such as color attenuation and turbidity. These phenomena are\nrepresented by distinct stylistic variations across different waterbodies, such\nas changes in tint and haze. While generative models are well-suited to capture\ncomplex patterns, they often lack the ability to model the non-uniform\nconditions of diverse underwater environments. To address these challenges, we\nemploy K-means clustering to partition a dataset into style-specific domains.\nWe use separate encoders to get latent spaces for style and content; we further\nintegrate these latent representations via Adaptive Instance Normalization\n(AdaIN) and decode the result to produce the final synthetic image. The model\nis trained independently on each style cluster to preserve domain-specific\ncharacteristics. Our framework demonstrates state-of-the-art performance,\nobtaining a Structural Similarity Index (SSIM) of 0.9012, an average Peak\nSignal-to-Noise Ratio (PSNR) of 32.5118 dB, and a Frechet Inception Distance\n(FID) of 13.3728.",
        "url": "http://arxiv.org/abs/2510.10782v1",
        "published_date": "2025-10-12T19:56:20+00:00",
        "updated_date": "2025-10-12T19:56:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sneha Varur",
            "Anirudh R Hanchinamani",
            "Tarun S Bagewadi",
            "Uma Mudenagudi",
            "Chaitra D Desai",
            "Sujata C",
            "Padmashree Desai",
            "Sumit Meharwade"
        ],
        "tldr": "The paper introduces DISC-GAN, a novel GAN-based framework that disentangles style and content with cluster-specific training to generate photorealistic underwater images, achieving state-of-the-art results in SSIM, PSNR, and FID.",
        "tldr_zh": "该论文介绍了一种新的基于GAN的框架DISC-GAN，它通过解耦风格和内容以及特定于集群的训练来生成逼真的水下图像，并在SSIM、PSNR和FID方面取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]