[
    {
        "title": "VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning",
        "summary": "Visual effects (VFX) are crucial to the expressive power of digital media,\nyet their creation remains a major challenge for generative AI. Prevailing\nmethods often rely on the one-LoRA-per-effect paradigm, which is\nresource-intensive and fundamentally incapable of generalizing to unseen\neffects, thus limiting scalability and creation. To address this challenge, we\nintroduce VFXMaster, the first unified, reference-based framework for VFX video\ngeneration. It recasts effect generation as an in-context learning task,\nenabling it to reproduce diverse dynamic effects from a reference video onto\ntarget content. In addition, it demonstrates remarkable generalization to\nunseen effect categories. Specifically, we design an in-context conditioning\nstrategy that prompts the model with a reference example. An in-context\nattention mask is designed to precisely decouple and inject the essential\neffect attributes, allowing a single unified model to master the effect\nimitation without information leakage. In addition, we propose an efficient\none-shot effect adaptation mechanism to boost generalization capability on\ntough unseen effects from a single user-provided video rapidly. Extensive\nexperiments demonstrate that our method effectively imitates various categories\nof effect information and exhibits outstanding generalization to out-of-domain\neffects. To foster future research, we will release our code, models, and a\ncomprehensive dataset to the community.",
        "url": "http://arxiv.org/abs/2510.25772v1",
        "published_date": "2025-10-29T17:59:53+00:00",
        "updated_date": "2025-10-29T17:59:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Baolu Li",
            "Yiming Zhang",
            "Qinghe Wang",
            "Liqian Ma",
            "Xiaoyu Shi",
            "Xintao Wang",
            "Pengfei Wan",
            "Zhenfei Yin",
            "Yunzhi Zhuge",
            "Huchuan Lu",
            "Xu Jia"
        ],
        "tldr": "The paper introduces VFXMaster, a unified, reference-based in-context learning framework for generating diverse VFX videos, demonstrating generalization to unseen effects using a novel conditioning strategy and adaptation mechanism.",
        "tldr_zh": "该论文介绍了VFXMaster，一个统一的、基于参考的上下文学习框架，用于生成各种VFX视频，通过新颖的条件反射策略和自适应机制，展示了对未见效果的泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image Generation",
        "summary": "Autoregressive (AR) image generation models are capable of producing\nhigh-fidelity images but often suffer from slow inference due to their\ninherently sequential, token-by-token decoding process. Speculative decoding,\nwhich employs a lightweight draft model to approximate the output of a larger\nAR model, has shown promise in accelerating text generation without\ncompromising quality. However, its application to image generation remains\nlargely underexplored. The challenges stem from a significantly larger sampling\nspace, which complicates the alignment between the draft and target model\noutputs, coupled with the inadequate use of the two-dimensional spatial\nstructure inherent in images, thereby limiting the modeling of local\ndependencies. To overcome these challenges, we introduce Hawk, a new approach\nthat harnesses the spatial structure of images to guide the speculative model\ntoward more accurate and efficient predictions. Experimental results on\nmultiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR\nmodels, while preserving both image fidelity and diversity.",
        "url": "http://arxiv.org/abs/2510.25739v1",
        "published_date": "2025-10-29T17:43:31+00:00",
        "updated_date": "2025-10-29T17:43:31+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhi-Kai Chen",
            "Jun-Peng Jiang",
            "Han-Jia Ye",
            "De-Chuan Zhan"
        ],
        "tldr": "The paper introduces Hawk, a method leveraging spatial context to speed up autoregressive text-to-image generation, achieving a 1.71x speedup while maintaining image quality and diversity.",
        "tldr_zh": "本文介绍了一种名为Hawk的方法，它利用空间上下文来加速自回归文本到图像的生成，在保持图像质量和多样性的同时，实现了1.71倍的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
        "summary": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.",
        "url": "http://arxiv.org/abs/2510.25590v1",
        "published_date": "2025-10-29T14:58:37+00:00",
        "updated_date": "2025-10-29T14:58:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pengtao Chen",
            "Xianfang Zeng",
            "Maosen Zhao",
            "Mingzhu Shen",
            "Peng Ye",
            "Bangyin Xiang",
            "Zhibo Wang",
            "Wei Cheng",
            "Gang Yu",
            "Tao Chen"
        ],
        "tldr": "The paper introduces RegionE, a framework that accelerates instruction-based image editing by adaptively applying different denoising strategies to edited and unedited regions, achieving significant speedups without sacrificing fidelity.",
        "tldr_zh": "该论文介绍了RegionE，一种通过自适应地对编辑区域和未编辑区域应用不同的去噪策略来加速基于指令的图像编辑的框架，在不牺牲保真度的情况下实现了显著的加速。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VividCam: Learning Unconventional Camera Motions from Virtual Synthetic Videos",
        "summary": "Although recent text-to-video generative models are getting more capable of\nfollowing external camera controls, imposed by either text descriptions or\ncamera trajectories, they still struggle to generalize to unconventional camera\nmotions, which is crucial in creating truly original and artistic videos. The\nchallenge lies in the difficulty of finding sufficient training videos with the\nintended uncommon camera motions. To address this challenge, we propose\nVividCam, a training paradigm that enables diffusion models to learn complex\ncamera motions from synthetic videos, releasing the reliance on collecting\nrealistic training videos. VividCam incorporates multiple disentanglement\nstrategies that isolates camera motion learning from synthetic appearance\nartifacts, ensuring more robust motion representation and mitigating domain\nshift. We demonstrate that our design synthesizes a wide range of precisely\ncontrolled and complex camera motions using surprisingly simple synthetic data.\nNotably, this synthetic data often consists of basic geometries within a\nlow-poly 3D scene and can be efficiently rendered by engines like Unity. Our\nvideo results can be found in https://wuqiuche.github.io/VividCamDemoPage/ .",
        "url": "http://arxiv.org/abs/2510.24904v1",
        "published_date": "2025-10-28T19:12:22+00:00",
        "updated_date": "2025-10-28T19:12:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiucheng Wu",
            "Handong Zhao",
            "Zhixin Shu",
            "Jing Shi",
            "Yang Zhang",
            "Shiyu Chang"
        ],
        "tldr": "VividCam introduces a training paradigm leveraging synthetic videos to enable text-to-video diffusion models to learn unconventional camera motions, overcoming the limitations of real-world training data and enhancing artistic video generation.",
        "tldr_zh": "VividCam提出了一种训练范式，利用合成视频使文本到视频的扩散模型能够学习非常规的相机运动，克服了真实世界训练数据的局限性，并增强了艺术视频的生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FreeArt3D: Training-Free Articulated Object Generation using 3D Diffusion",
        "summary": "Articulated 3D objects are central to many applications in robotics, AR/VR,\nand animation. Recent approaches to modeling such objects either rely on\noptimization-based reconstruction pipelines that require dense-view supervision\nor on feed-forward generative models that produce coarse geometric\napproximations and often overlook surface texture. In contrast, open-world 3D\ngeneration of static objects has achieved remarkable success, especially with\nthe advent of native 3D diffusion models such as Trellis. However, extending\nthese methods to articulated objects by training native 3D diffusion models\nposes significant challenges. In this work, we present FreeArt3D, a\ntraining-free framework for articulated 3D object generation. Instead of\ntraining a new model on limited articulated data, FreeArt3D repurposes a\npre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape\nprior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by\ntreating articulation as an additional generative dimension. Given a few images\ncaptured in different articulation states, FreeArt3D jointly optimizes the\nobject's geometry, texture, and articulation parameters without requiring\ntask-specific training or access to large-scale articulated datasets. Our\nmethod generates high-fidelity geometry and textures, accurately predicts\nunderlying kinematic structures, and generalizes well across diverse object\ncategories. Despite following a per-instance optimization paradigm, FreeArt3D\ncompletes in minutes and significantly outperforms prior state-of-the-art\napproaches in both quality and versatility.",
        "url": "http://arxiv.org/abs/2510.25765v1",
        "published_date": "2025-10-29T17:58:14+00:00",
        "updated_date": "2025-10-29T17:58:14+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Chuhao Chen",
            "Isabella Liu",
            "Xinyue Wei",
            "Hao Su",
            "Minghua Liu"
        ],
        "tldr": "FreeArt3D is a training-free framework that repurposes pre-trained static 3D diffusion models for articulated 3D object generation by extending Score Distillation Sampling (SDS) to the 3D-to-4D domain, achieving high-fidelity results without task-specific training.",
        "tldr_zh": "FreeArt3D是一个无需训练的框架，它通过将Score Distillation Sampling (SDS)扩展到3D到4D的领域，重新利用预训练的静态3D扩散模型来生成关节3D对象，在不需要特定任务训练的情况下实现高保真结果。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Balanced conic rectified flow",
        "summary": "Rectified flow is a generative model that learns smooth transport mappings\nbetween two distributions through an ordinary differential equation (ODE).\nUnlike diffusion-based generative models, which require costly numerical\nintegration of a generative ODE to sample images with state-of-the-art quality,\nrectified flow uses an iterative process called reflow to learn smooth and\nstraight ODE paths. This allows for relatively simple and efficient generation\nof high-quality images. However, rectified flow still faces several challenges.\n1) The reflow process requires a large number of generative pairs to preserve\nthe target distribution, leading to significant computational costs. 2) Since\nthe model is typically trained using only generated image pairs, its\nperformance heavily depends on the 1-rectified flow model, causing it to become\nbiased towards the generated data.\n  In this work, we experimentally expose the limitations of the original\nrectified flow and propose a novel approach that incorporates real images into\nthe training process. By preserving the ODE paths for real images, our method\neffectively reduces reliance on large amounts of generated data. Instead, we\ndemonstrate that the reflow process can be conducted efficiently using a much\nsmaller set of generated and real images. In CIFAR-10, we achieved\nsignificantly better FID scores, not only in one-step generation but also in\nfull-step simulations, while using only of the generative pairs compared to the\noriginal method. Furthermore, our approach induces straighter paths and avoids\nsaturation on generated images during reflow, leading to more robust ODE\nlearning while preserving the distribution of real images.",
        "url": "http://arxiv.org/abs/2510.25229v1",
        "published_date": "2025-10-29T07:06:01+00:00",
        "updated_date": "2025-10-29T07:06:01+00:00",
        "categories": [
            "cs.CV",
            "68T07, 68T45, 65C20",
            "I.2.10; I.4.9; I.2.6"
        ],
        "authors": [
            "Kim Shin Seong",
            "Mingi Kwon",
            "Jaeseok Jeong",
            "Youngjung Uh"
        ],
        "tldr": "This paper introduces a novel approach to rectified flow, a generative model, by incorporating real images into the training process to reduce reliance on generated data and improve performance with fewer generative pairs, resulting in better FID scores.",
        "tldr_zh": "本文提出了一种改进的 Rectified Flow 方法，通过将真实图像融入训练过程，减少对生成数据的依赖，并使用更少的生成对来提高性能，从而获得更好的 FID 分数。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation",
        "summary": "Generating hand grasps with language instructions is a widely studied topic\nthat benefits from embodied AI and VR/AR applications. While transferring into\nhand articulatied object interaction (HAOI), the hand grasps synthesis requires\nnot only object functionality but also long-term manipulation sequence along\nthe object deformation. This paper proposes a novel HAOI sequence generation\nframework SynHLMA, to synthesize hand language manipulation for articulated\nobjects. Given a complete point cloud of an articulated object, we utilize a\ndiscrete HAOI representation to model each hand object interaction frame. Along\nwith the natural language embeddings, the representations are trained by an\nHAOI manipulation language model to align the grasping process with its\nlanguage description in a shared representation space. A joint-aware loss is\nemployed to ensure hand grasps follow the dynamic variations of articulated\nobject joints. In this way, our SynHLMA achieves three typical hand\nmanipulation tasks for articulated objects of HAOI generation, HAOI prediction\nand HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset and\nexperimental results demonstrate the superior hand grasp sequence generation\nperformance comparing with state-of-the-art. We also show a robotics grasp\napplication that enables dexterous grasps execution from imitation learning\nusing the manipulation sequence provided by our SynHLMA. Our codes and datasets\nwill be made publicly available.",
        "url": "http://arxiv.org/abs/2510.25268v1",
        "published_date": "2025-10-29T08:27:00+00:00",
        "updated_date": "2025-10-29T08:27:00+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Wang zhi",
            "Yuyan Liu",
            "Liu Liu",
            "Li Zhang",
            "Ruixuan Lu",
            "Dan Guo"
        ],
        "tldr": "The paper introduces SynHLMA, a framework for synthesizing hand-object manipulation sequences for articulated objects, guided by language instructions, using a discrete HAOI representation and a joint-aware loss. It demonstrates superior performance on HAOI generation, prediction, and interpolation tasks.",
        "tldr_zh": "本文介绍 SynHLMA，一个用于合成铰接物体的手部-物体操纵序列的框架，该框架在语言指令的指导下，使用离散的 HAOI 表示和关节感知损失。 它展示了在 HAOI 生成、预测和插值任务方面的卓越性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]