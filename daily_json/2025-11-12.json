[
    {
        "title": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation",
        "summary": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms.",
        "url": "http://arxiv.org/abs/2511.07399v1",
        "published_date": "2025-11-10T18:51:28+00:00",
        "updated_date": "2025-11-11T02:53:28+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Tianrui Feng",
            "Zhi Li",
            "Shuo Yang",
            "Haocheng Xi",
            "Muyang Li",
            "Xiuyu Li",
            "Lvmin Zhang",
            "Keting Yang",
            "Kelly Peng",
            "Song Han",
            "Maneesh Agrawala",
            "Kurt Keutzer",
            "Akio Kodaira",
            "Chenfeng Xu"
        ],
        "tldr": "StreamDiffusionV2 is a training-free pipeline for interactive live streaming with video diffusion models that focuses on minimizing latency and scaling across multiple GPUs, achieving real-time performance with state-of-the-art models. It introduces an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and scalable pipeline orchestration.",
        "tldr_zh": "StreamDiffusionV2是一个用于交互式直播视频扩散模型的免训练pipeline，专注于最小化延迟和跨多个GPU的扩展，使用最先进的模型实现实时性能。它引入了感知SLO的批处理调度器和块调度器，以及sink-token引导的滚动KV缓存，运动感知噪声控制器和可扩展的pipeline编排。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist",
        "summary": "While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation $\\rightarrow$ multi-round editing $\\rightarrow$ object segmentation $\\rightarrow$ compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)",
        "url": "http://arxiv.org/abs/2511.08521v1",
        "published_date": "2025-11-11T17:58:13+00:00",
        "updated_date": "2025-11-12T02:02:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhengyang Liang",
            "Daoan Zhang",
            "Huichi Zhou",
            "Rui Huang",
            "Bobo Li",
            "Yuechen Zhang",
            "Shengqiong Wu",
            "Xiaohan Wang",
            "Jiebo Luo",
            "Lizi Liao",
            "Hao Fei"
        ],
        "tldr": "The paper introduces UniVA, an open-source multi-agent framework for general video processing tasks, enabling complex, iterative workflows combining understanding, segmentation, editing, and generation. It also includes UniVA-Bench, a benchmark suite for evaluating such systems.",
        "tldr_zh": "该论文介绍了一个名为UniVA的开源多智能体框架，用于通用的视频处理任务，实现理解、分割、编辑和生成等复杂迭代工作流程。它还包含UniVA-Bench，这是一个用于评估此类系统的基准测试套件。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist",
        "summary": "While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation $\\rightarrow$ multi-round editing $\\rightarrow$ object segmentation $\\rightarrow$ compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)",
        "url": "http://arxiv.org/abs/2511.08521v1",
        "published_date": "2025-11-11T17:58:13+00:00",
        "updated_date": "2025-11-12T02:02:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhengyang Liang",
            "Daoan Zhang",
            "Huichi Zhou",
            "Rui Huang",
            "Bobo Li",
            "Yuechen Zhang",
            "Shengqiong Wu",
            "Xiaohan Wang",
            "Jiebo Luo",
            "Lizi Liao",
            "Hao Fei"
        ],
        "tldr": "UniVA is an open-source multi-agent framework for universal video understanding, editing, and generation. It uses a Plan-and-Act architecture with hierarchical memory and introduces a new benchmark, UniVA-Bench.",
        "tldr_zh": "UniVA是一个开源的多智能体框架，用于通用视频理解、编辑和生成。它采用一种具有分层记忆的“计划-行动”架构，并引入了一个新的基准测试UniVA-Bench。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
        "summary": "We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Our project page is available at https://linzhanm.github.io/dimo.",
        "url": "http://arxiv.org/abs/2511.07409v1",
        "published_date": "2025-11-10T18:56:49+00:00",
        "updated_date": "2025-11-11T02:53:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Linzhan Mou",
            "Jiahui Lei",
            "Chen Wang",
            "Lingjie Liu",
            "Kostas Daniilidis"
        ],
        "tldr": "The paper introduces DIMO, a method for generating diverse 3D motions for arbitrary objects from a single image by leveraging video model priors and embedding motions into a latent space. It enables diverse 3D motion sampling and supports applications like motion interpolation and language-guided motion generation.",
        "tldr_zh": "本文介绍了DIMO，一种通过利用视频模型先验并将运动嵌入潜在空间，从单个图像为任意对象生成多样化3D运动的方法。它能够进行多样化的3D运动采样，并支持运动插值和语言引导的运动生成等应用。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier",
        "summary": "Diffusion models have achieved remarkable success in conditional image generation, yet their outputs often remain misaligned with human preferences. To address this, recent work has applied Direct Preference Optimization (DPO) to diffusion models, yielding significant improvements.~However, DPO-like methods exhibit two key limitations: 1) High computational cost,due to the entire model fine-tuning; 2) Sensitivity to reference model quality}, due to its tendency to introduce instability and bias. To overcome these limitations, we propose a novel framework for human preference alignment in diffusion models (PC-Diffusion), using a lightweight, trainable Preference Classifier that directly models the relative preference between samples. By restricting preference learning to this classifier, PC-Diffusion decouples preference alignment from the generative model, eliminating the need for entire model fine-tuning and reference model reliance.~We further provide theoretical guarantees for PC-Diffusion:1) PC-Diffusion ensures that the preference-guided distributions are consistently propagated across timesteps. 2)The training objective of the preference classifier is equivalent to DPO, but does not require a reference model.3) The proposed preference-guided correction can progressively steer generation toward preference-aligned regions.~Empirical results show that PC-Diffusion achieves comparable preference consistency to DPO while significantly reducing training costs and enabling efficient and stable preference-guided generation.",
        "url": "http://arxiv.org/abs/2511.07806v1",
        "published_date": "2025-11-11T03:53:06+00:00",
        "updated_date": "2025-11-12T01:19:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shaomeng Wang",
            "He Wang",
            "Xiaolu Wei",
            "Longquan Dai",
            "Jinhui Tang"
        ],
        "tldr": "The paper introduces PC-Diffusion, a novel method for aligning diffusion models with human preferences using a lightweight preference classifier, addressing limitations of DPO-like methods by reducing computational cost and eliminating reliance on reference models.",
        "tldr_zh": "该论文介绍了PC-Diffusion，一种使用轻量级偏好分类器将扩散模型与人类偏好对齐的新方法，通过降低计算成本和消除对参考模型的依赖，解决了类DPO方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Randomness: Understand the Order of the Noise in Diffusion",
        "summary": "In text-driven content generation (T2C) diffusion model, semantic of generated content is mostly attributed to the process of text embedding and attention mechanism interaction. The initial noise of the generation process is typically characterized as a random element that contributes to the diversity of the generated content. Contrary to this view, this paper reveals that beneath the random surface of noise lies strong analyzable patterns. Specifically, this paper first conducts a comprehensive analysis of the impact of random noise on the model's generation. We found that noise not only contains rich semantic information, but also allows for the erasure of unwanted semantics from it in an extremely simple way based on information theory, and using the equivalence between the generation process of diffusion model and semantic injection to inject semantics into the cleaned noise. Then, we mathematically decipher these observations and propose a simple but efficient training-free and universal two-step \"Semantic Erasure-Injection\" process to modulate the initial noise in T2C diffusion model. Experimental results demonstrate that our method is consistently effective across various T2C models based on both DiT and UNet architectures and presents a novel perspective for optimizing the generation of diffusion model, providing a universal tool for consistent generation.",
        "url": "http://arxiv.org/abs/2511.07756v1",
        "published_date": "2025-11-11T02:12:38+00:00",
        "updated_date": "2025-11-12T01:15:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Song Yan",
            "Min Li",
            "Bi Xinliang",
            "Jian Yang",
            "Yusen Zhang",
            "Guanye Xiong",
            "Yunwei Lan",
            "Tao Zhang",
            "Wei Zhai",
            "Zheng-Jun Zha"
        ],
        "tldr": "This paper challenges the assumption that initial noise in text-to-content diffusion models is purely random and proposes a training-free \"Semantic Erasure-Injection\" method to modulate the noise for improved and consistent content generation.",
        "tldr_zh": "本文挑战了文本到内容扩散模型中初始噪声是纯随机的假设，并提出了一种无需训练的“语义擦除-注入”方法来调制噪声，从而改进并一致地生成内容。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Taming Identity Consistency and Prompt Diversity in Diffusion Models via Latent Concatenation and Masked Conditional Flow Matching",
        "summary": "Subject-driven image generation aims to synthesize novel depictions of a specific subject across diverse contexts while preserving its core identity features. Achieving both strong identity consistency and high prompt diversity presents a fundamental trade-off. We propose a LoRA fine-tuned diffusion model employing a latent concatenation strategy, which jointly processes reference and target images, combined with a masked Conditional Flow Matching (CFM) objective. This approach enables robust identity preservation without architectural modifications. To facilitate large-scale training, we introduce a two-stage Distilled Data Curation Framework: the first stage leverages data restoration and VLM-based filtering to create a compact, high-quality seed dataset from diverse sources; the second stage utilizes these curated examples for parameter-efficient fine-tuning, thus scaling the generation capability across various subjects and contexts. Finally, for filtering and quality assessment, we present CHARIS, a fine-grained evaluation framework that performs attribute-level comparisons along five key axes: identity consistency, prompt adherence, region-wise color fidelity, visual quality, and transformation diversity.",
        "url": "http://arxiv.org/abs/2511.08061v1",
        "published_date": "2025-11-11T10:00:32+00:00",
        "updated_date": "2025-11-12T01:37:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Aditi Singhania",
            "Arushi Jain",
            "Krutik Malani",
            "Riddhi Dhawan",
            "Souymodip Chakraborty",
            "Vineet Batra",
            "Ankit Phogat"
        ],
        "tldr": "This paper introduces a LoRA fine-tuned diffusion model with latent concatenation and masked Conditional Flow Matching to improve identity consistency and prompt diversity in subject-driven image generation. They also present a distilled data curation framework and a fine-grained evaluation framework.",
        "tldr_zh": "本文提出了一种LoRA微调的扩散模型，采用潜在连接和掩码条件流匹配，以提高主体驱动图像生成中的身份一致性和提示多样性。同时，他们还提出了一个提炼的数据管理框架和一个细粒度的评估框架。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation",
        "summary": "Generating ground-level images from aerial views is a challenging task due to extreme viewpoint disparity, occlusions, and a limited field of view. We introduce Top2Ground, a novel diffusion-based method that directly generates photorealistic ground-view images from aerial input images without relying on intermediate representations such as depth maps or 3D voxels. Specifically, we condition the denoising process on a joint representation of VAE-encoded spatial features (derived from aerial RGB images and an estimated height map) and CLIP-based semantic embeddings. This design ensures the generation is both geometrically constrained by the scene's 3D structure and semantically consistent with its content. We evaluate Top2Ground on three diverse datasets: CVUSA, CVACT, and the Auto Arborist. Our approach shows 7.3% average improvement in SSIM across three benchmark datasets, showing Top2Ground can robustly handle both wide and narrow fields of view, highlighting its strong generalization capabilities.",
        "url": "http://arxiv.org/abs/2511.08258v1",
        "published_date": "2025-11-11T13:53:07+00:00",
        "updated_date": "2025-11-12T01:49:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jae Joong Lee",
            "Bedrich Benes"
        ],
        "tldr": "The paper introduces Top2Ground, a diffusion model that directly generates ground-level images from aerial views by conditioning on VAE-encoded spatial features and CLIP-based semantic embeddings, achieving improved SSIM scores on diverse datasets.",
        "tldr_zh": "该论文介绍了Top2Ground，一种扩散模型，通过结合VAE编码的空间特征和CLIP语义嵌入，直接从航空视图生成地面图像，并在不同数据集上实现了改进的SSIM分数。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Bridge: Universal Visual Perception Representations Generating",
        "summary": "Recent advances in diffusion models have achieved remarkable success in isolated computer vision tasks such as text-to-image generation, depth estimation, and optical flow. However, these models are often restricted by a ``single-task-single-model'' paradigm, severely limiting their generalizability and scalability in multi-task scenarios. Motivated by the cross-domain generalization ability of large language models, we propose a universal visual perception framework based on flow matching that can generate diverse visual representations across multiple tasks. Our approach formulates the process as a universal flow-matching problem from image patch tokens to task-specific representations rather than an independent generation or regression problem. By leveraging a strong self-supervised foundation model as the anchor and introducing a multi-scale, circular task embedding mechanism, our method learns a universal velocity field to bridge the gap between heterogeneous tasks, supporting efficient and flexible representation transfer. Extensive experiments on classification, detection, segmentation, depth estimation, and image-text retrieval demonstrate that our model achieves competitive performance in both zero-shot and fine-tuned settings, outperforming prior generalist and several specialist models. Ablation studies further validate the robustness, scalability, and generalization of our framework. Our work marks a significant step towards general-purpose visual perception, providing a solid foundation for future research in universal vision modeling.",
        "url": "http://arxiv.org/abs/2511.07877v1",
        "published_date": "2025-11-11T06:25:30+00:00",
        "updated_date": "2025-11-12T01:26:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yilin Gao",
            "Shuguang Dou",
            "Junzhou Li",
            "Zhiheng Yu",
            "Yin Li",
            "Dongsheng Jiang",
            "Shugong Xu"
        ],
        "tldr": "This paper introduces a universal visual perception framework, Visual Bridge, based on flow matching, aiming to generate diverse visual representations across various tasks and achieve general-purpose visual perception.",
        "tldr_zh": "该论文介绍了一种基于流匹配的通用视觉感知框架 Visual Bridge，旨在生成跨多项任务的各种视觉表示，并实现通用视觉感知。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Bridge: Universal Visual Perception Representations Generating",
        "summary": "Recent advances in diffusion models have achieved remarkable success in isolated computer vision tasks such as text-to-image generation, depth estimation, and optical flow. However, these models are often restricted by a ``single-task-single-model'' paradigm, severely limiting their generalizability and scalability in multi-task scenarios. Motivated by the cross-domain generalization ability of large language models, we propose a universal visual perception framework based on flow matching that can generate diverse visual representations across multiple tasks. Our approach formulates the process as a universal flow-matching problem from image patch tokens to task-specific representations rather than an independent generation or regression problem. By leveraging a strong self-supervised foundation model as the anchor and introducing a multi-scale, circular task embedding mechanism, our method learns a universal velocity field to bridge the gap between heterogeneous tasks, supporting efficient and flexible representation transfer. Extensive experiments on classification, detection, segmentation, depth estimation, and image-text retrieval demonstrate that our model achieves competitive performance in both zero-shot and fine-tuned settings, outperforming prior generalist and several specialist models. Ablation studies further validate the robustness, scalability, and generalization of our framework. Our work marks a significant step towards general-purpose visual perception, providing a solid foundation for future research in universal vision modeling.",
        "url": "http://arxiv.org/abs/2511.07877v1",
        "published_date": "2025-11-11T06:25:30+00:00",
        "updated_date": "2025-11-12T01:26:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yilin Gao",
            "Shuguang Dou",
            "Junzhou Li",
            "Zhiheng Yu",
            "Yin Li",
            "Dongsheng Jiang",
            "Shugong Xu"
        ],
        "tldr": "This paper introduces a universal visual perception framework, 'Visual Bridge,' based on flow matching, which generates diverse visual representations across multiple tasks, aiming for general-purpose visual perception similar to large language models.",
        "tldr_zh": "本文介绍了一种基于流匹配的通用视觉感知框架“Visual Bridge”，它可以跨多个任务生成不同的视觉表示，目标是实现类似于大型语言模型的通用视觉感知。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CLIP is All You Need for Human-like Semantic Representations in Stable Diffusion",
        "summary": "Latent diffusion models such as Stable Diffusion achieve state-of-the-art results on text-to-image generation tasks. However, the extent to which these models have a semantic understanding of the images they generate is not well understood. In this work, we investigate whether the internal representations used by these models during text-to-image generation contain semantic information that is meaningful to humans. To do so, we perform probing on Stable Diffusion with simple regression layers that predict semantic attributes for objects and evaluate these predictions against human annotations. Surprisingly, we find that this success can actually be attributed to the text encoding occurring in CLIP rather than the reverse diffusion process. We demonstrate that groups of specific semantic attributes have markedly different decoding accuracy than the average, and are thus represented to different degrees. Finally, we show that attributes become more difficult to disambiguate from one another during the inverse diffusion process, further demonstrating the strongest semantic representation of object attributes in CLIP. We conclude that the separately trained CLIP vision-language model is what determines the human-like semantic representation, and that the diffusion process instead takes the role of a visual decoder.",
        "url": "http://arxiv.org/abs/2511.08075v1",
        "published_date": "2025-11-11T10:22:45+00:00",
        "updated_date": "2025-11-12T01:38:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Cameron Braunstein",
            "Mariya Toneva",
            "Eddy Ilg"
        ],
        "tldr": "The paper investigates the semantic understanding of Stable Diffusion and finds that CLIP provides the primary human-like semantic representation used in the text-to-image generation process, rather than the diffusion process itself.",
        "tldr_zh": "该论文研究了Stable Diffusion的语义理解能力，发现CLIP提供了文本到图像生成过程中主要的人类语义表示，而不是扩散过程本身。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Human Motion Synthesis in 3D Scenes via Unified Scene Semantic Occupancy",
        "summary": "Human motion synthesis in 3D scenes relies heavily on scene comprehension, while current methods focus mainly on scene structure but ignore the semantic understanding. In this paper, we propose a human motion synthesis framework that take an unified Scene Semantic Occupancy (SSO) for scene representation, termed SSOMotion. We design a bi-directional tri-plane decomposition to derive a compact version of the SSO, and scene semantics are mapped to an unified feature space via CLIP encoding and shared linear dimensionality reduction. Such strategy can derive the fine-grained scene semantic structures while significantly reduce redundant computations. We further take these scene hints and movement direction derived from instructions for motion control via frame-wise scene query. Extensive experiments and ablation studies conducted on cluttered scenes using ShapeNet furniture, as well as scanned scenes from PROX and Replica datasets, demonstrate its cutting-edge performance while validating its effectiveness and generalization ability. Code will be publicly available at https://github.com/jingyugong/SSOMotion.",
        "url": "http://arxiv.org/abs/2511.07819v1",
        "published_date": "2025-11-11T04:33:16+00:00",
        "updated_date": "2025-11-12T01:21:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gong Jingyu",
            "Tong Kunkun",
            "Chen Zhuoran",
            "Yuan Chuanhan",
            "Chen Mingang",
            "Zhang Zhizhong",
            "Tan Xin",
            "Xie Yuan"
        ],
        "tldr": "This paper introduces SSOMotion, a framework for synthesizing human motion in 3D scenes using a unified scene semantic occupancy representation (SSO) and CLIP encoding for fine-grained scene understanding, demonstrating state-of-the-art performance on complex datasets.",
        "tldr_zh": "该论文提出了SSOMotion，一个用于在3D场景中合成人体运动的框架。该框架使用统一的场景语义占用表示（SSO）和CLIP编码，以实现精细的场景理解，并在复杂数据集上展示了最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "From Exploration to Exploitation: A Two-Stage Entropy RLVR Approach for Noise-Tolerant MLLM Training",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) for Multimodal Large Language Models (MLLMs) is highly dependent on high-quality labeled data, which is often scarce and prone to substantial annotation noise in real-world scenarios. Existing unsupervised RLVR methods, including pure entropy minimization, can overfit to incorrect labels and limit the crucial reward ranking signal for Group-Relative Policy Optimization (GRPO). To address these challenges and enhance noise tolerance, we propose a novel two-stage, token-level entropy optimization method for RLVR. This approach dynamically guides the model from exploration to exploitation during training. In the initial exploration phase, token-level entropy maximization promotes diverse and stochastic output generation, serving as a strong regularizer that prevents premature convergence to noisy labels and ensures sufficient intra-group variation, which enables more reliable reward gradient estimation in GRPO. As training progresses, the method transitions into the exploitation phase, where token-level entropy minimization encourages the model to produce confident and deterministic outputs, thereby consolidating acquired knowledge and refining prediction accuracy. Empirically, across three MLLM backbones - Qwen2-VL-2B, Qwen2-VL-7B, and Qwen2.5-VL-3B - spanning diverse noise settings and multiple tasks, our phased strategy consistently outperforms prior approaches by unifying and enhancing external, internal, and entropy-based methods, delivering robust and superior performance across the board.",
        "url": "http://arxiv.org/abs/2511.07738v1",
        "published_date": "2025-11-11T01:42:37+00:00",
        "updated_date": "2025-11-12T01:14:01+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Donglai Xu",
            "Hongzheng Yang",
            "Yuzhi Zhao",
            "Pingping Zhang",
            "Jinpeng Chen",
            "Wenao Ma",
            "Zhijian Hou",
            "Mengyang Wu",
            "Xiaolei Li",
            "Senkang Hu",
            "Ziyi Guan",
            "Jason Chun Lok Li",
            "Lai Man Po"
        ],
        "tldr": "This paper introduces a two-stage entropy optimization method for Reinforcement Learning with Verifiable Rewards (RLVR) to improve the noise tolerance of Multimodal Large Language Models (MLLMs) during training, showing consistent improvements across different models and tasks.",
        "tldr_zh": "本文提出了一种两阶段熵优化方法，用于可验证奖励的强化学习（RLVR），以提高多模态大型语言模型（MLLM）在训练过程中的噪声容忍度，并在不同的模型和任务中表现出持续的改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Inference-Time Scaling of Diffusion Models for Infrared Data Generation",
        "summary": "Infrared imagery enables temperature-based scene understanding using passive sensors, particularly under conditions of low visibility where traditional RGB imaging fails. Yet, developing downstream vision models for infrared applications is hindered by the scarcity of high-quality annotated data, due to the specialized expertise required for infrared annotation. While synthetic infrared image generation has the potential to accelerate model development by providing large-scale, diverse training data, training foundation-level generative diffusion models in the infrared domain has remained elusive due to limited datasets. In light of such data constraints, we explore an inference-time scaling approach using a domain-adapted CLIP-based verifier for enhanced infrared image generation quality. We adapt FLUX.1-dev, a state-of-the-art text-to-image diffusion model, to the infrared domain by finetuning it on a small sample of infrared images using parameter-efficient techniques. The trained verifier is then employed during inference to guide the diffusion sampling process toward higher quality infrared generations that better align with input text prompts. Empirically, we find that our approach leads to consistent improvements in generation quality, reducing FID scores on the KAIST Multispectral Pedestrian Detection Benchmark dataset by 10% compared to unguided baseline samples. Our results suggest that inference-time guidance offers a promising direction for bridging the domain gap in low-data infrared settings.",
        "url": "http://arxiv.org/abs/2511.07362v1",
        "published_date": "2025-11-10T18:18:38+00:00",
        "updated_date": "2025-11-11T02:52:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Kai A. Horstmann",
            "Maxim Clouser",
            "Kia Khezeli"
        ],
        "tldr": "This paper explores inference-time scaling of a text-to-image diffusion model for infrared image generation, using a CLIP-based verifier to improve quality in low-data settings. They demonstrate improved FID scores on the KAIST dataset.",
        "tldr_zh": "本文探索了文本到图像扩散模型在红外图像生成中的推理时缩放，使用基于CLIP的验证器来提高低数据环境下的质量。他们展示了KAIST数据集上改进的FID分数。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SynWeather: Weather Observation Data Synthesis across Multiple Regions and Variables via a General Diffusion Transformer",
        "summary": "With the advancement of meteorological instruments, abundant data has become available. Current approaches are typically focus on single-variable, single-region tasks and primarily rely on deterministic modeling. This limits unified synthesis across variables and regions, overlooks cross-variable complementarity and often leads to over-smoothed results. To address above challenges, we introduce SynWeather, the first dataset designed for Unified Multi-region and Multi-variable Weather Observation Data Synthesis. SynWeather covers four representative regions: the Continental United States, Europe, East Asia, and Tropical Cyclone regions, as well as provides high-resolution observations of key weather variables, including Composite Radar Reflectivity, Hourly Precipitation, Visible Light, and Microwave Brightness Temperature. In addition, we introduce SynWeatherDiff, a general and probabilistic weather synthesis model built upon the Diffusion Transformer framework to address the over-smoothed problem. Experiments on the SynWeather dataset demonstrate the effectiveness of our network compared with both task-specific and general models.",
        "url": "http://arxiv.org/abs/2511.08291v1",
        "published_date": "2025-11-11T14:24:49+00:00",
        "updated_date": "2025-11-12T01:51:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaiyi Xu",
            "Junchao Gong",
            "Zhiwang Zhou",
            "Zhangrui Li",
            "Yuandong Pu",
            "Yihao Liu",
            "Ben Fei",
            "Fenghua Ling",
            "Wenlong Zhang",
            "Lei Bei"
        ],
        "tldr": "The paper introduces SynWeather, a new dataset for unified multi-region and multi-variable weather observation data synthesis, and SynWeatherDiff, a diffusion transformer model for this task, demonstrating improved performance over existing methods.",
        "tldr_zh": "该论文介绍了SynWeather，一个新的用于统一多区域和多变量天气观测数据合成的数据集，以及SynWeatherDiff，一个用于此任务的扩散Transformer模型，并证明了其相对于现有方法的改进性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LayerEdit: Disentangled Multi-Object Editing via Conflict-Aware Multi-Layer Learning",
        "summary": "Text-driven multi-object image editing which aims to precisely modify multiple objects within an image based on text descriptions, has recently attracted considerable interest. Existing works primarily follow the localize-editing paradigm, focusing on independent object localization and editing while neglecting critical inter-object interactions. However, this work points out that the neglected attention entanglements in inter-object conflict regions, inherently hinder disentangled multi-object editing, leading to either inter-object editing leakage or intra-object editing constraints. We thereby propose a novel multi-layer disentangled editing framework LayerEdit, a training-free method which, for the first time, through precise object-layered decomposition and coherent fusion, enables conflict-free object-layered editing. Specifically, LayerEdit introduces a novel \"decompose-editingfusion\" framework, consisting of: (1) Conflict-aware Layer Decomposition module, which utilizes an attention-aware IoU scheme and time-dependent region removing, to enhance conflict awareness and suppression for layer decomposition. (2) Object-layered Editing module, to establish coordinated intra-layer text guidance and cross-layer geometric mapping, achieving disentangled semantic and structural modifications. (3) Transparency-guided Layer Fusion module, to facilitate structure-coherent inter-object layer fusion through precise transparency guidance learning. Extensive experiments verify the superiority of LayerEdit over existing methods, showing unprecedented intra-object controllability and inter-object coherence in complex multi-object scenarios. Codes are available at: https://github.com/fufy1024/LayerEdit.",
        "url": "http://arxiv.org/abs/2511.08251v1",
        "published_date": "2025-11-11T13:45:06+00:00",
        "updated_date": "2025-11-12T01:48:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fengyi Fu",
            "Mengqi Huang",
            "Lei Zhang",
            "Zhendong Mao"
        ],
        "tldr": "The paper introduces LayerEdit, a novel training-free framework for text-driven multi-object image editing that addresses inter-object conflict during editing by using layer decomposition and coherent fusion. It claims to achieve better disentanglement between objects during editing compared to existing methods.",
        "tldr_zh": "该论文介绍了一个名为LayerEdit的全新免训练框架，用于文本驱动的多对象图像编辑。该框架通过分层分解和连贯融合解决编辑过程中对象间的冲突。 该方法声称相比现有方法能在编辑过程中实现更好的对象解缠。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "UltraGS: Gaussian Splatting for Ultrasound Novel View Synthesis",
        "summary": "Ultrasound imaging is a cornerstone of non-invasive clinical diagnostics, yet its limited field of view complicates novel view synthesis. We propose \\textbf{UltraGS}, a Gaussian Splatting framework optimized for ultrasound imaging. First, we introduce a depth-aware Gaussian splatting strategy, where each Gaussian is assigned a learnable field of view, enabling accurate depth prediction and precise structural representation. Second, we design SH-DARS, a lightweight rendering function combining low-order spherical harmonics with ultrasound-specific wave physics, including depth attenuation, reflection, and scattering, to model tissue intensity accurately. Third, we contribute the Clinical Ultrasound Examination Dataset, a benchmark capturing diverse anatomical scans under real-world clinical protocols. Extensive experiments on three datasets demonstrate UltraGS's superiority, achieving state-of-the-art results in PSNR (up to 29.55), SSIM (up to 0.89), and MSE (as low as 0.002) while enabling real-time synthesis at 64.69 fps. The code and dataset are open-sourced at: https://github.com/Bean-Young/UltraGS.",
        "url": "http://arxiv.org/abs/2511.07743v1",
        "published_date": "2025-11-11T01:54:12+00:00",
        "updated_date": "2025-11-12T01:14:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuezhe Yang",
            "Wenjie Cai",
            "Dexin Yang",
            "Yufang Dong",
            "Xingbo Dong",
            "Zhe Jin"
        ],
        "tldr": "UltraGS introduces a Gaussian Splatting framework optimized for novel view synthesis in ultrasound imaging by incorporating depth-aware Gaussians and a specialized rendering function, achieving state-of-the-art results on a new clinical ultrasound dataset.",
        "tldr_zh": "UltraGS 提出了一个高斯溅射框架，专门为超声成像中的新视角合成进行了优化，通过结合深度感知高斯函数和专门的渲染函数，并在新的临床超声数据集上实现了最先进的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]