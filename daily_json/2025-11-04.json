[
    {
        "title": "Towards One-step Causal Video Generation via Adversarial Self-Distillation",
        "summary": "Recent hybrid video generation models combine autoregressive temporal\ndynamics with diffusion-based spatial denoising, but their sequential,\niterative nature leads to error accumulation and long inference times. In this\nwork, we propose a distillation-based framework for efficient causal video\ngeneration that enables high-quality synthesis with extremely limited denoising\nsteps. Our approach builds upon the Distribution Matching Distillation (DMD)\nframework and proposes a novel Adversarial Self-Distillation (ASD) strategy,\nwhich aligns the outputs of the student model's n-step denoising process with\nits (n+1)-step version at the distribution level. This design provides smoother\nsupervision by bridging small intra-student gaps and more informative guidance\nby combining teacher knowledge with locally consistent student behavior,\nsubstantially improving training stability and generation quality in extremely\nfew-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame\nEnhancement (FFE) strategy, which allocates more denoising steps to the initial\nframes to mitigate error propagation while applying larger skipping steps to\nlater frames. Extensive experiments on VBench demonstrate that our method\nsurpasses state-of-the-art approaches in both one-step and two-step video\ngeneration. Notably, our framework produces a single distilled model that\nflexibly supports multiple inference-step settings, eliminating the need for\nrepeated re-distillation and enabling efficient, high-quality video synthesis.",
        "url": "http://arxiv.org/abs/2511.01419v1",
        "published_date": "2025-11-03T10:12:47+00:00",
        "updated_date": "2025-11-03T10:12:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongqi Yang",
            "Huayang Huang",
            "Xu Peng",
            "Xiaobin Hu",
            "Donghao Luo",
            "Jiangning Zhang",
            "Chengjie Wang",
            "Yu Wu"
        ],
        "tldr": "The paper presents an Adversarial Self-Distillation (ASD) framework for efficient one-step causal video generation, addressing the limitations of iterative denoising methods and achieving state-of-the-art results on VBench.",
        "tldr_zh": "该论文提出了一种对抗性自蒸馏（ASD）框架，用于高效的单步因果视频生成，解决了迭代去噪方法的局限性，并在VBench上取得了最先进的结果。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "MotionStream: Real-Time Video Generation with Interactive Motion Controls",
        "summary": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience.",
        "url": "http://arxiv.org/abs/2511.01266v1",
        "published_date": "2025-11-03T06:37:53+00:00",
        "updated_date": "2025-11-03T06:37:53+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Joonghyuk Shin",
            "Zhengqi Li",
            "Richard Zhang",
            "Jun-Yan Zhu",
            "Jaesik Park",
            "Eli Schechtman",
            "Xun Huang"
        ],
        "tldr": "MotionStream enables real-time, controllable video generation by distilling a text-to-video model into a causal architecture with sliding-window attention and attention sinks, achieving state-of-the-art results with sub-second latency.",
        "tldr_zh": "MotionStream通过将文本到视频模型提炼成具有滑动窗口注意力和注意力汇的因果架构，实现了实时的、可控的视频生成，并以亚秒级延迟实现了最先进的结果。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment",
        "summary": "Foundation models in video generation are demonstrating remarkable\ncapabilities as potential world models for simulating the physical world.\nHowever, their application in high-stakes domains like surgery, which demand\ndeep, specialized causal knowledge rather than general physical rules, remains\na critical unexplored gap. To systematically address this challenge, we present\nSurgVeo, the first expert-curated benchmark for video generation model\nevaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,\nfour-tiered framework tailored to assess model outputs from basic appearance to\ncomplex surgical strategy. On the basis of the SurgVeo benchmark, we task the\nadvanced Veo-3 model with a zero-shot prediction task on surgical clips from\nlaparoscopic and neurosurgical procedures. A panel of four board-certified\nsurgeons evaluates the generated videos according to the SPP. Our results\nreveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual\nPerceptual Plausibility, it fails critically at higher levels of the SPP,\nincluding Instrument Operation Plausibility, Environment Feedback Plausibility,\nand Surgical Intent Plausibility. This work provides the first quantitative\nevidence of the chasm between visually convincing mimicry and causal\nunderstanding in surgical AI. Our findings from SurgVeo and the SPP establish a\ncrucial foundation and roadmap for developing future models capable of\nnavigating the complexities of specialized, real-world healthcare domains.",
        "url": "http://arxiv.org/abs/2511.01775v1",
        "published_date": "2025-11-03T17:28:54+00:00",
        "updated_date": "2025-11-03T17:28:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Zhen Chen",
            "Qing Xu",
            "Jinlin Wu",
            "Biao Yang",
            "Yuhao Zhai",
            "Geng Guo",
            "Jing Zhang",
            "Yinlu Ding",
            "Nassir Navab",
            "Jiebo Luo"
        ],
        "tldr": "This paper introduces SurgVeo, a benchmark for evaluating surgical video generation models, and demonstrates that current advanced models like Veo-3 struggle with causal understanding in surgical contexts despite visual plausibility.",
        "tldr_zh": "本文介绍了SurgVeo，一个用于评估手术视频生成模型的基准，并表明，尽管像Veo-3这样的先进模型具有视觉上的合理性，但在手术环境中，它们在因果理解方面仍存在不足。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process",
        "summary": "Vision-language-action (VLA) models aim to understand natural language\ninstructions and visual observations and to execute corresponding actions as an\nembodied agent. Recent work integrates future images into the\nunderstanding-acting loop, yielding unified VLAs that jointly understand,\ngenerate, and act -- reading text and images and producing future images and\nactions. However, these models either rely on external experts for modality\nunification or treat image generation and action prediction as separate\nprocesses, limiting the benefits of direct synergy between these tasks. Our\ncore philosophy is to optimize generation and action jointly through a\nsynchronous denoising process, where the iterative refinement enables actions\nto evolve from initialization, under constant and sufficient visual guidance.\nWe ground this philosophy in our proposed Unified Diffusion VLA and Joint\nDiscrete Denoising Diffusion Process (JD3P), which is a joint diffusion process\nthat integrates multiple modalities into a single denoising trajectory to serve\nas the key mechanism enabling understanding, generation, and acting to be\nintrinsically synergistic. Our model and theory are built on a unified\ntokenized space of all modalities and a hybrid attention mechanism. We further\npropose a two-stage training pipeline and several inference-time techniques\nthat optimize performance and efficiency. Our approach achieves\nstate-of-the-art performance on benchmarks such as CALVIN, LIBERO, and\nSimplerEnv with 4$\\times$ faster inference than autoregressive methods, and we\ndemonstrate its effectiveness through in-depth analysis and real-world\nevaluations. Our project page is available at\nhttps://irpn-eai.github.io/UD-VLA.github.io/.",
        "url": "http://arxiv.org/abs/2511.01718v1",
        "published_date": "2025-11-03T16:26:54+00:00",
        "updated_date": "2025-11-03T16:26:54+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Jiayi Chen",
            "Wenxuan Song",
            "Pengxiang Ding",
            "Ziyang Zhou",
            "Han Zhao",
            "Feilong Tang",
            "Donglin Wang",
            "Haoang Li"
        ],
        "tldr": "This paper introduces a Unified Diffusion VLA model (UD-VLA) using a Joint Discrete Denoising Diffusion Process (JD3P) for vision-language-action tasks, achieving SOTA results with faster inference.",
        "tldr_zh": "本文介绍了一种统一的扩散VLA模型（UD-VLA），它采用联合离散去噪扩散过程（JD3P）来执行视觉-语言-动作任务，并以更快的推理速度实现了SOTA结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback",
        "summary": "Relighting is a crucial task with both practical demand and artistic value,\nand recent diffusion models have shown strong potential by enabling rich and\ncontrollable lighting effects. However, as they are typically optimized in\nsemantic latent space, where proximity does not guarantee physical correctness\nin visual space, they often produce unrealistic results, such as overexposed\nhighlights, misaligned shadows, and incorrect occlusions. We address this with\nUniLumos, a unified relighting framework for both images and videos that brings\nRGB-space geometry feedback into a flow matching backbone. By supervising the\nmodel with depth and normal maps extracted from its outputs, we explicitly\nalign lighting effects with the scene structure, enhancing physical\nplausibility. Nevertheless, this feedback requires high-quality outputs for\nsupervision in visual space, making standard multi-step denoising\ncomputationally expensive. To mitigate this, we employ path consistency\nlearning, allowing supervision to remain effective even under few-step training\nregimes. To enable fine-grained relighting control and supervision, we design a\nstructured six-dimensional annotation protocol capturing core illumination\nattributes. Building upon this, we propose LumosBench, a disentangled\nattribute-level benchmark that evaluates lighting controllability via large\nvision-language models, enabling automatic and interpretable assessment of\nrelighting precision across individual dimensions. Extensive experiments\ndemonstrate that UniLumos achieves state-of-the-art relighting quality with\nsignificantly improved physical consistency, while delivering a 20x speedup for\nboth image and video relighting. Code is available at\nhttps://github.com/alibaba-damo-academy/Lumos-Custom.",
        "url": "http://arxiv.org/abs/2511.01678v1",
        "published_date": "2025-11-03T15:41:41+00:00",
        "updated_date": "2025-11-03T15:41:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ropeway Liu",
            "Hangjie Yuan",
            "Bo Dong",
            "Jiazheng Xing",
            "Jinwang Wang",
            "Rui Zhao",
            "Yan Xing",
            "Weihua Chen",
            "Fan Wang"
        ],
        "tldr": "UniLumos is a fast and unified image and video relighting framework that uses RGB-space geometry feedback within a flow matching backbone and path consistency learning to achieve state-of-the-art, physically plausible results with 20x speedup, evaluated with a new disentangled benchmark.",
        "tldr_zh": "UniLumos是一个快速且统一的图像和视频重照明框架，它在流量匹配骨干网络中使用RGB空间几何反馈和路径一致性学习，以实现最先进的、物理上合理的结果，并实现了20倍的加速。该方法通过一个新的解耦基准进行评估。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation",
        "summary": "The unification of understanding and generation within a single multi-modal\nlarge model (MLLM) remains one significant challenge, largely due to the\ndichotomy between continuous and discrete visual tokenizations. Continuous\ntokenizer (CT) achieves strong performance by bridging multiple\nindependently-trained understanding modules and generation modules, but suffers\nfrom complex multi-stage pipelines and substantial engineering overhead.\nConversely, discrete tokenizers (DT) offer a conceptually elegant idea by\nquantizing each image into a primitive, but inevitably leading to information\nloss and performance degradation. To resolve this tension, we question the\nbinary choice between CT and DT, inspired by the wave-particle duality of\nlight, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).\nWe treat visual data as a flexible composition of image primitives derived from\nquantized codebooks, with the crucial insight that the primitive number\nassigned to each visual sample is adaptively determined according to its\ncomplexity: simple instances use a few primitives, emulating discrete\ntokenization, while complex instances use many, approximating continuous\ntokenization. Two core components are designed: Diverse Quantitative\nPrimitives, which encourage primitives orthogonality to better populate\ninformation space, and Dynamic Primitive Allocator, which assesses sample\ncomplexity to determine the optimal set of primitives. Extensive experiments on\nreconstruction, retrieval and classification show that CDD-VT achieves superior\nperformance over to specialized CT and DT, effectively getting strong result\nwithin a concise and scalable MLLM.",
        "url": "http://arxiv.org/abs/2511.01593v1",
        "published_date": "2025-11-03T13:58:32+00:00",
        "updated_date": "2025-11-03T13:58:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yizhu Chen",
            "Chen Ju",
            "Zhicheng Wang",
            "Shuai Xiao",
            "Xu Chen",
            "Jinsong Lan",
            "Xiaoyong Zhu",
            "Ying Chen"
        ],
        "tldr": "This paper introduces a Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT) that adaptively tokenizes images based on their complexity, achieving superior performance in multi-modal tasks compared to purely continuous or discrete tokenizers.",
        "tldr_zh": "本文介绍了一种连续-离散双重视觉标记器 (CDD-VT)，它根据图像的复杂性自适应地标记图像，与纯粹的连续或离散标记器相比，在多模态任务中实现了卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation",
        "summary": "Recent studies have identified Direct Preference Optimization (DPO) as an\nefficient and reward-free approach to improving video generation quality.\nHowever, existing methods largely follow image-domain paradigms and are mainly\ndeveloped on small-scale models (approximately 2B parameters), limiting their\nability to address the unique challenges of video tasks, such as costly data\nconstruction, unstable training, and heavy memory consumption. To overcome\nthese limitations, we introduce a GT-Pair that automatically builds\nhigh-quality preference pairs by using real videos as positives and\nmodel-generated videos as negatives, eliminating the need for any external\nannotation. We further present Reg-DPO, which incorporates the SFT loss as a\nregularization term into the DPO loss to enhance training stability and\ngeneration fidelity. Additionally, by combining the FSDP framework with\nmultiple memory optimization techniques, our approach achieves nearly three\ntimes higher training capacity than using FSDP alone. Extensive experiments on\nboth I2V and T2V tasks across multiple datasets demonstrate that our method\nconsistently outperforms existing approaches, delivering superior video\ngeneration quality.",
        "url": "http://arxiv.org/abs/2511.01450v2",
        "published_date": "2025-11-03T11:04:22+00:00",
        "updated_date": "2025-11-05T16:11:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jie Du",
            "Xinyu Gong",
            "Qingshan Tan",
            "Wen Li",
            "Yangming Cheng",
            "Weitao Wang",
            "Chenlu Zhan",
            "Suhui Wu",
            "Hao Zhang",
            "Jun Zhang"
        ],
        "tldr": "The paper introduces Reg-DPO, a DPO-based video generation method incorporating SFT regularization and a novel GT-Pair data construction strategy to improve training stability, fidelity, and memory efficiency, achieving superior results on I2V and T2V tasks.",
        "tldr_zh": "该论文介绍了Reg-DPO，一种基于DPO的视频生成方法，它结合了SFT正则化和新颖的GT-Pair数据构建策略，以提高训练稳定性、保真度和内存效率，并在I2V和T2V任务上取得了优异的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion",
        "summary": "Recent advancements in text-to-image diffusion models have significantly\nimproved the personalization and stylization of generated images. However,\nprevious studies have only assessed content similarity under a single style\nintensity. In our experiments, we observe that increasing style intensity leads\nto a significant loss of content features, resulting in a suboptimal\ncontent-style frontier. To address this, we propose a novel approach to expand\nthe content-style frontier by leveraging Content-Style Subspace Blending and a\nContent-Style Balance loss. Our method improves content similarity across\nvarying style intensities, significantly broadening the content-style frontier.\nExtensive experiments demonstrate that our approach outperforms existing\ntechniques in both qualitative and quantitative evaluations, achieving superior\ncontent-style trade-off with significantly lower Inverted Generational Distance\n(IGD) and Generational Distance (GD) scores compared to current methods.",
        "url": "http://arxiv.org/abs/2511.01355v1",
        "published_date": "2025-11-03T09:03:45+00:00",
        "updated_date": "2025-11-03T09:03:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Linhao Huang"
        ],
        "tldr": "This paper introduces a novel approach, Content-Style Subspace Blending with a Content-Style Balance loss, to improve content similarity across varying style intensities in text-to-image diffusion models, effectively expanding the content-style frontier.",
        "tldr_zh": "该论文提出了一种新的方法，即内容-风格子空间融合与内容-风格平衡损失，以提高文本到图像扩散模型中不同风格强度下的内容相似性，从而有效地扩展内容-风格边界。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "UniREditBench: A Unified Reasoning-based Image Editing Benchmark",
        "summary": "Recent advances in multi-modal generative models have driven substantial\nimprovements in image editing. However, current generative models still\nstruggle with handling diverse and complex image editing tasks that require\nimplicit reasoning, underscoring the need for a comprehensive benchmark to\nsystematically assess their performance across various reasoning scenarios.\nExisting benchmarks primarily focus on single-object attribute transformation\nin realistic scenarios, which, while effective, encounter two key challenges:\n(1) they largely overlook multi-object interactions as well as game-world\nscenarios that involve human-defined rules, which are common in real-life\napplications; (2) they only rely on textual references to evaluate the\ngenerated images, potentially leading to systematic misjudgments, especially in\ncomplex reasoning scenarios. To this end, this work proposes UniREditBench, a\nunified benchmark for reasoning-based image editing evaluation. It comprises\n2,700 meticulously curated samples, covering both real- and game-world\nscenarios across 8 primary dimensions and 18 sub-dimensions. To improve\nevaluation reliability, we introduce multimodal dual-reference evaluation,\nproviding both textual and ground-truth image references for each sample\nassessment. Furthermore, we design an automated multi-scenario data synthesis\npipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with\nhigh-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel\non this dataset and develop UniREdit-Bagel, demonstrating substantial\nimprovements in both in-domain and out-of-distribution settings. Through\nthorough benchmarking of both open-source and closed-source image editing\nmodels, we reveal their strengths and weaknesses across various aspects.",
        "url": "http://arxiv.org/abs/2511.01295v1",
        "published_date": "2025-11-03T07:24:57+00:00",
        "updated_date": "2025-11-03T07:24:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feng Han",
            "Yibin Wang",
            "Chenglin Li",
            "Zheming Liang",
            "Dianyi Wang",
            "Yang Jiao",
            "Zhipeng Wei",
            "Chao Gong",
            "Cheng Jin",
            "Jingjing Chen",
            "Jiaqi Wang"
        ],
        "tldr": "This paper introduces UniREditBench, a new benchmark for evaluating reasoning-based image editing models using multi-modal dual-reference evaluation, and presents UniREdit-Data-100K, a large-scale synthetic dataset with chain-of-thought annotations.",
        "tldr_zh": "该论文介绍了一个新的基准测试 UniREditBench，用于评估基于推理的图像编辑模型，它使用多模态双重参考评估，并提出了 UniREdit-Data-100K，一个具有思维链注释的大规模合成数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation",
        "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm for\nseamlessly unifying text and image understanding and generation. However,\nprevailing evaluations treat these abilities in isolation, such that tasks with\nmultimodal inputs and outputs are scored primarily through unimodal reasoning,\ni.e., textual benchmarks emphasize language-based reasoning, while visual\nbenchmarks emphasize reasoning outcomes manifested in the pixels. We introduce\nROVER to address this pressing need to test reciprocal cross-modal reasoning,\nthe use of one modality to guide, verify, or refine outputs in the other, an\nability central to the vision of unified multimodal intelligence. ROVER is a\nhuman-annotated benchmark that explicitly targets reciprocal cross-modal\nreasoning, which contains 1312 tasks grounded in 1876 images, spanning two\ncomplementary settings. Verbally-augmented reasoning for visual generation\nevaluates whether models can use verbal prompts and reasoning chains to guide\nfaithful image synthesis. Visually-augmented reasoning for verbal generation\nevaluates whether models can generate intermediate visualizations that\nstrengthen their own reasoning processes for question answering. Experiments on\n17 unified models reveal two key findings: (i) Cross-modal reasoning determines\nvisual generation quality, with interleaved models significantly outperforming\nnon-interleaved ones; notably, combining strong unimodal models fails to\nachieve comparable reasoning. (ii) Models show dissociation between physical\nand symbolic reasoning: they succeed at interpreting perceptual concepts\nliterally but fail to construct visual abstractions for symbolic tasks, where\nfaulty reasoning harms performance. These results highlight reciprocal\ncross-modal reasoning as a critical frontier for enabling true omnimodal\ngeneration.",
        "url": "http://arxiv.org/abs/2511.01163v1",
        "published_date": "2025-11-03T02:27:46+00:00",
        "updated_date": "2025-11-03T02:27:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongyuan Liang",
            "Wei Chow",
            "Feng Li",
            "Ziqiao Ma",
            "Xiyao Wang",
            "Jiageng Mao",
            "Jiuhai Chen",
            "Jiatao Gu",
            "Yue Wang",
            "Furong Huang"
        ],
        "tldr": "This paper introduces ROVER, a benchmark for evaluating reciprocal cross-modal reasoning in unified multimodal models, revealing limitations in current models' ability to use one modality to guide the other for generation tasks.",
        "tldr_zh": "该论文介绍了ROVER，一个用于评估统一多模态模型中互惠跨模态推理的基准，揭示了当前模型在使用一种模态来指导另一种模态进行生成任务时的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fractional Diffusion Bridge Models",
        "summary": "We present Fractional Diffusion Bridge Models (FDBM), a novel generative\ndiffusion bridge framework driven by an approximation of the rich and\nnon-Markovian fractional Brownian motion (fBM). Real stochastic processes\nexhibit a degree of memory effects (correlations in time), long-range\ndependencies, roughness and anomalous diffusion phenomena that are not captured\nin standard diffusion or bridge modeling due to the use of Brownian motion\n(BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM),\nwe construct FDBM that enable tractable inference while preserving the\nnon-Markovian nature of fBM. We prove the existence of a coupling-preserving\ngenerative diffusion bridge and leverage it for future state prediction from\npaired training data. We then extend our formulation to the Schr\\\"{o}dinger\nbridge problem and derive a principled loss function to learn the unpaired data\ntranslation. We evaluate FDBM on both tasks: predicting future protein\nconformations from aligned data, and unpaired image translation. In both\nsettings, FDBM achieves superior performance compared to the Brownian\nbaselines, yielding lower root mean squared deviation (RMSD) of C$_\\alpha$\natomic positions in protein structure prediction and lower Fr\\'echet Inception\nDistance (FID) in unpaired image translation.",
        "url": "http://arxiv.org/abs/2511.01795v1",
        "published_date": "2025-11-03T17:51:10+00:00",
        "updated_date": "2025-11-03T17:51:10+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.RO",
            "stat.ML"
        ],
        "authors": [
            "Gabriel Nobis",
            "Maximilian Springenberg",
            "Arina Belova",
            "Rembert Daems",
            "Christoph Knochenhauer",
            "Manfred Opper",
            "Tolga Birdal",
            "Wojciech Samek"
        ],
        "tldr": "This paper introduces Fractional Diffusion Bridge Models (FDBM) using a Markovian approximation of fractional Brownian motion to improve performance in tasks like protein conformation prediction and unpaired image translation compared to standard Brownian motion-based models.",
        "tldr_zh": "本文介绍了分数扩散桥模型 (FDBM)，该模型使用分数布朗运动的马尔可夫近似来提高蛋白质构象预测和非配对图像翻译等任务的性能，与基于标准布朗运动的模型相比有所提升。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image",
        "summary": "In this work, we introduce \\textbf{Wonder3D++}, a novel method for\nefficiently generating high-fidelity textured meshes from single-view images.\nRecent methods based on Score Distillation Sampling (SDS) have shown the\npotential to recover 3D geometry from 2D diffusion priors, but they typically\nsuffer from time-consuming per-shape optimization and inconsistent geometry. In\ncontrast, certain works directly produce 3D information via fast network\ninferences, but their results are often of low quality and lack geometric\ndetails. To holistically improve the quality, consistency, and efficiency of\nsingle-view reconstruction tasks, we propose a cross-domain diffusion model\nthat generates multi-view normal maps and the corresponding color images. To\nensure the consistency of generation, we employ a multi-view cross-domain\nattention mechanism that facilitates information exchange across views and\nmodalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that\ndrives high-quality surfaces from the multi-view 2D representations in only\nabout $3$ minute in a coarse-to-fine manner. Our extensive evaluations\ndemonstrate that our method achieves high-quality reconstruction results,\nrobust generalization, and good efficiency compared to prior works. Code\navailable at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.",
        "url": "http://arxiv.org/abs/2511.01767v1",
        "published_date": "2025-11-03T17:24:18+00:00",
        "updated_date": "2025-11-03T17:24:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuxiao Yang",
            "Xiao-Xiao Long",
            "Zhiyang Dou",
            "Cheng Lin",
            "Yuan Liu",
            "Qingsong Yan",
            "Yuexin Ma",
            "Haoqian Wang",
            "Zhiqiang Wu",
            "Wei Yin"
        ],
        "tldr": "Wonder3D++ proposes a cross-domain diffusion model for generating high-fidelity 3D textured meshes from single images, leveraging multi-view normal maps and a cascaded mesh extraction algorithm for improved quality, consistency, and efficiency.",
        "tldr_zh": "Wonder3D++提出了一种跨域扩散模型，用于从单张图像生成高保真度的3D纹理网格。该方法利用多视角法线贴图和级联网格提取算法，以提高质量、一致性和效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Driving scenario generation and evaluation using a structured layer representation and foundational models",
        "summary": "Rare and challenging driving scenarios are critical for autonomous vehicle\ndevelopment. Since they are difficult to encounter, simulating or generating\nthem using generative models is a popular approach. Following previous efforts\nto structure driving scenario representations in a layer model, we propose a\nstructured five-layer model to improve the evaluation and generation of rare\nscenarios. We use this model alongside large foundational models to generate\nnew driving scenarios using a data augmentation strategy. Unlike previous\nrepresentations, our structure introduces subclasses and characteristics for\nevery agent of the scenario, allowing us to compare them using an embedding\nspecific to our layer-model. We study and adapt two metrics to evaluate the\nrelevance of a synthetic dataset in the context of a structured representation:\nthe diversity score estimates how different the scenarios of a dataset are from\none another, while the originality score calculates how similar a synthetic\ndataset is from a real reference set. This paper showcases both metrics in\ndifferent generation setup, as well as a qualitative evaluation of synthetic\nvideos generated from structured scenario descriptions. The code and extended\nresults can be found at https://github.com/Valgiz/5LMSG.",
        "url": "http://arxiv.org/abs/2511.01541v1",
        "published_date": "2025-11-03T13:04:55+00:00",
        "updated_date": "2025-11-03T13:04:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Arthur Hubert",
            "Gamal Elghazaly",
            "Raphaël Frank"
        ],
        "tldr": "This paper introduces a structured five-layer model and foundational models to generate and evaluate rare driving scenarios using data augmentation. They evaluate the generated scenarios using diversity and originality metrics.",
        "tldr_zh": "本文介绍了一个结构化的五层模型和基础模型，用于使用数据增强来生成和评估罕见的驾驶场景。他们使用多样性和原创性指标来评估生成的场景。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Example-Based Feature Painting on Textures",
        "summary": "In this work, we propose a system that covers the complete workflow for\nachieving controlled authoring and editing of textures that present distinctive\nlocal characteristics. These include various effects that change the surface\nappearance of materials, such as stains, tears, holes, abrasions,\ndiscoloration, and more. Such alterations are ubiquitous in nature, and\nincluding them in the synthesis process is crucial for generating realistic\ntextures. We introduce a novel approach for creating textures with such\nblemishes, adopting a learning-based approach that leverages unlabeled\nexamples. Our approach does not require manual annotations by the user;\ninstead, it detects the appearance-altering features through unsupervised\nanomaly detection. The various textural features are then automatically\nclustered into semantically coherent groups, which are used to guide the\nconditional generation of images. Our pipeline as a whole goes from a small\nimage collection to a versatile generative model that enables the user to\ninteractively create and paint features on textures of arbitrary size. Notably,\nthe algorithms we introduce for diffusion-based editing and infinite stationary\ntexture generation are generic and should prove useful in other contexts as\nwell. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html",
        "url": "http://arxiv.org/abs/2511.01513v1",
        "published_date": "2025-11-03T12:26:50+00:00",
        "updated_date": "2025-11-03T12:26:50+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Andrei-Timotei Ardelean",
            "Tim Weyrich"
        ],
        "tldr": "The paper introduces a novel system for authoring and editing textures with realistic blemishes by leveraging unsupervised anomaly detection and conditional image generation. It enables users to interactively create and paint features on textures of arbitrary size.",
        "tldr_zh": "该论文介绍了一种新型系统，通过利用无监督异常检测和条件图像生成，来创作和编辑具有真实瑕疵的纹理。它使用户能够交互式地在任意大小的纹理上创建和绘制特征。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA",
        "summary": "The expansion of instruction-tuning data has enabled foundation language\nmodels to exhibit improved instruction adherence and superior performance\nacross diverse downstream tasks. Semantically-rich 3D human motion is being\nprogressively integrated with these foundation models to enhance multimodal\nunderstanding and cross-modal generation capabilities. However, the modality\ngap between human motion and text raises unresolved concerns about catastrophic\nforgetting during this integration. In addition, developing\nautoregressive-compatible pose representations that preserve generalizability\nacross heterogeneous downstream tasks remains a critical technical barrier. To\naddress these issues, we propose the Human Motion-Vision-Language Model\n(HMVLM), a unified framework based on the Mixture of Expert Low-Rank\nAdaption(MoE LoRA) strategy. The framework leverages the gating network to\ndynamically allocate LoRA expert weights based on the input prompt, enabling\nsynchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting\nduring instruction-tuning, we introduce a novel zero expert that preserves the\npre-trained parameters for general linguistic tasks. For pose representation,\nwe implement body-part-specific tokenization by partitioning the human body\ninto different joint groups, enhancing the spatial resolution of the\nrepresentation. Experiments show that our method effectively alleviates\nknowledge forgetting during instruction-tuning and achieves remarkable\nperformance across diverse human motion downstream tasks.",
        "url": "http://arxiv.org/abs/2511.01463v1",
        "published_date": "2025-11-03T11:22:10+00:00",
        "updated_date": "2025-11-03T11:22:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.GR",
            "68T45",
            "I.2.10; I.3.7"
        ],
        "authors": [
            "Lei Hu",
            "Yongjing Ye",
            "Shihong Xia"
        ],
        "tldr": "The paper introduces HMVLM, a unified framework leveraging MoE LoRA to integrate 3D human motion into foundation language models for enhanced multimodal understanding and generation, addressing catastrophic forgetting and improving pose representation.",
        "tldr_zh": "该论文介绍了HMVLM，一个统一的框架，利用MoE LoRA将3D人体运动整合到基础语言模型中，以增强多模态理解和生成能力，解决了灾难性遗忘问题并改进了姿势表示。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark",
        "summary": "We review human evaluation practices in automated, speech-driven 3D gesture\ngeneration and find a lack of standardisation and frequent use of flawed\nexperimental setups. This leads to a situation where it is impossible to know\nhow different methods compare, or what the state of the art is. In order to\naddress common shortcomings of evaluation design, and to standardise future\nuser studies in gesture-generation works, we introduce a detailed human\nevaluation protocol for the widely-used BEAT2 motion-capture dataset. Using\nthis protocol, we conduct large-scale crowdsourced evaluation to rank six\nrecent gesture-generation models -- each trained by its original authors --\nacross two key evaluation dimensions: motion realism and speech-gesture\nalignment. Our results provide strong evidence that 1) newer models do not\nconsistently outperform earlier approaches; 2) published claims of high motion\nrealism or speech-gesture alignment may not hold up under rigorous evaluation;\nand 3) the field must adopt disentangled assessments of motion quality and\nmultimodal alignment for accurate benchmarking in order to make progress.\nFinally, in order to drive standardisation and enable new evaluation research,\nwe will release five hours of synthetic motion from the benchmarked models;\nover 750 rendered video stimuli from the user studies -- enabling new\nevaluations without model reimplementation required -- alongside our\nopen-source rendering script, and the 16,000 pairwise human preference votes\ncollected for our benchmark.",
        "url": "http://arxiv.org/abs/2511.01233v1",
        "published_date": "2025-11-03T05:17:28+00:00",
        "updated_date": "2025-11-03T05:17:28+00:00",
        "categories": [
            "cs.CV",
            "cs.GR",
            "cs.HC",
            "I.3; I.2"
        ],
        "authors": [
            "Rajmund Nagy",
            "Hendric Voss",
            "Thanh Hoang-Minh",
            "Mihail Tsakov",
            "Teodor Nikolov",
            "Zeyi Zhang",
            "Tenglong Ao",
            "Sicheng Yang",
            "Shaoli Huang",
            "Yongkang Cheng",
            "M. Hamza Mughal",
            "Rishabh Dabral",
            "Kiran Chhatre",
            "Christian Theobalt",
            "Libin Liu",
            "Stefan Kopp",
            "Rachel McDonnell",
            "Michael Neff",
            "Taras Kucherenko",
            "Youngwoo Yoon",
            "Gustav Eje Henter"
        ],
        "tldr": "This paper addresses the lack of standardized and reliable human evaluation in speech-driven gesture generation, introduces a rigorous evaluation protocol and benchmark, and finds inconsistencies in prior claims of state-of-the-art performance.",
        "tldr_zh": "本文指出语音驱动的手势生成领域缺乏标准化和可靠的人工评估，引入了一种严格的评估协议和基准，并发现先前关于最先进性能的声明存在不一致之处。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images",
        "summary": "Compared to traditional methods, Deep Learning (DL) becomes a key technology\nfor computer vision tasks. Synthetic data generation is an interesting use case\nfor DL, especially in the field of medical imaging such as Magnetic Resonance\nImaging (MRI). The need for this task since the original MRI data is limited.\nThe generation of realistic medical images is completely difficult and\nchallenging. Generative Adversarial Networks (GANs) are useful for creating\nsynthetic medical images. In this paper, we propose a DL based methodology for\ncreating synthetic MRI data using the Deep Convolutional Generative Adversarial\nNetwork (DC-GAN) to address the problem of limited data. We also employ a\nConvolutional Neural Network (CNN) classifier to classify the brain tumor using\nsynthetic data and real MRI data. CNN is used to evaluate the quality and\nutility of the synthetic images. The classification result demonstrates\ncomparable performance on real and synthetic images, which validates the\neffectiveness of GAN-generated images for downstream tasks.",
        "url": "http://arxiv.org/abs/2511.01574v1",
        "published_date": "2025-11-03T13:42:44+00:00",
        "updated_date": "2025-11-03T13:42:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Md Sumon Ali",
            "Muzammil Behzad"
        ],
        "tldr": "This paper proposes a DC-GAN based method for generating synthetic brain tumor MRI images to address limited data availability, and validates the quality of generated images using a CNN classifier.",
        "tldr_zh": "该论文提出了一种基于DC-GAN的方法，用于生成合成的脑肿瘤MRI图像，以解决数据有限的问题，并通过CNN分类器验证生成的图像质量。",
        "relevance_score": 7,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]