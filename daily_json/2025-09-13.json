[
    {
        "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis",
        "summary": "Arbitrary resolution image generation provides a consistent visual experience\nacross devices, having extensive applications for producers and consumers.\nCurrent diffusion models increase computational demand quadratically with\nresolution, causing 4K image generation delays over 100 seconds. To solve this,\nwe explore the second generation upon the latent diffusion models, where the\nfixed latent generated by diffusion models is regarded as the content\nrepresentation and we propose to decode arbitrary resolution images with a\ncompact generated latent using a one-step generator. Thus, we present the\n\\textbf{InfGen}, replacing the VAE decoder with the new generator, for\ngenerating images at any resolution from a fixed-size latent without retraining\nthe diffusion models, which simplifies the process, reducing computational\ncomplexity and can be applied to any model using the same latent space.\nExperiments show InfGen is capable of improving many models into the arbitrary\nhigh-resolution era while cutting 4K image generation time to under 10 seconds.",
        "url": "http://arxiv.org/abs/2509.10441v1",
        "published_date": "2025-09-12T17:48:57+00:00",
        "updated_date": "2025-09-12T17:48:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tao Han",
            "Wanghan Xu",
            "Junchao Gong",
            "Xiaoyu Yue",
            "Song Guo",
            "Luping Zhou",
            "Lei Bai"
        ],
        "tldr": "InfGen addresses the computational cost of high-resolution image generation by using a one-step generator to decode arbitrary resolution images from a fixed-size latent space produced by diffusion models, significantly reducing generation time.",
        "tldr_zh": "InfGen 通过使用一步生成器从扩散模型生成的固定大小潜在空间解码任意分辨率的图像，解决了高分辨率图像生成计算成本过高的问题，从而显著缩短了生成时间。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching",
        "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
        "url": "http://arxiv.org/abs/2509.10312v1",
        "published_date": "2025-09-12T14:53:45+00:00",
        "updated_date": "2025-09-12T14:53:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhixin Zheng",
            "Xinyu Wang",
            "Chang Zou",
            "Shaobo Wang",
            "Linfeng Zhang"
        ],
        "tldr": "The paper introduces Cluster-Driven Feature Caching (ClusCa), a method to accelerate diffusion transformers by spatially clustering tokens and only computing a representative token in each cluster, achieving significant speedup without retraining.",
        "tldr_zh": "该论文介绍了集群驱动特征缓存（ClusCa），一种加速扩散Transformer的方法，通过对token进行空间聚类，并在每个集群中仅计算一个代表性token，从而在不重新训练的情况下实现显著加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization",
        "summary": "Vector quantization (VQ) is a key component in discrete tokenizers for image\ngeneration, but its training is often unstable due to straight-through\nestimation bias, one-step-behind updates, and sparse codebook gradients, which\nlead to suboptimal reconstruction performance and low codebook usage. In this\nwork, we analyze these fundamental challenges and provide a simple yet\neffective solution. To maintain high codebook usage in VQ networks (VQN) during\nlearning annealing and codebook size expansion, we propose VQBridge, a robust,\nscalable, and efficient projector based on the map function method. VQBridge\noptimizes code vectors through a compress-process-recover pipeline, enabling\nstable and effective codebook training. By combining VQBridge with learning\nannealing, our VQN achieves full (100%) codebook usage across diverse codebook\nconfigurations, which we refer to as FVQ (FullVQ). Through extensive\nexperiments, we demonstrate that FVQ is effective, scalable, and generalizable:\nit attains 100% codebook usage even with a 262k-codebook, achieves\nstate-of-the-art reconstruction performance, consistently improves with larger\ncodebooks, higher vector channels, or longer training, and remains effective\nacross different VQ variants. Moreover, when integrated with LlamaGen, FVQ\nsignificantly enhances image generation performance, surpassing visual\nautoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,\nhighlighting the importance of high-quality tokenizers for strong\nautoregressive image generation.",
        "url": "http://arxiv.org/abs/2509.10140v1",
        "published_date": "2025-09-12T11:08:21+00:00",
        "updated_date": "2025-09-12T11:08:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifan Chang",
            "Jie Qin",
            "Limeng Qiao",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Lin Ma",
            "Xingang Wang"
        ],
        "tldr": "The paper introduces VQBridge and FVQ, a method for training vector-quantized networks with 100% codebook utilization, leading to improved reconstruction and image generation performance, especially when integrated with LlamaGen.",
        "tldr_zh": "该论文介绍了VQBridge和FVQ，一种能够以100%代码本利用率训练向量量化网络的方法，从而提高重建和图像生成性能，尤其是在与LlamaGen集成时。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey",
        "summary": "Event camera sensors are bio-inspired sensors which asynchronously capture\nper-pixel brightness changes and output a stream of events encoding the\npolarity, location and time of these changes. These systems are witnessing\nrapid advancements as an emerging field, driven by their low latency, reduced\npower consumption, and ultra-high capture rates. This survey explores the\nevolution of fusing event-stream captured with traditional frame-based capture,\nhighlighting how this synergy significantly benefits various video restoration\nand 3D reconstruction tasks. The paper systematically reviews major deep\nlearning contributions to image/video enhancement and restoration, focusing on\ntwo dimensions: temporal enhancement (such as frame interpolation and motion\ndeblurring) and spatial enhancement (including super-resolution, low-light and\nHDR enhancement, and artifact reduction). This paper also explores how the 3D\nreconstruction domain evolves with the advancement of event driven fusion.\nDiverse topics are covered, with in-depth discussions on recent works for\nimproving visual quality under challenging conditions. Additionally, the survey\ncompiles a comprehensive list of openly available datasets, enabling\nreproducible research and benchmarking. By consolidating recent progress and\ninsights, this survey aims to inspire further research into leveraging event\ncamera systems, especially in combination with deep learning, for advanced\nvisual media restoration and enhancement.",
        "url": "http://arxiv.org/abs/2509.09971v1",
        "published_date": "2025-09-12T05:16:54+00:00",
        "updated_date": "2025-09-12T05:16:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aupendu Kar",
            "Vishnu Raj",
            "Guan-Ming Su"
        ],
        "tldr": "This survey explores the fusion of event camera data with traditional frame-based capture for video restoration and 3D reconstruction, focusing on deep learning contributions and providing a dataset compilation.",
        "tldr_zh": "该综述探讨了事件相机数据与传统帧捕获融合在视频修复和 3D 重建中的应用，重点关注深度学习的贡献，并提供了一个数据集汇编。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]