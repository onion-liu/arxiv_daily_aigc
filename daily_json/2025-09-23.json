[
    {
        "title": "ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation",
        "summary": "Generating high-fidelity images of humans with fine-grained control over\nattributes such as hairstyle and clothing remains a core challenge in\npersonalized text-to-image synthesis. While prior methods emphasize identity\npreservation from a reference image, they lack modularity and fail to provide\ndisentangled control over specific visual attributes. We introduce a new\nparadigm for attribute-specific image prompting, in which distinct sets of\nreference images are used to guide the generation of individual aspects of\nhuman appearance, such as hair, clothing, and identity. Our method encodes\nthese inputs into attribute-specific tokens, which are injected into a\npre-trained text-to-image diffusion model. This enables compositional and\ndisentangled control over multiple visual factors, even across multiple people\nwithin a single image. To promote natural composition and robust\ndisentanglement, we curate a cross-reference training dataset featuring\nsubjects in diverse poses and expressions, and propose a multi-attribute\ncross-reference training strategy that encourages the model to generate\nfaithful outputs from misaligned attribute inputs while adhering to both\nidentity and textual conditioning. Extensive experiments show that our method\nachieves state-of-the-art performance in accurately following both visual and\ntextual prompts. Our framework paves the way for more configurable human image\nsynthesis by combining visual prompting with text-driven generation. Webpage is\navailable at: https://snap-research.github.io/composeme/.",
        "url": "http://arxiv.org/abs/2509.18092v1",
        "published_date": "2025-09-22T17:59:30+00:00",
        "updated_date": "2025-09-22T17:59:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guocheng Gordon Qian",
            "Daniil Ostashev",
            "Egor Nemchinov",
            "Avihay Assouline",
            "Sergey Tulyakov",
            "Kuan-Chieh Jackson Wang",
            "Kfir Aberman"
        ],
        "tldr": "The paper introduces a novel attribute-specific image prompting method for controllable human image generation, utilizing distinct reference images for individual attributes like hair and clothing within a text-to-image diffusion model.",
        "tldr_zh": "该论文介绍了一种新的属性特定图像提示方法，用于可控的人类图像生成，利用不同的参考图像来控制头发和服装等各个属性，并将其应用到文本到图像的扩散模型中。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous Tissue Synthesis in Histopathology",
        "summary": "Synthetic data generation in histopathology faces unique challenges:\npreserving tissue heterogeneity, capturing subtle morphological features, and\nscaling to unannotated datasets. We present a latent diffusion model that\ngenerates realistic heterogeneous histopathology images through a novel\ndual-conditioning approach combining semantic segmentation maps with\ntissue-specific visual crops. Unlike existing methods that rely on text prompts\nor abstract visual embeddings, our approach preserves critical morphological\ndetails by directly incorporating raw tissue crops from corresponding semantic\nregions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches\nensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we\nintroduce a self-supervised extension that clusters whole-slide images into 100\ntissue types using foundation model embeddings, automatically generating\npseudo-semantic maps for training. Our method synthesizes high-fidelity images\nwith precise region-wise annotations, achieving superior performance on\ndownstream segmentation tasks. When evaluated on annotated datasets, models\ntrained on our synthetic data show competitive performance to those trained on\nreal data, demonstrating the utility of controlled heterogeneous tissue\ngeneration. In quantitative evaluation, prompt-guided synthesis reduces Frechet\nDistance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower\nFD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on\nsynthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within\n1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA\nwhole-slide images without manual annotations, our framework offers a practical\nsolution for an urgent need for generating diverse, annotated histopathology\ndata, addressing a critical bottleneck in computational pathology.",
        "url": "http://arxiv.org/abs/2509.17847v1",
        "published_date": "2025-09-22T14:41:43+00:00",
        "updated_date": "2025-09-22T14:41:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Saghir Alfasly",
            "Wataru Uegami",
            "MD Enamul Hoq",
            "Ghazal Alabtah",
            "H. R. Tizhoosh"
        ],
        "tldr": "This paper presents a novel diffusion model for generating realistic and diverse histopathology images by conditioning on semantic segmentation maps and tissue-specific visual crops, achieving significant improvements in downstream segmentation tasks and reducing the need for manual annotations.",
        "tldr_zh": "本文提出了一种新颖的扩散模型，通过以语义分割图和组织特异性视觉裁剪为条件，生成逼真且多样化的组织病理学图像，从而在下游分割任务中实现显着改进，并减少了手动注释的需求。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment",
        "summary": "Training-free video object editing aims to achieve precise object-level\nmanipulation, including object insertion, swapping, and deletion. However, it\nfaces significant challenges in maintaining fidelity and temporal consistency.\nExisting methods, often designed for U-Net architectures, suffer from two\nprimary limitations: inaccurate inversion due to first-order solvers, and\ncontextual conflicts caused by crude \"hard\" feature replacement. These issues\nare more challenging in Diffusion Transformers (DiTs), where the unsuitability\nof prior layer-selection heuristics makes effective guidance challenging. To\naddress these limitations, we introduce ContextFlow, a novel training-free\nframework for DiT-based video object editing. In detail, we first employ a\nhigh-order Rectified Flow solver to establish a robust editing foundation. The\ncore of our framework is Adaptive Context Enrichment (for specifying what to\nedit), a mechanism that addresses contextual conflicts. Instead of replacing\nfeatures, it enriches the self-attention context by concatenating Key-Value\npairs from parallel reconstruction and editing paths, empowering the model to\ndynamically fuse information. Additionally, to determine where to apply this\nenrichment (for specifying where to edit), we propose a systematic, data-driven\nanalysis to identify task-specific vital layers. Based on a novel Guidance\nResponsiveness Metric, our method pinpoints the most influential DiT blocks for\ndifferent tasks (e.g., insertion, swapping), enabling targeted and highly\neffective guidance. Extensive experiments show that ContextFlow significantly\noutperforms existing training-free methods and even surpasses several\nstate-of-the-art training-based approaches, delivering temporally coherent,\nhigh-fidelity results.",
        "url": "http://arxiv.org/abs/2509.17818v1",
        "published_date": "2025-09-22T14:13:31+00:00",
        "updated_date": "2025-09-22T14:13:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiyang Chen",
            "Xuanhua He",
            "Xiujun Ma",
            "Yue Ma"
        ],
        "tldr": "The paper introduces ContextFlow, a training-free framework for video object editing using Diffusion Transformers, which leverages a high-order solver and adaptive context enrichment to significantly improve fidelity and temporal consistency compared to existing methods.",
        "tldr_zh": "该论文介绍了ContextFlow，一个基于扩散Transformer的无训练视频对象编辑框架，它利用高阶求解器和自适应上下文丰富来显著提高保真度和时间一致性，相比于现有的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Qwen3-Omni Technical Report",
        "summary": "We present Qwen3-Omni, a single multimodal model that, for the first time,\nmaintains state-of-the-art performance across text, image, audio, and video\nwithout any degradation relative to single-modal counterparts. Qwen3-Omni\nmatches the performance of same-sized single-modal models within the Qwen\nseries and excels particularly on audio tasks. Across 36 audio and audio-visual\nbenchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall\nSOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,\nSeed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE\narchitecture that unifies perception and generation across text, images, audio,\nand video, yielding fluent text and natural real-time speech. It supports text\ninteraction in 119 languages, speech understanding in 19 languages, and speech\ngeneration in 10 languages. To reduce first-packet latency in streaming\nsynthesis, Talker autoregressively predicts discrete speech codecs using a\nmulti-codebook scheme. Leveraging the representational capacity of these\ncodebooks, we replace computationally intensive block-wise diffusion with a\nlightweight causal ConvNet, enabling streaming from the first codec frame. In\ncold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet\nlatency of 234 ms. To further strengthen multimodal reasoning, we introduce a\nThinking model that explicitly reasons over inputs from any modality. Since the\nresearch community currently lacks a general-purpose audio captioning model, we\nfine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which\nproduces detailed, low-hallucination captions for arbitrary audio inputs.\nQwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and\nQwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0\nlicense.",
        "url": "http://arxiv.org/abs/2509.17765v1",
        "published_date": "2025-09-22T13:26:24+00:00",
        "updated_date": "2025-09-22T13:26:24+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "eess.AS"
        ],
        "authors": [
            "Jin Xu",
            "Zhifang Guo",
            "Hangrui Hu",
            "Yunfei Chu",
            "Xiong Wang",
            "Jinzheng He",
            "Yuxuan Wang",
            "Xian Shi",
            "Ting He",
            "Xinfa Zhu",
            "Yuanjun Lv",
            "Yongqi Wang",
            "Dake Guo",
            "He Wang",
            "Linhan Ma",
            "Pei Zhang",
            "Xinyu Zhang",
            "Hongkun Hao",
            "Zishan Guo",
            "Baosong Yang",
            "Bin Zhang",
            "Ziyang Ma",
            "Xipin Wei",
            "Shuai Bai",
            "Keqin Chen",
            "Xuejing Liu",
            "Peng Wang",
            "Mingkun Yang",
            "Dayiheng Liu",
            "Xingzhang Ren",
            "Bo Zheng",
            "Rui Men",
            "Fan Zhou",
            "Bowen Yu",
            "Jianxin Yang",
            "Le Yu",
            "Jingren Zhou",
            "Junyang Lin"
        ],
        "tldr": "Qwen3-Omni is a novel multimodal model achieving state-of-the-art performance across text, image, audio, and video, with notable success in audio tasks and real-time speech synthesis, and public availability under Apache 2.0.",
        "tldr_zh": "Qwen3-Omni 是一款新型多模态模型，在文本、图像、音频和视频方面均实现了最先进的性能。它在音频任务和实时语音合成方面表现出色，并以 Apache 2.0 许可公开提供。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained Semantic Guidance",
        "summary": "Amodal completion, generating invisible parts of occluded objects, is vital\nfor applications like image editing and AR. Prior methods face challenges with\ndata needs, generalization, or error accumulation in progressive pipelines. We\npropose a Collaborative Multi-Agent Reasoning Framework based on upfront\ncollaborative reasoning to overcome these issues. Our framework uses multiple\nagents to collaboratively analyze occlusion relationships and determine\nnecessary boundary expansion, yielding a precise mask for inpainting.\nConcurrently, an agent generates fine-grained textual descriptions, enabling\nFine-Grained Semantic Guidance. This ensures accurate object synthesis and\nprevents the regeneration of occluders or other unwanted elements, especially\nwithin large inpainting areas. Furthermore, our method directly produces\nlayered RGBA outputs guided by visible masks and attention maps from a\nDiffusion Transformer, eliminating extra segmentation. Extensive evaluations\ndemonstrate our framework achieves state-of-the-art visual quality.",
        "url": "http://arxiv.org/abs/2509.17757v1",
        "published_date": "2025-09-22T13:20:06+00:00",
        "updated_date": "2025-09-22T13:20:06+00:00",
        "categories": [
            "cs.CV",
            "cs.MA"
        ],
        "authors": [
            "Hongxing Fan",
            "Lipeng Wang",
            "Haohua Chen",
            "Zehuan Huang",
            "Jiangtao Wu",
            "Lu Sheng"
        ],
        "tldr": "This paper introduces a collaborative multi-agent framework for amodal completion that leverages fine-grained semantic guidance and a Diffusion Transformer to directly generate layered RGBA outputs, achieving state-of-the-art visual quality.",
        "tldr_zh": "本文提出了一种用于非模态补全的协作多代理框架，该框架利用细粒度语义引导和扩散变换器直接生成分层RGBA输出，从而实现最先进的视觉质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models",
        "summary": "Recent advances in video insertion based on diffusion models are impressive.\nHowever, existing methods rely on complex control signals but struggle with\nsubject consistency, limiting their practical applicability. In this paper, we\nfocus on the task of Mask-free Video Insertion and aim to resolve three key\nchallenges: data scarcity, subject-scene equilibrium, and insertion\nharmonization. To address the data scarcity, we propose a new data pipeline\nInsertPipe, constructing diverse cross-pair data automatically. Building upon\nour data pipeline, we develop OmniInsert, a novel unified framework for\nmask-free video insertion from both single and multiple subject references.\nSpecifically, to maintain subject-scene equilibrium, we introduce a simple yet\neffective Condition-Specific Feature Injection mechanism to distinctly inject\nmulti-source conditions and propose a novel Progressive Training strategy that\nenables the model to balance feature injection from subjects and source video.\nMeanwhile, we design the Subject-Focused Loss to improve the detailed\nappearance of the subjects. To further enhance insertion harmonization, we\npropose an Insertive Preference Optimization methodology to optimize the model\nby simulating human preferences, and incorporate a Context-Aware Rephraser\nmodule during reference to seamlessly integrate the subject into the original\nscenes. To address the lack of a benchmark for the field, we introduce\nInsertBench, a comprehensive benchmark comprising diverse scenes with\nmeticulously selected subjects. Evaluation on InsertBench indicates OmniInsert\noutperforms state-of-the-art closed-source commercial solutions. The code will\nbe released.",
        "url": "http://arxiv.org/abs/2509.17627v1",
        "published_date": "2025-09-22T11:35:55+00:00",
        "updated_date": "2025-09-22T11:35:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinshu Chen",
            "Xinghui Li",
            "Xu Bai",
            "Tianxiang Ma",
            "Pengze Zhang",
            "Zhuowei Chen",
            "Gen Li",
            "Lijie Liu",
            "Songtao Zhao",
            "Bingchuan Li",
            "Qian He"
        ],
        "tldr": "The paper introduces OmniInsert, a mask-free video insertion framework based on diffusion transformer models, addressing data scarcity, subject-scene equilibrium, and insertion harmonization through a new data pipeline, condition-specific feature injection, progressive training, and insertive preference optimization. It outperforms state-of-the-art methods on a newly introduced benchmark.",
        "tldr_zh": "这篇论文介绍了OmniInsert，一个基于扩散Transformer模型的无掩码视频插入框架。 该框架通过新的数据管道、条件特定的特征注入、渐进式训练和插入偏好优化，解决了数据稀缺、主体-场景平衡和插入协调等问题。 在新发布的benchmark上，OmniInsert的表现优于当前最优方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation",
        "summary": "Generating realistic 3D point clouds is a fundamental problem in computer\nvision with applications in remote sensing, robotics, and digital object\nmodeling. Existing generative approaches primarily capture geometry, and when\nsemantics are considered, they are typically imposed post hoc through external\nsegmentation or clustering rather than integrated into the generative process\nitself. We propose a diffusion-based framework that embeds per-point semantic\nconditioning directly within generation. Each point is associated with a\nconditional variable corresponding to its semantic label, which guides the\ndiffusion dynamics and enables the joint synthesis of geometry and semantics.\nThis design produces point clouds that are both structurally coherent and\nsegmentation-aware, with object parts explicitly represented during synthesis.\nThrough a comparative analysis of guided and unguided diffusion processes, we\ndemonstrate the significant impact of conditional variables on diffusion\ndynamics and generation quality. Extensive experiments validate the efficacy of\nour approach, producing detailed and accurate 3D point clouds tailored to\nspecific parts and features.",
        "url": "http://arxiv.org/abs/2509.17206v1",
        "published_date": "2025-09-21T19:19:36+00:00",
        "updated_date": "2025-09-21T19:19:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Gunner Stone",
            "Sushmita Sarker",
            "Alireza Tavakkoli"
        ],
        "tldr": "This paper introduces a diffusion-based method for generating 3D point clouds that incorporates semantic information directly into the generative process, resulting in structurally coherent and segmentation-aware outputs.",
        "tldr_zh": "本文提出了一种基于扩散的3D点云生成方法，该方法将语义信息直接融入生成过程，从而产生结构连贯且具有分割意识的输出。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "4D-MoDe: Towards Editable and Scalable Volumetric Streaming via Motion-Decoupled 4D Gaussian Compression",
        "summary": "Volumetric video has emerged as a key medium for immersive telepresence and\naugmented/virtual reality, enabling six-degrees-of-freedom (6DoF) navigation\nand realistic spatial interactions. However, delivering high-quality dynamic\nvolumetric content at scale remains challenging due to massive data volume,\ncomplex motion, and limited editability of existing representations. In this\npaper, we present 4D-MoDe, a motion-decoupled 4D Gaussian compression framework\ndesigned for scalable and editable volumetric video streaming. Our method\nintroduces a layered representation that explicitly separates static\nbackgrounds from dynamic foregrounds using a lookahead-based motion\ndecomposition strategy, significantly reducing temporal redundancy and enabling\nselective background/foreground streaming. To capture continuous motion\ntrajectories, we employ a multi-resolution motion estimation grid and a\nlightweight shared MLP, complemented by a dynamic Gaussian compensation\nmechanism to model emergent content. An adaptive grouping scheme dynamically\ninserts background keyframes to balance temporal consistency and compression\nefficiency. Furthermore, an entropy-aware training pipeline jointly optimizes\nthe motion fields and Gaussian parameters under a rate-distortion (RD)\nobjective, while employing range-based and KD-tree compression to minimize\nstorage overhead. Extensive experiments on multiple datasets demonstrate that\n4D-MoDe consistently achieves competitive reconstruction quality with an order\nof magnitude lower storage cost (e.g., as low as \\textbf{11.4} KB/frame)\ncompared to state-of-the-art methods, while supporting practical applications\nsuch as background replacement and foreground-only streaming.",
        "url": "http://arxiv.org/abs/2509.17506v1",
        "published_date": "2025-09-22T08:35:46+00:00",
        "updated_date": "2025-09-22T08:35:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Houqiang Zhong",
            "Zihan Zheng",
            "Qiang Hu",
            "Yuan Tian",
            "Ning Cao",
            "Lan Xu",
            "Xiaoyun Zhang",
            "Zhengxue Cheng",
            "Li Song",
            "Wenjun Zhang"
        ],
        "tldr": "The paper introduces 4D-MoDe, a motion-decoupled 4D Gaussian compression framework for scalable and editable volumetric video streaming, achieving significant compression gains and supporting editing functionalities like background replacement.",
        "tldr_zh": "该论文介绍了4D-MoDe，一种运动解耦的4D高斯压缩框架，用于可扩展和可编辑的 volumetric video 流传输，实现了显著的压缩收益，并支持背景替换等编辑功能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]