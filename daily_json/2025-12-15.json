[
    {
        "title": "No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching",
        "summary": "Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.",
        "url": "http://arxiv.org/abs/2512.12604v1",
        "published_date": "2025-12-14T09:02:18+00:00",
        "updated_date": "2025-12-14T09:02:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tingyan Wen",
            "Haoyu Li",
            "Yihuang Chen",
            "Xing Zhou",
            "Lifei Zhu",
            "Xueqian Wang"
        ],
        "tldr": "The paper introduces X-Slim, a novel caching-based acceleration technique for diffusion models that optimizes the speed-quality trade-off by intelligently reusing features across timesteps, blocks, and tokens, achieving significant speedups and even improved fidelity in image and video generation tasks.",
        "tldr_zh": "该论文介绍了一种名为X-Slim的新型基于缓存的扩散模型加速技术，通过智能地重用跨时间步、块和令牌的特征来优化速度-质量权衡，在图像和视频生成任务中实现了显著的加速，甚至提高了保真度。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation",
        "summary": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.",
        "url": "http://arxiv.org/abs/2512.12595v1",
        "published_date": "2025-12-14T08:28:50+00:00",
        "updated_date": "2025-12-14T08:28:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Karthikeya KV"
        ],
        "tldr": "This paper introduces a vision-enhanced LLM framework using rectified flow and bidirectional tokenization for high-resolution image synthesis and multimodal data interpretation, achieving improved fidelity and efficiency compared to diffusion models.",
        "tldr_zh": "本文介绍了一种视觉增强的LLM框架，该框架使用校正流和双向标记化进行高分辨率图像合成和多模态数据解释，与扩散模型相比，实现了更高的保真度和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Endless World: Real-Time 3D-Aware Long Video Generation",
        "summary": "Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.",
        "url": "http://arxiv.org/abs/2512.12430v1",
        "published_date": "2025-12-13T19:06:12+00:00",
        "updated_date": "2025-12-13T19:06:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ke Zhang",
            "Yiqun Mei",
            "Jiacong Xu",
            "Vishal M. Patel"
        ],
        "tldr": "The paper introduces Endless World, a real-time framework for generating infinite, 3D-consistent videos using a conditional autoregressive training strategy and 3D-aware attention, achieving competitive performance in visual fidelity and spatial consistency.",
        "tldr_zh": "该论文介绍了 Endless World，一个用于生成无限、3D 一致视频的实时框架，它采用条件自回归训练策略和 3D 感知注意力机制，在视觉保真度和空间一致性方面实现了有竞争力的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning",
        "summary": "Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.",
        "url": "http://arxiv.org/abs/2512.12756v1",
        "published_date": "2025-12-14T16:41:29+00:00",
        "updated_date": "2025-12-14T16:41:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yue Jiang",
            "Dingkang Yang",
            "Minghao Han",
            "Jinghang Han",
            "Zizhi Chen",
            "Yizhou Liu",
            "Mingcheng Li",
            "Peng Zhai",
            "Lihua Zhang"
        ],
        "tldr": "The paper introduces FysicsWorld, a new multimodal benchmark for any-to-any evaluation across image, video, audio, and text, designed to address limitations in existing benchmarks and advance full-modality architectures.",
        "tldr_zh": "该论文介绍了FysicsWorld，一个新的多模态基准，用于跨图像、视频、音频和文本的任意到任意评估，旨在解决现有基准的局限性并推进全模态架构。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
        "summary": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.",
        "url": "http://arxiv.org/abs/2512.12751v1",
        "published_date": "2025-12-14T16:23:51+00:00",
        "updated_date": "2025-12-14T16:23:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenya Yang",
            "Zhe Liu",
            "Yuxiang Lu",
            "Liping Hou",
            "Chenxuan Miao",
            "Siyi Peng",
            "Bailan Feng",
            "Xiang Bai",
            "Hengshuang Zhao"
        ],
        "tldr": "GenieDrive is a novel framework for physics-aware driving video generation that uses 4D occupancy as a physics-informed foundation, achieving improvements in forecasting accuracy, inference speed, and video quality while using a relatively small number of parameters.",
        "tldr_zh": "GenieDrive是一个新颖的物理感知驾驶视频生成框架，它使用4D occupancy作为物理信息基础，在预测准确性、推理速度和视频质量方面都取得了改进，同时使用了相对较少的参数。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robust Motion Generation using Part-level Reliable Data from Videos",
        "summary": "Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.\n  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as \"credible\". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.\n  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/",
        "url": "http://arxiv.org/abs/2512.12703v1",
        "published_date": "2025-12-14T14:15:16+00:00",
        "updated_date": "2025-12-14T14:15:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Boyuan Li",
            "Sipeng Zheng",
            "Bin Cao",
            "Ruihua Song",
            "Zongqing Lu"
        ],
        "tldr": "The paper introduces a robust part-aware masked autoregression model using credible part-level data from web videos to enhance human motion generation, addressing the data quality challenges from occlusions and off-screen captures. It also introduces a large-scale motion dataset, K700-M.",
        "tldr_zh": "该论文提出了一种稳健的、具有部分感知的掩码自回归模型，该模型利用从网络视频中提取的可信部分级数据来增强人体运动生成，解决了由遮挡和屏幕外捕捉造成的数据质量挑战。 该论文还介绍了一个大型运动数据集 K700-M。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling",
        "summary": "Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.",
        "url": "http://arxiv.org/abs/2512.12675v1",
        "published_date": "2025-12-14T12:58:19+00:00",
        "updated_date": "2025-12-14T12:58:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuran Wang",
            "Bohan Zeng",
            "Chengzhuo Tong",
            "Wenxuan Liu",
            "Yang Shi",
            "Xiaochen Ma",
            "Hao Liang",
            "Yuanxing Zhang",
            "Wentao Zhang"
        ],
        "tldr": "The paper proposes Scone, a unified understanding-generation model for subject-driven image generation that addresses both composition and distinction (identifying subjects in multi-subject scenarios). It includes a new benchmark for evaluating these capabilities.",
        "tldr_zh": "该论文提出了Scone，一个统一的理解-生成模型，用于解决在多主体场景中进行主体驱动的图像生成时，构图和区分（识别主体）的问题。它还包括一个新的基准来评估这些能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Geometry-Aware Scene-Consistent Image Generation",
        "summary": "We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.",
        "url": "http://arxiv.org/abs/2512.12598v1",
        "published_date": "2025-12-14T08:35:04+00:00",
        "updated_date": "2025-12-14T08:35:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cong Xie",
            "Che Wang",
            "Yan Zhang",
            "Zheng Pan",
            "Han Zou",
            "Zhenpeng Zhan"
        ],
        "tldr": "This paper introduces a method for geometry-aware scene-consistent image generation, balancing scene preservation and prompt adherence by using a scene-consistent data construction pipeline and a geometry-guided attention loss.",
        "tldr_zh": "该论文介绍了一种几何感知场景一致的图像生成方法，通过使用场景一致的数据构造流水线和几何引导的注意力损失，平衡了场景保持和提示语遵从。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generative Spatiotemporal Data Augmentation",
        "summary": "We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.",
        "url": "http://arxiv.org/abs/2512.12508v1",
        "published_date": "2025-12-14T01:18:48+00:00",
        "updated_date": "2025-12-14T01:18:48+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jinfan Zhou",
            "Lixin Luo",
            "Sungmin Eum",
            "Heesung Kwon",
            "Jeong Joon Park"
        ],
        "tldr": "This paper introduces a novel spatiotemporal data augmentation technique using video diffusion models to generate realistic 3D spatial and temporal variations for improving model performance, especially in low-data regimes like UAV imagery. The methodology includes practical guidelines for implementation and annotation transfer.",
        "tldr_zh": "本文介绍了一种新的时空数据增强技术，使用视频扩散模型生成逼真的3D空间和时间变化，以提高模型性能，尤其是在UAV图像等数据稀缺的情况下。该方法还包括实施和注释转移的实用指南。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence",
        "summary": "Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.",
        "url": "http://arxiv.org/abs/2512.12768v1",
        "published_date": "2025-12-14T17:05:11+00:00",
        "updated_date": "2025-12-14T17:05:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Tianjiao Yu",
            "Xinzhuo Li",
            "Yifan Shen",
            "Yuanzhe Liu",
            "Ismini Lourentzou"
        ],
        "tldr": "CoRe3D introduces a reasoning framework for 3D understanding and generation, coupling semantic chain-of-thought inference with structured spatial reasoning to achieve better local consistency and alignment with linguistic descriptions.",
        "tldr_zh": "CoRe3D 引入了一个用于 3D 理解和生成的推理框架，将语义链式思维推理与结构化空间推理相结合，以实现更好的局部一致性和与语言描述的对齐。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]