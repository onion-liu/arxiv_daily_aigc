[
    {
        "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
        "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.",
        "url": "http://arxiv.org/abs/2512.08765v1",
        "published_date": "2025-12-09T16:13:55+00:00",
        "updated_date": "2025-12-09T16:13:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruihang Chu",
            "Yefei He",
            "Zhekai Chen",
            "Shiwei Zhang",
            "Xiaogang Xu",
            "Bin Xia",
            "Dingdong Wang",
            "Hongwei Yi",
            "Xihui Liu",
            "Hengshuang Zhao",
            "Yu Liu",
            "Yingya Zhang",
            "Yujiu Yang"
        ],
        "tldr": "The paper introduces Wan-Move, a scalable framework for motion-controllable video generation using latent trajectory guidance, achieving high-quality control rivaling commercial tools and accompanied by a new benchmark dataset, MoveBench, for comprehensive evaluation.",
        "tldr_zh": "该论文介绍了Wan-Move，一个利用潜在轨迹引导实现运动可控视频生成的框架，该框架具有可扩展性，实现了可媲美商业工具的高质量控制，并附带一个新的基准数据集MoveBench，用于综合评估。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
        "summary": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo",
        "url": "http://arxiv.org/abs/2512.07831v1",
        "published_date": "2025-12-08T18:59:01+00:00",
        "updated_date": "2025-12-08T18:59:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiehui Huang",
            "Yuechen Zhang",
            "Xu He",
            "Yuan Gao",
            "Zhi Cen",
            "Bin Xia",
            "Yan Zhou",
            "Xin Tao",
            "Pengfei Wan",
            "Jiaya Jia"
        ],
        "tldr": "UnityVideo is a unified multi-modal, multi-task framework for video generation, leveraging multiple modalities like segmentation masks, skeletons, and depth maps, to enhance world-aware video generation with improved quality and zero-shot generalization.",
        "tldr_zh": "UnityVideo是一个统一的多模态、多任务视频生成框架，利用分割掩码、骨骼和深度图等多种模态，以增强具有改进的质量和零样本泛化能力的、具有世界感知的视频生成。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling",
        "summary": "Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.",
        "url": "http://arxiv.org/abs/2512.07821v1",
        "published_date": "2025-12-08T18:54:12+00:00",
        "updated_date": "2025-12-08T18:54:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shaoheng Fang",
            "Hanwen Jiang",
            "Yunpeng Bai",
            "Niloy J. Mitra",
            "Qixing Huang"
        ],
        "tldr": "WorldReel introduces a 4D video generation method that produces spatio-temporally consistent videos by jointly generating RGB frames and 4D scene representations, trained on a combination of synthetic and real data.",
        "tldr_zh": "WorldReel 提出了一种 4D 视频生成方法，通过联合生成 RGB 帧和 4D 场景表示来生成时空一致的视频，并在合成数据和真实数据的组合上进行训练。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory",
        "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.",
        "url": "http://arxiv.org/abs/2512.07802v1",
        "published_date": "2025-12-08T18:32:24+00:00",
        "updated_date": "2025-12-08T18:32:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaochong An",
            "Menglin Jia",
            "Haonan Qiu",
            "Zijian Zhou",
            "Xiaoke Huang",
            "Zhiheng Liu",
            "Weiming Ren",
            "Kumara Kahatapitiya",
            "Ding Liu",
            "Sen He",
            "Chenyang Zhang",
            "Tao Xiang",
            "Fanny Yang",
            "Serge Belongie",
            "Tian Xie"
        ],
        "tldr": "The paper introduces OneStory, a novel framework for multi-shot video generation that uses an adaptive memory and frame selection module to improve long-range coherence and enables controllable long-form video storytelling.",
        "tldr_zh": "该论文介绍了OneStory，一种新颖的多镜头视频生成框架，它使用自适应记忆和帧选择模块来提高长程连贯性，并实现可控的长篇视频叙事。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Generation is Required for Data-Efficient Perception",
        "summary": "It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.",
        "url": "http://arxiv.org/abs/2512.08854v1",
        "published_date": "2025-12-09T17:47:28+00:00",
        "updated_date": "2025-12-09T17:47:28+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jack Brady",
            "Bernhard Schölkopf",
            "Thomas Kipf",
            "Simon Buchholz",
            "Wieland Brendel"
        ],
        "tldr": "This paper argues and demonstrates both theoretically and empirically that generative models are inherently better suited for compositional generalization in visual perception tasks compared to non-generative models, particularly when data is limited, due to the ease of enforcing necessary inductive biases.",
        "tldr_zh": "该论文从理论和实践角度论证并展示了生成模型更适合视觉感知任务中的组合泛化，尤其是在数据有限的情况下，这是因为生成模型更容易强制执行必要的归纳偏置。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LoFA: Learning to Predict Personalized Priors for Fast Adaptation of Visual Generative Models",
        "summary": "Personalizing visual generative models to meet specific user needs has gained increasing attention, yet current methods like Low-Rank Adaptation (LoRA) remain impractical due to their demand for task-specific data and lengthy optimization. While a few hypernetwork-based approaches attempt to predict adaptation weights directly, they struggle to map fine-grained user prompts to complex LoRA distributions, limiting their practical applicability. To bridge this gap, we propose LoFA, a general framework that efficiently predicts personalized priors for fast model adaptation. We first identify a key property of LoRA: structured distribution patterns emerge in the relative changes between LoRA and base model parameters. Building on this, we design a two-stage hypernetwork: first predicting relative distribution patterns that capture key adaptation regions, then using these to guide final LoRA weight prediction. Extensive experiments demonstrate that our method consistently predicts high-quality personalized priors within seconds, across multiple tasks and user prompts, even outperforming conventional LoRA that requires hours of processing. Project page: https://jaeger416.github.io/lofa/.",
        "url": "http://arxiv.org/abs/2512.08785v1",
        "published_date": "2025-12-09T16:39:31+00:00",
        "updated_date": "2025-12-09T16:39:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiming Hao",
            "Mutian Xu",
            "Chongjie Ye",
            "Jie Qin",
            "Shunlin Lu",
            "Yipeng Qin",
            "Xiaoguang Han"
        ],
        "tldr": "The paper introduces LoFA, a hypernetwork-based framework for rapidly predicting personalized priors for LoRA-based visual generative models, addressing the limitations of existing adaptation techniques.",
        "tldr_zh": "该论文介绍了LoFA，一个基于超网络的框架，用于快速预测LoRA视觉生成模型的个性化先验，解决了现有适配技术的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank",
        "summary": "The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\\mname} achieves a state-of-the-art FID of \\textbf{2.40} within 400k steps, significantly outperforming comparable methods.",
        "url": "http://arxiv.org/abs/2512.08648v1",
        "published_date": "2025-12-09T14:39:26+00:00",
        "updated_date": "2025-12-09T14:39:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shaofeng Zhang",
            "Xuanqi Chen",
            "Ning Liao",
            "Haoxiang Zhao",
            "Xiaoxing Wang",
            "Haoru Tan",
            "Sitong Wu",
            "Xiaosong Jia",
            "Qi Fan",
            "Junchi Yan"
        ],
        "tldr": "The paper introduces Repulsor, a plug-and-play training framework that accelerates generative modeling by using a contrastive memory bank to improve representation learning without relying on external encoders, achieving state-of-the-art results on ImageNet-256.",
        "tldr_zh": "该论文介绍了一种名为Repulsor的即插即用训练框架，通过使用对比记忆库来改进表征学习，从而加速生成建模，无需依赖外部编码器，并在ImageNet-256上实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation",
        "summary": "Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.",
        "url": "http://arxiv.org/abs/2512.08542v1",
        "published_date": "2025-12-09T12:39:39+00:00",
        "updated_date": "2025-12-09T12:39:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "math.NA"
        ],
        "authors": [
            "Zhigang Jia",
            "Duan Wang",
            "Hengkai Wang",
            "Yajun Xie",
            "Meixiang Zhao",
            "Xiaoyu Zhao"
        ],
        "tldr": "The paper introduces a novel Wasserstein Quaternion GAN that leverages quaternion representation and a new quaternion Wasserstein distance to improve color image generation by addressing inter-channel correlation and providing a theoretical framework for color image dataset comparison, achieving superior image quality and generation efficiency.",
        "tldr_zh": "该论文介绍了一种新的Wasserstein四元数GAN，它利用四元数表示和新的四元数Wasserstein距离，通过解决通道间相关性并为彩色图像数据集比较提供理论框架来改善彩色图像生成，从而实现卓越的图像质量和生成效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation",
        "summary": "Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\\times$256, TransDiff attains 4.3$\\times$ lossless speedup, and NextStep-1 achieves 3$\\times$ acceleration on text-conditioned generation. Code will be available at https://github.com/aSleepyTree/Fast-ARDiff.",
        "url": "http://arxiv.org/abs/2512.08537v1",
        "published_date": "2025-12-09T12:35:18+00:00",
        "updated_date": "2025-12-09T12:35:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhen Zou",
            "Xiaoxiao Ma",
            "Jie Huang",
            "Zichao Yu",
            "Feng Zhao"
        ],
        "tldr": "The paper introduces Fast-ARDiff, a framework that accelerates autoregressive-diffusion models by jointly optimizing both components using entropy-informed speculative decoding and a dynamic scheduler, achieving significant speedups in image and text-conditioned generation.",
        "tldr_zh": "该论文介绍了 Fast-ARDiff，一个通过联合优化自回归和扩散模型组件来加速自回归-扩散模型的框架。该框架采用基于熵的推测解码和动态调度器，在图像和文本条件生成方面实现了显著的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement",
        "summary": "Although recent 3D-native generators have made great progress in synthesizing reliable geometry, they still fall short in achieving realistic appearances. A key obstacle lies in the lack of diverse and high-quality real-world 3D assets with rich texture details, since capturing such data is intrinsically difficult due to the diverse scales of scenes, non-rigid motions of objects, and the limited precision of 3D scanners. We introduce Photo3D, a framework for advancing photorealistic 3D generation, which is driven by the image data generated by the GPT-4o-Image model. Considering that the generated images can distort 3D structures due to their lack of multi-view consistency, we design a structure-aligned multi-view synthesis pipeline and construct a detail-enhanced multi-view dataset paired with 3D geometry. Building on it, we present a realistic detail enhancement scheme that leverages perceptual feature adaptation and semantic structure matching to enforce appearance consistency with realistic details while preserving the structural consistency with the 3D-native geometry. Our scheme is general to different 3D-native generators, and we present dedicated training strategies to facilitate the optimization of geometry-texture coupled and decoupled 3D-native generation paradigms. Experiments demonstrate that Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.",
        "url": "http://arxiv.org/abs/2512.08535v1",
        "published_date": "2025-12-09T12:33:48+00:00",
        "updated_date": "2025-12-09T12:33:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyue Liang",
            "Zhinyuan Ma",
            "Lingchen Sun",
            "Yanjun Guo",
            "Lei Zhang"
        ],
        "tldr": "Photo3D enhances photorealistic 3D generation by leveraging GPT-4o-Image generated images and a structure-aligned detail enhancement scheme, achieving state-of-the-art results.",
        "tldr_zh": "Photo3D通过利用GPT-4o-Image生成的图像和一个结构对齐的细节增强方案来提高照片般真实的3D生成效果，实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation",
        "summary": "Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing.",
        "url": "http://arxiv.org/abs/2512.08534v1",
        "published_date": "2025-12-09T12:31:00+00:00",
        "updated_date": "2025-12-09T12:31:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhangli Hu",
            "Ye Chen",
            "Jiajun Yao",
            "Bingbing Ni"
        ],
        "tldr": "The paper introduces PaintFlow, a unified multimodal framework for interactive oil painting generation and editing using reference images, sketches, and text prompts, addressing limitations of existing methods by incorporating novel training and style transfer techniques.",
        "tldr_zh": "该论文介绍了一个统一的多模态框架PaintFlow，用于交互式油画生成和编辑，它采用了参考图像、草图和文本提示，并通过引入新的训练和风格迁移技术，解决了现有方法的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond the Noise: Aligning Prompts with Latent Representations in Diffusion Models",
        "summary": "Conditional diffusion models rely on language-to-image alignment methods to steer the generation towards semantically accurate outputs. Despite the success of this architecture, misalignment and hallucinations remain common issues and require automatic misalignment detection tools to improve quality, for example by applying them in a Best-of-N (BoN) post-generation setting. Unfortunately, measuring the alignment after the generation is an expensive step since we need to wait for the overall generation to finish to determine prompt adherence. In contrast, this work hypothesizes that text/image misalignments can be detected early in the denoising process, enabling real-time alignment assessment without waiting for the complete generation. In particular, we propose NoisyCLIP a method that measures semantic alignment in the noisy latent space. This work is the first to explore and benchmark prompt-to-latent misalignment detection during image generation using dual encoders in the reverse diffusion process. We evaluate NoisyCLIP qualitatively and quantitatively and find it reduces computational cost by 50% while achieving 98% of CLIP alignment performance in BoN settings. This approach enables real-time alignment assessment during generation, reducing costs without sacrificing semantic fidelity.",
        "url": "http://arxiv.org/abs/2512.08505v1",
        "published_date": "2025-12-09T11:45:01+00:00",
        "updated_date": "2025-12-09T11:45:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vasco Ramos",
            "Regev Cohen",
            "Idan Szpektor",
            "Joao Magalhaes"
        ],
        "tldr": "The paper introduces NoisyCLIP, a method for detecting text/image misalignment in diffusion models early in the denoising process, achieving comparable alignment performance to CLIP with a 50% reduction in computational cost in Best-of-N settings.",
        "tldr_zh": "该论文介绍了NoisyCLIP，一种在扩散模型中早期检测文本/图像不对齐的方法，在 Best-of-N 设置中，以 50% 的计算成本降低实现了与 CLIP 相当的对齐性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation",
        "summary": "Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.",
        "url": "http://arxiv.org/abs/2512.08294v1",
        "published_date": "2025-12-09T06:49:33+00:00",
        "updated_date": "2025-12-09T06:49:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yexin Liu",
            "Manyuan Zhang",
            "Yueze Wang",
            "Hongyu Li",
            "Dian Zheng",
            "Weiming Zhang",
            "Changsheng Lu",
            "Xunliang Cai",
            "Yan Feng",
            "Peng Pei",
            "Harry Yang"
        ],
        "tldr": "The paper introduces OpenSubject, a large-scale video-derived dataset for subject-driven image generation and manipulation, and demonstrates its effectiveness in improving performance, especially in complex scenes.",
        "tldr_zh": "该论文介绍了 OpenSubject，一个大规模的视频衍生数据集，用于主体驱动的图像生成和操控，并演示了其在提高性能方面的有效性，特别是在复杂场景中。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
        "summary": "Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.",
        "url": "http://arxiv.org/abs/2512.08269v1",
        "published_date": "2025-12-09T05:53:39+00:00",
        "updated_date": "2025-12-09T05:53:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Taewoong Kang",
            "Kinam Kim",
            "Dohyeon Kim",
            "Minho Park",
            "Junha Hyung",
            "Jaegul Choo"
        ],
        "tldr": "The paper proposes EgoX, a framework for generating egocentric videos from a single exocentric video by leveraging large-scale video diffusion models with LoRA and a geometry-guided self-attention mechanism to achieve geometric coherence and high visual fidelity.",
        "tldr_zh": "该论文提出了EgoX，一个从单个外部视角视频生成自我中心视角视频的框架。它通过LoRA利用大型视频扩散模型，并结合几何引导的自注意力机制，以实现几何一致性和高视觉保真度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models",
        "summary": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \\textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \\emph{High sample efficiency}, achieving better performance under same training samples (2) \\emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \\emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \\textbf{2.4$\\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.",
        "url": "http://arxiv.org/abs/2512.08153v1",
        "published_date": "2025-12-09T01:17:34+00:00",
        "updated_date": "2025-12-09T01:17:34+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Zheng Ding",
            "Weirui Ye"
        ],
        "tldr": "TreeGRPO, a novel RL framework, accelerates the alignment of generative models with human preferences by using a search tree approach during RL post-training, achieving 2.4x faster training and superior efficiency-reward trade-offs.",
        "tldr_zh": "TreeGRPO是一个新颖的强化学习框架，通过在强化学习后训练中使用搜索树方法，加速了生成模型与人类偏好的对齐，实现了2.4倍的训练速度提升和更卓越的效率-奖励权衡。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation",
        "summary": "Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.",
        "url": "http://arxiv.org/abs/2512.07829v1",
        "published_date": "2025-12-08T18:57:26+00:00",
        "updated_date": "2025-12-08T18:57:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuan Gao",
            "Chen Chen",
            "Tianrong Chen",
            "Jiatao Gu"
        ],
        "tldr": "The paper introduces Feature Auto-Encoder (FAE), a simple framework that adapts pre-trained visual representations for image generation with a single attention layer, achieving strong performance on class-conditional and text-to-image benchmarks.",
        "tldr_zh": "该论文介绍了特征自动编码器(FAE)，这是一个简单的框架，可以通过单个注意力层将预训练的视觉表示适应于图像生成，并在类条件和文本到图像的基准测试中都取得了优异性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Modular Neural Image Signal Processing",
        "summary": "This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM",
        "url": "http://arxiv.org/abs/2512.08564v1",
        "published_date": "2025-12-09T13:04:08+00:00",
        "updated_date": "2025-12-09T13:04:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mahmoud Afifi",
            "Zhongling Wang",
            "Ran Zhang",
            "Michael S. Brown"
        ],
        "tldr": "This paper introduces a modular neural ISP framework for high-quality image rendering from raw inputs, offering improvements in scalability, debuggability, generalization, and user control through a user-interactive photo-editing tool.",
        "tldr_zh": "本文介绍了一种模块化的神经图像信号处理（ISP）框架, 用于从原始输入中渲染高质量图像, 并通过用户交互式照片编辑工具在可扩展性、可调试性、泛化性和用户控制方面进行了改进。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention",
        "summary": "Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.",
        "url": "http://arxiv.org/abs/2512.08477v1",
        "published_date": "2025-12-09T10:51:45+00:00",
        "updated_date": "2025-12-09T10:51:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Huiguo He",
            "Pengyu Yan",
            "Ziqi Yi",
            "Weizhi Zhong",
            "Zheng Liu",
            "Yejun Tang",
            "Huan Yang",
            "Kun Gai",
            "Guanbin Li",
            "Lianwen Jin"
        ],
        "tldr": "The paper introduces ContextDrag, a new drag-based image editing method that leverages contextual information through token injection and position-consistent attention for precise and coherent image manipulation.",
        "tldr_zh": "该论文介绍了ContextDrag，一种新的基于拖拽的图像编辑方法，通过令牌注入和位置一致性注意力来利用上下文信息，实现精确和连贯的图像操作。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SCU-CGAN: Enhancing Fire Detection through Synthetic Fire Image Generation and Dataset Augmentation",
        "summary": "Fire has long been linked to human life, causing severe disasters and losses. Early detection is crucial, and with the rise of home IoT technologies, household fire detection systems have emerged. However, the lack of sufficient fire datasets limits the performance of detection models. We propose the SCU-CGAN model, which integrates U-Net, CBAM, and an additional discriminator to generate realistic fire images from nonfire images. We evaluate the image quality and confirm that SCU-CGAN outperforms existing models. Specifically, SCU-CGAN achieved a 41.5% improvement in KID score compared to CycleGAN, demonstrating the superior quality of the generated fire images. Furthermore, experiments demonstrate that the augmented dataset significantly improves the accuracy of fire detection models without altering their structure. For the YOLOv5 nano model, the most notable improvement was observed in the mAP@0.5:0.95 metric, which increased by 56.5%, highlighting the effectiveness of the proposed approach.",
        "url": "http://arxiv.org/abs/2512.08362v1",
        "published_date": "2025-12-09T08:38:11+00:00",
        "updated_date": "2025-12-09T08:38:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ju-Young Kim",
            "Ji-Hong Park",
            "Gun-Woo Kim"
        ],
        "tldr": "The paper introduces SCU-CGAN, a conditional GAN model for generating synthetic fire images to augment fire detection datasets, leading to significant improvements in fire detection accuracy, particularly for YOLOv5 nano.",
        "tldr_zh": "该论文介绍了SCU-CGAN，一种条件GAN模型，用于生成合成火灾图像以扩充火灾检测数据集，从而显著提高了火灾检测的准确性，特别是对于YOLOv5 nano模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata",
        "summary": "Biological systems exhibit remarkable morphogenetic plasticity, where a single genome can encode various specialized cellular structures triggered by local chemical signals. In the domain of Deep Learning, Differentiable Neural Cellular Automata (NCA) have emerged as a paradigm to mimic this self-organization. However, existing NCA research has predominantly focused on continuous texture synthesis or single-target object recovery, leaving the challenge of class-conditional structural generation largely unexplored. In this work, we propose a novel Conditional Neural Cellular Automata (c-NCA) architecture capable of growing distinct topological structures - specifically MNIST digits - from a single generic seed, guided solely by a spatially broadcasted class vector. Unlike traditional generative models (e.g., GANs, VAEs) that rely on global reception fields, our model enforces strict locality and translation equivariance. We demonstrate that by injecting a one-hot condition into the cellular perception field, a single set of local rules can learn to break symmetry and self-assemble into ten distinct geometric attractors. Experimental results show that our c-NCA achieves stable convergence, correctly forming digit topologies from a single pixel, and exhibits robustness characteristic of biological systems. This work bridges the gap between texture-based NCAs and structural pattern formation, offering a lightweight, biologically plausible alternative for conditional generation.",
        "url": "http://arxiv.org/abs/2512.08360v1",
        "published_date": "2025-12-09T08:36:54+00:00",
        "updated_date": "2025-12-09T08:36:54+00:00",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Ali Sakour"
        ],
        "tldr": "This paper introduces Conditional Neural Cellular Automata (c-NCA) for class-conditional generation of MNIST digits, demonstrating emergent structure formation from a single seed guided by a class vector, with biological plausibility and locality constraints.",
        "tldr_zh": "本文介绍了一种条件神经元胞自动机 (c-NCA)，用于 MNIST 数字的类别条件生成，展示了在类别向量引导下从单个种子中涌现的结构形成，具有生物合理性和局部性约束。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]