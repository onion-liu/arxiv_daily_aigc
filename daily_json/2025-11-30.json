[
    {
        "title": "Visual Generation Tuning",
        "summary": "Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.",
        "url": "http://arxiv.org/abs/2511.23469v1",
        "published_date": "2025-11-28T18:57:13+00:00",
        "updated_date": "2025-11-28T18:57:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Guo",
            "Sinan Du",
            "Jingfeng Yao",
            "Wenyu Liu",
            "Bo Li",
            "Haoxiang Cao",
            "Kun Gai",
            "Chun Yuan",
            "Kai Wu",
            "Xinggang Wang"
        ],
        "tldr": "The paper introduces Visual Generation Tuning (VGT), a novel paradigm to equip VLMs with visual generation capabilities by efficiently aligning semantic encoders with pixel decoders, achieving SOTA results in image reconstruction and visual generation tasks.",
        "tldr_zh": "该论文介绍了视觉生成调优（VGT），这是一种新型范式，通过高效地将语义编码器与像素解码器对齐，使VLM具备视觉生成能力，并在图像重建和视觉生成任务中取得了SOTA结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement",
        "summary": "Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.",
        "url": "http://arxiv.org/abs/2511.23475v1",
        "published_date": "2025-11-28T18:59:01+00:00",
        "updated_date": "2025-11-28T18:59:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhizhou Zhong",
            "Yicheng Ji",
            "Zhe Kong",
            "Yiying Liu",
            "Jiarui Wang",
            "Jiasun Feng",
            "Lupeng Liu",
            "Xiangyi Wang",
            "Yanjia Li",
            "Yuqing She",
            "Ying Qin",
            "Huan Li",
            "Shuiyang Mao",
            "Wei Liu",
            "Wenhan Luo"
        ],
        "tldr": "AnyTalker is a multi-person talking video generation framework that uses a novel identity-aware attention mechanism within a Diffusion Transformer and a training pipeline leveraging single-person videos and interactivity refinement with limited multi-person data, demonstrating improved lip synchronization, visual quality, and interactivity.",
        "tldr_zh": "AnyTalker是一个多人说话视频生成框架，它在Diffusion Transformer中使用了一种新颖的身份感知注意力机制，并采用了一种利用单人视频和少量多人数据进行交互改进的训练流程，从而显著地提高了唇部同步、视觉质量和交互性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model",
        "summary": "Recent advances in generative world models have enabled remarkable progress in creating open-ended game environments, evolving from static scene synthesis toward dynamic, interactive simulation. However, current approaches remain limited by rigid action schemas and high annotation costs, restricting their ability to model diverse in-game interactions and player-driven dynamics. To address these challenges, we introduce Hunyuan-GameCraft-2, a new paradigm of instruction-driven interaction for generative game world modeling. Instead of relying on fixed keyboard inputs, our model allows users to control game video contents through natural language prompts, keyboard, or mouse signals, enabling flexible and semantically rich interaction within generated worlds. We formally defined the concept of interactive video data and developed an automated process to transform large-scale, unstructured text-video pairs into causally aligned interactive datasets. Built upon a 14B image-to-video Mixture-of-Experts(MoE) foundation model, our model incorporates a text-driven interaction injection mechanism for fine-grained control over camera motion, character behavior, and environment dynamics. We introduce an interaction-focused benchmark, InterBench, to evaluate interaction performance comprehensively. Extensive experiments demonstrate that our model generates temporally coherent and causally grounded interactive game videos that faithfully respond to diverse and free-form user instructions such as \"open the door\", \"draw a torch\", or \"trigger an explosion\".",
        "url": "http://arxiv.org/abs/2511.23429v1",
        "published_date": "2025-11-28T18:26:39+00:00",
        "updated_date": "2025-11-28T18:26:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junshu Tang",
            "Jiacheng Liu",
            "Jiaqi Li",
            "Longhuang Wu",
            "Haoyu Yang",
            "Penghao Zhao",
            "Siruis Gong",
            "Xiang Yuan",
            "Shuai Shao",
            "Qinglin Lu"
        ],
        "tldr": "The paper introduces Hunyuan-GameCraft-2, a model that generates interactive game environments controllable via natural language, keyboard, or mouse, addressing limitations of existing game world models with a new instruction-driven paradigm.",
        "tldr_zh": "该论文介绍了Hunyuan-GameCraft-2，一个可以通过自然语言、键盘或鼠标控制的交互式游戏环境生成模型，通过一种新的指令驱动范式解决了现有游戏世界模型的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]