[
    {
        "title": "VDOT: Efficient Unified Video Creation via Optimal Transport Distillation",
        "summary": "The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.",
        "url": "http://arxiv.org/abs/2512.06802v1",
        "published_date": "2025-12-07T11:31:00+00:00",
        "updated_date": "2025-12-07T11:31:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yutong Wang",
            "Haiyu Zhang",
            "Tianfan Xue",
            "Yu Qiao",
            "Yaohui Wang",
            "Chang Xu",
            "Xinyuan Chen"
        ],
        "tldr": "The paper introduces VDOT, an efficient unified video creation model that leverages optimal transport for distribution matching distillation and a discriminator to improve video quality, achieving comparable results to models with many more denoising steps. They also propose a benchmark dataset to facilitate research in video generation.",
        "tldr_zh": "该论文介绍了VDOT，一种高效的统一视频生成模型，利用最优传输进行分布匹配蒸馏，并使用判别器提高视频质量，实现了与更多去噪步骤模型相当的结果。 他们还提出了一个基准数据集以促进视频生成的研究。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Scaling Zero-Shot Reference-to-Video Generation",
        "summary": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.",
        "url": "http://arxiv.org/abs/2512.06905v1",
        "published_date": "2025-12-07T16:10:25+00:00",
        "updated_date": "2025-12-07T16:10:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zijian Zhou",
            "Shikun Liu",
            "Haozhe Liu",
            "Haonan Qiu",
            "Zhaochong An",
            "Weiming Ren",
            "Zhiheng Liu",
            "Xiaoke Huang",
            "Kam Woh Ng",
            "Tian Xie",
            "Xiao Han",
            "Yuren Cong",
            "Hang Li",
            "Chuyan Zhu",
            "Aditya Patel",
            "Tao Xiang",
            "Sen He"
        ],
        "tldr": "The paper introduces Saber, a zero-shot reference-to-video generation framework trained on video-text pairs, achieving state-of-the-art performance on OpenS2V-Eval without requiring explicit reference image-video-text triplets by using a masked training strategy and attention-based model design.",
        "tldr_zh": "该论文介绍了Saber，一个零样本参考到视频生成框架，通过使用掩码训练策略和基于注意力的模型设计，仅使用视频-文本对进行训练，并在OpenS2V-Eval上实现了最先进的性能，而无需显式的参考图像-视频-文本三元组。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment",
        "summary": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.",
        "url": "http://arxiv.org/abs/2512.06628v1",
        "published_date": "2025-12-07T02:28:06+00:00",
        "updated_date": "2025-12-07T02:28:06+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Ruicheng Zhang",
            "Mingyang Zhang",
            "Jun Zhou",
            "Zhangrui Guo",
            "Xiaofan Liu",
            "Zunnan Xu",
            "Zhizhou Zhong",
            "Puxin Yan",
            "Haocheng Luo",
            "Xiu Li"
        ],
        "tldr": "The paper introduces MIND-V, a hierarchical video generation framework for long-horizon robotic manipulation using reinforcement learning to ensure physical plausibility, achieving state-of-the-art performance in embodied data synthesis.",
        "tldr_zh": "该论文介绍了MIND-V，一个用于长时程机器人操作的分层视频生成框架，它使用强化学习来确保物理上的合理性，并在具身数据合成方面实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "JoPano: Unified Panorama Generation via Joint Modeling",
        "summary": "Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.",
        "url": "http://arxiv.org/abs/2512.06885v1",
        "published_date": "2025-12-07T15:19:26+00:00",
        "updated_date": "2025-12-07T15:19:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wancheng Feng",
            "Chen An",
            "Zhenliang He",
            "Meina Kan",
            "Shiguang Shan",
            "Lukun Wang"
        ],
        "tldr": "JoPano introduces a DiT-based approach with a Joint-Face Adapter to unify text-to-panorama and view-to-panorama generation, addressing limitations of existing U-Net based methods and independent task modeling.",
        "tldr_zh": "JoPano 提出了一种基于 DiT 的方法，通过 Joint-Face Adapter 统一了文本到全景图和视图到全景图生成，解决了现有基于 U-Net 的方法和独立任务建模的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]