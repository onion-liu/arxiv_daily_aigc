[
    {
        "title": "Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation",
        "summary": "Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising\nmethod to generate factual and up-to-date responses of Multimodal Large\nLanguage Models (MLLMs) by incorporating non-parametric knowledge from external\nknowledge bases. However, existing MRAG approaches suffer from static retrieval\nstrategies, inflexible modality selection, and suboptimal utilization of\nretrieved information, leading to three critical challenges: determining when\nto retrieve, what modality to incorporate, and how to utilize retrieved\ninformation effectively. To address these challenges, we introduce Windsock, a\nquery-dependent module making decisions on retrieval necessity and modality\nselection, effectively reducing computational overhead and improving response\nquality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction\nTuning, an adaptive training strategy that enhances MLLMs' ability to utilize\nretrieved information while maintaining robustness against noise. Moreover, we\nadopt a self-assessment approach leveraging knowledge within MLLMs to convert\nquestion-answering datasets to MRAG training datasets. Extensive experiments\ndemonstrate that our proposed method significantly improves the generation\nquality by 17.07% while reducing 8.95% retrieval times.",
        "url": "http://arxiv.org/abs/2510.22694v1",
        "published_date": "2025-10-26T14:36:16+00:00",
        "updated_date": "2025-10-26T14:36:16+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.IR"
        ],
        "authors": [
            "Shu Zhao",
            "Tianyi Shen",
            "Nilesh Ahuja",
            "Omesh Tickoo",
            "Vijaykrishnan Narayanan"
        ],
        "tldr": "The paper introduces Windsock, an adaptive multimodal retrieval-augmented generation method that dynamically selects retrieval necessity and modality, combined with DANCE Instruction Tuning for noise resistance, resulting in improved generation quality and reduced retrieval time.",
        "tldr_zh": "该论文介绍了一种自适应多模态检索增强生成方法Windsock，它动态选择检索必要性和模态，结合 DANCE 指令微调以提高抗噪能力，从而提高生成质量并减少检索时间。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RoboSVG: A Unified Framework for Interactive SVG Generation with Multi-modal Guidance",
        "summary": "Scalable Vector Graphics (SVGs) are fundamental to digital design and robot\ncontrol, encoding not only visual structure but also motion paths in\ninteractive drawings. In this work, we introduce RoboSVG, a unified multimodal\nframework for generating interactive SVGs guided by textual, visual, and\nnumerical signals. Given an input query, the RoboSVG model first produces\nmultimodal guidance, then synthesizes candidate SVGs through dedicated\ngeneration modules, and finally refines them under numerical guidance to yield\nhigh-quality outputs. To support this framework, we construct RoboDraw, a\nlarge-scale dataset of one million examples, each pairing an SVG generation\ncondition (e.g., text, image, and partial SVG) with its corresponding\nground-truth SVG code. RoboDraw dataset enables systematic study of four tasks,\nincluding basic generation (Text-to-SVG, Image-to-SVG) and interactive\ngeneration (PartialSVG-to-SVG, PartialImage-to-SVG). Extensive experiments\ndemonstrate that RoboSVG achieves superior query compliance and visual fidelity\nacross tasks, establishing a new state of the art in versatile SVG generation.\nThe dataset and source code of this project will be publicly available soon.",
        "url": "http://arxiv.org/abs/2510.22684v1",
        "published_date": "2025-10-26T13:57:08+00:00",
        "updated_date": "2025-10-26T13:57:08+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Jiuniu Wang",
            "Gongjie Zhang",
            "Quanhao Qian",
            "Junlong Gao",
            "Deli Zhao",
            "Ran Xu"
        ],
        "tldr": "The paper introduces RoboSVG, a multimodal framework for generating interactive SVGs from text, images, and partial SVGs, supported by a large-scale dataset called RoboDraw. Experiments show state-of-the-art SVG generation performance and high query compliance.",
        "tldr_zh": "该论文介绍了 RoboSVG，一个多模态框架，用于从文本、图像和部分 SVG 生成交互式 SVG。该框架基于一个名为 RoboDraw 的大型数据集。实验结果表明，RoboSVG 在 SVG 生成任务上达到了最先进的性能，并且具有很高的查询依从性",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Attention Decomposition For Training Free Diffusion Editing",
        "summary": "Diffusion models achieve remarkable fidelity in image synthesis, yet precise\ncontrol over their outputs for targeted editing remains challenging. A key step\ntoward controllability is to identify interpretable directions in the model's\nlatent representations that correspond to semantic attributes. Existing\napproaches for finding interpretable directions typically rely on sampling\nlarge sets of images or training auxiliary networks, which limits efficiency.\nWe propose an analytical method that derives semantic editing directions\ndirectly from the pretrained parameters of diffusion models, requiring neither\nadditional data nor fine-tuning. Our insight is that self-attention weight\nmatrices encode rich structural information about the data distribution learned\nduring training. By computing the eigenvectors of these weight matrices, we\nobtain robust and interpretable editing directions. Experiments demonstrate\nthat our method produces high-quality edits across multiple datasets while\nreducing editing time significantly by 60% over current benchmarks.",
        "url": "http://arxiv.org/abs/2510.22650v1",
        "published_date": "2025-10-26T12:22:56+00:00",
        "updated_date": "2025-10-26T12:22:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tharun Anand",
            "Mohammad Hassan Vali",
            "Arno Solin"
        ],
        "tldr": "This paper introduces a training-free method for editing diffusion model generated images by analyzing self-attention weights to find interpretable editing directions, showing a 60% speed improvement.",
        "tldr_zh": "本文介绍了一种无需训练的扩散模型图像编辑方法，通过分析自注意力权重来寻找可解释的编辑方向，并展示了60%的速度提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DynaPose4D: High-Quality 4D Dynamic Content Generation via Pose Alignment Loss",
        "summary": "Recent advancements in 2D and 3D generative models have expanded the\ncapabilities of computer vision. However, generating high-quality 4D dynamic\ncontent from a single static image remains a significant challenge. Traditional\nmethods have limitations in modeling temporal dependencies and accurately\ncapturing dynamic geometry changes, especially when considering variations in\ncamera perspective. To address this issue, we propose DynaPose4D, an innovative\nsolution that integrates 4D Gaussian Splatting (4DGS) techniques with\nCategory-Agnostic Pose Estimation (CAPE) technology. This framework uses 3D\nGaussian Splatting to construct a 3D model from single images, then predicts\nmulti-view pose keypoints based on one-shot support from a chosen view,\nleveraging supervisory signals to enhance motion consistency. Experimental\nresults show that DynaPose4D achieves excellent coherence, consistency, and\nfluidity in dynamic motion generation. These findings not only validate the\nefficacy of the DynaPose4D framework but also indicate its potential\napplications in the domains of computer vision and animation production.",
        "url": "http://arxiv.org/abs/2510.22473v1",
        "published_date": "2025-10-26T01:11:13+00:00",
        "updated_date": "2025-10-26T01:11:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jing Yang",
            "Yufeng Yang"
        ],
        "tldr": "DynaPose4D introduces a novel framework using 4D Gaussian Splatting and pose estimation to generate high-quality 4D dynamic content from a single image, addressing limitations in modeling temporal dependencies and dynamic geometry changes.",
        "tldr_zh": "DynaPose4D 提出了一个新颖的框架，该框架使用 4D 高斯溅射和姿势估计从单个图像生成高质量的 4D 动态内容，解决了在建模时间依赖性和动态几何变化方面的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hollywood Town: Long-Video Generation via Cross-Modal Multi-Agent Orchestration",
        "summary": "Recent advancements in multi-agent systems have demonstrated significant\npotential for enhancing creative task performance, such as long video\ngeneration. This study introduces three innovations to improve multi-agent\ncollaboration. First, we propose OmniAgent, a hierarchical, graph-based\nmulti-agent framework for long video generation that leverages a\nfilm-production-inspired architecture to enable modular specialization and\nscalable inter-agent collaboration. Second, inspired by context engineering, we\npropose hypergraph nodes that enable temporary group discussions among agents\nlacking sufficient context, reducing individual memory requirements while\nensuring adequate contextual information. Third, we transition from directed\nacyclic graphs (DAGs) to directed cyclic graphs with limited retries, allowing\nagents to reflect and refine outputs iteratively, thereby improving earlier\nstages through feedback from subsequent nodes. These contributions lay the\ngroundwork for developing more robust multi-agent systems in creative tasks.",
        "url": "http://arxiv.org/abs/2510.22431v1",
        "published_date": "2025-10-25T20:34:18+00:00",
        "updated_date": "2025-10-25T20:34:18+00:00",
        "categories": [
            "cs.MA",
            "cs.CV"
        ],
        "authors": [
            "Zheng Wei",
            "Mingchen Li",
            "Zeqian Zhang",
            "Ruibin Yuan",
            "Pan Hui",
            "Huamin Qu",
            "James Evans",
            "Maneesh Agrawala",
            "Anyi Rao"
        ],
        "tldr": "The paper presents a novel multi-agent framework, OmniAgent, for long video generation, incorporating hypergraphs for context sharing and directed cyclic graphs for iterative refinement.",
        "tldr_zh": "该论文提出了一个新的多智能体框架OmniAgent，用于长视频生成，融合了用于上下文共享的超图和用于迭代改进的有向循环图。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AesCrop: Aesthetic-driven Cropping Guided by Composition",
        "summary": "Aesthetic-driven image cropping is crucial for applications like view\nrecommendation and thumbnail generation, where visual appeal significantly\nimpacts user engagement. A key factor in visual appeal is composition--the\ndeliberate arrangement of elements within an image. Some methods have\nsuccessfully incorporated compositional knowledge through evaluation-based and\nregression-based paradigms. However, evaluation-based methods lack globality\nwhile regression-based methods lack diversity. Recently, hybrid approaches that\nintegrate both paradigms have emerged, bridging the gap between these two to\nachieve better diversity and globality. Notably, existing hybrid methods do not\nincorporate photographic composition guidance, a key attribute that defines\nphotographic aesthetics. In this work, we introduce AesCrop, a\ncomposition-aware hybrid image-cropping model that integrates a VMamba image\nencoder, augmented with a novel Mamba Composition Attention Bias (MCAB) and a\ntransformer decoder to perform end-to-end rank-based image cropping, generating\nmultiple crops along with the corresponding quality scores. By explicitly\nencoding compositional cues into the attention mechanism, MCAB directs AesCrop\nto focus on the most compositionally salient regions. Extensive experiments\ndemonstrate that AesCrop outperforms current state-of-the-art methods,\ndelivering superior quantitative metrics and qualitatively more pleasing crops.",
        "url": "http://arxiv.org/abs/2510.22528v1",
        "published_date": "2025-10-26T04:30:02+00:00",
        "updated_date": "2025-10-26T04:30:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yen-Hong Wong",
            "Lai-Kuan Wong"
        ],
        "tldr": "AesCrop is a novel aesthetic-driven image cropping model that uses a Mamba-based architecture and a novel Mamba Composition Attention Bias to generate high-quality crops by incorporating photographic composition guidance.",
        "tldr_zh": "AesCrop是一种新颖的美学驱动的图像裁剪模型，它使用基于Mamba的架构和一种新颖的Mamba构图注意力偏差，通过结合摄影构图指导来生成高质量的裁剪。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "LAMP: Data-Efficient Linear Affine Weight-Space Models for Parameter-Controlled 3D Shape Generation and Extrapolation",
        "summary": "Generating high-fidelity 3D geometries that satisfy specific parameter\nconstraints has broad applications in design and engineering. However, current\nmethods typically rely on large training datasets and struggle with\ncontrollability and generalization beyond the training distributions. To\novercome these limitations, we introduce LAMP (Linear Affine Mixing of\nParametric shapes), a data-efficient framework for controllable and\ninterpretable 3D generation. LAMP first aligns signed distance function (SDF)\ndecoders by overfitting each exemplar from a shared initialization, then\nsynthesizes new geometries by solving a parameter-constrained mixing problem in\nthe aligned weight space. To ensure robustness, we further propose a safety\nmetric that detects geometry validity via linearity mismatch. We evaluate LAMP\non two 3D parametric benchmarks: DrivAerNet++ and BlendedNet. We found that\nLAMP enables (i) controlled interpolation within bounds with as few as 100\nsamples, (ii) safe extrapolation by up to 100% parameter difference beyond\ntraining ranges, (iii) physics performance-guided optimization under fixed\nparameters. LAMP significantly outperforms conditional autoencoder and Deep\nNetwork Interpolation (DNI) baselines in both extrapolation and data\nefficiency. Our results demonstrate that LAMP advances controllable,\ndata-efficient, and safe 3D generation for design exploration, dataset\ngeneration, and performance-driven optimization.",
        "url": "http://arxiv.org/abs/2510.22491v1",
        "published_date": "2025-10-26T02:12:20+00:00",
        "updated_date": "2025-10-26T02:12:20+00:00",
        "categories": [
            "cs.LG",
            "cs.CE",
            "cs.CV"
        ],
        "authors": [
            "Ghadi Nehme",
            "Yanxia Zhang",
            "Dule Shu",
            "Matt Klenk",
            "Faez Ahmed"
        ],
        "tldr": "The paper introduces LAMP, a data-efficient framework for controllable 3D shape generation using linear affine mixing in the weight space of SDF decoders, demonstrating superior performance in interpolation and extrapolation compared to existing methods.",
        "tldr_zh": "该论文介绍了一种名为LAMP的数据高效框架，它使用SDF解码器权重空间中的线性仿射混合来实现可控的3D形状生成，并在插值和外推方面表现优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]