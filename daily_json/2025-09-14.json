[
    {
        "title": "EditDuet: A Multi-Agent System for Video Non-Linear Editing",
        "summary": "Automated tools for video editing and assembly have applications ranging from\nfilmmaking and advertisement to content creation for social media. Previous\nvideo editing work has mainly focused on either retrieval or user interfaces,\nleaving actual editing to the user. In contrast, we propose to automate the\ncore task of video editing, formulating it as sequential decision making\nprocess. Ours is a multi-agent approach. We design an Editor agent and a Critic\nagent. The Editor takes as input a collection of video clips together with\nnatural language instructions and uses tools commonly found in video editing\nsoftware to produce an edited sequence. On the other hand, the Critic gives\nnatural language feedback to the editor based on the produced sequence or\nrenders it if it is satisfactory. We introduce a learning-based approach for\nenabling effective communication across specialized agents to address the\nlanguage-driven video editing task. Finally, we explore an LLM-as-a-judge\nmetric for evaluating the quality of video editing system and compare it with\ngeneral human preference. We evaluate our system's output video sequences\nqualitatively and quantitatively through a user study and find that our system\nvastly outperforms existing approaches in terms of coverage, time constraint\nsatisfaction, and human preference.",
        "url": "http://arxiv.org/abs/2509.10761v1",
        "published_date": "2025-09-13T00:27:02+00:00",
        "updated_date": "2025-09-13T00:27:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Marcelo Sandoval-Castaneda",
            "Bryan Russell",
            "Josef Sivic",
            "Gregory Shakhnarovich",
            "Fabian Caba Heilbron"
        ],
        "tldr": "This paper introduces EditDuet, a multi-agent system comprising an Editor and a Critic agent, to automate video editing based on natural language instructions, outperforming existing methods in user studies.",
        "tldr_zh": "本文介绍了一个名为EditDuet的多智能体系统，该系统包含一个Editor和一个Critic智能体，旨在根据自然语言指令自动进行视频编辑，并在用户研究中优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation",
        "summary": "We present Stable Part Diffusion 4D (SP4D), a framework for generating paired\nRGB and kinematic part videos from monocular inputs. Unlike conventional part\nsegmentation methods that rely on appearance-based semantic cues, SP4D learns\nto produce kinematic parts - structural components aligned with object\narticulation and consistent across views and time. SP4D adopts a dual-branch\ndiffusion model that jointly synthesizes RGB frames and corresponding part\nsegmentation maps. To simplify the architecture and flexibly enable different\npart counts, we introduce a spatial color encoding scheme that maps part masks\nto continuous RGB-like images. This encoding allows the segmentation branch to\nshare the latent VAE from the RGB branch, while enabling part segmentation to\nbe recovered via straightforward post-processing. A Bidirectional Diffusion\nFusion (BiDiFuse) module enhances cross-branch consistency, supported by a\ncontrastive part consistency loss to promote spatial and temporal alignment of\npart predictions. We demonstrate that the generated 2D part maps can be lifted\nto 3D to derive skeletal structures and harmonic skinning weights with few\nmanual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,\na curated dataset of over 20K rigged objects selected and processed from\nObjaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part\nvideo sequences. Experiments show that SP4D generalizes strongly to diverse\nscenarios, including real-world videos, novel generated objects, and rare\narticulated poses, producing kinematic-aware outputs suitable for downstream\nanimation and motion-related tasks.",
        "url": "http://arxiv.org/abs/2509.10687v1",
        "published_date": "2025-09-12T20:39:43+00:00",
        "updated_date": "2025-09-12T20:39:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Zhang",
            "Chun-Han Yao",
            "Simon Donné",
            "Narendra Ahuja",
            "Varun Jampani"
        ],
        "tldr": "The paper introduces Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs using a dual-branch diffusion model and a novel spatial color encoding scheme. It includes a new dataset, KinematicParts20K, and demonstrates generalization to real-world scenarios.",
        "tldr_zh": "该论文介绍了Stable Part Diffusion 4D (SP4D)，一个利用双分支扩散模型和新型空间颜色编码方案，从单目输入生成配对的RGB和运动部件视频的框架。它包括一个新的数据集KinematicParts20K，并展示了对真实世界场景的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]