[
    {
        "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
        "summary": "Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video.",
        "url": "http://arxiv.org/abs/2510.05096v1",
        "published_date": "2025-10-06T17:58:02+00:00",
        "updated_date": "2025-10-06T17:58:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.MA",
            "cs.MM"
        ],
        "authors": [
            "Zeyu Zhu",
            "Kevin Qinghong Lin",
            "Mike Zheng Shou"
        ],
        "tldr": "The paper introduces PaperTalker, a benchmark dataset and multi-agent framework for automatically generating presentation videos from scientific papers, addressing the challenges of multi-modal information integration and efficient generation.",
        "tldr_zh": "该论文介绍了PaperTalker，一个基准数据集和多智能体框架，用于从科学论文中自动生成演示视频，解决了多模态信息集成和高效生成方面的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation",
        "summary": "Recent video generation models can produce smooth and visually appealing\nclips, but they often struggle to synthesize complex dynamics with a coherent\nchain of consequences. Accurately modeling visual outcomes and state\ntransitions over time remains a core challenge. In contrast, large language and\nmultimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and\nfuture prediction capabilities. To bridge these strengths, we introduce VChain,\na novel inference-time chain-of-visual-thought framework that injects visual\nreasoning signals from multimodal models into video generation. Specifically,\nVChain contains a dedicated pipeline that leverages large multimodal models to\ngenerate a sparse set of critical keyframes as snapshots, which are then used\nto guide the sparse inference-time tuning of a pre-trained video generator only\nat these key moments. Our approach is tuning-efficient, introduces minimal\noverhead and avoids dense supervision. Extensive experiments on complex,\nmulti-step scenarios show that VChain significantly enhances the quality of\ngenerated videos.",
        "url": "http://arxiv.org/abs/2510.05094v1",
        "published_date": "2025-10-06T17:57:59+00:00",
        "updated_date": "2025-10-06T17:57:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziqi Huang",
            "Ning Yu",
            "Gordon Chen",
            "Haonan Qiu",
            "Paul Debevec",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces VChain, a framework that uses multimodal models to inject visual reasoning into video generation by generating keyframes to guide a pre-trained video generator, enhancing the quality of generated videos.",
        "tldr_zh": "该论文介绍了VChain，一个利用多模态模型将视觉推理注入视频生成的框架，通过生成关键帧来指导预训练的视频生成器，从而提高生成的视频质量。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Bridging Text and Video Generation: A Survey",
        "summary": "Text-to-video (T2V) generation technology holds potential to transform\nmultiple domains such as education, marketing, entertainment, and assistive\ntechnologies for individuals with visual or reading comprehension challenges,\nby creating coherent visual content from natural language prompts. From its\ninception, the field has advanced from adversarial models to diffusion-based\nmodels, yielding higher-fidelity, temporally consistent outputs. Yet challenges\npersist, such as alignment, long-range coherence, and computational efficiency.\nAddressing this evolving landscape, we present a comprehensive survey of\ntext-to-video generative models, tracing their development from early GANs and\nVAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these\nmodels work, what limitations they addressed in their predecessors, and why\nshifts toward new architectural paradigms were necessary to overcome challenges\nin quality, coherence, and control. We provide a systematic account of the\ndatasets, which the surveyed text-to-video models were trained and evaluated\non, and, to support reproducibility and assess the accessibility of training\nsuch models, we detail their training configurations, including their hardware\nspecifications, GPU counts, batch sizes, learning rates, optimizers, epochs,\nand other key hyperparameters. Further, we outline the evaluation metrics\ncommonly used for evaluating such models and present their performance across\nstandard benchmarks, while also discussing the limitations of these metrics and\nthe emerging shift toward more holistic, perception-aligned evaluation\nstrategies. Finally, drawing from our analysis, we outline the current open\nchallenges and propose a few promising future directions, laying out a\nperspective for future researchers to explore and build upon in advancing T2V\nresearch and applications.",
        "url": "http://arxiv.org/abs/2510.04999v1",
        "published_date": "2025-10-06T16:39:05+00:00",
        "updated_date": "2025-10-06T16:39:05+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Nilay Kumar",
            "Priyansh Bhandari",
            "G. Maragatham"
        ],
        "tldr": "This paper is a comprehensive survey of text-to-video generation models, covering their evolution, datasets, training configurations, evaluation metrics, and future directions.",
        "tldr_zh": "本文全面综述了文本到视频生成模型，涵盖了其发展历程、数据集、训练配置、评估指标以及未来方向。",
        "relevance_score": 10,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Pulp Motion: Framing-aware multimodal camera and human motion generation",
        "summary": "Treating human motion and camera trajectory generation separately overlooks a\ncore principle of cinematography: the tight interplay between actor performance\nand camera work in the screen space. In this paper, we are the first to cast\nthis task as a text-conditioned joint generation, aiming to maintain consistent\non-screen framing while producing two heterogeneous, yet intrinsically linked,\nmodalities: human motion and camera trajectories. We propose a simple,\nmodel-agnostic framework that enforces multimodal coherence via an auxiliary\nmodality: the on-screen framing induced by projecting human joints onto the\ncamera. This on-screen framing provides a natural and effective bridge between\nmodalities, promoting consistency and leading to more precise joint\ndistribution. We first design a joint autoencoder that learns a shared latent\nspace, together with a lightweight linear transform from the human and camera\nlatents to a framing latent. We then introduce auxiliary sampling, which\nexploits this linear transform to steer generation toward a coherent framing\nmodality. To support this task, we also introduce the PulpMotion dataset, a\nhuman-motion and camera-trajectory dataset with rich captions, and high-quality\nhuman motions. Extensive experiments across DiT- and MAR-based architectures\nshow the generality and effectiveness of our method in generating on-frame\ncoherent human-camera motions, while also achieving gains on textual alignment\nfor both modalities. Our qualitative results yield more cinematographically\nmeaningful framings setting the new state of the art for this task. Code,\nmodels and data are available in our\n\\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project\npage}.",
        "url": "http://arxiv.org/abs/2510.05097v1",
        "published_date": "2025-10-06T17:58:34+00:00",
        "updated_date": "2025-10-06T17:58:34+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Robin Courant",
            "Xi Wang",
            "David Loiseaux",
            "Marc Christie",
            "Vicky Kalogeiton"
        ],
        "tldr": "This paper introduces a novel framework for joint generation of human motion and camera trajectories conditioned on text, emphasizing coherent on-screen framing using a shared latent space and a new dataset called PulpMotion.",
        "tldr_zh": "本文介绍了一种新的框架，用于在文本条件下联合生成人体运动和相机轨迹，强调使用共享潜在空间和一个名为 PulpMotion 的新数据集来实现连贯的屏幕取景。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Character Mixing for Video Generation",
        "summary": "Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where\ncharacters interact naturally across different worlds? We study inter-character\ninteraction in text-to-video generation, where the key challenge is to preserve\neach character's identity and behaviors while enabling coherent cross-context\ninteraction. This is difficult because characters may never have coexisted and\nbecause mixing styles often causes style delusion, where realistic characters\nappear cartoonish or vice versa. We introduce a framework that tackles these\nissues with Cross-Character Embedding (CCE), which learns identity and\nbehavioral logic across multimodal sources, and Cross-Character Augmentation\n(CCA), which enriches training with synthetic co-existence and mixed-style\ndata. Together, these techniques allow natural interactions between previously\nuncoexistent characters without losing stylistic fidelity. Experiments on a\ncurated benchmark of cartoons and live-action series with 10 characters show\nclear improvements in identity preservation, interaction quality, and\nrobustness to style delusion, enabling new forms of generative\nstorytelling.Additional results and videos are available on our project page:\nhttps://tingtingliao.github.io/mimix/.",
        "url": "http://arxiv.org/abs/2510.05093v1",
        "published_date": "2025-10-06T17:57:39+00:00",
        "updated_date": "2025-10-06T17:57:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tingting Liao",
            "Chongjian Ge",
            "Guangyi Liu",
            "Hao Li",
            "Yi Zhou"
        ],
        "tldr": "This paper introduces a framework for text-to-video generation that allows characters from different styles and contexts to interact naturally by preserving their identities and behaviors, using Cross-Character Embedding and Augmentation techniques.",
        "tldr_zh": "本文提出了一种文本到视频生成的框架，该框架允许来自不同风格和背景的角色自然地互动，通过使用跨角色嵌入和增强技术来保持其身份和行为。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Factuality Matters: When Image Generation and Editing Meet Structured Visuals",
        "summary": "While modern visual generation models excel at creating aesthetically\npleasing natural images, they struggle with producing or editing structured\nvisuals like charts, diagrams, and mathematical figures, which demand\ncomposition planning, text rendering, and multimodal reasoning for factual\nfidelity. To address this, we present the first comprehensive, systematic\ninvestigation of this domain, encompassing data construction, model training,\nand an evaluation benchmark. First, we construct a large-scale dataset of 1.3\nmillion high-quality structured image pairs derived from executable drawing\nprograms and augmented with chain-of-thought reasoning annotations. Building on\nit, we train a unified model that integrates a VLM with FLUX.1 Kontext via a\nlightweight connector for enhanced multimodal understanding. A three-stage\ntraining curriculum enables progressive feature alignment, knowledge infusion,\nand reasoning-augmented generation, further boosted by an external reasoner at\ninference time. Finally, we introduce StructBench, a novel benchmark for\ngeneration and editing with over 1,700 challenging instances, and an\naccompanying evaluation metric, StructScore, which employs a multi-round Q\\&A\nprotocol to assess fine-grained factual accuracy. Evaluations of 15 models\nreveal that even leading closed-source systems remain far from satisfactory.\nOur model attains strong editing performance, and inference-time reasoning\nyields consistent gains across diverse architectures. By releasing the dataset,\nmodel, and benchmark, we aim to advance unified multimodal foundations for\nstructured visuals.",
        "url": "http://arxiv.org/abs/2510.05091v1",
        "published_date": "2025-10-06T17:56:55+00:00",
        "updated_date": "2025-10-06T17:56:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Le Zhuo",
            "Songhao Han",
            "Yuandong Pu",
            "Boxiang Qiu",
            "Sayak Paul",
            "Yue Liao",
            "Yihao Liu",
            "Jie Shao",
            "Xi Chen",
            "Si Liu",
            "Hongsheng Li"
        ],
        "tldr": "This paper introduces a new dataset, model, and benchmark (StructBench) for generating and editing structured visuals with improved factual accuracy, addressing a weakness in current visual generation models.",
        "tldr_zh": "本文介绍了一个新的数据集、模型和基准测试 (StructBench)，用于生成和编辑具有更高事实准确性的结构化视觉内容，解决了当前视觉生成模型的一个弱点。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder",
        "summary": "Large-scale text-to-image diffusion models have become the backbone of modern\nimage editing, yet text prompts alone do not offer adequate control over the\nediting process. Two properties are especially desirable: disentanglement,\nwhere changing one attribute does not unintentionally alter others, and\ncontinuous control, where the strength of an edit can be smoothly adjusted. We\nintroduce a method for disentangled and continuous editing through token-level\nmanipulation of text embeddings. The edits are applied by manipulating the\nembeddings along carefully chosen directions, which control the strength of the\ntarget attribute. To identify such directions, we employ a Sparse Autoencoder\n(SAE), whose sparse latent space exposes semantically isolated dimensions. Our\nmethod operates directly on text embeddings without modifying the diffusion\nprocess, making it model agnostic and broadly applicable to various image\nsynthesis backbones. Experiments show that it enables intuitive and efficient\nmanipulations with continuous control across diverse attributes and domains.",
        "url": "http://arxiv.org/abs/2510.05081v1",
        "published_date": "2025-10-06T17:51:04+00:00",
        "updated_date": "2025-10-06T17:51:04+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ronen Kamenetsky",
            "Sara Dorfman",
            "Daniel Garibi",
            "Roni Paiss",
            "Or Patashnik",
            "Daniel Cohen-Or"
        ],
        "tldr": "This paper introduces SAEdit, a method for disentangled and continuous image editing via token-level manipulation of text embeddings using a Sparse Autoencoder to control the strength of attributes in text-to-image diffusion models.",
        "tldr_zh": "本文介绍了SAEdit，一种通过稀疏自动编码器，利用token级别操作文本嵌入，从而实现解耦和连续图像编辑的方法，可以控制文本到图像扩散模型中属性的强度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization",
        "summary": "Tokenizers are a key component of state-of-the-art generative image models,\nextracting the most important features from the signal while reducing data\ndimension and redundancy. Most current tokenizers are based on KL-regularized\nvariational autoencoders (KL-VAE), trained with reconstruction, perceptual and\nadversarial losses. Diffusion decoders have been proposed as a more principled\nalternative to model the distribution over images conditioned on the latent.\nHowever, matching the performance of KL-VAE still requires adversarial losses,\nas well as a higher decoding time due to iterative sampling. To address these\nlimitations, we introduce a new pixel diffusion decoder architecture for\nimproved scaling and training stability, benefiting from transformer components\nand GAN-free training. We use distillation to replicate the performance of the\ndiffusion decoder in an efficient single-step decoder. This makes SSDD the\nfirst diffusion decoder optimized for single-step reconstruction trained\nwithout adversarial losses, reaching higher reconstruction quality and faster\nsampling than KL-VAE. In particular, SSDD improves reconstruction FID from\n$0.87$ to $0.50$ with $1.4\\times$ higher throughput and preserve generation\nquality of DiTs with $3.8\\times$ faster sampling. As such, SSDD can be used as\na drop-in replacement for KL-VAE, and for building higher-quality and faster\ngenerative models.",
        "url": "http://arxiv.org/abs/2510.04961v1",
        "published_date": "2025-10-06T15:57:31+00:00",
        "updated_date": "2025-10-06T15:57:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Théophane Vallaeys",
            "Jakob Verbeek",
            "Matthieu Cord"
        ],
        "tldr": "The paper introduces SSDD, a single-step diffusion decoder for image tokenization, outperforming KL-VAEs in reconstruction quality and sampling speed without adversarial losses. It can be used as a drop-in replacement for KL-VAE tokenizers.",
        "tldr_zh": "本文介绍了一种用于图像标记的单步扩散解码器SSDD，它在重建质量和采样速度上优于KL-VAEs，并且没有对抗性损失。它可以作为KL-VAE标记器的直接替代品。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement",
        "summary": "In recent years, multi-concept personalization for text-to-image (T2I)\ndiffusion models to represent several subjects in an image has gained much more\nattention. The main challenge of this task is \"concept mixing\", where multiple\nlearned concepts interfere or blend undesirably in the output image. To address\nthis issue, in this paper, we present ConceptSplit, a novel framework to split\nthe individual concepts through training and inference. Our framework comprises\ntwo key components. First, we introduce Token-wise Value Adaptation (ToVA), a\nmerging-free training method that focuses exclusively on adapting the value\nprojection in cross-attention. Based on our empirical analysis, we found that\nmodifying the key projection, a common approach in existing methods, can\ndisrupt the attention mechanism and lead to concept mixing. Second, we propose\nLatent Optimization for Disentangled Attention (LODA), which alleviates\nattention entanglement during inference by optimizing the input latent. Through\nextensive qualitative and quantitative experiments, we demonstrate that\nConceptSplit achieves robust multi-concept personalization, mitigating\nunintended concept interference. Code is available at\nhttps://github.com/KU-VGI/ConceptSplit",
        "url": "http://arxiv.org/abs/2510.04668v1",
        "published_date": "2025-10-06T10:22:46+00:00",
        "updated_date": "2025-10-06T10:22:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Habin Lim",
            "Yeongseob Won",
            "Juwon Seo",
            "Gyeong-Moon Park"
        ],
        "tldr": "The paper introduces ConceptSplit, a novel framework for multi-concept personalization in text-to-image diffusion models that aims to mitigate concept mixing by adapting value projection and optimizing the input latent.",
        "tldr_zh": "该论文介绍了 ConceptSplit，这是一种用于文本到图像扩散模型中多概念个性化的新型框架，旨在通过调整值投影和优化输入潜在变量来减轻概念混合。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator",
        "summary": "Deep generative models have made significant advances in generating complex\ncontent, yet conditional generation remains a fundamental challenge. Existing\nconditional generative adversarial networks often struggle to balance the dual\nobjectives of assessing authenticity and conditional alignment of input samples\nwithin their conditional discriminators. To address this, we propose a novel\ndiscriminator design that integrates three key capabilities: unconditional\ndiscrimination, matching-aware supervision to enhance alignment sensitivity,\nand adaptive weighting to dynamically balance all objectives. Specifically, we\nintroduce Sum of Naturalness and Alignment (SONA), which employs separate\nprojections for naturalness (authenticity) and alignment in the final layer\nwith an inductive bias, supported by dedicated objective functions and an\nadaptive weighting mechanism. Extensive experiments on class-conditional\ngeneration tasks show that \\ours achieves superior sample quality and\nconditional alignment compared to state-of-the-art methods. Furthermore, we\ndemonstrate its effectiveness in text-to-image generation, confirming the\nversatility and robustness of our approach.",
        "url": "http://arxiv.org/abs/2510.04576v1",
        "published_date": "2025-10-06T08:26:06+00:00",
        "updated_date": "2025-10-06T08:26:06+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Yuhta Takida",
            "Satoshi Hayakawa",
            "Takashi Shibuya",
            "Masaaki Imaizumi",
            "Naoki Murata",
            "Bac Nguyen",
            "Toshimitsu Uesaka",
            "Chieh-Hsin Lai",
            "Yuki Mitsufuji"
        ],
        "tldr": "This paper introduces SONA, a novel discriminator for conditional GANs that balances authenticity and conditional alignment through unconditional discrimination, matching-aware supervision, and adaptive weighting, demonstrating superior performance in class-conditional and text-to-image generation tasks.",
        "tldr_zh": "本文介绍了一种名为SONA的新型判别器，用于条件GAN，通过无条件判别、匹配感知监督和自适应加权来平衡真实性和条件对齐，并在类别条件和文本到图像生成任务中表现出卓越性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling",
        "summary": "Recent diffusion models achieve the state-of-the-art performance in image\ngeneration, but often suffer from semantic inconsistencies or hallucinations.\nWhile various inference-time guidance methods can enhance generation, they\noften operate indirectly by relying on external signals or architectural\nmodifications, which introduces additional computational overhead. In this\npaper, we propose Tangential Amplifying Guidance (TAG), a more efficient and\ndirect guidance method that operates solely on trajectory signals without\nmodifying the underlying diffusion model. TAG leverages an intermediate sample\nas a projection basis and amplifies the tangential components of the estimated\nscores with respect to this basis to correct the sampling trajectory. We\nformalize this guidance process by leveraging a first-order Taylor expansion,\nwhich demonstrates that amplifying the tangential component steers the state\ntoward higher-probability regions, thereby reducing inconsistencies and\nenhancing sample quality. TAG is a plug-and-play, architecture-agnostic module\nthat improves diffusion sampling fidelity with minimal computational addition,\noffering a new perspective on diffusion guidance.",
        "url": "http://arxiv.org/abs/2510.04533v1",
        "published_date": "2025-10-06T06:53:29+00:00",
        "updated_date": "2025-10-06T06:53:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyunmin Cho",
            "Donghoon Ahn",
            "Susung Hong",
            "Jee Eun Kim",
            "Seungryong Kim",
            "Kyong Hwan Jin"
        ],
        "tldr": "The paper introduces Tangential Amplifying Guidance (TAG), a novel, efficient, and architecture-agnostic guidance method for diffusion models that reduces semantic inconsistencies by amplifying tangential components of estimated scores during sampling.",
        "tldr_zh": "该论文介绍了切向放大引导(TAG), 一种新颖、高效且架构无关的扩散模型引导方法, 通过在采样过程中放大估计分数的切向分量来减少语义不一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation",
        "summary": "Diffusion models have achieved impressive results in generating high-quality\nimages. Yet, they often struggle to faithfully align the generated images with\nthe input prompts. This limitation arises from synchronous denoising, where all\npixels simultaneously evolve from random noise to clear images. As a result,\nduring generation, the prompt-related regions can only reference the unrelated\nregions at the same noise level, failing to obtain clear context and ultimately\nimpairing text-to-image alignment. To address this issue, we propose\nasynchronous diffusion models -- a novel framework that allocates distinct\ntimesteps to different pixels and reformulates the pixel-wise denoising\nprocess. By dynamically modulating the timestep schedules of individual pixels,\nprompt-related regions are denoised more gradually than unrelated regions,\nthereby allowing them to leverage clearer inter-pixel context. Consequently,\nthese prompt-related regions achieve better alignment in the final images.\nExtensive experiments demonstrate that our asynchronous diffusion models can\nsignificantly improve text-to-image alignment across diverse prompts. The code\nrepository for this work is available at https://github.com/hu-zijing/AsynDM.",
        "url": "http://arxiv.org/abs/2510.04504v1",
        "published_date": "2025-10-06T05:45:56+00:00",
        "updated_date": "2025-10-06T05:45:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zijing Hu",
            "Yunze Tong",
            "Fengda Zhang",
            "Junkun Yuan",
            "Jun Xiao",
            "Kun Kuang"
        ],
        "tldr": "The paper introduces asynchronous diffusion models to improve text-to-image alignment by dynamically adjusting denoising timesteps for different pixels, allowing prompt-related regions to leverage clearer inter-pixel context.",
        "tldr_zh": "该论文提出了一种异步扩散模型，通过为不同的像素动态调整去噪时间步长来改善文本到图像的对齐，从而使与提示相关的区域能够利用更清晰的像素间上下文。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization",
        "summary": "Visual autoregressive (AR) generation offers a promising path toward unifying\nvision and language models, yet its performance remains suboptimal against\ndiffusion models. Prior work often attributes this gap to tokenizer limitations\nand rasterization ordering. In this work, we identify a core bottleneck from\nthe perspective of generator-tokenizer inconsistency, i.e., the AR-generated\ntokens may not be well-decoded by the tokenizer. To address this, we propose\nreAR, a simple training strategy introducing a token-wise regularization\nobjective: when predicting the next token, the causal transformer is also\ntrained to recover the visual embedding of the current token and predict the\nembedding of the target token under a noisy context. It requires no changes to\nthe tokenizer, generation order, inference pipeline, or external models.\nDespite its simplicity, reAR substantially improves performance. On ImageNet,\nit reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard\nrasterization-based tokenizer. When applied to advanced tokenizers, it achieves\na gFID of 1.42 with only 177M parameters, matching the performance with larger\nstate-of-the-art diffusion models (675M).",
        "url": "http://arxiv.org/abs/2510.04450v1",
        "published_date": "2025-10-06T02:48:13+00:00",
        "updated_date": "2025-10-06T02:48:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiyuan He",
            "Yicong Li",
            "Haotian Ye",
            "Jinghao Wang",
            "Xinyao Liao",
            "Pheng-Ann Heng",
            "Stefano Ermon",
            "James Zou",
            "Angela Yao"
        ],
        "tldr": "The paper introduces reAR, a generator-tokenizer consistency regularization method for visual autoregressive models, improving image generation performance to match state-of-the-art diffusion models with fewer parameters.",
        "tldr_zh": "该论文介绍了reAR，一种用于视觉自回归模型的生成器-tokenizer一致性正则化方法，提高了图像生成性能，以更少的参数匹配了最先进的扩散模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator",
        "summary": "World models that support controllable\n  and editable spatiotemporal environments are valuable\n  for robotics, enabling scalable training data, repro ducible evaluation, and\nflexible task design. While\n  recent text-to-video models generate realistic dynam ics, they are\nconstrained to 2D views and offer limited\n  interaction. We introduce MorphoSim, a language guided framework that\ngenerates 4D scenes with\n  multi-view consistency and object-level controls. From\n  natural language instructions, MorphoSim produces\n  dynamic environments where objects can be directed,\n  recolored, or removed, and scenes can be observed\n  from arbitrary viewpoints. The framework integrates\n  trajectory-guided generation with feature field dis tillation, allowing edits\nto be applied interactively\n  without full re-generation. Experiments show that Mor phoSim maintains high\nscene fidelity while enabling\n  controllability and editability. The code is available\n  at https://github.com/eric-ai-lab/Morph4D.",
        "url": "http://arxiv.org/abs/2510.04390v1",
        "published_date": "2025-10-05T22:55:17+00:00",
        "updated_date": "2025-10-05T22:55:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Xuehai He",
            "Shijie Zhou",
            "Thivyanth Venkateswaran",
            "Kaizhi Zheng",
            "Ziyu Wan",
            "Achuta Kadambi",
            "Xin Eric Wang"
        ],
        "tldr": "MorphoSim is a language-guided framework for generating controllable and editable 4D scenes with multi-view consistency, enabling object manipulation and scene observation from arbitrary viewpoints. It integrates trajectory-guided generation with feature field distillation for interactive edits.",
        "tldr_zh": "MorphoSim是一个语言引导的框架，用于生成具有多视角一致性的可控和可编辑的4D场景，从而实现对象操作和从任意视点观察场景。它集成了轨迹引导生成和特征场蒸馏，以实现交互式编辑。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG",
        "summary": "This paper proposes \"3Dify,\" a procedural 3D computer graphics (3D-CG)\ngeneration framework utilizing Large Language Models (LLMs). The framework\nenables users to generate 3D-CG content solely through natural language\ninstructions. 3Dify is built upon Dify, an open-source platform for AI\napplication development, and incorporates several state-of-the-art LLM-related\ntechnologies such as the Model Context Protocol (MCP) and Retrieval-Augmented\nGeneration (RAG). For 3D-CG generation support, 3Dify automates the operation\nof various Digital Content Creation (DCC) tools via MCP. When DCC tools do not\nsupport MCP-based interaction, the framework employs the Computer-Using Agent\n(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,\nto enhance image generation quality, 3Dify allows users to provide feedback by\nselecting preferred images from multiple candidates. The LLM then learns\nvariable patterns from these selections and applies them to subsequent\ngenerations. Furthermore, 3Dify supports the integration of locally deployed\nLLMs, enabling users to utilize custom-developed models and to reduce both time\nand monetary costs associated with external API calls by leveraging their own\ncomputational resources.",
        "url": "http://arxiv.org/abs/2510.04536v1",
        "published_date": "2025-10-06T07:00:06+00:00",
        "updated_date": "2025-10-06T07:00:06+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Shun-ichiro Hayashi",
            "Daichi Mukunoki",
            "Tetsuya Hoshino",
            "Satoshi Ohshima",
            "Takahiro Katagiri"
        ],
        "tldr": "The paper introduces 3Dify, a framework leveraging LLMs, MCP, and RAG for procedural 3D content generation from natural language instructions, integrating DCC tools and user feedback for improved quality and cost reduction.",
        "tldr_zh": "该论文介绍了3Dify，一个利用LLMs，MCP和RAG的框架，用于从自然语言指令程序化生成3D内容，集成了DCC工具和用户反馈，以提高质量并降低成本。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement",
        "summary": "Recent advances in image generation and editing technologies have enabled\nstate-of-the-art models to achieve impressive results in general domains.\nHowever, when applied to e-commerce scenarios, these general models often\nencounter consistency limitations. To address this challenge, we introduce\nTBStar-Edit, an new image editing model tailored for the e-commerce domain.\nThrough rigorous data engineering, model architecture design and training\nstrategy, TBStar-Edit achieves precise and high-fidelity image editing while\nmaintaining the integrity of product appearance and layout. Specifically, for\ndata engineering, we establish a comprehensive data construction pipeline,\nencompassing data collection, construction, filtering, and augmentation, to\nacquire high-quality, instruction-following, and strongly consistent editing\ndata to support model training. For model architecture design, we design a\nhierarchical model framework consisting of a base model, pattern shifting\nmodules, and consistency enhancement modules. For model training, we adopt a\ntwo-stage training strategy to enhance the consistency preservation: first\nstage for editing pattern shifting, and second stage for consistency\nenhancement. Each stage involves training different modules with separate\ndatasets. Finally, we conduct extensive evaluations of TBStar-Edit on a\nself-proposed e-commerce benchmark, and the results demonstrate that\nTBStar-Edit outperforms existing general-domain editing models in both\nobjective metrics (VIE Score) and subjective user preference.",
        "url": "http://arxiv.org/abs/2510.04483v1",
        "published_date": "2025-10-06T04:46:42+00:00",
        "updated_date": "2025-10-06T04:46:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Fang",
            "Zechao Zhan",
            "Weixin Feng",
            "Ziwei Huang",
            "XuBin Li",
            "Tiezheng Ge"
        ],
        "tldr": "The paper introduces TBStar-Edit, a novel image editing model specifically designed for e-commerce scenarios, addressing the consistency issues encountered by general models. It uses a tailored architecture, data engineering pipeline, and two-stage training strategy to achieve high-fidelity editing with preserved product integrity, outperforming existing models on an e-commerce benchmark.",
        "tldr_zh": "该论文介绍了TBStar-Edit，一种专为电商场景设计的新型图像编辑模型，旨在解决通用模型遇到的一致性问题。它采用定制的架构、数据工程流程和两阶段训练策略，以实现高保真编辑并保持产品完整性， 在电商基准测试中优于现有模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]