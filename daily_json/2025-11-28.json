[
    {
        "title": "Canvas-to-Image: Compositional Image Generation with Multimodal Controls",
        "summary": "While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.",
        "url": "http://arxiv.org/abs/2511.21691v1",
        "published_date": "2025-11-26T18:59:56+00:00",
        "updated_date": "2025-11-26T18:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yusuf Dalva",
            "Guocheng Gordon Qian",
            "Maya Goldenberg",
            "Tsai-Shien Chen",
            "Kfir Aberman",
            "Sergey Tulyakov",
            "Pinar Yanardag",
            "Kuan-Chieh Jackson Wang"
        ],
        "tldr": "The paper introduces Canvas-to-Image, a unified framework for compositional image generation using diffusion models. It consolidates heterogeneous controls (text, reference images, layouts, poses) into a single canvas interface and trains the model to reason across multiple control modalities.",
        "tldr_zh": "该论文介绍了Canvas-to-Image，一个使用扩散模型进行组合图像生成的统一框架。它将不同的控制信号（文本、参考图像、布局、姿势）整合到一个画布界面中，并训练模型以跨多种控制方式进行推理。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    }
]