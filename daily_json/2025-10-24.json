[
    {
        "title": "HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives",
        "summary": "State-of-the-art text-to-video models excel at generating isolated clips but\nfall short of creating the coherent, multi-shot narratives, which are the\nessence of storytelling. We bridge this \"narrative gap\" with HoloCine, a model\nthat generates entire scenes holistically to ensure global consistency from the\nfirst shot to the last. Our architecture achieves precise directorial control\nthrough a Window Cross-Attention mechanism that localizes text prompts to\nspecific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within\nshots but sparse between them) ensures the efficiency required for minute-scale\ngeneration. Beyond setting a new state-of-the-art in narrative coherence,\nHoloCine develops remarkable emergent abilities: a persistent memory for\ncharacters and scenes, and an intuitive grasp of cinematic techniques. Our work\nmarks a pivotal shift from clip synthesis towards automated filmmaking, making\nend-to-end cinematic creation a tangible future. Our code is available at:\nhttps://holo-cine.github.io/.",
        "url": "http://arxiv.org/abs/2510.20822v1",
        "published_date": "2025-10-23T17:59:59+00:00",
        "updated_date": "2025-10-23T17:59:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yihao Meng",
            "Hao Ouyang",
            "Yue Yu",
            "Qiuyu Wang",
            "Wen Wang",
            "Ka Leong Cheng",
            "Hanlin Wang",
            "Yixuan Li",
            "Cheng Chen",
            "Yanhong Zeng",
            "Yujun Shen",
            "Huamin Qu"
        ],
        "tldr": "HoloCine generates coherent, multi-shot video narratives by ensuring global consistency across scenes and enabling directorial control via window cross-attention and sparse inter-shot self-attention, demonstrating emergent abilities in character persistence and cinematic understanding.",
        "tldr_zh": "HoloCine通过确保场景间的全局一致性，以及通过窗口交叉注意力和稀疏镜头间自注意力实现导演控制，从而生成连贯的多镜头视频叙事，并在角色持久性和电影理解方面展示了新兴能力。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "AutoScape: Geometry-Consistent Long-Horizon Scene Generation",
        "summary": "This paper proposes AutoScape, a long-horizon driving scene generation\nframework. At its core is a novel RGB-D diffusion model that iteratively\ngenerates sparse, geometrically consistent keyframes, serving as reliable\nanchors for the scene's appearance and geometry. To maintain long-range\ngeometric consistency, the model 1) jointly handles image and depth in a shared\nlatent space, 2) explicitly conditions on the existing scene geometry (i.e.,\nrendered point clouds) from previously generated keyframes, and 3) steers the\nsampling process with a warp-consistent guidance. Given high-quality RGB-D\nkeyframes, a video diffusion model then interpolates between them to produce\ndense and coherent video frames. AutoScape generates realistic and\ngeometrically consistent driving videos of over 20 seconds, improving the\nlong-horizon FID and FVD scores over the prior state-of-the-art by 48.6\\% and\n43.0\\%, respectively.",
        "url": "http://arxiv.org/abs/2510.20726v1",
        "published_date": "2025-10-23T16:44:34+00:00",
        "updated_date": "2025-10-23T16:44:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiacheng Chen",
            "Ziyu Jiang",
            "Mingfu Liang",
            "Bingbing Zhuang",
            "Jong-Chyi Su",
            "Sparsh Garg",
            "Ying Wu",
            "Manmohan Chandraker"
        ],
        "tldr": "AutoScape introduces a novel RGB-D diffusion model for generating long-horizon, geometrically consistent driving scene videos, achieving significant improvements in FID and FVD scores compared to existing methods.",
        "tldr_zh": "AutoScape 提出了一个新颖的 RGB-D 扩散模型，用于生成长时程、几何一致的驾驶场景视频，与现有方法相比，在 FID 和 FVD 分数上取得了显著改进。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling",
        "summary": "Prompt design plays a crucial role in text-to-video (T2V) generation, yet\nuser-provided prompts are often short, unstructured, and misaligned with\ntraining data, limiting the generative potential of diffusion-based T2V models.\nWe present \\textbf{RAPO++}, a cross-stage prompt optimization framework that\nunifies training-data--aligned refinement, test-time iterative scaling, and\nlarge language model (LLM) fine-tuning to substantially improve T2V generation\nwithout modifying the underlying generative backbone. In \\textbf{Stage 1},\nRetrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with\nsemantically relevant modifiers retrieved from a relation graph and refactors\nthem to match training distributions, enhancing compositionality and\nmulti-object fidelity. \\textbf{Stage 2} introduces Sample-Specific Prompt\nOptimization (SSPO), a closed-loop mechanism that iteratively refines prompts\nusing multi-source feedback -- including semantic alignment, spatial fidelity,\ntemporal coherence, and task-specific signals such as optical flow -- yielding\nprogressively improved video generation quality. \\textbf{Stage 3} leverages\noptimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing\ntask-specific optimization patterns and enabling efficient, high-quality prompt\ngeneration even before inference. Extensive experiments across five\nstate-of-the-art T2V models and five benchmarks demonstrate that RAPO++\nachieves significant gains in semantic alignment, compositional reasoning,\ntemporal stability, and physical plausibility, outperforming existing methods\nby large margins. Our results highlight RAPO++ as a model-agnostic,\ncost-efficient, and scalable solution that sets a new standard for prompt\noptimization in T2V generation. The code is available at\nhttps://github.com/Vchitect/RAPO.",
        "url": "http://arxiv.org/abs/2510.20206v1",
        "published_date": "2025-10-23T04:45:09+00:00",
        "updated_date": "2025-10-23T04:45:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bingjie Gao",
            "Qianli Ma",
            "Xiaoxue Wu",
            "Shuai Yang",
            "Guanzhou Lan",
            "Haonan Zhao",
            "Jiaxuan Chen",
            "Qingyang Liu",
            "Yu Qiao",
            "Xinyuan Chen",
            "Yaohui Wang",
            "Li Niu"
        ],
        "tldr": "The paper introduces RAPO++, a three-stage prompt optimization framework for text-to-video generation that refines prompts using retrieval augmentation, iterative scaling with multi-source feedback, and LLM fine-tuning, demonstrating significant improvements across multiple benchmarks and models.",
        "tldr_zh": "该论文提出了RAPO++，一个用于文本到视频生成的三阶段提示优化框架。该框架利用检索增强、多源的迭代缩放以及语言模型微调来改进提示，并在多个基准测试和模型上展示了显著的性能提升。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers",
        "summary": "Inspired by the performance and scalability of autoregressive large language\nmodels (LLMs), transformer-based models have seen recent success in the visual\ndomain. This study investigates a transformer adaptation for video prediction\nwith a simple end-to-end approach, comparing various spatiotemporal\nself-attention layouts. Focusing on causal modeling of physical simulations\nover time; a common shortcoming of existing video-generative approaches, we\nattempt to isolate spatiotemporal reasoning via physical object tracking\nmetrics and unsupervised training on physical simulation datasets. We introduce\na simple yet effective pure transformer model for autoregressive video\nprediction, utilizing continuous pixel-space representations for video\nprediction. Without the need for complex training strategies or latent\nfeature-learning components, our approach significantly extends the time\nhorizon for physically accurate predictions by up to 50% when compared with\nexisting latent-space approaches, while maintaining comparable performance on\ncommon video quality metrics. In addition, we conduct interpretability\nexperiments to identify network regions that encode information useful to\nperform accurate estimations of PDE simulation parameters via probing models,\nand find that this generalizes to the estimation of out-of-distribution\nsimulation parameters. This work serves as a platform for further\nattention-based spatiotemporal modeling of videos via a simple, parameter\nefficient, and interpretable approach.",
        "url": "http://arxiv.org/abs/2510.20807v1",
        "published_date": "2025-10-23T17:58:45+00:00",
        "updated_date": "2025-10-23T17:58:45+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Dean L Slack",
            "G Thomas Hudson",
            "Thomas Winterbottom",
            "Noura Al Moubayed"
        ],
        "tldr": "The paper introduces a simple, end-to-end transformer model for autoregressive video prediction of physical simulations, achieving longer prediction horizons and interpretability without complex latent space learning.",
        "tldr_zh": "该论文介绍了一种简单的端到端transformer模型，用于物理模拟的自回归视频预测，无需复杂的潜在空间学习即可实现更长的预测范围和可解释性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "AlphaFlow: Understanding and Improving MeanFlow Models",
        "summary": "MeanFlow has recently emerged as a powerful framework for few-step generative\nmodeling trained from scratch, but its success is not yet fully understood. In\nthis work, we show that the MeanFlow objective naturally decomposes into two\nparts: trajectory flow matching and trajectory consistency. Through gradient\nanalysis, we find that these terms are strongly negatively correlated, causing\noptimization conflict and slow convergence. Motivated by these insights, we\nintroduce $\\alpha$-Flow, a broad family of objectives that unifies trajectory\nflow matching, Shortcut Model, and MeanFlow under one formulation. By adopting\na curriculum strategy that smoothly anneals from trajectory flow matching to\nMeanFlow, $\\alpha$-Flow disentangles the conflicting objectives, and achieves\nbetter convergence. When trained from scratch on class-conditional ImageNet-1K\n256x256 with vanilla DiT backbones, $\\alpha$-Flow consistently outperforms\nMeanFlow across scales and settings. Our largest $\\alpha$-Flow-XL/2+ model\nachieves new state-of-the-art results using vanilla DiT backbones, with FID\nscores of 2.58 (1-NFE) and 2.15 (2-NFE).",
        "url": "http://arxiv.org/abs/2510.20771v1",
        "published_date": "2025-10-23T17:45:06+00:00",
        "updated_date": "2025-10-23T17:45:06+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Huijie Zhang",
            "Aliaksandr Siarohin",
            "Willi Menapace",
            "Michael Vasilkovsky",
            "Sergey Tulyakov",
            "Qing Qu",
            "Ivan Skorokhodov"
        ],
        "tldr": "The paper identifies conflicting objectives in MeanFlow, a few-step generative modeling framework, and introduces α-Flow, which mitigates these conflicts through a curriculum strategy, achieving state-of-the-art results on ImageNet generation with DiT backbones.",
        "tldr_zh": "该论文指出 MeanFlow（一种少步生成建模框架）中存在冲突的目标，并提出了 α-Flow，通过课程策略缓解这些冲突，并在使用 DiT 主干网络的 ImageNet 生成方面取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion",
        "summary": "Diffusion Transformer models can generate images with remarkable fidelity and\ndetail, yet training them at ultra-high resolutions remains extremely costly\ndue to the self-attention mechanism's quadratic scaling with the number of\nimage tokens. In this paper, we introduce Dynamic Position Extrapolation\n(DyPE), a novel, training-free method that enables pre-trained diffusion\ntransformers to synthesize images at resolutions far beyond their training\ndata, with no additional sampling cost. DyPE takes advantage of the spectral\nprogression inherent to the diffusion process, where low-frequency structures\nconverge early, while high-frequencies take more steps to resolve.\nSpecifically, DyPE dynamically adjusts the model's positional encoding at each\ndiffusion step, matching their frequency spectrum with the current stage of the\ngenerative process. This approach allows us to generate images at resolutions\nthat exceed the training resolution dramatically, e.g., 16 million pixels using\nFLUX. On multiple benchmarks, DyPE consistently improves performance and\nachieves state-of-the-art fidelity in ultra-high-resolution image generation,\nwith gains becoming even more pronounced at higher resolutions. Project page is\navailable at https://noamissachar.github.io/DyPE/.",
        "url": "http://arxiv.org/abs/2510.20766v1",
        "published_date": "2025-10-23T17:42:14+00:00",
        "updated_date": "2025-10-23T17:42:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Noam Issachar",
            "Guy Yariv",
            "Sagie Benaim",
            "Yossi Adi",
            "Dani Lischinski",
            "Raanan Fattal"
        ],
        "tldr": "The paper introduces DyPE, a training-free method for ultra-high-resolution image generation using diffusion transformers, which dynamically adjusts positional encodings during the diffusion process to improve fidelity at high resolutions.",
        "tldr_zh": "该论文介绍了DyPE，一种使用扩散Transformer进行超高分辨率图像生成的免训练方法，它在扩散过程中动态调整位置编码，从而提高高分辨率下的保真度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset",
        "summary": "Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable\nprogress. However, two key challenges remain : 1) the absence of a large-scale\nhigh-quality UHR T2I dataset, and (2) the neglect of tailored training\nstrategies for fine-grained detail synthesis in UHR scenarios. To tackle the\nfirst challenge, we introduce \\textbf{UltraHR-100K}, a high-quality dataset of\n100K UHR images with rich captions, offering diverse content and strong visual\nfidelity. Each image exceeds 3K resolution and is rigorously curated based on\ndetail richness, content complexity, and aesthetic quality. To tackle the\nsecond challenge, we propose a frequency-aware post-training method that\nenhances fine-detail generation in T2I diffusion models. Specifically, we\ndesign (i) \\textit{Detail-Oriented Timestep Sampling (DOTS)} to focus learning\non detail-critical denoising steps, and (ii) \\textit{Soft-Weighting Frequency\nRegularization (SWFR)}, which leverages Discrete Fourier Transform (DFT) to\nsoftly constrain frequency components, encouraging high-frequency detail\npreservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks\ndemonstrate that our approach significantly improves the fine-grained detail\nquality and overall fidelity of UHR image generation. The code is available at\n\\href{https://github.com/NJU-PCALab/UltraHR-100k}{here}.",
        "url": "http://arxiv.org/abs/2510.20661v1",
        "published_date": "2025-10-23T15:34:53+00:00",
        "updated_date": "2025-10-23T15:34:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Zhao",
            "En Ci",
            "Yunzhe Xu",
            "Tiehan Fan",
            "Shanyan Guan",
            "Yanhao Ge",
            "Jian Yang",
            "Ying Tai"
        ],
        "tldr": "The paper introduces UltraHR-100K, a large-scale high-quality ultra-high-resolution (UHR) text-to-image dataset, and proposes a frequency-aware post-training method with Detail-Oriented Timestep Sampling and Soft-Weighting Frequency Regularization to improve fine-grained detail synthesis in UHR image generation.",
        "tldr_zh": "该论文介绍了UltraHR-100K，一个大规模高质量的超高分辨率（UHR）文本到图像数据集，并提出了一种频率感知后训练方法，结合细节导向时间步采样和软权重频率正则化，以提高UHR图像生成中的精细细节合成。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging",
        "summary": "Recent progress in vision-language modeling for 3D medical imaging has been\nfueled by large-scale computed tomography (CT) corpora with paired free-text\nreports, stronger architectures, and powerful pretrained models. This has\nenabled applications such as automated report generation and text-conditioned\n3D image synthesis. Yet, current approaches struggle with high-resolution,\nlong-sequence volumes: contrastive pretraining often yields vision encoders\nthat are misaligned with clinical language, and slice-wise tokenization blurs\nfine anatomy, reducing diagnostic performance on downstream tasks. We introduce\nBTB3D (Better Tokens for Better 3D), a causal convolutional encoder-decoder\nthat unifies 2D and 3D training and inference while producing compact,\nfrequency-aware volumetric tokens. A three-stage training curriculum enables\n(i) local reconstruction, (ii) overlapping-window tiling, and (iii)\nlong-context decoder refinement, during which the model learns from short slice\nexcerpts yet generalizes to scans exceeding 300 slices without additional\nmemory overhead. BTB3D sets a new state-of-the-art on two key tasks: it\nimproves BLEU scores and increases clinical F1 by 40% over CT2Rep, CT-CHAT, and\nMerlin for report generation; and it reduces FID by 75% and halves FVD compared\nto GenerateCT and MedSyn for text-to-CT synthesis, producing anatomically\nconsistent 512*512*241 volumes. These results confirm that precise\nthree-dimensional tokenization, rather than larger language backbones alone, is\nessential for scalable vision-language modeling in 3D medical imaging. The\ncodebase is available at: https://github.com/ibrahimethemhamamci/BTB3D",
        "url": "http://arxiv.org/abs/2510.20639v1",
        "published_date": "2025-10-23T15:13:13+00:00",
        "updated_date": "2025-10-23T15:13:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ibrahim Ethem Hamamci",
            "Sezgin Er",
            "Suprosanna Shit",
            "Hadrien Reynaud",
            "Dong Yang",
            "Pengfei Guo",
            "Marc Edgar",
            "Daguang Xu",
            "Bernhard Kainz",
            "Bjoern Menze"
        ],
        "tldr": "The paper introduces BTB3D, a novel causal convolutional encoder-decoder for 3D medical imaging that achieves state-of-the-art results in report generation and text-to-CT synthesis by improving 3D tokenization. The impact is a significant boost in performance with higher quality outputs validated by clinical measures, and anatomically-accurate 3D volumes via text instructions.",
        "tldr_zh": "该论文介绍了一种名为 BTB3D 的新型因果卷积编码器-解码器，用于 3D 医学成像，通过改进 3D 标记化在自动报告生成和文本到 CT 合成方面取得了最先进的结果。该研究的核心影响在于性能的显著提升，通过临床措施验证了更高质量的输出，并通过文本指令生成解剖学上准确的 3D 容积。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GenColorBench: A Color Evaluation Benchmark for Text-to-Image Generation Models",
        "summary": "Recent years have seen impressive advances in text-to-image generation, with\nimage generative or unified models producing high-quality images from text. Yet\nthese models still struggle with fine-grained color controllability, often\nfailing to accurately match colors specified in text prompts. While existing\nbenchmarks evaluate compositional reasoning and prompt adherence, none\nsystematically assess color precision. Color is fundamental to human visual\nperception and communication, critical for applications from art to design\nworkflows requiring brand consistency. However, current benchmarks either\nneglect color or rely on coarse assessments, missing key capabilities such as\ninterpreting RGB values or aligning with human expectations. To this end, we\npropose GenColorBench, the first comprehensive benchmark for text-to-image\ncolor generation, grounded in color systems like ISCC-NBS and CSS3/X11,\nincluding numerical colors which are absent elsewhere. With 44K color-focused\nprompts covering 400+ colors, it reveals models' true capabilities via\nperceptual and automated assessments. Evaluations of popular text-to-image\nmodels using GenColorBench show performance variations, highlighting which\ncolor conventions models understand best and identifying failure modes. Our\nGenColorBench assessments will guide improvements in precise color generation.\nThe benchmark will be made public upon acceptance.",
        "url": "http://arxiv.org/abs/2510.20586v1",
        "published_date": "2025-10-23T14:12:55+00:00",
        "updated_date": "2025-10-23T14:12:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muhammad Atif Butt",
            "Alexandra Gomez-Villa",
            "Tao Wu",
            "Javier Vazquez-Corral",
            "Joost Van De Weijer",
            "Kai Wang"
        ],
        "tldr": "The paper introduces GenColorBench, a new benchmark for evaluating color precision in text-to-image generation models, addressing the lack of systematic color assessment in existing benchmarks.",
        "tldr_zh": "该论文介绍了 GenColorBench，这是一个用于评估文本到图像生成模型中颜色精确度的新基准，旨在解决现有基准中缺乏系统颜色评估的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Positional Encoding Field",
        "summary": "Diffusion Transformers (DiTs) have emerged as the dominant architecture for\nvisual generation, powering state-of-the-art image and video models. By\nrepresenting images as patch tokens with positional encodings (PEs), DiTs\ncombine Transformer scalability with spatial and temporal inductive biases. In\nthis work, we revisit how DiTs organize visual content and discover that patch\ntokens exhibit a surprising degree of independence: even when PEs are\nperturbed, DiTs still produce globally coherent outputs, indicating that\nspatial coherence is primarily governed by PEs. Motivated by this finding, we\nintroduce the Positional Encoding Field (PE-Field), which extends positional\nencodings from the 2D plane to a structured 3D field. PE-Field incorporates\ndepth-aware encodings for volumetric reasoning and hierarchical encodings for\nfine-grained sub-patch control, enabling DiTs to model geometry directly in 3D\nspace. Our PE-Field-augmented DiT achieves state-of-the-art performance on\nsingle-image novel view synthesis and generalizes to controllable spatial image\nediting.",
        "url": "http://arxiv.org/abs/2510.20385v1",
        "published_date": "2025-10-23T09:32:37+00:00",
        "updated_date": "2025-10-23T09:32:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunpeng Bai",
            "Haoxiang Li",
            "Qixing Huang"
        ],
        "tldr": "This paper introduces the Positional Encoding Field (PE-Field) for Diffusion Transformers (DiTs), enabling improved 3D reasoning and spatial control in image generation, achieving state-of-the-art results in novel view synthesis and image editing.",
        "tldr_zh": "该论文介绍了用于扩散Transformer (DiT) 的位置编码场 (PE-Field)，从而提高了图像生成中的3D推理和空间控制，并在新视角合成和图像编辑方面实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EditInfinity: Image Editing with Binary-Quantized Generative Models",
        "summary": "Adapting pretrained diffusion-based generative models for text-driven image\nediting with negligible tuning overhead has demonstrated remarkable potential.\nA classical adaptation paradigm, as followed by these methods, first infers the\ngenerative trajectory inversely for a given source image by image inversion,\nthen performs image editing along the inferred trajectory guided by the target\ntext prompts. However, the performance of image editing is heavily limited by\nthe approximation errors introduced during image inversion by diffusion models,\nwhich arise from the absence of exact supervision in the intermediate\ngenerative steps. To circumvent this issue, we investigate the\nparameter-efficient adaptation of VQ-based generative models for image editing,\nand leverage their inherent characteristic that the exact intermediate\nquantized representations of a source image are attainable, enabling more\neffective supervision for precise image inversion. Specifically, we propose\n\\emph{EditInfinity}, which adapts \\emph{Infinity}, a binary-quantized\ngenerative model, for image editing. We propose an efficient yet effective\nimage inversion mechanism that integrates text prompting rectification and\nimage style preservation, enabling precise image inversion. Furthermore, we\ndevise a holistic smoothing strategy which allows our \\emph{EditInfinity} to\nperform image editing with high fidelity to source images and precise semantic\nalignment to the text prompts. Extensive experiments on the PIE-Bench benchmark\nacross \"add\", \"change\", and \"delete\" editing operations, demonstrate the\nsuperior performance of our model compared to state-of-the-art diffusion-based\nbaselines. Code available at: https://github.com/yx-chen-ust/EditInfinity.",
        "url": "http://arxiv.org/abs/2510.20217v1",
        "published_date": "2025-10-23T05:06:24+00:00",
        "updated_date": "2025-10-23T05:06:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahuan Wang",
            "Yuxin Chen",
            "Jun Yu",
            "Guangming Lu",
            "Wenjie Pei"
        ],
        "tldr": "The paper introduces EditInfinity, a method for text-driven image editing using binary-quantized generative models, which achieves superior performance compared to diffusion-based methods by leveraging precise image inversion.",
        "tldr_zh": "本文提出了EditInfinity，一种使用二值量化生成模型进行文本驱动图像编辑的方法。通过利用精确的图像反演，该方法比基于扩散的方法表现出更优越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "FlowCycle: Pursuing Cycle-Consistent Flows for Text-based Editing",
        "summary": "Recent advances in pre-trained text-to-image flow models have enabled\nremarkable progress in text-based image editing. Mainstream approaches always\nadopt a corruption-then-restoration paradigm, where the source image is first\ncorrupted into an ``intermediate state'' and then restored to the target image\nunder the prompt guidance. However, current methods construct this intermediate\nstate in a target-agnostic manner, i.e., they primarily focus on realizing\nsource image reconstruction while neglecting the semantic gaps towards the\nspecific editing target. This design inherently results in limited editability\nor inconsistency when the desired modifications substantially deviate from the\nsource. In this paper, we argue that the intermediate state should be\ntarget-aware, i.e., selectively corrupting editing-relevant contents while\npreserving editing-irrelevant ones. To this end, we propose FlowCycle, a novel\ninversion-free and flow-based editing framework that parameterizes corruption\nwith learnable noises and optimizes them through a cycle-consistent process. By\niteratively editing the source to the target and recovering back to the source\nwith dual consistency constraints, FlowCycle learns to produce a target-aware\nintermediate state, enabling faithful modifications while preserving source\nconsistency. Extensive ablations have demonstrated that FlowCycle achieves\nsuperior editing quality and consistency over state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2510.20212v1",
        "published_date": "2025-10-23T04:58:29+00:00",
        "updated_date": "2025-10-23T04:58:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanghao Wang",
            "Zhen Wang",
            "Long Chen"
        ],
        "tldr": "The paper introduces FlowCycle, a flow-based text-to-image editing framework that uses a cycle-consistent process to learn a target-aware intermediate state, improving editing quality and consistency. It addresses the limitation of current methods that neglect semantic gaps between source and target images during the corruption stage.",
        "tldr_zh": "该论文介绍了一种名为FlowCycle的基于流的文本到图像编辑框架，该框架使用循环一致性过程来学习目标感知的中间状态，从而提高编辑质量和一致性。它解决了当前方法在源图像和目标图像之间的语义差距。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Attentive Convolution: Unifying the Expressivity of Self-Attention with Convolutional Efficiency",
        "summary": "Self-attention (SA) has become the cornerstone of modern vision backbones for\nits powerful expressivity over traditional Convolutions (Conv). However, its\nquadratic complexity remains a critical bottleneck for practical applications.\nGiven that Conv offers linear complexity and strong visual priors, continuing\nefforts have been made to promote the renaissance of Conv. However, a\npersistent performance chasm remains, highlighting that these modernizations\nhave not yet captured the intrinsic expressivity that defines SA. In this\npaper, we re-examine the design of the CNNs, directed by a key question: what\nprinciples give SA its edge over Conv? As a result, we reveal two fundamental\ninsights that challenge the long-standing design intuitions in prior research\n(e.g., Receptive field). The two findings are: (1) \\textit{Adaptive routing}:\nSA dynamically regulates positional information flow according to semantic\ncontent, whereas Conv employs static kernels uniformly across all positions.\n(2) \\textit{Lateral inhibition}: SA induces score competition among token\nweighting, effectively suppressing redundancy and sharpening representations,\nwhereas Conv filters lack such inhibitory dynamics and exhibit considerable\nredundancy. Based on this, we propose \\textit{Attentive Convolution} (ATConv),\na principled reformulation of the convolutional operator that intrinsically\ninjects these principles. Interestingly, with only $3\\times3$ kernels, ATConv\nconsistently outperforms various SA mechanisms in fundamental vision tasks.\nBuilding on ATConv, we introduce AttNet, a CNN family that can attain\n\\textbf{84.4\\%} ImageNet-1K Top-1 accuracy with only 27M parameters. In\ndiffusion-based image generation, replacing all SA with the proposed $3\\times\n3$ ATConv in SiT-XL/2 reduces ImageNet FID by 0.15 in 400k steps with faster\nsampling. Code is available at: github.com/price112/Attentive-Convolution.",
        "url": "http://arxiv.org/abs/2510.20092v1",
        "published_date": "2025-10-23T00:25:17+00:00",
        "updated_date": "2025-10-23T00:25:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Yu",
            "Haoyu Chen",
            "Yan Jiang",
            "Wei Peng",
            "Zhaodong Sun",
            "Samuel Kaski",
            "Guoying Zhao"
        ],
        "tldr": "This paper introduces Attentive Convolution (ATConv), a novel convolutional operator inspired by self-attention mechanisms, showing superior performance and efficiency compared to both standard convolutions and self-attention in image classification and generation tasks.",
        "tldr_zh": "本文介绍了注意力卷积 (ATConv)，这是一种受自注意力机制启发的新型卷积算子，在图像分类和生成任务中表现出比标准卷积和自注意力更优越的性能和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks",
        "summary": "We propose a new GAN-based unsupervised model for disentangled representation\nlearning. The new model is discovered in an attempt to utilize the Information\nBottleneck (IB) framework to the optimization of GAN, thereby named IB-GAN. The\narchitecture of IB-GAN is partially similar to that of InfoGAN but has a\ncritical difference; an intermediate layer of the generator is leveraged to\nconstrain the mutual information between the input and the generated output.\nThe intermediate stochastic layer can serve as a learnable latent distribution\nthat is trained with the generator jointly in an end-to-end fashion. As a\nresult, the generator of IB-GAN can harness the latent space in a disentangled\nand interpretable manner. With the experiments on dSprites and Color-dSprites\ndataset, we demonstrate that IB-GAN achieves competitive disentanglement scores\nto those of state-of-the-art \\b{eta}-VAEs and outperforms InfoGAN. Moreover,\nthe visual quality and the diversity of samples generated by IB-GAN are often\nbetter than those by \\b{eta}-VAEs and Info-GAN in terms of FID score on CelebA\nand 3D Chairs dataset.",
        "url": "http://arxiv.org/abs/2510.20165v1",
        "published_date": "2025-10-23T03:24:48+00:00",
        "updated_date": "2025-10-23T03:24:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68T45 (Machine learning in discrete mathematics), 68T07 (Artificial\n  neural networks and deep learning)"
        ],
        "authors": [
            "Insu Jeon",
            "Wonkwang Lee",
            "Myeongjang Pyeon",
            "Gunhee Kim"
        ],
        "tldr": "This paper introduces IB-GAN, a novel GAN-based unsupervised model that utilizes the Information Bottleneck (IB) framework to achieve disentangled representation learning, demonstrating competitive or superior performance compared to existing methods like InfoGAN and β-VAEs on various datasets.",
        "tldr_zh": "该论文介绍了 IB-GAN，一种新的基于 GAN 的无监督模型，它利用信息瓶颈（IB）框架来实现解耦的表征学习，并在各种数据集上展示了与 InfoGAN 和 β-VAE 等现有方法相比具有竞争力或更优越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback",
        "summary": "Although recent advancements in diffusion models have significantly enriched\nthe quality of generated images, challenges remain in synthesizing pixel-based\nhuman-drawn sketches, a representative example of abstract expression. To\ncombat these challenges, we propose StableSketcher, a novel framework that\nempowers diffusion models to generate hand-drawn sketches with high prompt\nfidelity. Within this framework, we fine-tune the variational autoencoder to\noptimize latent decoding, enabling it to better capture the characteristics of\nsketches. In parallel, we integrate a new reward function for reinforcement\nlearning based on visual question answering, which improves text-image\nalignment and semantic consistency. Extensive experiments demonstrate that\nStableSketcher generates sketches with improved stylistic fidelity, achieving\nbetter alignment with prompts compared to the Stable Diffusion baseline.\nAdditionally, we introduce SketchDUO, to the best of our knowledge, the first\ndataset comprising instance-level sketches paired with captions and\nquestion-answer pairs, thereby addressing the limitations of existing datasets\nthat rely on image-label pairs. Our code and dataset will be made publicly\navailable upon acceptance.",
        "url": "http://arxiv.org/abs/2510.20093v1",
        "published_date": "2025-10-23T00:27:32+00:00",
        "updated_date": "2025-10-23T00:27:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiho Park",
            "Sieun Choi",
            "Jaeyoon Seo",
            "Jihie Kim"
        ],
        "tldr": "The paper introduces StableSketcher, a framework that enhances diffusion models for pixel-based sketch generation using VQA-based reinforcement learning and a fine-tuned VAE. It also introduces a new dataset, SketchDUO, for instance-level sketches.",
        "tldr_zh": "该论文介绍了 StableSketcher，一个通过使用基于 VQA 的强化学习和微调的 VAE 来增强用于基于像素的草图生成的扩散模型的框架。它还介绍了一个新的数据集 SketchDUO，用于实例级别的草图。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]