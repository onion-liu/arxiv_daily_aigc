[
    {
        "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback",
        "summary": "Instruction-based image editing has achieved remarkable progress; however,\nmodels solely trained via supervised fine-tuning often overfit to annotated\npatterns, hindering their ability to explore and generalize beyond training\ndistributions. To this end, we introduce Edit-R1, a novel post-training\nframework for instruction-based image editing based on policy optimization.\nSpecifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a\nlikelihood-free policy optimization method consistent with the flow matching\nforward process, thereby enabling the use of higher-order samplers and more\nefficient training. Another key challenge here is the absence of a universal\nreward model, resulting from the diverse nature of editing instructions and\ntasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)\nas a unified, training-free reward model, leveraging its output logits to\nprovide fine-grained feedback. Furthermore, we carefully design a low-variance\ngroup filtering mechanism to reduce MLLM scoring noise and stabilize\noptimization. UniWorld-V2, trained with this framework, achieves\n\\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,\nscoring 4.49 and 7.83, respectively. Crucially, our framework is\nmodel-agnostic, delivering substantial performance gains when applied to\ndiverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its\nwide applicability. Code and models are publicly available at\nhttps://github.com/PKU-YuanGroup/UniWorld-V2.",
        "url": "http://arxiv.org/abs/2510.16888v1",
        "published_date": "2025-10-19T15:38:06+00:00",
        "updated_date": "2025-10-19T15:38:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zongjian Li",
            "Zheyuan Liu",
            "Qihui Zhang",
            "Bin Lin",
            "Shenghai Yuan",
            "Zhiyuan Yan",
            "Yang Ye",
            "Wangbo Yu",
            "Yuwei Niu",
            "Li Yuan"
        ],
        "tldr": "The paper introduces UniWorld-V2, a novelReinforcement Learning based post-training framework for instruction-based image editing, which leverages Diffusion Negative-aware Finetuning and a Multimodal Large Language Model as a reward model, achieving state-of-the-art results on image editing benchmarks.",
        "tldr_zh": "该论文介绍了UniWorld-V2，一种新颖的基于强化学习的指令图像编辑后训练框架，它利用了扩散负感知微调和多模态大型语言模型作为奖励模型，在图像编辑基准测试中取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis",
        "summary": "Generative models, especially Diffusion Models, have demonstrated remarkable\ncapability in generating high-quality synthetic data, including medical images.\nHowever, traditional class-conditioned generative models often struggle to\ngenerate images that accurately represent specific medical categories, limiting\ntheir usefulness for applications such as skin cancer diagnosis. To address\nthis problem, we propose a classification-induced diffusion model, namely,\nClass-N-Diff, to simultaneously generate and classify dermoscopic images. Our\nClass-N-Diff model integrates a classifier within a diffusion model to guide\nimage generation based on its class conditions. Thus, the model has better\ncontrol over class-conditioned image synthesis, resulting in more realistic and\ndiverse images. Additionally, the classifier demonstrates improved performance,\nhighlighting its effectiveness for downstream diagnostic tasks. This unique\nintegration in our Class-N-Diff makes it a robust tool for enhancing the\nquality and utility of diffusion model-based synthetic dermoscopic image\ngeneration. Our code is available at https://github.com/Munia03/Class-N-Diff.",
        "url": "http://arxiv.org/abs/2510.16887v1",
        "published_date": "2025-10-19T15:37:41+00:00",
        "updated_date": "2025-10-19T15:37:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nusrat Munia",
            "Abdullah Imran"
        ],
        "tldr": "The paper introduces Class-N-Diff, a classification-induced diffusion model that enhances class-conditioned dermoscopic image generation for improved skin cancer diagnosis, demonstrating superior image realism and diagnostic performance.",
        "tldr_zh": "该论文介绍了 Class-N-Diff，一种分类引导的扩散模型，增强了类条件下的皮肤镜图像生成，从而改进了皮肤癌诊断，并展示了卓越的图像真实性和诊断性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Personalized Image Filter: Mastering Your Photographic Style",
        "summary": "Photographic style, as a composition of certain photographic concepts, is the\ncharm behind renowned photographers. But learning and transferring photographic\nstyle need a profound understanding of how the photo is edited from the unknown\noriginal appearance. Previous works either fail to learn meaningful\nphotographic concepts from reference images, or cannot preserve the content of\nthe content image. To tackle these issues, we proposed a Personalized Image\nFilter (PIF). Based on a pretrained text-to-image diffusion model, the\ngenerative prior enables PIF to learn the average appearance of photographic\nconcepts, as well as how to adjust them according to text prompts. PIF then\nlearns the photographic style of reference images with the textual inversion\ntechnique, by optimizing the prompts for the photographic concepts. PIF shows\noutstanding performance in extracting and transferring various kinds of\nphotographic style. Project page: https://pif.pages.dev/",
        "url": "http://arxiv.org/abs/2510.16791v1",
        "published_date": "2025-10-19T11:03:21+00:00",
        "updated_date": "2025-10-19T11:03:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengxuan Zhu",
            "Shuchen Weng",
            "Jiacong Fang",
            "Peixuan Zhang",
            "Si Li",
            "Chao Xu",
            "Boxin Shi"
        ],
        "tldr": "The paper introduces Personalized Image Filter (PIF), a method that leverages a text-to-image diffusion model and textual inversion to learn and transfer photographic styles from reference images based on textual prompts.",
        "tldr_zh": "该论文介绍了 Personalized Image Filter (PIF)，一种利用文本到图像扩散模型和文本反演技术，基于文本提示，从参考图像中学习和迁移摄影风格的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Region in Context: Text-condition Image editing with Human-like semantic reasoning",
        "summary": "Recent research has made significant progress in localizing and editing image\nregions based on text. However, most approaches treat these regions in\nisolation, relying solely on local cues without accounting for how each part\ncontributes to the overall visual and semantic composition. This often results\nin inconsistent edits, unnatural transitions, or loss of coherence across the\nimage. In this work, we propose Region in Context, a novel framework for\ntext-conditioned image editing that performs multilevel semantic alignment\nbetween vision and language, inspired by the human ability to reason about\nedits in relation to the whole scene. Our method encourages each region to\nunderstand its role within the global image context, enabling precise and\nharmonized changes. At its core, the framework introduces a dual-level guidance\nmechanism: regions are represented with full-image context and aligned with\ndetailed region-level descriptions, while the entire image is simultaneously\nmatched to a comprehensive scene-level description generated by a large\nvision-language model. These descriptions serve as explicit verbal references\nof the intended content, guiding both local modifications and global structure.\nExperiments show that it produces more coherent and instruction-aligned\nresults. Code is available at:\nhttps://github.com/thuyvuphuong/Region-in-Context.git",
        "url": "http://arxiv.org/abs/2510.16772v1",
        "published_date": "2025-10-19T09:36:02+00:00",
        "updated_date": "2025-10-19T09:36:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Thuy Phuong Vu",
            "Dinh-Cuong Hoang",
            "Minhhuy Le",
            "Phan Xuan Tan"
        ],
        "tldr": "The paper introduces \"Region in Context,\" a framework for text-conditioned image editing that leverages both region-level and scene-level semantic alignment, yielding more coherent and instruction-aligned results.",
        "tldr_zh": "该论文介绍了一个名为“上下文区域”的框架，用于文本条件下的图像编辑，该框架利用区域级别和场景级别的语义对齐，从而产生更连贯且与指令对齐的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "End-to-end Listen, Look, Speak and Act",
        "summary": "Human interaction is inherently multimodal and full-duplex: we listen while\nwatching, speak while acting, and fluidly adapt to turn-taking and\ninterruptions. Realizing these capabilities is essential for building models\nsimulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),\nwhich, to our knowledge, is the first full-duplex, end-to-end model that\nsimultaneously perceives and generates across vision, text, speech, and action\nwithin a single architecture, enabling interaction patterns previously out of\nreach, yielding more natural, human-like behaviors. At its core is a novel\nSA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each\nmodality to specialized experts and fuses them through a unified attention\nbackbone. This provides a generalizable solution for joint multimodal\nperception and concurrent generation, leveraging strong pre-trained components\nwhile enabling efficient modality integration and mitigating modality\ninterference. On speech-interaction and robot-manipulation benchmarks, ELLSA\nmatches modality-specific baselines, while uniquely supporting advanced\nmultimodal and full-duplex behaviors such as dialogue and action turn-taking,\ndefective instruction rejection, speaking-while-acting, context-grounded visual\nquestion answering, and action barge-ins. We contend that ELLSA represents a\nstep toward more natural and general interactive intelligence, contributing to\nthe broader pursuit of artificial general intelligence. All data, code and\nmodel checkpoints will be released upon acceptance.",
        "url": "http://arxiv.org/abs/2510.16756v1",
        "published_date": "2025-10-19T08:45:46+00:00",
        "updated_date": "2025-10-19T08:45:46+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.RO",
            "eess.AS"
        ],
        "authors": [
            "Siyin Wang",
            "Wenyi Yu",
            "Xianzhao Chen",
            "Xiaohai Tian",
            "Jun Zhang",
            "Lu Lu",
            "Chao Zhang"
        ],
        "tldr": "The paper introduces ELLSA, a novel end-to-end multimodal model that simultaneously perceives and generates across vision, text, speech, and action, enabling more natural human-like interaction.",
        "tldr_zh": "该论文介绍了ELLSA，一种新型端到端多模态模型，能够同时感知和生成视觉、文本、语音和动作，从而实现更自然的人类交互。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling",
        "summary": "While inference-time scaling through search has revolutionized Large Language\nModels, translating these gains to image generation has proven difficult.\nRecent attempts to apply search strategies to continuous diffusion models show\nlimited benefits, with simple random sampling often performing best. We\ndemonstrate that the discrete, sequential nature of visual autoregressive\nmodels enables effective search for image generation. We show that beam search\nsubstantially improves text-to-image generation, enabling a 2B parameter\nautoregressive model to outperform a 12B parameter diffusion model across\nbenchmarks. Systematic ablations show that this advantage comes from the\ndiscrete token space, which allows early pruning and computational reuse, and\nour verifier analysis highlights trade-offs between speed and reasoning\ncapability. These findings suggest that model architecture, not just scale, is\ncritical for inference-time optimization in visual generation.",
        "url": "http://arxiv.org/abs/2510.16751v1",
        "published_date": "2025-10-19T08:28:06+00:00",
        "updated_date": "2025-10-19T08:28:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Erik Riise",
            "Mehmet Onurcan Kaya",
            "Dim P. Papadopoulos"
        ],
        "tldr": "This paper demonstrates that visual autoregressive models, leveraging beam search in the discrete token space, outperform diffusion models in text-to-image generation at inference time, even with fewer parameters. The advantage stems from efficient pruning and computational reuse.",
        "tldr_zh": "本文证明了视觉自回归模型通过在离散token空间中利用波束搜索，在推理时间方面优于扩散模型，即使参数较少。优势来源于高效的剪枝和计算复用。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]