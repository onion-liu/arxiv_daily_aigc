[
    {
        "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models",
        "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.",
        "url": "http://arxiv.org/abs/2510.02300v1",
        "published_date": "2025-10-02T17:59:06+00:00",
        "updated_date": "2025-10-02T17:59:06+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Runqian Wang",
            "Yilun Du"
        ],
        "tldr": "This paper introduces Equilibrium Matching (EqM), a generative modeling framework using an implicit energy landscape for optimization-based sampling, achieving state-of-the-art image generation performance and offering a unified approach to various image tasks.",
        "tldr_zh": "这篇论文介绍了平衡匹配（EqM），一种使用隐性能量景观进行基于优化的采样的生成建模框架，实现了最先进的图像生成性能，并为各种图像任务提供了一种统一的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Pack and Force Your Memory: Long-form and Consistent Video Generation",
        "summary": "Long-form video generation presents a dual challenge: models must capture\nlong-range dependencies while preventing the error accumulation inherent in\nautoregressive decoding. To address these challenges, we make two\ncontributions. First, for dynamic context modeling, we propose MemoryPack, a\nlearnable context-retrieval mechanism that leverages both textual and image\ninformation as global guidance to jointly model short- and long-term\ndependencies, achieving minute-level temporal consistency. This design scales\ngracefully with video length, preserves computational efficiency, and maintains\nlinear complexity. Second, to mitigate error accumulation, we introduce Direct\nForcing, an efficient single-step approximating strategy that improves\ntraining-inference alignment and thereby curtails error propagation during\ninference. Together, MemoryPack and Direct Forcing substantially enhance the\ncontext consistency and reliability of long-form video generation, advancing\nthe practical usability of autoregressive video models.",
        "url": "http://arxiv.org/abs/2510.01784v1",
        "published_date": "2025-10-02T08:22:46+00:00",
        "updated_date": "2025-10-02T08:22:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiaofei Wu",
            "Guozhen Zhang",
            "Zhiyong Xu",
            "Yuan Zhou",
            "Qinglin Lu",
            "Xuming He"
        ],
        "tldr": "This paper introduces MemoryPack and Direct Forcing to improve long-form video generation by enhancing temporal consistency and mitigating error accumulation, respectively.",
        "tldr_zh": "该论文介绍了MemoryPack和Direct Forcing，分别用于提高长视频生成的时间一致性和减少误差累积，从而改善长视频生成质量。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation",
        "summary": "State-of-the-art text-to-image models excel at realism but collapse on\nmulti-human prompts - duplicating faces, merging identities, and miscounting\nindividuals. We introduce DisCo (Reinforcement with Diversity Constraints), the\nfirst RL-based framework to directly optimize identity diversity in multi-human\ngeneration. DisCo fine-tunes flow-matching models via Group-Relative Policy\nOptimization (GRPO) with a compositional reward that (i) penalizes intra-image\nfacial similarity, (ii) discourages cross-sample identity repetition, (iii)\nenforces accurate person counts, and (iv) preserves visual fidelity through\nhuman preference scores. A single-stage curriculum stabilizes training as\ncomplexity scales, requiring no extra annotations. On the DiverseHumans\nTestset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global\nIdentity Spread - surpassing both open-source and proprietary methods (e.g.,\nGemini, GPT-Image) while maintaining competitive perceptual quality. Our\nresults establish DisCo as a scalable, annotation-free solution that resolves\nthe long-standing identity crisis in generative models and sets a new benchmark\nfor compositional multi-human generation.",
        "url": "http://arxiv.org/abs/2510.01399v1",
        "published_date": "2025-10-01T19:28:51+00:00",
        "updated_date": "2025-10-01T19:28:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shubhankar Borse",
            "Farzad Farhadzadeh",
            "Munawar Hayat",
            "Fatih Porikli"
        ],
        "tldr": "The paper introduces DisCo, a reinforcement learning framework with diversity constraints, to improve identity diversity in multi-human image generation, outperforming existing methods without requiring extra annotations.",
        "tldr_zh": "该论文介绍了DisCo，一个具有多样性约束的强化学习框架，旨在提高多人图像生成中的身份多样性，无需额外注释即可胜过现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation",
        "summary": "Text-to-image diffusion models trained on a fixed set of resolutions often\nfail to generalize, even when asked to generate images at lower resolutions\nthan those seen during training. High-resolution text-to-image generators are\ncurrently unable to easily offer an out-of-the-box budget-efficient alternative\nto their users who might not need high-resolution images. We identify a key\ntechnical insight in diffusion models that when addressed can help tackle this\nlimitation: Noise schedulers have unequal perceptual effects across\nresolutions. The same level of noise removes disproportionately more signal\nfrom lower-resolution images than from high-resolution images, leading to a\ntrain-test mismatch. We propose NoiseShift, a training-free method that\nrecalibrates the noise level of the denoiser conditioned on resolution size.\nNoiseShift requires no changes to model architecture or sampling schedule and\nis compatible with existing models. When applied to Stable Diffusion 3, Stable\nDiffusion 3.5, and Flux-Dev, quality at low resolutions is significantly\nimproved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and\nFlux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by\n10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results\ndemonstrate the effectiveness of NoiseShift in mitigating resolution-dependent\nartifacts and enhancing the quality of low-resolution image generation.",
        "url": "http://arxiv.org/abs/2510.02307v1",
        "published_date": "2025-10-02T17:59:43+00:00",
        "updated_date": "2025-10-02T17:59:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ruozhen He",
            "Moayed Haji-Ali",
            "Ziyan Yang",
            "Vicente Ordonez"
        ],
        "tldr": "The paper presents NoiseShift, a training-free method to recalibrate noise levels in diffusion models conditioned on resolution, improving low-resolution image generation quality without model changes, and demonstrating significant FID improvements on Stable Diffusion models and Flux-Dev.",
        "tldr_zh": "该论文提出了 NoiseShift，一种无需训练的方法，可根据分辨率重新校准扩散模型中的噪声水平，从而提高低分辨率图像的生成质量，无需更改模型，并展示了 Stable Diffusion 模型和 Flux-Dev 在 FID 方面的显着改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Continual Personalization for Diffusion Models",
        "summary": "Updating diffusion models in an incremental setting would be practical in\nreal-world applications yet computationally challenging. We present a novel\nlearning strategy of Concept Neuron Selection (CNS), a simple yet effective\napproach to perform personalization in a continual learning scheme. CNS\nuniquely identifies neurons in diffusion models that are closely related to the\ntarget concepts. In order to mitigate catastrophic forgetting problems while\npreserving zero-shot text-to-image generation ability, CNS finetunes concept\nneurons in an incremental manner and jointly preserves knowledge learned of\nprevious concepts. Evaluation of real-world datasets demonstrates that CNS\nachieves state-of-the-art performance with minimal parameter adjustments,\noutperforming previous methods in both single and multi-concept personalization\nworks. CNS also achieves fusion-free operation, reducing memory storage and\nprocessing time for continual personalization.",
        "url": "http://arxiv.org/abs/2510.02296v1",
        "published_date": "2025-10-02T17:58:56+00:00",
        "updated_date": "2025-10-02T17:58:56+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Yu-Chien Liao",
            "Jr-Jen Chen",
            "Chi-Pin Huang",
            "Ci-Siang Lin",
            "Meng-Lin Wu",
            "Yu-Chiang Frank Wang"
        ],
        "tldr": "The paper introduces Concept Neuron Selection (CNS), a novel continual learning approach for personalizing diffusion models by selectively fine-tuning concept-related neurons, mitigating catastrophic forgetting and preserving zero-shot generation capabilities with minimal parameter adjustments.",
        "tldr_zh": "该论文介绍了一种名为概念神经元选择 (CNS) 的新型持续学习方法，通过选择性地微调与概念相关的神经元来个性化扩散模型，从而减少灾难性遗忘并保留零样本生成能力，且只需最少的参数调整。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Test-Time Anchoring for Discrete Diffusion Posterior Sampling",
        "summary": "We study the problem of posterior sampling using pretrained discrete\ndiffusion foundation models, aiming to recover images from noisy measurements\nwithout retraining task-specific models. While diffusion models have achieved\nremarkable success in generative modeling, most advances rely on continuous\nGaussian diffusion. In contrast, discrete diffusion offers a unified framework\nfor jointly modeling categorical data such as text and images. Beyond\nunification, discrete diffusion provides faster inference, finer control, and\nprincipled training-free Bayesian inference, making it particularly well-suited\nfor posterior sampling. However, existing approaches to discrete diffusion\nposterior sampling face severe challenges: derivative-free guidance yields\nsparse signals, continuous relaxations limit applicability, and split Gibbs\nsamplers suffer from the curse of dimensionality. To overcome these\nlimitations, we introduce Anchored Posterior Sampling (APS) for masked\ndiffusion foundation models, built on two key innovations -- quantized\nexpectation for gradient-like guidance in discrete embedding space, and\nanchored remasking for adaptive decoding. Our approach achieves\nstate-of-the-art performance among discrete diffusion samplers across linear\nand nonlinear inverse problems on the standard benchmarks. We further\ndemonstrate the benefits of our approach in training-free stylization and\ntext-guided editing.",
        "url": "http://arxiv.org/abs/2510.02291v1",
        "published_date": "2025-10-02T17:58:37+00:00",
        "updated_date": "2025-10-02T17:58:37+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Litu Rout",
            "Andreas Lugmayr",
            "Yasamin Jafarian",
            "Srivatsan Varadharajan",
            "Constantine Caramanis",
            "Sanjay Shakkottai",
            "Ira Kemelmacher-Shlizerman"
        ],
        "tldr": "The paper introduces Anchored Posterior Sampling (APS) for discrete diffusion models, a novel method for posterior sampling that addresses the limitations of existing techniques and achieves state-of-the-art performance in inverse problems and editing tasks.",
        "tldr_zh": "该论文介绍了锚定后验采样 (APS)，这是一种用于离散扩散模型的新颖后验采样方法，解决了现有技术的局限性，并在反问题和编辑任务中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MultiModal Action Conditioned Video Generation",
        "summary": "Current video models fail as world model as they lack fine-graiend control.\nGeneral-purpose household robots require real-time fine motor control to handle\ndelicate tasks and urgent situations. In this work, we introduce fine-grained\nmultimodal actions to capture such precise control. We consider senses of\nproprioception, kinesthesia, force haptics, and muscle activation. Such\nmultimodal senses naturally enables fine-grained interactions that are\ndifficult to simulate with text-conditioned generative models. To effectively\nsimulate fine-grained multisensory actions, we develop a feature learning\nparadigm that aligns these modalities while preserving the unique information\neach modality provides. We further propose a regularization scheme to enhance\ncausality of the action trajectory features in representing intricate\ninteraction dynamics. Experiments show that incorporating multimodal senses\nimproves simulation accuracy and reduces temporal drift. Extensive ablation\nstudies and downstream applications demonstrate the effectiveness and\npracticality of our work.",
        "url": "http://arxiv.org/abs/2510.02287v1",
        "published_date": "2025-10-02T17:57:06+00:00",
        "updated_date": "2025-10-02T17:57:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yichen Li",
            "Antonio Torralba"
        ],
        "tldr": "The paper introduces a multimodal action-conditioned video generation model that incorporates proprioception, kinesthesia, force haptics, and muscle activation to achieve fine-grained control in simulated environments, demonstrating improved simulation accuracy and reduced temporal drift.",
        "tldr_zh": "该论文介绍了一种多模态动作条件视频生成模型，该模型结合了本体感受、动觉、力触觉和肌肉激活，以在模拟环境中实现细粒度控制，从而提高了模拟精度并减少了时间漂移。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning to Generate Object Interactions with Physics-Guided Video Diffusion",
        "summary": "Recent models for video generation have achieved remarkable progress and are\nnow deployed in film, social media production, and advertising. Beyond their\ncreative potential, such models also hold promise as world simulators for\nrobotics and embodied decision making. Despite strong advances, however,\ncurrent approaches still struggle to generate physically plausible object\ninteractions and lack physics-grounded control mechanisms. To address this\nlimitation, we introduce KineMask, an approach for physics-guided video\ngeneration that enables realistic rigid body control, interactions, and\neffects. Given a single image and a specified object velocity, our method\ngenerates videos with inferred motions and future object interactions. We\npropose a two-stage training strategy that gradually removes future motion\nsupervision via object masks. Using this strategy we train video diffusion\nmodels (VDMs) on synthetic scenes of simple interactions and demonstrate\nsignificant improvements of object interactions in real scenes. Furthermore,\nKineMask integrates low-level motion control with high-level textual\nconditioning via predictive scene descriptions, leading to effective support\nfor synthesis of complex dynamical phenomena. Extensive experiments show that\nKineMask achieves strong improvements over recent models of comparable size.\nAblation studies further highlight the complementary roles of low- and\nhigh-level conditioning in VDMs. Our code, model, and data will be made\npublicly available.",
        "url": "http://arxiv.org/abs/2510.02284v1",
        "published_date": "2025-10-02T17:56:46+00:00",
        "updated_date": "2025-10-02T17:56:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "David Romero",
            "Ariana Bermudez",
            "Hao Li",
            "Fabio Pizzati",
            "Ivan Laptev"
        ],
        "tldr": "The paper introduces KineMask, a physics-guided video diffusion approach for generating realistic object interactions with rigid body control, demonstrating improved performance on synthetic and real-world scenes.",
        "tldr_zh": "该论文介绍了KineMask，一种基于物理引导的视频扩散方法，用于生成具有刚体控制的逼真物体交互，并在合成和真实场景中表现出改进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
        "summary": "Diffusion models have revolutionized image and video generation, achieving\nunprecedented visual quality. However, their reliance on transformer\narchitectures incurs prohibitively high computational costs, particularly when\nextending generation to long videos. Recent work has explored autoregressive\nformulations for long video generation, typically by distilling from\nshort-horizon bidirectional teachers. Nevertheless, given that teacher models\ncannot synthesize long videos, the extrapolation of student models beyond their\ntraining horizon often leads to pronounced quality degradation, arising from\nthe compounding of errors within the continuous latent space. In this paper, we\npropose a simple yet effective approach to mitigate quality degradation in\nlong-horizon video generation without requiring supervision from long-video\nteachers or retraining on long video datasets. Our approach centers on\nexploiting the rich knowledge of teacher models to provide guidance for the\nstudent model through sampled segments drawn from self-generated long videos.\nOur method maintains temporal consistency while scaling video length by up to\n20x beyond teacher's capability, avoiding common issues such as over-exposure\nand error-accumulation without recomputing overlapping frames like previous\nmethods. When scaling up the computation, our method shows the capability of\ngenerating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the\nmaximum span supported by our base model's position embedding and more than 50x\nlonger than that of our baseline model. Experiments on standard benchmarks and\nour proposed improved benchmark demonstrate that our approach substantially\noutperforms baseline methods in both fidelity and consistency. Our long-horizon\nvideos demo can be found at https://self-forcing-plus-plus.github.io/",
        "url": "http://arxiv.org/abs/2510.02283v1",
        "published_date": "2025-10-02T17:55:42+00:00",
        "updated_date": "2025-10-02T17:55:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Justin Cui",
            "Jie Wu",
            "Ming Li",
            "Tao Yang",
            "Xiaojie Li",
            "Rui Wang",
            "Andrew Bai",
            "Yuanhao Ban",
            "Cho-Jui Hsieh"
        ],
        "tldr": "This paper introduces Self-Forcing++, a method to improve the quality and temporal consistency of long-horizon video generation using diffusion models, achieving up to 4-minute videos without long-video teacher supervision or retraining.",
        "tldr_zh": "本文介绍了 Self-Forcing++，一种用于改进长时程视频生成的质量和时间一致性的方法，它使用扩散模型，无需长视频教师监督或重新训练即可生成长达 4 分钟的视频。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing",
        "summary": "Drag-based image editing has long suffered from distortions in the target\nregion, largely because the priors of earlier base models, Stable Diffusion,\nare insufficient to project optimized latents back onto the natural image\nmanifold. With the shift from UNet-based DDPMs to more scalable DiT with flow\nmatching (e.g., SD3.5, FLUX), generative priors have become significantly\nstronger, enabling advances across diverse editing tasks. However, drag-based\nediting has yet to benefit from these stronger priors. This work proposes the\nfirst framework to effectively harness FLUX's rich prior for drag-based\nediting, dubbed DragFlow, achieving substantial gains over baselines. We first\nshow that directly applying point-based drag editing to DiTs performs poorly:\nunlike the highly compressed features of UNets, DiT features are insufficiently\nstructured to provide reliable guidance for point-wise motion supervision. To\novercome this limitation, DragFlow introduces a region-based editing paradigm,\nwhere affine transformations enable richer and more consistent feature\nsupervision. Additionally, we integrate pretrained open-domain personalization\nadapters (e.g., IP-Adapter) to enhance subject consistency, while preserving\nbackground fidelity through gradient mask-based hard constraints. Multimodal\nlarge language models (MLLMs) are further employed to resolve task ambiguities.\nFor evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)\nfeaturing region-level dragging instructions. Extensive experiments on\nDragBench-DR and ReD Bench show that DragFlow surpasses both point-based and\nregion-based baselines, setting a new state-of-the-art in drag-based image\nediting. Code and datasets will be publicly available upon publication.",
        "url": "http://arxiv.org/abs/2510.02253v1",
        "published_date": "2025-10-02T17:39:13+00:00",
        "updated_date": "2025-10-02T17:39:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Zihan Zhou",
            "Shilin Lu",
            "Shuli Leng",
            "Shaocong Zhang",
            "Zhuming Lian",
            "Xinlei Yu",
            "Adams Wai-Kin Kong"
        ],
        "tldr": "DragFlow is a new drag-based image editing framework leveraging the strong priors of DiT models like FLUX, using region-based supervision and integration of personalization adapters to achieve state-of-the-art results.",
        "tldr_zh": "DragFlow是一个新的基于拖拽的图像编辑框架，它利用了DiT模型（如FLUX）的强大先验知识，通过基于区域的监督和个性化适配器的集成，实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FreeViS: Training-free Video Stylization with Inconsistent References",
        "summary": "Video stylization plays a key role in content creation, but it remains a\nchallenging problem. Na\\\"ively applying image stylization frame-by-frame hurts\ntemporal consistency and reduces style richness. Alternatively, training a\ndedicated video stylization model typically requires paired video data and is\ncomputationally expensive. In this paper, we propose FreeViS, a training-free\nvideo stylization framework that generates stylized videos with rich style\ndetails and strong temporal coherence. Our method integrates multiple stylized\nreferences to a pretrained image-to-video (I2V) model, effectively mitigating\nthe propagation errors observed in prior works, without introducing flickers\nand stutters. In addition, it leverages high-frequency compensation to\nconstrain the content layout and motion, together with flow-based motion cues\nto preserve style textures in low-saliency regions. Through extensive\nevaluations, FreeViS delivers higher stylization fidelity and superior temporal\nconsistency, outperforming recent baselines and achieving strong human\npreference. Our training-free pipeline offers a practical and economic solution\nfor high-quality, temporally coherent video stylization. The code and videos\ncan be accessed via https://xujiacong.github.io/FreeViS/",
        "url": "http://arxiv.org/abs/2510.01686v1",
        "published_date": "2025-10-02T05:27:06+00:00",
        "updated_date": "2025-10-02T05:27:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiacong Xu",
            "Yiqun Mei",
            "Ke Zhang",
            "Vishal M. Patel"
        ],
        "tldr": "The paper introduces FreeViS, a training-free video stylization framework that leverages multiple stylized references and a pretrained image-to-video model to achieve temporally coherent and stylistically rich video stylization.",
        "tldr_zh": "该论文介绍了一种名为FreeViS的免训练视频风格化框架，该框架利用多个风格化参考和预训练的图像到视频模型，以实现时间连贯且风格丰富的视频风格化。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Growing Visual Generative Capacity for Pre-Trained MLLMs",
        "summary": "Multimodal large language models (MLLMs) extend the success of language\nmodels to visual understanding, and recent efforts have sought to build unified\nMLLMs that support both understanding and generation. However, constructing\nsuch models remains challenging: hybrid approaches combine continuous\nembeddings with diffusion or flow-based objectives, producing high-quality\nimages but breaking the autoregressive paradigm, while pure autoregressive\napproaches unify text and image prediction over discrete visual tokens but\noften face trade-offs between semantic alignment and pixel-level fidelity. In\nthis work, we present Bridge, a pure autoregressive unified MLLM that augments\npre-trained visual understanding models with generative ability through a\nMixture-of-Transformers architecture, enabling both image understanding and\ngeneration within a single next-token prediction framework. To further improve\nvisual generation fidelity, we propose a semantic-to-pixel discrete\nrepresentation that integrates compact semantic tokens with fine-grained pixel\ntokens, achieving strong language alignment and precise description of visual\ndetails with only a 7.9% increase in sequence length. Extensive experiments\nacross diverse multimodal benchmarks demonstrate that Bridge achieves\ncompetitive or superior results in both understanding and generation\nbenchmarks, while requiring less training data and reduced training time\ncompared to prior unified MLLMs.",
        "url": "http://arxiv.org/abs/2510.01546v1",
        "published_date": "2025-10-02T00:40:02+00:00",
        "updated_date": "2025-10-02T00:40:02+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Hanyu Wang",
            "Jiaming Han",
            "Ziyan Yang",
            "Qi Zhao",
            "Shanchuan Lin",
            "Xiangyu Yue",
            "Abhinav Shrivastava",
            "Zhenheng Yang",
            "Hao Chen"
        ],
        "tldr": "The paper introduces Bridge, a unified MLLM using a Mixture-of-Transformers architecture for both image understanding and generation, achieving competitive performance with less training data.",
        "tldr_zh": "该论文介绍了Bridge，一种统一的MLLM，使用混合Transformer架构进行图像理解和生成，并在更少训练数据的情况下实现了有竞争力的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Better Optimization For Listwise Preference in Diffusion Models",
        "summary": "Reinforcement learning from human feedback (RLHF) has proven effectiveness\nfor aligning text-to-image (T2I) diffusion models with human preferences.\nAlthough Direct Preference Optimization (DPO) is widely adopted for its\ncomputational efficiency and avoidance of explicit reward modeling, its\napplications to diffusion models have primarily relied on pairwise preferences.\nThe precise optimization of listwise preferences remains largely unaddressed.\nIn practice, human feedback on image preferences often contains implicit ranked\ninformation, which conveys more precise human preferences than pairwise\ncomparisons. In this work, we propose Diffusion-LPO, a simple and effective\nframework for Listwise Preference Optimization in diffusion models with\nlistwise data. Given a caption, we aggregate user feedback into a ranked list\nof images and derive a listwise extension of the DPO objective under the\nPlackett-Luce model. Diffusion-LPO enforces consistency across the entire\nranking by encouraging each sample to be preferred over all of its lower-ranked\nalternatives. We empirically demonstrate the effectiveness of Diffusion-LPO\nacross various tasks, including text-to-image generation, image editing, and\npersonalized preference alignment. Diffusion-LPO consistently outperforms\npairwise DPO baselines on visual quality and preference alignment.",
        "url": "http://arxiv.org/abs/2510.01540v1",
        "published_date": "2025-10-02T00:26:37+00:00",
        "updated_date": "2025-10-02T00:26:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiamu Bai",
            "Xin Yu",
            "Meilong Xu",
            "Weitao Lu",
            "Xin Pan",
            "Kiwan Maeng",
            "Daniel Kifer",
            "Jian Wang",
            "Yu Wang"
        ],
        "tldr": "The paper introduces Diffusion-LPO, a method for optimizing diffusion models using listwise preferences derived from ranked image feedback, outperforming pairwise DPO baselines in text-to-image generation, image editing and personalized preference alignment.",
        "tldr_zh": "该论文介绍了Diffusion-LPO，一种使用来自排序图像反馈的列表式偏好优化扩散模型的方法，在文本到图像生成、图像编辑和个性化偏好对齐方面优于成对DPO基线。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Purrception: Variational Flow Matching for Vector-Quantized Image Generation",
        "summary": "We introduce Purrception, a variational flow matching approach for\nvector-quantized image generation that provides explicit categorical\nsupervision while maintaining continuous transport dynamics. Our method adapts\nVariational Flow Matching to vector-quantized latents by learning categorical\nposteriors over codebook indices while computing velocity fields in the\ncontinuous embedding space. This combines the geometric awareness of continuous\nmethods with the discrete supervision of categorical approaches, enabling\nuncertainty quantification over plausible codes and temperature-controlled\ngeneration. We evaluate Purrception on ImageNet-1k 256x256 generation. Training\nconverges faster than both continuous flow matching and discrete flow matching\nbaselines while achieving competitive FID scores with state-of-the-art models.\nThis demonstrates that Variational Flow Matching can effectively bridge\ncontinuous transport and discrete supervision for improved training efficiency\nin image generation.",
        "url": "http://arxiv.org/abs/2510.01478v1",
        "published_date": "2025-10-01T21:41:30+00:00",
        "updated_date": "2025-10-01T21:41:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Răzvan-Andrei Matişan",
            "Vincent Tao Hu",
            "Grigory Bartosh",
            "Björn Ommer",
            "Cees G. M. Snoek",
            "Max Welling",
            "Jan-Willem van de Meent",
            "Mohammad Mahdi Derakhshani",
            "Floor Eijkelboom"
        ],
        "tldr": "Purrception combines variational flow matching with vector-quantized image generation, achieving faster convergence and competitive FID scores on ImageNet-1k. It bridges continuous transport and discrete supervision for efficient image generation.",
        "tldr_zh": "Purrception将变分流匹配与向量量化图像生成相结合，在ImageNet-1k上实现了更快的收敛速度和具有竞争力的FID分数。它弥合了连续传输和离散监督之间的差距，从而提高了图像生成的效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Image Generation Based on Image Style Extraction",
        "summary": "Image generation based on text-to-image generation models is a task with\npractical application scenarios that fine-grained styles cannot be precisely\ndescribed and controlled in natural language, while the guidance information of\nstylized reference images is difficult to be directly aligned with the textual\nconditions of traditional textual guidance generation. This study focuses on\nhow to maximize the generative capability of the pretrained generative model,\nby obtaining fine-grained stylistic representations from a single given\nstylistic reference image, and injecting the stylistic representations into the\ngenerative body without changing the structural framework of the downstream\ngenerative model, so as to achieve fine-grained controlled stylized image\ngeneration. In this study, we propose a three-stage training style\nextraction-based image generation method, which uses a style encoder and a\nstyle projection layer to align the style representations with the textual\nrepresentations to realize fine-grained textual cue-based style guide\ngeneration. In addition, this study constructs the Style30k-captions dataset,\nwhose samples contain a triad of images, style labels, and text descriptions,\nto train the style encoder and style projection layer in this experiment.",
        "url": "http://arxiv.org/abs/2510.01347v1",
        "published_date": "2025-10-01T18:23:09+00:00",
        "updated_date": "2025-10-01T18:23:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuochen Chang"
        ],
        "tldr": "This paper proposes a three-stage training method for fine-grained stylized image generation using a style encoder and projection layer, trained on a newly constructed dataset (Style30k-captions) to align style representations with textual cues.",
        "tldr_zh": "本文提出了一种三阶段训练方法，用于生成精细风格化图像。该方法使用风格编码器和投影层，并在新建的数据集（Style30k-captions）上进行训练，以将风格表示与文本线索对齐。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing",
        "summary": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges\nwith view, temporal, and non-editing region consistency, as well as with\nhandling complex text instructions. To address these issues, we propose\n4DGS-Craft, a consistent and interactive 4DGS editing framework. We first\nintroduce a 4D-aware InstructPix2Pix model to ensure both view and temporal\nconsistency. This model incorporates 4D VGGT geometry features extracted from\nthe initial scene, enabling it to capture underlying 4D geometric structures\nduring editing. We further enhance this model with a multi-view grid module\nthat enforces consistency by iteratively refining multi-view input images while\njointly optimizing the underlying 4D scene. Furthermore, we preserve the\nconsistency of non-edited regions through a novel Gaussian selection mechanism,\nwhich identifies and optimizes only the Gaussians within the edited regions.\nBeyond consistency, facilitating user interaction is also crucial for effective\n4DGS editing. Therefore, we design an LLM-based module for user intent\nunderstanding. This module employs a user instruction template to define atomic\nediting operations and leverages an LLM for reasoning. As a result, our\nframework can interpret user intent and decompose complex instructions into a\nlogical sequence of atomic operations, enabling it to handle intricate user\ncommands and further enhance editing performance. Compared to related works,\nour approach enables more consistent and controllable 4D scene editing. Our\ncode will be made available upon acceptance.",
        "url": "http://arxiv.org/abs/2510.01991v1",
        "published_date": "2025-10-02T13:13:19+00:00",
        "updated_date": "2025-10-02T13:13:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lei Liu",
            "Can Wang",
            "Zhenghao Chen",
            "Dong Xu"
        ],
        "tldr": "This paper introduces 4DGS-Craft, a framework for consistent and interactive editing of 4D Gaussian Splatting scenes using a 4D-aware InstructPix2Pix model, a multi-view grid module, and an LLM-based user intent understanding module.",
        "tldr_zh": "本文介绍了 4DGS-Craft，一个用于一致且可交互式编辑 4D 高斯溅射场景的框架，它使用了一个 4D 感知的 InstructPix2Pix 模型、一个多视图网格模块和一个基于 LLM 的用户意图理解模块。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "$\\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models",
        "summary": "The integration of online reinforcement learning (RL) into diffusion and flow\nmodels has recently emerged as a promising approach for aligning generative\nmodels with human preferences. Stochastic sampling via Stochastic Differential\nEquations (SDE) is employed during the denoising process to generate diverse\ndenoising directions for RL exploration. While existing methods effectively\nexplore potential high-value samples, they suffer from sub-optimal preference\nalignment due to sparse and narrow reward signals. To address these challenges,\nwe propose a novel Granular-GRPO ($\\text{G}^2$RPO ) framework that achieves\nprecise and comprehensive reward assessments of sampling directions in\nreinforcement learning of flow models. Specifically, a Singular Stochastic\nSampling strategy is introduced to support step-wise stochastic exploration\nwhile enforcing a high correlation between the reward and the injected noise,\nthereby facilitating a faithful reward for each SDE perturbation. Concurrently,\nto eliminate the bias inherent in fixed-granularity denoising, we introduce a\nMulti-Granularity Advantage Integration module that aggregates advantages\ncomputed at multiple diffusion scales, producing a more comprehensive and\nrobust evaluation of the sampling directions. Experiments conducted on various\nreward models, including both in-domain and out-of-domain evaluations,\ndemonstrate that our $\\text{G}^2$RPO significantly outperforms existing\nflow-based GRPO baselines,highlighting its effectiveness and robustness.",
        "url": "http://arxiv.org/abs/2510.01982v1",
        "published_date": "2025-10-02T12:57:12+00:00",
        "updated_date": "2025-10-02T12:57:12+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Yujie Zhou",
            "Pengyang Ling",
            "Jiazi Bu",
            "Yibin Wang",
            "Yuhang Zang",
            "Jiaqi Wang",
            "Li Niu",
            "Guangtao Zhai"
        ],
        "tldr": "The paper introduces $\\text{G}^2$RPO, a novel framework that improves reward assessment in reinforcement learning for flow models by using a Singular Stochastic Sampling strategy and a Multi-Granularity Advantage Integration module. Experiments show it outperforms existing methods.",
        "tldr_zh": "该论文介绍了$\\text{G}^2$RPO，一种新颖的框架，通过使用单数随机抽样策略和多粒度优势集成模块，改进了流模型强化学习中的奖励评估。实验表明，该方法优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks",
        "summary": "Latent representations are critical for the performance and robustness of\nmachine learning models, as they encode the essential features of data in a\ncompact and informative manner. However, in vision tasks, these representations\nare often affected by noisy or irrelevant features, which can degrade the\nmodel's performance and generalization capabilities. This paper presents a\nnovel approach for enhancing latent representations using unsupervised Dynamic\nFeature Selection (DFS). For each instance, the proposed method identifies and\nremoves misleading or redundant information in images, ensuring that only the\nmost relevant features contribute to the latent space. By leveraging an\nunsupervised framework, our approach avoids reliance on labeled data, making it\nbroadly applicable across various domains and datasets. Experiments conducted\non image datasets demonstrate that models equipped with unsupervised DFS\nachieve significant improvements in generalization performance across various\ntasks, including clustering and image generation, while incurring a minimal\nincrease in the computational cost.",
        "url": "http://arxiv.org/abs/2510.01758v1",
        "published_date": "2025-10-02T07:46:59+00:00",
        "updated_date": "2025-10-02T07:46:59+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Bruno Corcuera",
            "Carlos Eiras-Franco",
            "Brais Cancela"
        ],
        "tldr": "This paper introduces an unsupervised Dynamic Feature Selection (DFS) method to improve the robustness and generalization of latent representations in vision tasks by removing noisy or irrelevant features. Experiments show improvements in clustering and image generation.",
        "tldr_zh": "本文提出了一种无监督的动态特征选择（DFS）方法，通过去除噪声或不相关的特征，来提高视觉任务中潜在表示的鲁棒性和泛化能力。实验表明，该方法在聚类和图像生成方面有所改进。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]