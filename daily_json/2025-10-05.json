[
    {
        "title": "Diverse Text-to-Image Generation via Contrastive Noise Optimization",
        "summary": "Text-to-image (T2I) diffusion models have demonstrated impressive performance\nin generating high-fidelity images, largely enabled by text-guided inference.\nHowever, this advantage often comes with a critical drawback: limited\ndiversity, as outputs tend to collapse into similar modes under strong text\nguidance. Existing approaches typically optimize intermediate latents or text\nconditions during inference, but these methods deliver only modest gains or\nremain sensitive to hyperparameter tuning. In this work, we introduce\nContrastive Noise Optimization, a simple yet effective method that addresses\nthe diversity issue from a distinct perspective. Unlike prior techniques that\nadapt intermediate latents, our approach shapes the initial noise to promote\ndiverse outputs. Specifically, we develop a contrastive loss defined in the\nTweedie data space and optimize a batch of noise latents. Our contrastive\noptimization repels instances within the batch to maximize diversity while\nkeeping them anchored to a reference sample to preserve fidelity. We further\nprovide theoretical insights into the mechanism of this preprocessing to\nsubstantiate its effectiveness. Extensive experiments across multiple T2I\nbackbones demonstrate that our approach achieves a superior quality-diversity\nPareto frontier while remaining robust to hyperparameter choices.",
        "url": "http://arxiv.org/abs/2510.03813v1",
        "published_date": "2025-10-04T13:51:32+00:00",
        "updated_date": "2025-10-04T13:51:32+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Byungjun Kim",
            "Soobin Um",
            "Jong Chul Ye"
        ],
        "tldr": "This paper introduces Contrastive Noise Optimization, a method to improve the diversity of text-to-image generation by optimizing the initial noise with a contrastive loss, achieving improved quality-diversity trade-off and robustness to hyperparameter tuning.",
        "tldr_zh": "本文介绍了一种对比噪声优化方法，通过使用对比损失优化初始噪声来提高文本到图像生成的质量，从而实现更好的质量-多样性权衡，并且对超参数调整具有鲁棒性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bridging the Gap Between Multimodal Foundation Models and World Models",
        "summary": "Humans understand the world through the integration of multiple sensory\nmodalities, enabling them to perceive, reason about, and imagine dynamic\nphysical processes. Inspired by this capability, multimodal foundation models\n(MFMs) have emerged as powerful tools for multimodal understanding and\ngeneration. However, today's MFMs fall short of serving as effective world\nmodels. They lack the essential ability such as perform counterfactual\nreasoning, simulate dynamics, understand the spatiotemporal information,\ncontrol generated visual outcomes, and perform multifaceted reasoning. We\ninvestigates what it takes to bridge the gap between multimodal foundation\nmodels and world models. We begin by improving the reasoning capabilities of\nMFMs through discriminative tasks and equipping MFMs with structured reasoning\nskills, such as causal inference, counterfactual thinking, and spatiotemporal\nreasoning, enabling them to go beyond surface correlations and understand\ndeeper relationships within visual and textual data. Next, we explore\ngenerative capabilities of multimodal foundation models across both image and\nvideo modalities, introducing new frameworks for structured and controllable\ngeneration. Our approaches incorporate scene graphs, multimodal conditioning,\nand multimodal alignment strategies to guide the generation process, ensuring\nconsistency with high-level semantics and fine-grained user intent. We further\nextend these techniques to controllable 4D generation, enabling interactive,\neditable, and morphable object synthesis over time and space.",
        "url": "http://arxiv.org/abs/2510.03727v1",
        "published_date": "2025-10-04T08:14:20+00:00",
        "updated_date": "2025-10-04T08:14:20+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Xuehai He"
        ],
        "tldr": "This paper explores improving multimodal foundation models (MFMs) to bridge the gap towards world models by enhancing reasoning capabilities, structured generation, and controllable 4D generation.",
        "tldr_zh": "本文旨在探究如何增强多模态基础模型（MFM），以弥合其与世界模型之间的差距，主要通过提升推理能力、结构化生成和可控的4D生成来实现。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!",
        "summary": "Achieving streaming, fine-grained control over the outputs of autoregressive\nvideo diffusion models remains challenging, making it difficult to ensure that\nthey consistently align with user expectations. To bridge this gap, we propose\n\\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new\ntask that enables users to modify generated videos \\emph{anytime} on\n\\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and\nSG-I2V, REVEL unifies drag-style video manipulation as editing and animating\nvideo frames with both supporting user-specified translation, deformation, and\nrotation effects, making drag operations versatile. In resolving REVEL, we\nobserve: \\emph{i}) drag-induced perturbations accumulate in latent space,\ncausing severe latent distribution drift that halts the drag process;\n\\emph{ii}) streaming drag is easily disturbed by context frames, thereby\nyielding visually unnatural outcomes. We thus propose a training-free approach,\n\\textbf{DragStream}, comprising: \\emph{i}) an adaptive distribution\nself-rectification strategy that leverages neighboring frames' statistics to\neffectively constrain the drift of latent embeddings; \\emph{ii}) a\nspatial-frequency selective optimization mechanism, allowing the model to fully\nexploit contextual information while mitigating its interference via\nselectively propagating visual cues along generation. Our method can be\nseamlessly integrated into existing autoregressive video diffusion models, and\nextensive experiments firmly demonstrate the effectiveness of our DragStream.",
        "url": "http://arxiv.org/abs/2510.03550v1",
        "published_date": "2025-10-03T22:38:35+00:00",
        "updated_date": "2025-10-03T22:38:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junbao Zhou",
            "Yuan Zhou",
            "Kesen Zhao",
            "Qingshan Xu",
            "Beier Zhu",
            "Richang Hong",
            "Hanwang Zhang"
        ],
        "tldr": "The paper introduces REVEL, a new task for interactive video manipulation via drag, and proposes DragStream, a training-free approach to address latent drift and contextual interference in streaming drag-oriented video editing.",
        "tldr_zh": "该论文介绍了REVEL，一项通过拖拽进行交互式视频操作的新任务，并提出了DragStream，一种无需训练的方法，用于解决流式拖拽视频编辑中的潜在漂移和上下文干扰问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]