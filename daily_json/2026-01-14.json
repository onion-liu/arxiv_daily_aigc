[
    {
        "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
        "summary": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\\% improvement on ImageNet classification, a 6.3\\% gain on NLP, a 12.6\\% improvement on image generation, and a 41\\% enhancement on video generation under the same time complexity.",
        "url": "http://arxiv.org/abs/2601.07832v1",
        "published_date": "2026-01-12T18:59:18+00:00",
        "updated_date": "2026-01-12T18:59:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kewei Zhang",
            "Ye Huang",
            "Yufan Deng",
            "Jincheng Yu",
            "Junsong Chen",
            "Huan Ling",
            "Enze Xie",
            "Daquan Zhou"
        ],
        "tldr": "This paper introduces Multi-Head Linear Attention (MHLA), a method that improves linear attention's expressiveness by addressing global context collapse, achieving significant performance gains in image, NLP, and especially video & image generation tasks while maintaining linear complexity.",
        "tldr_zh": "本文介绍了多头线性注意力（MHLA），该方法通过解决全局上下文崩溃问题来提高线性注意力的表达能力，在图像、自然语言处理，尤其是视频和图像生成任务中实现了显著的性能提升，同时保持了线性复杂度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "End-to-End Video Character Replacement without Structural Guidance",
        "summary": "Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha",
        "url": "http://arxiv.org/abs/2601.08587v1",
        "published_date": "2026-01-13T14:10:34+00:00",
        "updated_date": "2026-01-13T14:10:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhengbo Xu",
            "Jie Ma",
            "Ziheng Wang",
            "Zhan Peng",
            "Jun Liang",
            "Jing Li"
        ],
        "tldr": "The paper introduces MoCha, a novel end-to-end video character replacement framework that requires only a single frame mask and overcomes limitations of prior works by utilizing condition-aware RoPE, RL-based refinement, and a comprehensive data construction pipeline with specialized datasets.",
        "tldr_zh": "该论文介绍了一种名为MoCha的全新端到端视频角色替换框架，仅需单帧掩码，并通过使用条件感知RoPE、基于强化学习的优化以及包含专门数据集的综合数据构建流程，克服了现有方法的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Safer Mobile Agents: Scalable Generation and Evaluation of Diverse Scenarios for VLMs",
        "summary": "Vision Language Models (VLMs) are increasingly deployed in autonomous vehicles and mobile systems, making it crucial to evaluate their ability to support safer decision-making in complex environments. However, existing benchmarks inadequately cover diverse hazardous situations, especially anomalous scenarios with spatio-temporal dynamics. While image editing models are a promising means to synthesize such hazards, it remains challenging to generate well-formulated scenarios that include moving, intrusive, and distant objects frequently observed in the real world. To address this gap, we introduce \\textbf{HazardForge}, a scalable pipeline that leverages image editing models to generate these scenarios with layout decision algorithms, and validation modules. Using HazardForge, we construct \\textbf{MovSafeBench}, a multiple-choice question (MCQ) benchmark comprising 7,254 images and corresponding QA pairs across 13 object categories, covering both normal and anomalous objects. Experiments using MovSafeBench show that VLM performance degrades notably under conditions including anomalous objects, with the largest drop in scenarios requiring nuanced motion understanding.",
        "url": "http://arxiv.org/abs/2601.08470v1",
        "published_date": "2026-01-13T11:55:31+00:00",
        "updated_date": "2026-01-13T11:55:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Takara Taniguchi",
            "Kuniaki Saito",
            "Atsushi Hashimoto"
        ],
        "tldr": "The paper introduces HazardForge, a pipeline for generating challenging scenarios for VLMs in autonomous driving, and MovSafeBench, a benchmark demonstrating VLM performance degradation in anomalous situations requiring motion understanding.",
        "tldr_zh": "本文介绍HazardForge，一个用于生成自动驾驶领域VLM的挑战性场景的流程，以及MovSafeBench，一个基准，表明VLM在需要运动理解的异常情况下性能下降。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training-Free Distribution Adaptation for Diffusion Models via Maximum Mean Discrepancy Guidance",
        "summary": "Pre-trained diffusion models have emerged as powerful generative priors for both unconditional and conditional sample generation, yet their outputs often deviate from the characteristics of user-specific target data. Such mismatches are especially problematic in domain adaptation tasks, where only a few reference examples are available and retraining the diffusion model is infeasible. Existing inference-time guidance methods can adjust sampling trajectories, but they typically optimize surrogate objectives such as classifier likelihoods rather than directly aligning with the target distribution. We propose MMD Guidance, a training-free mechanism that augments the reverse diffusion process with gradients of the Maximum Mean Discrepancy (MMD) between generated samples and a reference dataset. MMD provides reliable distributional estimates from limited data, exhibits low variance in practice, and is efficiently differentiable, which makes it particularly well-suited for the guidance task. Our framework naturally extends to prompt-aware adaptation in conditional generation models via product kernels. Also, it can be applied with computational efficiency in latent diffusion models (LDMs), since guidance is applied in the latent space of the LDM. Experiments on synthetic and real-world benchmarks demonstrate that MMD Guidance can achieve distributional alignment while preserving sample fidelity.",
        "url": "http://arxiv.org/abs/2601.08379v1",
        "published_date": "2026-01-13T09:42:57+00:00",
        "updated_date": "2026-01-13T09:42:57+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Matina Mahdizadeh Sani",
            "Nima Jamali",
            "Mohammad Jalali",
            "Farzan Farnia"
        ],
        "tldr": "This paper introduces MMD Guidance, a training-free method to adapt pre-trained diffusion models to target data distributions by incorporating Maximum Mean Discrepancy gradients into the reverse diffusion process, effectively aligning generated samples with a reference dataset.",
        "tldr_zh": "本文介绍了一种名为MMD Guidance的免训练方法，通过将最大平均差异（MMD）梯度融入反向扩散过程，使预训练的扩散模型适应目标数据分布，从而有效地将生成的样本与参考数据集对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "IGAN: A New Inception-based Model for Stable and High-Fidelity Image Synthesis Using Generative Adversarial Networks",
        "summary": "Generative Adversarial Networks (GANs) face a significant challenge of striking an optimal balance between high-quality image generation and training stability. Recent techniques, such as DCGAN, BigGAN, and StyleGAN, improve visual fidelity; however, such techniques usually struggle with mode collapse and unstable gradients at high network depth. This paper proposes a novel GAN structural model that incorporates deeper inception-inspired convolution and dilated convolution. This novel model is termed the Inception Generative Adversarial Network (IGAN). The IGAN model generates high-quality synthetic images while maintaining training stability, by reducing mode collapse as well as preventing vanishing and exploding gradients. Our proposed IGAN model achieves the Frechet Inception Distance (FID) of 13.12 and 15.08 on the CUB-200 and ImageNet datasets, respectively, representing a 28-33% improvement in FID over the state-of-the-art GANs. Additionally, the IGAN model attains an Inception Score (IS) of 9.27 and 68.25, reflecting improved image diversity and generation quality. Finally, the two techniques of dropout and spectral normalization are utilized in both the generator and discriminator structures to further mitigate gradient explosion and overfitting. These findings confirm that the IGAN model potentially balances training stability with image generation quality, constituting a scalable and computationally efficient framework for high-fidelity image synthesis.",
        "url": "http://arxiv.org/abs/2601.08332v1",
        "published_date": "2026-01-13T08:42:46+00:00",
        "updated_date": "2026-01-13T08:42:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ahmed A. Hashim",
            "Ali Al-Shuwaili",
            "Asraa Saeed",
            "Ali Al-Bayaty"
        ],
        "tldr": "The paper introduces IGAN, a novel GAN architecture incorporating inception-inspired and dilated convolutions, achieving improved Fréchet Inception Distance (FID) and Inception Score (IS) on CUB-200 and ImageNet datasets while maintaining training stability.",
        "tldr_zh": "该论文介绍了一种新的GAN架构IGAN，它结合了受Inception启发的卷积和空洞卷积，在CUB-200和ImageNet数据集上实现了改进的Fréchet Inception Distance (FID) 和 Inception Score (IS)，同时保持了训练的稳定性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UM-Text: A Unified Multimodal Model for Image Understanding",
        "summary": "With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2601.08321v1",
        "published_date": "2026-01-13T08:18:49+00:00",
        "updated_date": "2026-01-13T08:18:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lichen Ma",
            "Xiaolong Fu",
            "Gaojing Zhou",
            "Zipeng Guo",
            "Ting Zhu",
            "Yichun Liu",
            "Yu Shi",
            "Jason Li",
            "Junshi Huang"
        ],
        "tldr": "The paper introduces UM-Text, a unified multimodal model for visual text editing using natural language instructions, focusing on style consistency and improved glyph generation through a novel architecture, loss function, and training strategy, along with a new large-scale dataset.",
        "tldr_zh": "该论文介绍了UM-Text，一个用于自然语言指令引导下的视觉文本编辑的统一多模态模型，专注于风格一致性，并通过一种新颖的架构、损失函数和训练策略来改进字形生成，同时还提供了一个新的大规模数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
        "summary": "Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.",
        "url": "http://arxiv.org/abs/2601.08303v1",
        "published_date": "2026-01-13T07:46:46+00:00",
        "updated_date": "2026-01-13T07:46:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dongting Hu",
            "Aarush Gupta",
            "Magzhan Gabidolla",
            "Arpit Sahni",
            "Huseyin Coskun",
            "Yanyu Li",
            "Yerlan Idelbayev",
            "Ahsan Mahmood",
            "Aleksei Lebedev",
            "Dishani Lahiri",
            "Anujraaj Goyal",
            "Ju Hu",
            "Mingming Gong",
            "Sergey Tulyakov",
            "Anil Kag"
        ],
        "tldr": "SnapGen++ introduces an efficient diffusion transformer framework for on-device image generation, using sparse attention, elastic training, and knowledge-guided distillation to achieve high-fidelity results with low latency on edge devices.",
        "tldr_zh": "SnapGen++ 提出了一种高效的扩散 Transformer 框架，用于在边缘设备上进行图像生成。它采用稀疏注意力机制、弹性训练和知识引导的蒸馏方法，以在边缘设备上实现高保真和低延迟的图像生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Prompts to Deployment: Auto-Curated Domain-Specific Dataset Generation via Diffusion Models",
        "summary": "In this paper, we present an automated pipeline for generating domain-specific synthetic datasets with diffusion models, addressing the distribution shift between pre-trained models and real-world deployment environments. Our three-stage framework first synthesizes target objects within domain-specific backgrounds through controlled inpainting. The generated outputs are then validated via a multi-modal assessment that integrates object detection, aesthetic scoring, and vision-language alignment. Finally, a user-preference classifier is employed to capture subjective selection criteria. This pipeline enables the efficient construction of high-quality, deployable datasets while reducing reliance on extensive real-world data collection.",
        "url": "http://arxiv.org/abs/2601.08095v1",
        "published_date": "2026-01-13T00:29:25+00:00",
        "updated_date": "2026-01-13T00:29:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dongsik Yoon",
            "Jongeun Kim"
        ],
        "tldr": "This paper introduces an automated pipeline for generating domain-specific datasets using diffusion models, incorporating object detection, aesthetic scoring, and user preference classification to bridge the gap between pre-trained models and real-world deployments.",
        "tldr_zh": "本文介绍了一个自动化流程，利用扩散模型生成领域特定的数据集，结合了目标检测、美学评分和用户偏好分类，以弥合预训练模型与现实世界部署之间的差距。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TP-Blend: Textual-Prompt Attention Pairing for Precise Object-Style Blending in Diffusion Models",
        "summary": "Current text-conditioned diffusion editors handle single object replacement well but struggle when a new object and a new style must be introduced simultaneously. We present Twin-Prompt Attention Blend (TP-Blend), a lightweight training-free framework that receives two separate textual prompts, one specifying a blend object and the other defining a target style, and injects both into a single denoising trajectory. TP-Blend is driven by two complementary attention processors. Cross-Attention Object Fusion (CAOF) first averages head-wise attention to locate spatial tokens that respond strongly to either prompt, then solves an entropy-regularised optimal transport problem that reassigns complete multi-head feature vectors to those positions. CAOF updates feature vectors at the full combined dimensionality of all heads (e.g., 640 dimensions in SD-XL), preserving rich cross-head correlations while keeping memory low. Self-Attention Style Fusion (SASF) injects style at every self-attention layer through Detail-Sensitive Instance Normalization. A lightweight one-dimensional Gaussian filter separates low- and high-frequency components; only the high-frequency residual is blended back, imprinting brush-stroke-level texture without disrupting global geometry. SASF further swaps the Key and Value matrices with those derived from the style prompt, enforcing context-aware texture modulation that remains independent of object fusion. Extensive experiments show that TP-Blend produces high-resolution, photo-realistic edits with precise control over both content and appearance, surpassing recent baselines in quantitative fidelity, perceptual quality, and inference speed.",
        "url": "http://arxiv.org/abs/2601.08011v1",
        "published_date": "2026-01-12T21:30:10+00:00",
        "updated_date": "2026-01-12T21:30:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Xin Jin",
            "Yichuan Zhong",
            "Yapeng Tian"
        ],
        "tldr": "The paper introduces TP-Blend, a training-free framework for text-conditioned diffusion models that allows for simultaneous object replacement and style transfer using twin prompts and complementary attention processors. It achieves high-resolution, photorealistic edits with precise control and outperforms existing baselines.",
        "tldr_zh": "该论文介绍了一种名为TP-Blend的免训练框架，用于文本条件扩散模型，它允许使用双提示和互补注意力处理器同时进行对象替换和风格迁移。此方法实现了高分辨率、逼真的编辑，并且性能优于现有的基线方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Moonworks Lunara Aesthetic Dataset",
        "summary": "The dataset spans diverse artistic styles, including regionally grounded aesthetics from the Middle East, Northern Europe, East Asia, and South Asia, alongside general categories such as sketch and oil painting. All images are generated using the Moonworks Lunara model and intentionally crafted to embody distinct, high-quality aesthetic styles, yielding a first-of-its-kind dataset with substantially higher aesthetic scores, exceeding even aesthetics-focused datasets, and general-purpose datasets by a larger margin. Each image is accompanied by a human-refined prompt and structured annotations that jointly describe salient objects, attributes, relationships, and stylistic cues. Unlike large-scale web-derived datasets that emphasize breadth over precision, the Lunara Aesthetic Dataset prioritizes aesthetic quality, stylistic diversity, and licensing transparency, and is released under the Apache 2.0 license to support research and unrestricted academic and commercial use.",
        "url": "http://arxiv.org/abs/2601.07941v1",
        "published_date": "2026-01-12T19:11:41+00:00",
        "updated_date": "2026-01-12T19:11:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yan Wang",
            "M M Sayeef Abdullah",
            "Partho Hassan",
            "Sabit Hassan"
        ],
        "tldr": "The paper introduces Moonworks Lunara Aesthetic Dataset, a novel, high-quality, and diverse image dataset generated by the Moonworks Lunara model, designed for aesthetic-focused research with detailed annotations and an Apache 2.0 license.",
        "tldr_zh": "该论文介绍了一个名为 Moonworks Lunara Aesthetic Dataset 的新型高质量图像数据集，该数据集由 Moonworks Lunara 模型生成，专为美学研究而设计，具有详细的注释并采用 Apache 2.0 许可。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 10,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Tuning-free Visual Effect Transfer across Videos",
        "summary": "We present RefVFX, a new framework that transfers complex temporal effects from a reference video onto a target video or image in a feed-forward manner. While existing methods excel at prompt-based or keyframe-conditioned editing, they struggle with dynamic temporal effects such as dynamic lighting changes or character transformations, which are difficult to describe via text or static conditions. Transferring a video effect is challenging, as the model must integrate the new temporal dynamics with the input video's existing motion and appearance. % To address this, we introduce a large-scale dataset of triplets, where each triplet consists of a reference effect video, an input image or video, and a corresponding output video depicting the transferred effect. Creating this data is non-trivial, especially the video-to-video effect triplets, which do not exist naturally. To generate these, we propose a scalable automated pipeline that creates high-quality paired videos designed to preserve the input's motion and structure while transforming it based on some fixed, repeatable effect. We then augment this data with image-to-video effects derived from LoRA adapters and code-based temporal effects generated through programmatic composition. Building on our new dataset, we train our reference-conditioned model using recent text-to-video backbones. Experimental results demonstrate that RefVFX produces visually consistent and temporally coherent edits, generalizes across unseen effect categories, and outperforms prompt-only baselines in both quantitative metrics and human preference. See our website https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/",
        "url": "http://arxiv.org/abs/2601.07833v2",
        "published_date": "2026-01-12T18:59:32+00:00",
        "updated_date": "2026-01-13T03:17:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maxwell Jones",
            "Rameen Abdal",
            "Or Patashnik",
            "Ruslan Salakhutdinov",
            "Sergey Tulyakov",
            "Jun-Yan Zhu",
            "Kuan-Chieh Jackson Wang"
        ],
        "tldr": "RefVFX introduces a framework and large-scale dataset for transferring complex temporal visual effects from a reference video to a target video or image without needing tuning, outperforming prompt-based methods.",
        "tldr_zh": "RefVFX 提出了一个框架和一个大规模数据集，用于将复杂的时序视觉效果从参考视频转移到目标视频或图像，无需调整，优于基于提示的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing",
        "summary": "The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models. Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes. In this paper, we introduce 3DGS-Drag -- a point-based 3D editing framework that provides efficient, intuitive drag manipulation of real 3D scenes. Our approach bridges the gap between deformation-based and 2D-editing-based 3D editing methods, addressing their limitations to geometry-related content editing. We leverage two key innovations: deformation guidance utilizing 3D Gaussian Splatting for consistent geometric modifications and diffusion guidance for content correction and visual quality enhancement. A progressive editing strategy further supports aggressive 3D drag edits. Our method enables a wide range of edits, including motion change, shape adjustment, inpainting, and content extension. Experimental results demonstrate the effectiveness of 3DGS-Drag in various scenes, achieving state-of-the-art performance in geometry-related 3D content editing. Notably, the editing is efficient, taking 10 to 20 minutes on a single RTX 4090 GPU.",
        "url": "http://arxiv.org/abs/2601.07963v1",
        "published_date": "2026-01-12T19:57:31+00:00",
        "updated_date": "2026-01-12T19:57:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahua Dong",
            "Yu-Xiong Wang"
        ],
        "tldr": "The paper introduces 3DGS-Drag, a novel point-based 3D editing framework that enables intuitive drag manipulation of real 3D scenes by leveraging deformation and diffusion guidance, achieving state-of-the-art performance with efficient editing.",
        "tldr_zh": "这篇论文介绍了3DGS-Drag，一种新的基于点的3D编辑框架，通过利用变形和扩散引导，能够对真实的3D场景进行直观的拖动操作，并以高效的编辑实现了最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]