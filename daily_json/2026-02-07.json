[
    {
        "title": "GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?",
        "summary": "The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.",
        "url": "http://arxiv.org/abs/2602.06013v1",
        "published_date": "2026-02-05T18:52:48+00:00",
        "updated_date": "2026-02-05T18:52:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ruihang Li",
            "Leigang Qu",
            "Jingxu Zhang",
            "Dongnan Gui",
            "Mengde Xu",
            "Xiaosong Zhang",
            "Han Hu",
            "Wenjie Wang",
            "Jiaqi Wang"
        ],
        "tldr": "The paper introduces GenArena, a pairwise comparison framework for evaluating visual generation models, demonstrating improved consistency and human alignment compared to traditional pointwise scoring, and achieving state-of-the-art correlation with human preferences.",
        "tldr_zh": "该论文介绍了 GenArena，一个用于评估视觉生成模型的成对比较框架。相比于传统的点状评分方法，GenArena 在一致性和与人类对齐方面有所提升，并且实现了与人类偏好相关的最高水平。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Context Forcing: Consistent Autoregressive Video Generation with Long Context",
        "summary": "Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \\textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \\textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \\textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.",
        "url": "http://arxiv.org/abs/2602.06028v1",
        "published_date": "2026-02-05T18:58:01+00:00",
        "updated_date": "2026-02-05T18:58:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuo Chen",
            "Cong Wei",
            "Sun Sun",
            "Ping Nie",
            "Kai Zhou",
            "Ge Zhang",
            "Ming-Hsuan Yang",
            "Wenhu Chen"
        ],
        "tldr": "The paper introduces 'Context Forcing,' a novel long-context video generation framework that addresses the student-teacher mismatch problem by training a long-context student model with a long-context teacher, achieving state-of-the-art consistency for videos exceeding 20 seconds. It also introduces a slow-fast memory architecture for context management.",
        "tldr_zh": "该论文介绍了'Context Forcing'，一种新的长上下文视频生成框架，通过使用长上下文Teacher模型训练长上下文Student模型来解决Student-Teacher不匹配问题，实现了超过20秒视频的最先进一致性。它还引入了一种用于上下文管理的慢速-快速记忆架构。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RISE-Video: Can Video Generators Decode Implicit World Rules?",
        "summary": "While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \\textit{Reasoning Alignment}, \\textit{Temporal Consistency}, \\textit{Physical Rationality}, and \\textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.",
        "url": "http://arxiv.org/abs/2602.05986v1",
        "published_date": "2026-02-05T18:36:10+00:00",
        "updated_date": "2026-02-05T18:36:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mingxin Liu",
            "Shuran Ma",
            "Shibei Meng",
            "Xiangyu Zhao",
            "Zicheng Zhang",
            "Shaofeng Zhang",
            "Zhihang Zhong",
            "Peixian Chen",
            "Haoyu Cao",
            "Xing Sun",
            "Haodong Duan",
            "Xue Yang"
        ],
        "tldr": "The paper introduces RISE-Video, a new benchmark for evaluating the reasoning capabilities of text-to-video generation models, revealing deficiencies in current models' ability to handle implicit world rules.",
        "tldr_zh": "该论文介绍 RISE-Video，一个新的用于评估文本到视频生成模型推理能力的基准，揭示了当前模型在处理隐含世界规则方面的不足。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation",
        "summary": "Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.",
        "url": "http://arxiv.org/abs/2602.05966v1",
        "published_date": "2026-02-05T18:21:02+00:00",
        "updated_date": "2026-02-05T18:21:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mirlan Karimov",
            "Teodora Spasojevic",
            "Markus Braun",
            "Julian Wiederer",
            "Vasileios Belagiannis",
            "Marc Pollefeys"
        ],
        "tldr": "The paper introduces Localized Semantic Alignment (LSA), a fine-tuning method for video generation models that improves temporal consistency by aligning semantic features between ground-truth and generated video clips, without needing control signals at inference.",
        "tldr_zh": "本文提出了一种名为局部语义对齐(LSA)的视频生成模型微调方法，通过对齐真实视频片段和生成视频片段之间的语义特征，提高了时间一致性，且在推理时不依赖于控制信号。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching",
        "summary": "Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.",
        "url": "http://arxiv.org/abs/2602.05951v1",
        "published_date": "2026-02-05T18:08:20+00:00",
        "updated_date": "2026-02-05T18:08:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Junwan Kim",
            "Jiho Park",
            "Seonghu Jeon",
            "Seungryong Kim"
        ],
        "tldr": "This paper introduces a method for learning a condition-dependent source distribution in flow matching for text-to-image generation, leading to faster convergence and improved performance.",
        "tldr_zh": "本文提出了一种在Flow Matching中学习条件相关源分布的方法，用于文本到图像生成，从而实现更快的收敛并提高性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]