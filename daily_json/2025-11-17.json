[
    {
        "title": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "summary": "We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.",
        "url": "http://arxiv.org/abs/2511.12609v1",
        "published_date": "2025-11-16T14:10:55+00:00",
        "updated_date": "2025-11-16T14:10:55+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yunxin Li",
            "Xinyu Chen",
            "Shenyuan Jiang",
            "Haoyuan Shi",
            "Zhenyu Liu",
            "Xuanyu Zhang",
            "Nanhao Deng",
            "Zhenran Xu",
            "Yicheng Ma",
            "Meishan Zhang",
            "Baotian Hu",
            "Min Zhang"
        ],
        "tldr": "Uni-MoE 2.0-Omni is a new open-source omnimodal large model leveraging Mixture-of-Experts, progressive training, and curated multimodal data to achieve state-of-the-art or competitive performance across various multimodal tasks, including image, text, and speech generation.",
        "tldr_zh": "Uni-MoE 2.0-Omni 是一个全新的开源全模态大型模型，它利用混合专家模型(MoE)，渐进式训练和精心策划的多模态数据，在各种多模态任务(包括图像、文本和语音生成)中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Denoising Vision Transformer Autoencoder with Spectral Self-Regularization",
        "summary": "Variational autoencoders (VAEs) typically encode images into a compact latent space, reducing computational cost but introducing an optimization dilemma: a higher-dimensional latent space improves reconstruction fidelity but often hampers generative performance. Recent methods attempt to address this dilemma by regularizing high-dimensional latent spaces using external vision foundation models (VFMs). However, it remains unclear how high-dimensional VAE latents affect the optimization of generative models. To our knowledge, our analysis is the first to reveal that redundant high-frequency components in high-dimensional latent spaces hinder the training convergence of diffusion models and, consequently, degrade generation quality. To alleviate this problem, we propose a spectral self-regularization strategy to suppress redundant high-frequency noise while simultaneously preserving reconstruction quality. The resulting Denoising-VAE, a ViT-based autoencoder that does not rely on VFMs, produces cleaner, lower-noise latents, leading to improved generative quality and faster optimization convergence. We further introduce a spectral alignment strategy to facilitate the optimization of Denoising-VAE-based generative models. Our complete method enables diffusion models to converge approximately 2$\\times$ faster than with SD-VAE, while achieving state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26) and competitive generation performance (gFID = 1.82) on the ImageNet 256$\\times$256 benchmark.",
        "url": "http://arxiv.org/abs/2511.12633v1",
        "published_date": "2025-11-16T15:00:32+00:00",
        "updated_date": "2025-11-16T15:00:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xunzhi Xiang",
            "Xingye Tian",
            "Guiyu Zhang",
            "Yabo Chen",
            "Shaofeng Zhang",
            "Xuebo Wang",
            "Xin Tao",
            "Qi Fan"
        ],
        "tldr": "This paper introduces a spectral self-regularization strategy for Vision Transformer Autoencoders to remove high-frequency noise in latent spaces, improving generative quality and optimization speed for diffusion models without relying on external vision foundation models.",
        "tldr_zh": "本文提出了一种用于视觉Transformer自编码器的频谱自正则化策略，以消除潜在空间中的高频噪声，从而提高扩散模型的生成质量和优化速度，而无需依赖外部视觉基础模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation",
        "summary": "While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.",
        "url": "http://arxiv.org/abs/2511.12631v1",
        "published_date": "2025-11-16T14:52:54+00:00",
        "updated_date": "2025-11-16T14:52:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yushe Cao",
            "Dianxi Shi",
            "Xing Fu",
            "Xuechao Zou",
            "Haikuo Peng",
            "Xueqi Li",
            "Chun Yu",
            "Junliang Xing"
        ],
        "tldr": "The paper introduces MDiTFace, a diffusion transformer framework with decoupled attention for high-fidelity face generation conditioned on semantic masks and text, achieving state-of-the-art performance with significant computational efficiency improvements.",
        "tldr_zh": "该论文介绍了MDiTFace，一个具有解耦注意力的扩散 Transformer 框架，用于基于语义掩码和文本的高保真面部生成，实现了最先进的性能，并显著提高了计算效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction",
        "summary": "We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.",
        "url": "http://arxiv.org/abs/2511.12578v1",
        "published_date": "2025-11-16T12:41:07+00:00",
        "updated_date": "2025-11-16T12:41:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yukuo Ma",
            "Cong Liu",
            "Junke Wang",
            "Junqi Liu",
            "Haibin Huang",
            "Zuxuan Wu",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "tldr": "TempoMaster generates long videos efficiently by predicting the next frame rate, starting with a low-frame-rate blueprint and progressively refining visual details and motion continuity.",
        "tldr_zh": "TempoMaster通过预测下一帧速率来高效生成长视频，从低帧速率的蓝图开始，逐步细化视觉细节和运动连续性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]