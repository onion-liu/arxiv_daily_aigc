[
    {
        "title": "Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency",
        "summary": "This work represents the first effort to scale up continuous-time consistency\ndistillation to general application-level image and video diffusion models.\nAlthough continuous-time consistency model (sCM) is theoretically principled\nand empirically powerful for accelerating academic-scale diffusion, its\napplicability to large-scale text-to-image and video tasks remains unclear due\nto infrastructure challenges in Jacobian-vector product (JVP) computation and\nthe limitations of standard evaluation benchmarks. We first develop a\nparallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on\nmodels with over 10 billion parameters and high-dimensional video tasks. Our\ninvestigation reveals fundamental quality limitations of sCM in fine-detail\ngeneration, which we attribute to error accumulation and the \"mode-covering\"\nnature of its forward-divergence objective. To remedy this, we propose the\nscore-regularized continuous-time consistency model (rCM), which incorporates\nscore distillation as a long-skip regularizer. This integration complements sCM\nwith the \"mode-seeking\" reverse divergence, effectively improving visual\nquality while maintaining high generation diversity. Validated on large-scale\nmodels (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM\nmatches or surpasses the state-of-the-art distillation method DMD2 on quality\nmetrics while offering notable advantages in diversity, all without GAN tuning\nor extensive hyperparameter searches. The distilled models generate\nhigh-fidelity samples in only $1\\sim4$ steps, accelerating diffusion sampling\nby $15\\times\\sim50\\times$. These results position rCM as a practical and\ntheoretically grounded framework for advancing large-scale diffusion\ndistillation.",
        "url": "http://arxiv.org/abs/2510.08431v1",
        "published_date": "2025-10-09T16:45:30+00:00",
        "updated_date": "2025-10-09T16:45:30+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Kaiwen Zheng",
            "Yuji Wang",
            "Qianli Ma",
            "Huayu Chen",
            "Jintao Zhang",
            "Yogesh Balaji",
            "Jianfei Chen",
            "Ming-Yu Liu",
            "Jun Zhu",
            "Qinsheng Zhang"
        ],
        "tldr": "The paper introduces score-regularized continuous-time consistency model (rCM) to improve the quality and diversity of large-scale image and video diffusion models, achieving significant acceleration and performance gains over existing methods.",
        "tldr_zh": "该论文介绍了分数正则化连续时间一致性模型 (rCM)，以提高大规模图像和视频扩散模型的质量和多样性，与现有方法相比，实现了显著的加速和性能提升。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos",
        "summary": "Unified multimodal models have shown promising results in multimodal content\ngeneration and editing but remain largely limited to the image domain. In this\nwork, we present UniVideo, a versatile framework that extends unified modeling\nto the video domain. UniVideo adopts a dual-stream design, combining a\nMultimodal Large Language Model (MLLM) for instruction understanding with a\nMultimodal DiT (MMDiT) for video generation. This design enables accurate\ninterpretation of complex multimodal instructions while preserving visual\nconsistency. Built on this architecture, UniVideo unifies diverse video\ngeneration and editing tasks under a single multimodal instruction paradigm and\nis jointly trained across them. Extensive experiments demonstrate that UniVideo\nmatches or surpasses state-of-the-art task-specific baselines in\ntext/image-to-video generation, in-context video generation and in-context\nvideo editing. Notably, the unified design of UniVideo enables two forms of\ngeneralization. First, UniVideo supports task composition, such as combining\nediting with style transfer, by integrating multiple capabilities within a\nsingle instruction. Second, even without explicit training on free-form video\nediting, UniVideo transfers its editing capability from large-scale image\nediting data to this setting, handling unseen instructions such as\ngreen-screening characters or changing materials within a video. Beyond these\ncore capabilities, UniVideo also supports visual-prompt-based video generation,\nwhere the MLLM interprets visual prompts and guides the MMDiT during synthesis.\nTo foster future research, we will release our model and code.",
        "url": "http://arxiv.org/abs/2510.08377v1",
        "published_date": "2025-10-09T16:01:30+00:00",
        "updated_date": "2025-10-09T16:01:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cong Wei",
            "Quande Liu",
            "Zixuan Ye",
            "Qiulin Wang",
            "Xintao Wang",
            "Pengfei Wan",
            "Kun Gai",
            "Wenhu Chen"
        ],
        "tldr": "The paper introduces UniVideo, a unified framework for video understanding, generation, and editing, leveraging a dual-stream architecture with an MLLM and MMDiT, demonstrating strong performance and generalization capabilities across various tasks.",
        "tldr_zh": "该论文介绍了 UniVideo，一个用于视频理解、生成和编辑的统一框架，利用具有 MLLM 和 MMDiT 的双流架构，展示了在各种任务中的强大性能和泛化能力。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation",
        "summary": "Video diffusion models (DMs) have enabled high-quality video synthesis.\nHowever, their computation costs scale quadratically with sequence length\nbecause self-attention has quadratic complexity. While linear attention lowers\nthe cost, fully replacing quadratic attention requires expensive pretraining\ndue to the limited expressiveness of linear attention and the complexity of\nspatiotemporal modeling in video generation. In this paper, we present\nLinVideo, an efficient data-free post-training framework that replaces a target\nnumber of self-attention modules with linear attention while preserving the\noriginal model's performance. First, we observe a significant disparity in the\nreplaceability of different layers. Instead of manual or heuristic choices, we\nframe layer selection as a binary classification problem and propose selective\ntransfer, which automatically and progressively converts layers to linear\nattention with minimal performance impact. Additionally, to overcome the\nineffectiveness and inefficiency of existing objectives for this transfer\nprocess, we introduce an anytime distribution matching (ADM) objective that\naligns the distributions of samples across any timestep along the sampling\ntrajectory. This objective is efficient and recovers model performance.\nExtensive experiments show that our method achieves a 1.25-2.00x speedup while\npreserving generation quality, and our 4-step distilled model further delivers\na 15.92x latency reduction with minimal visual quality drop.",
        "url": "http://arxiv.org/abs/2510.08318v1",
        "published_date": "2025-10-09T15:03:39+00:00",
        "updated_date": "2025-10-09T15:03:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yushi Huang",
            "Xingtong Ge",
            "Ruihao Gong",
            "Chengtao Lv",
            "Jun Zhang"
        ],
        "tldr": "The paper introduces LinVideo, a post-training framework for video diffusion models that achieves O(n) attention complexity by selectively replacing quadratic self-attention layers with linear attention layers, resulting in significant speedups with minimal performance degradation.",
        "tldr_zh": "该论文介绍了 LinVideo，一种视频扩散模型的后训练框架，通过选择性地将二次自注意力层替换为线性注意力层来实现 O(n) 的注意力复杂度，从而在性能下降最小的情况下显著提高速度。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing",
        "summary": "Image editing with natural language has gained significant popularity, yet\nexisting methods struggle with intricate object intersections and fine-grained\nspatial relationships due to the lack of an explicit reasoning process. While\nChain-of-Thought (CoT) has been explored to enhance reasoning, purely textual\nCoT or CoT augmented with coordinate information is fundamentally limited in\nits ability to represent intricate visual layouts and lacks the necessary\nvisual cues to guide the generation of fine-grained, pixel-level details. To\naddress these challenges, we propose Multimodal Reasoning Edit (MURE), a novel\nframework that shifts the visual editing process from purely text-based\nreasoning to a series of interleaved textual and visual rationales. Our\nframework performs image editing using a natively multimodal, interleaved\ntext-image CoT. This approach generates a step-by-step chain of reasoning where\na textual description is followed by a corresponding visual cue, such as a\npositional mask that defined intended edited regions or a representation of new\ncontent. Furthermore, to mitigate the hallucination phenomenon of large\nlanguage models, we introduce Multimodal Deep Confidence (MMDC) reasoning\nparadigm. This paradigm explores a tree of visual reasoning paths at each step.\nBy pruning low-quality branches using a deep confidence score from a reward\nmodel, it ensures the model consistently follows a high-quality trajectory\ntowards the final edited result. The proposed method decomposes complex editing\ntasks into interdependent sub-tasks, achieving greater precision at each stage\nand yielding high-fidelity edited results. We define the formulation for\ninterleaved text-image chains and release the first CoT-Edit-14K dataset,\ncomprising 14K high-quality editing examples. Extensive experiments show that\nour method yields significant improvements across three image editing\nbenchmarks.",
        "url": "http://arxiv.org/abs/2510.08157v1",
        "published_date": "2025-10-09T12:36:51+00:00",
        "updated_date": "2025-10-09T12:36:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhentao Zou",
            "Zhengrong Yue",
            "Kunpeng Du",
            "Binlei Bao",
            "Hanting Li",
            "Haizhen Xie",
            "Guozheng Xu",
            "Yue Zhou",
            "Yali Wang",
            "Jie Hu",
            "Xue Jiang",
            "Xinghao Chen"
        ],
        "tldr": "The paper introduces MURE, a novel framework for image editing that uses interleaved text-image Chain-of-Thought reasoning and Multimodal Deep Confidence to address the limitations of text-based methods in handling intricate object intersections and fine-grained spatial relationships.",
        "tldr_zh": "该论文介绍了MURE，一种新颖的图像编辑框架，它使用交错的文本-图像链式思维推理和多模态深度置信度，以解决基于文本的方法在处理复杂的对象交叉和精细的空间关系方面的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Controllable Video Synthesis via Variational Inference",
        "summary": "Many video workflows benefit from a mixture of user controls with varying\ngranularity, from exact 4D object trajectories and camera paths to coarse text\nprompts, while existing video generative models are typically trained for fixed\ninput formats. We develop a video synthesis method that addresses this need and\ngenerates samples with high controllability for specified elements while\nmaintaining diversity for under-specified ones. We cast the task as variational\ninference to approximate a composed distribution, leveraging multiple video\ngeneration backbones to account for all task constraints collectively. To\naddress the optimization challenge, we break down the problem into step-wise KL\ndivergence minimization over an annealed sequence of distributions, and further\npropose a context-conditioned factorization technique that reduces modes in the\nsolution space to circumvent local optima. Experiments suggest that our method\nproduces samples with improved controllability, diversity, and 3D consistency\ncompared to prior works.",
        "url": "http://arxiv.org/abs/2510.07670v1",
        "published_date": "2025-10-09T01:48:16+00:00",
        "updated_date": "2025-10-09T01:48:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haoyi Duan",
            "Yunzhi Zhang",
            "Yilun Du",
            "Jiajun Wu"
        ],
        "tldr": "This paper introduces a controllable video synthesis method using variational inference, allowing users to specify various control elements while maintaining diversity. It outperforms existing methods in controllability, diversity, and 3D consistency.",
        "tldr_zh": "本文提出了一种基于变分推理的可控视频合成方法，允许用户指定各种控制元素，同时保持多样性。该方法在可控性、多样性和3D一致性方面优于现有方法。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "MultiCOIN: Multi-Modal COntrollable Video INbetweening",
        "summary": "Video inbetweening creates smooth and natural transitions between two image\nframes, making it an indispensable tool for video editing and long-form video\nsynthesis. Existing works in this domain are unable to generate large, complex,\nor intricate motions. In particular, they cannot accommodate the versatility of\nuser intents and generally lack fine control over the details of intermediate\nframes, leading to misalignment with the creative mind. To fill these gaps, we\nintroduce \\modelname{}, a video inbetweening framework that allows multi-modal\ncontrols, including depth transition and layering, motion trajectories, text\nprompts, and target regions for movement localization, while achieving a\nbalance between flexibility, ease of use, and precision for fine-grained video\ninterpolation. To achieve this, we adopt the Diffusion Transformer (DiT)\narchitecture as our video generative model, due to its proven capability to\ngenerate high-quality long videos. To ensure compatibility between DiT and our\nmulti-modal controls, we map all motion controls into a common sparse and\nuser-friendly point-based representation as the video/noise input. Further, to\nrespect the variety of controls which operate at varying levels of granularity\nand influence, we separate content controls and motion controls into two\nbranches to encode the required features before guiding the denoising process,\nresulting in two generators, one for motion and the other for content. Finally,\nwe propose a stage-wise training strategy to ensure that our model learns the\nmulti-modal controls smoothly. Extensive qualitative and quantitative\nexperiments demonstrate that multi-modal controls enable a more dynamic,\ncustomizable, and contextually accurate visual narrative.",
        "url": "http://arxiv.org/abs/2510.08561v1",
        "published_date": "2025-10-09T17:59:27+00:00",
        "updated_date": "2025-10-09T17:59:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maham Tanveer",
            "Yang Zhou",
            "Simon Niklaus",
            "Ali Mahdavi Amiri",
            "Hao Zhang",
            "Krishna Kumar Singh",
            "Nanxuan Zhao"
        ],
        "tldr": "The paper introduces MultiCOIN, a multi-modal controllable video inbetweening framework using Diffusion Transformers, enabling fine-grained control over intermediate frames via depth, motion trajectories, text prompts, and target regions.",
        "tldr_zh": "本文介绍了MultiCOIN，一个多模态可控视频插帧框架，使用扩散Transformer，可以通过深度、运动轨迹、文本提示和目标区域对中间帧进行细粒度控制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Kontinuous Kontext: Continuous Strength Control for Instruction-based Image Editing",
        "summary": "Instruction-based image editing offers a powerful and intuitive way to\nmanipulate images through natural language. Yet, relying solely on text\ninstructions limits fine-grained control over the extent of edits. We introduce\nKontinuous Kontext, an instruction-driven editing model that provides a new\ndimension of control over edit strength, enabling users to adjust edits\ngradually from no change to a fully realized result in a smooth and continuous\nmanner. Kontinuous Kontext extends a state-of-the-art image editing model to\naccept an additional input, a scalar edit strength which is then paired with\nthe edit instruction, enabling explicit control over the extent of the edit. To\ninject this scalar information, we train a lightweight projector network that\nmaps the input scalar and the edit instruction to coefficients in the model's\nmodulation space. For training our model, we synthesize a diverse dataset of\nimage-edit-instruction-strength quadruplets using existing generative models,\nfollowed by a filtering stage to ensure quality and consistency. Kontinuous\nKontext provides a unified approach for fine-grained control over edit strength\nfor instruction driven editing from subtle to strong across diverse operations\nsuch as stylization, attribute, material, background, and shape changes,\nwithout requiring attribute-specific training.",
        "url": "http://arxiv.org/abs/2510.08532v1",
        "published_date": "2025-10-09T17:51:03+00:00",
        "updated_date": "2025-10-09T17:51:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Rishubh Parihar",
            "Or Patashnik",
            "Daniil Ostashev",
            "R. Venkatesh Babu",
            "Daniel Cohen-Or",
            "Kuan-Chieh Wang"
        ],
        "tldr": "This paper introduces Kontinuous Kontext, an instruction-driven image editing model that allows continuous control over the strength of the edit, trained on a synthesized dataset of image-edit-instruction-strength quadruplets.",
        "tldr_zh": "该论文介绍了一种名为 Kontinuous Kontext 的指令驱动图像编辑模型，该模型允许对编辑强度进行连续控制，并使用图像-编辑-指令-强度四元组的合成数据集进行训练。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control",
        "summary": "We present FlexTraj, a framework for image-to-video generation with flexible\npoint trajectory control. FlexTraj introduces a unified point-based motion\nrepresentation that encodes each point with a segmentation ID, a temporally\nconsistent trajectory ID, and an optional color channel for appearance cues,\nenabling both dense and sparse trajectory control. Instead of injecting\ntrajectory conditions into the video generator through token concatenation or\nControlNet, FlexTraj employs an efficient sequence-concatenation scheme that\nachieves faster convergence, stronger controllability, and more efficient\ninference, while maintaining robustness under unaligned conditions. To train\nsuch a unified point trajectory-controlled video generator, FlexTraj adopts an\nannealing training strategy that gradually reduces reliance on complete\nsupervision and aligned condition. Experimental results demonstrate that\nFlexTraj enables multi-granularity, alignment-agnostic trajectory control for\nvideo generation, supporting various applications such as motion cloning,\ndrag-based image-to-video, motion interpolation, camera redirection, flexible\naction control and mesh animations.",
        "url": "http://arxiv.org/abs/2510.08527v1",
        "published_date": "2025-10-09T17:50:22+00:00",
        "updated_date": "2025-10-09T17:50:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiyuan Zhang",
            "Can Wang",
            "Dongdong Chen",
            "Jing Liao"
        ],
        "tldr": "FlexTraj is a framework for image-to-video generation that uses a unified point-based motion representation with trajectory control, trained with an annealing strategy for robustness under unaligned conditions, supporting various applications.",
        "tldr_zh": "FlexTraj 是一个图像到视频生成的框架，它使用统一的基于点的运动表示并进行轨迹控制。该框架通过退火训练策略来提高在未对齐条件下的鲁棒性，并支持多种应用。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InstructX: Towards Unified Visual Editing with MLLM Guidance",
        "summary": "With recent advances in Multimodal Large Language Models (MLLMs) showing\nstrong visual understanding and reasoning, interest is growing in using them to\nimprove the editing performance of diffusion models. Despite rapid progress,\nmost studies lack an in-depth analysis of MLLM design choices. Moreover, the\nintegration of MLLMs and diffusion models remains an open challenge in some\ndifficult tasks, such as video editing. In this paper, we present InstructX, a\nunified framework for image and video editing. Specifically, we conduct a\ncomprehensive study on integrating MLLMs and diffusion models for\ninstruction-driven editing across diverse tasks. Building on this study, we\nanalyze the cooperation and distinction between images and videos in unified\nmodeling. (1) We show that training on image data can lead to emergent video\nediting capabilities without explicit supervision, thereby alleviating the\nconstraints imposed by scarce video training data. (2) By incorporating\nmodality-specific MLLM features, our approach effectively unifies image and\nvideo editing tasks within a single model. Extensive experiments demonstrate\nthat our method can handle a broad range of image and video editing tasks and\nachieves state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2510.08485v1",
        "published_date": "2025-10-09T17:26:09+00:00",
        "updated_date": "2025-10-09T17:26:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chong Mou",
            "Qichao Sun",
            "Yanze Wu",
            "Pengze Zhang",
            "Xinghui Li",
            "Fulong Ye",
            "Songtao Zhao",
            "Qian He"
        ],
        "tldr": "InstructX is a unified framework for image and video editing using MLLMs and diffusion models, demonstrating that training on image data can induce video editing capabilities and achieve state-of-the-art performance.",
        "tldr_zh": "InstructX是一个统一的图像和视频编辑框架，它使用MLLM和扩散模型，表明在图像数据上训练可以诱导视频编辑能力，并实现最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reinforcing Diffusion Models by Direct Group Preference Optimization",
        "summary": "While reinforcement learning methods such as Group Relative Preference\nOptimization (GRPO) have significantly enhanced Large Language Models, adapting\nthem to diffusion models remains challenging. In particular, GRPO demands a\nstochastic policy, yet the most cost-effective diffusion samplers are based on\ndeterministic ODEs. Recent work addresses this issue by using inefficient\nSDE-based samplers to induce stochasticity, but this reliance on model-agnostic\nGaussian noise leads to slow convergence. To resolve this conflict, we propose\nDirect Group Preference Optimization (DGPO), a new online RL algorithm that\ndispenses with the policy-gradient framework entirely. DGPO learns directly\nfrom group-level preferences, which utilize relative information of samples\nwithin groups. This design eliminates the need for inefficient stochastic\npolicies, unlocking the use of efficient deterministic ODE samplers and faster\ntraining. Extensive results show that DGPO trains around 20 times faster than\nexisting state-of-the-art methods and achieves superior performance on both\nin-domain and out-of-domain reward metrics. Code is available at\nhttps://github.com/Luo-Yihong/DGPO.",
        "url": "http://arxiv.org/abs/2510.08425v1",
        "published_date": "2025-10-09T16:40:43+00:00",
        "updated_date": "2025-10-09T16:40:43+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Yihong Luo",
            "Tianyang Hu",
            "Jing Tang"
        ],
        "tldr": "The paper introduces Direct Group Preference Optimization (DGPO), a new reinforcement learning algorithm for diffusion models that directly learns from group-level preferences, allowing for efficient deterministic sampling and faster training compared to existing methods.",
        "tldr_zh": "该论文介绍了一种新的扩散模型强化学习算法，即直接群组偏好优化（DGPO）。DGPO直接从群组层面的偏好中学习，从而可以使用高效的确定性采样方法并实现比现有方法更快的训练速度。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VideoVerse: How Far is Your T2V Generator from a World Model?",
        "summary": "The recent rapid advancement of Text-to-Video (T2V) generation technologies,\nwhich are critical to build ``world models'', makes the existing benchmarks\nincreasingly insufficient to evaluate state-of-the-art T2V models. First,\ncurrent evaluation dimensions, such as per-frame aesthetic quality and temporal\nconsistency, are no longer able to differentiate state-of-the-art T2V models.\nSecond, event-level temporal causality, which not only distinguishes video from\nother modalities but also constitutes a crucial component of world models, is\nseverely underexplored in existing benchmarks. Third, existing benchmarks lack\na systematic assessment of world knowledge, which are essential capabilities\nfor building world models. To address these issues, we introduce VideoVerse, a\ncomprehensive benchmark that focuses on evaluating whether a T2V model could\nunderstand complex temporal causality and world knowledge in the real world. We\ncollect representative videos across diverse domains (e.g., natural landscapes,\nsports, indoor scenes, science fiction, chemical and physical experiments) and\nextract their event-level descriptions with inherent temporal causality, which\nare then rewritten into text-to-video prompts by independent annotators. For\neach prompt, we design a suite of binary evaluation questions from the\nperspective of dynamic and static properties, with a total of ten carefully\ndefined evaluation dimensions. In total, our VideoVerse comprises 300 carefully\ncurated prompts, involving 815 events and 793 binary evaluation questions.\nConsequently, a human preference aligned QA-based evaluation pipeline is\ndeveloped by using modern vision-language models. Finally, we perform a\nsystematic evaluation of state-of-the-art open-source and closed-source T2V\nmodels on VideoVerse, providing in-depth analysis on how far the current T2V\ngenerators are from world models.",
        "url": "http://arxiv.org/abs/2510.08398v1",
        "published_date": "2025-10-09T16:18:20+00:00",
        "updated_date": "2025-10-09T16:18:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeqing Wang",
            "Xinyu Wei",
            "Bairui Li",
            "Zhen Guo",
            "Jinrui Zhang",
            "Hongyang Wei",
            "Keze Wang",
            "Lei Zhang"
        ],
        "tldr": "The paper introduces VideoVerse, a new benchmark for Text-to-Video models that assesses their understanding of temporal causality and world knowledge, addressing limitations in existing benchmarks.",
        "tldr_zh": "本文介绍了VideoVerse，一个新的文本到视频模型的基准，用于评估它们对时间因果关系和世界知识的理解，解决了现有基准的局限性",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Real-Time Motion-Controllable Autoregressive Video Diffusion",
        "summary": "Real-time motion-controllable video generation remains challenging due to the\ninherent latency of bidirectional diffusion models and the lack of effective\nautoregressive (AR) approaches. Existing AR video diffusion models are limited\nto simple control signals or text-to-video generation, and often suffer from\nquality degradation and motion artifacts in few-step generation. To address\nthese challenges, we propose AR-Drag, the first RL-enhanced few-step AR video\ndiffusion model for real-time image-to-video generation with diverse motion\ncontrol. We first fine-tune a base I2V model to support basic motion control,\nthen further improve it via reinforcement learning with a trajectory-based\nreward model. Our design preserves the Markov property through a Self-Rollout\nmechanism and accelerates training by selectively introducing stochasticity in\ndenoising steps. Extensive experiments demonstrate that AR-Drag achieves high\nvisual fidelity and precise motion alignment, significantly reducing latency\ncompared with state-of-the-art motion-controllable VDMs, while using only 1.3B\nparameters. Additional visualizations can be found on our project page:\nhttps://kesenzhao.github.io/AR-Drag.github.io/.",
        "url": "http://arxiv.org/abs/2510.08131v1",
        "published_date": "2025-10-09T12:17:11+00:00",
        "updated_date": "2025-10-09T12:17:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kesen Zhao",
            "Jiaxin Shi",
            "Beier Zhu",
            "Junbao Zhou",
            "Xiaolong Shen",
            "Yuan Zhou",
            "Qianru Sun",
            "Hanwang Zhang"
        ],
        "tldr": "The paper introduces AR-Drag, a reinforcement learning-enhanced autoregressive video diffusion model for real-time, motion-controllable image-to-video generation, achieving high fidelity and precise motion alignment with reduced latency and a relatively small parameter size.",
        "tldr_zh": "该论文介绍了 AR-Drag，一种强化学习增强的自回归视频扩散模型，用于实时、可运动控制的图像到视频生成，以相对较小的参数规模实现了高保真度和精确的运动对齐，并降低了延迟。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RetouchLLM: Training-free White-box Image Retouching",
        "summary": "Image retouching not only enhances visual quality but also serves as a means\nof expressing personal preferences and emotions. However, existing\nlearning-based approaches require large-scale paired data and operate as black\nboxes, making the retouching process opaque and limiting their adaptability to\nhandle diverse, user- or image-specific adjustments. In this work, we propose\nRetouchLLM, a training-free white-box image retouching system, which requires\nno training data and performs interpretable, code-based retouching directly on\nhigh-resolution images. Our framework progressively enhances the image in a\nmanner similar to how humans perform multi-step retouching, allowing\nexploration of diverse adjustment paths. It comprises of two main modules: a\nvisual critic that identifies differences between the input and reference\nimages, and a code generator that produces executable codes. Experiments\ndemonstrate that our approach generalizes well across diverse retouching\nstyles, while natural language-based user interaction enables interpretable and\ncontrollable adjustments tailored to user intent.",
        "url": "http://arxiv.org/abs/2510.08054v1",
        "published_date": "2025-10-09T10:40:49+00:00",
        "updated_date": "2025-10-09T10:40:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Moon Ye-Bin",
            "Roy Miles",
            "Tae-Hyun Oh",
            "Ismail Elezi",
            "Jiankang Deng"
        ],
        "tldr": "RetouchLLM is a training-free, white-box image retouching system that uses a visual critic and code generator to perform interpretable and controllable adjustments on high-resolution images, guided by natural language.",
        "tldr_zh": "RetouchLLM是一个无需训练的白盒图像修饰系统，它使用视觉评论器和代码生成器，在高分辨率图像上执行可解释和可控的调整，并由自然语言指导。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving",
        "summary": "Generative models have been widely applied to world modeling for environment\nsimulation and future state prediction. With advancements in autonomous\ndriving, there is a growing demand not only for high-fidelity video generation\nunder various controls, but also for producing diverse and meaningful\ninformation such as depth estimation. To address this, we propose CVD-STORM, a\ncross-view video diffusion model utilizing a spatial-temporal reconstruction\nVariational Autoencoder (VAE) that generates long-term, multi-view videos with\n4D reconstruction capabilities under various control inputs. Our approach first\nfine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its\nability to encode 3D structures and temporal dynamics. Subsequently, we\nintegrate this VAE into the video diffusion process to significantly improve\ngeneration quality. Experimental results demonstrate that our model achieves\nsubstantial improvements in both FID and FVD metrics. Additionally, the\njointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic\nscenes, providing valuable geometric information for comprehensive scene\nunderstanding.",
        "url": "http://arxiv.org/abs/2510.07944v1",
        "published_date": "2025-10-09T08:41:58+00:00",
        "updated_date": "2025-10-09T08:41:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianrui Zhang",
            "Yichen Liu",
            "Zilin Guo",
            "Yuxin Guo",
            "Jingcheng Ni",
            "Chenjing Ding",
            "Dan Xu",
            "Lewei Lu",
            "Zehuan Wu"
        ],
        "tldr": "The paper introduces CVD-STORM, a cross-view video diffusion model for autonomous driving that uses a spatial-temporal reconstruction VAE to generate long-term, multi-view videos with 4D reconstruction capabilities, achieving improved FID and FVD scores.",
        "tldr_zh": "该论文介绍了CVD-STORM，一个用于自动驾驶的跨视角视频扩散模型，它使用一个时空重建VAE来生成具有4D重建能力的长时程、多视角视频，并实现了改进的FID和FVD分数。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TTOM: Test-Time Optimization and Memorization for Compositional Video Generation",
        "summary": "Video Foundation Models (VFMs) exhibit remarkable visual generation\nperformance, but struggle in compositional scenarios (e.g., motion, numeracy,\nand spatial relation). In this work, we introduce Test-Time Optimization and\nMemorization (TTOM), a training-free framework that aligns VFM outputs with\nspatiotemporal layouts during inference for better text-image alignment. Rather\nthan direct intervention to latents or attention per-sample in existing work,\nwe integrate and optimize new parameters guided by a general layout-attention\nobjective. Furthermore, we formulate video generation within a streaming\nsetting, and maintain historical optimization contexts with a parametric memory\nmechanism that supports flexible operations, such as insert, read, update, and\ndelete. Notably, we found that TTOM disentangles compositional world knowledge,\nshowing powerful transferability and generalization. Experimental results on\nthe T2V-CompBench and Vbench benchmarks establish TTOM as an effective,\npractical, scalable, and efficient framework to achieve cross-modal alignment\nfor compositional video generation on the fly.",
        "url": "http://arxiv.org/abs/2510.07940v1",
        "published_date": "2025-10-09T08:37:00+00:00",
        "updated_date": "2025-10-09T08:37:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Leigang Qu",
            "Ziyang Wang",
            "Na Zheng",
            "Wenjie Wang",
            "Liqiang Nie",
            "Tat-Seng Chua"
        ],
        "tldr": "The paper introduces TTOM, a training-free framework that aligns Video Foundation Model outputs with spatiotemporal layouts during inference using optimization and memorization techniques, achieving better text-image alignment in compositional video generation tasks.",
        "tldr_zh": "该论文介绍了TTOM，一个无需训练的框架，利用优化和记忆技术在推理过程中将视频基础模型输出与时空布局对齐，从而在组合视频生成任务中实现更好的文本-图像对齐。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MONKEY: Masking ON KEY-Value Activation Adapter for Personalization",
        "summary": "Personalizing diffusion models allows users to generate new images that\nincorporate a given subject, allowing more control than a text prompt. These\nmodels often suffer somewhat when they end up just recreating the subject\nimage, and ignoring the text prompt. We observe that one popular method for\npersonalization, the IP-Adapter automatically generates masks that we\ndefinitively segment the subject from the background during inference. We\npropose to use this automatically generated mask on a second pass to mask the\nimage tokens, thus restricting them to the subject, not the background,\nallowing the text prompt to attend to the rest of the image. For text prompts\ndescribing locations and places, this produces images that accurately depict\nthe subject while definitively matching the prompt. We compare our method to a\nfew other test time personalization methods, and find our method displays high\nprompt and source image alignment.",
        "url": "http://arxiv.org/abs/2510.07656v1",
        "published_date": "2025-10-09T01:20:06+00:00",
        "updated_date": "2025-10-09T01:20:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "James Baker"
        ],
        "tldr": "The paper introduces MONKEY, a method that improves personalized image generation in diffusion models by masking image tokens based on automatically generated subject masks, enhancing prompt alignment while preserving subject fidelity.",
        "tldr_zh": "本文介绍了一种名为MONKEY的方法，该方法通过基于自动生成的主题掩码来屏蔽图像令牌，从而改进扩散模型中的个性化图像生成，增强提示对齐，同时保持主题保真度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Rectified-CFG++ for Flow Based Models",
        "summary": "Classifier-free guidance (CFG) is the workhorse for steering large diffusion\nmodels toward text-conditioned targets, yet its native application to rectified\nflow (RF) based models provokes severe off-manifold drift, yielding visual\nartifacts, text misalignment, and brittle behaviour. We present\nRectified-CFG++, an adaptive predictor-corrector guidance that couples the\ndeterministic efficiency of rectified flows with a geometry-aware conditioning\nrule. Each inference step first executes a conditional RF update that anchors\nthe sample near the learned transport path, then applies a weighted conditional\ncorrection that interpolates between conditional and unconditional velocity\nfields. We prove that the resulting velocity field is marginally consistent and\nthat its trajectories remain within a bounded tubular neighbourhood of the data\nmanifold, ensuring stability across a wide range of guidance strengths.\nExtensive experiments on large-scale text-to-image models (Flux, Stable\nDiffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms\nstandard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and\nT2I-CompBench. Project page: https://rectified-cfgpp.github.io/",
        "url": "http://arxiv.org/abs/2510.07631v1",
        "published_date": "2025-10-09T00:00:47+00:00",
        "updated_date": "2025-10-09T00:00:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shreshth Saini",
            "Shashank Gupta",
            "Alan C. Bovik"
        ],
        "tldr": "The paper introduces Rectified-CFG++, an improved classifier-free guidance method for rectified flow models that addresses off-manifold drift and enhances text-to-image generation quality, demonstrating superior performance on large-scale models.",
        "tldr_zh": "该论文介绍了Rectified-CFG++，一种改进的用于校正流模型的无分类器指导方法，解决了流形漂移问题，并提高了文本到图像的生成质量，并在大型模型上表现出卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PickStyle: Video-to-Video Style Transfer with Context-Style Adapters",
        "summary": "We address the task of video style transfer with diffusion models, where the\ngoal is to preserve the context of an input video while rendering it in a\ntarget style specified by a text prompt. A major challenge is the lack of\npaired video data for supervision. We propose PickStyle, a video-to-video style\ntransfer framework that augments pretrained video diffusion backbones with\nstyle adapters and benefits from paired still image data with source-style\ncorrespondences for training. PickStyle inserts low-rank adapters into the\nself-attention layers of conditioning modules, enabling efficient\nspecialization for motion-style transfer while maintaining strong alignment\nbetween video content and style. To bridge the gap between static image\nsupervision and dynamic video, we construct synthetic training clips from\npaired images by applying shared augmentations that simulate camera motion,\nensuring temporal priors are preserved. In addition, we introduce Context-Style\nClassifier-Free Guidance (CS-CFG), a novel factorization of classifier-free\nguidance into independent text (style) and video (context) directions. CS-CFG\nensures that context is preserved in generated video while the style is\neffectively transferred. Experiments across benchmarks show that our approach\nachieves temporally coherent, style-faithful, and content-preserving video\ntranslations, outperforming existing baselines both qualitatively and\nquantitatively.",
        "url": "http://arxiv.org/abs/2510.07546v1",
        "published_date": "2025-10-08T21:02:55+00:00",
        "updated_date": "2025-10-08T21:02:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Soroush Mehraban",
            "Vida Adeli",
            "Jacob Rommann",
            "Babak Taati",
            "Kyryl Truskovskyi"
        ],
        "tldr": "PickStyle proposes a video style transfer framework using diffusion models and style adapters trained on paired image data with synthetic motion augmentations and a novel Context-Style Classifier-Free Guidance to achieve temporally coherent and style-accurate video translations.",
        "tldr_zh": "PickStyle 提出了一种视频风格迁移框架，它使用扩散模型和风格适配器，在配对的图像数据上进行训练，并结合合成运动增强和一个新颖的上下文-风格无分类器指导（Context-Style Classifier-Free Guidance）来实现时间上连贯且风格准确的视频转换。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models",
        "summary": "Traditional multimodal learners find unified representations for tasks like\nvisual question answering, but rely heavily on paired datasets. However, an\noverlooked yet potentially powerful question is: can one leverage auxiliary\nunpaired multimodal data to directly enhance representation learning in a\ntarget modality? We introduce UML: Unpaired Multimodal Learner, a\nmodality-agnostic training paradigm in which a single model alternately\nprocesses inputs from different modalities while sharing parameters across\nthem. This design exploits the assumption that different modalities are\nprojections of a shared underlying reality, allowing the model to benefit from\ncross-modal structure without requiring explicit pairs. Theoretically, under\nlinear data-generating assumptions, we show that unpaired auxiliary data can\nyield representations strictly more informative about the data-generating\nprocess than unimodal training. Empirically, we show that using unpaired data\nfrom auxiliary modalities -- such as text, audio, or images -- consistently\nimproves downstream performance across diverse unimodal targets such as image\nand audio. Our project page: https://unpaired-multimodal.github.io/",
        "url": "http://arxiv.org/abs/2510.08492v1",
        "published_date": "2025-10-09T17:32:23+00:00",
        "updated_date": "2025-10-09T17:32:23+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Sharut Gupta",
            "Shobhita Sundaram",
            "Chenyu Wang",
            "Stefanie Jegelka",
            "Phillip Isola"
        ],
        "tldr": "The paper introduces a new training paradigm, UML, that leverages unpaired multimodal data to improve unimodal representation learning by sharing parameters across modalities, showing improved performance in image and audio tasks.",
        "tldr_zh": "该论文介绍了一种新的训练范式 UML，它利用非配对的多模态数据，通过跨模态共享参数来提高单模态表征学习，并在图像和音频任务中表现出性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion Models for Text-Guided Image Inpainting",
        "summary": "Text-guided image inpainting aims at reconstructing the masked regions as per\ntext prompts, where the longstanding challenges lie in the preservation for\nunmasked regions, while achieving the semantics consistency between unmasked\nand inpainted masked regions. Previous arts failed to address both of them,\nalways with either of them to be remedied. Such facts, as we observed, stem\nfrom the entanglement of the hybrid (e.g., mid-and-low) frequency bands that\nencode varied image properties, which exhibit different robustness to text\nprompts during the denoising process. In this paper, we propose a\nnull-text-null frequency-aware diffusion models, dubbed \\textbf{NTN-Diff}, for\ntext-guided image inpainting, by decomposing the semantics consistency across\nmasked and unmasked regions into the consistencies as per each frequency band,\nwhile preserving the unmasked regions, to circumvent two challenges in a row.\nBased on the diffusion process, we further divide the denoising process into\nearly (high-level noise) and late (low-level noise) stages, where the\nmid-and-low frequency bands are disentangled during the denoising process. As\nobserved, the stable mid-frequency band is progressively denoised to be\nsemantically aligned during text-guided denoising process, which, meanwhile,\nserves as the guidance to the null-text denoising process to denoise\nlow-frequency band for the masked regions, followed by a subsequent text-guided\ndenoising process at late stage, to achieve the semantics consistency for\nmid-and-low frequency bands across masked and unmasked regions, while preserve\nthe unmasked regions. Extensive experiments validate the superiority of\nNTN-Diff over the state-of-the-art diffusion models to text-guided diffusion\nmodels. Our code can be accessed from https://github.com/htyjers/NTN-Diff.",
        "url": "http://arxiv.org/abs/2510.08273v1",
        "published_date": "2025-10-09T14:30:34+00:00",
        "updated_date": "2025-10-09T14:30:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haipeng Liu",
            "Yang Wang",
            "Meng Wang"
        ],
        "tldr": "The paper introduces NTN-Diff, a novel diffusion model for text-guided image inpainting that disentangles frequency bands during denoising to balance reconstruction quality and semantic consistency between masked and unmasked regions. It claims to outperform existing diffusion models in this task.",
        "tldr_zh": "该论文介绍了一种名为 NTN-Diff 的新型扩散模型，用于文本引导的图像修复。该模型在去噪过程中解耦了不同频率段，以平衡重建质量和被遮盖与未遮盖区域之间的语义一致性。 它声称在这项任务中优于现有的扩散模型。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "InstructUDrag: Joint Text Instructions and Object Dragging for Interactive Image Editing",
        "summary": "Text-to-image diffusion models have shown great potential for image editing,\nwith techniques such as text-based and object-dragging methods emerging as key\napproaches. However, each of these methods has inherent limitations: text-based\nmethods struggle with precise object positioning, while object dragging methods\nare confined to static relocation. To address these issues, we propose\nInstructUDrag, a diffusion-based framework that combines text instructions with\nobject dragging, enabling simultaneous object dragging and text-based image\nediting. Our framework treats object dragging as an image reconstruction\nprocess, divided into two synergistic branches. The moving-reconstruction\nbranch utilizes energy-based gradient guidance to move objects accurately,\nrefining cross-attention maps to enhance relocation precision. The text-driven\nediting branch shares gradient signals with the reconstruction branch, ensuring\nconsistent transformations and allowing fine-grained control over object\nattributes. We also employ DDPM inversion and inject prior information into\nnoise maps to preserve the structure of moved objects. Extensive experiments\ndemonstrate that InstructUDrag facilitates flexible, high-fidelity image\nediting, offering both precision in object relocation and semantic control over\nimage content.",
        "url": "http://arxiv.org/abs/2510.08181v1",
        "published_date": "2025-10-09T13:06:49+00:00",
        "updated_date": "2025-10-09T13:06:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoran Yu",
            "Yi Shi"
        ],
        "tldr": "The paper introduces InstructUDrag, a diffusion-based image editing framework combining text instructions and object dragging for precise object relocation and semantic control of image content.",
        "tldr_zh": "该论文介绍了InstructUDrag，一个基于扩散模型的图像编辑框架，它结合了文本指令和对象拖动，以实现精确的对象重定位和图像内容的语义控制。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints",
        "summary": "Compositional training has been the de-facto paradigm in existing Multimodal\nLarge Language Models (MLLMs), where pre-trained vision encoders are connected\nwith pre-trained LLMs through continuous multimodal pre-training. However, the\nmultimodal scaling property of this paradigm remains difficult to explore due\nto the separated training. In this paper, we focus on the native training of\nMLLMs in an end-to-end manner and systematically study its design space and\nscaling property under a practical setting, i.e., data constraint. Through\ncareful study of various choices in MLLM, we obtain the optimal\nmeta-architecture that best balances performance and training cost. After that,\nwe further explore the scaling properties of the native MLLM and indicate the\npositively correlated scaling relationship between visual encoders and LLMs.\nBased on these findings, we propose a native MLLM called NaViL, combined with a\nsimple and cost-effective recipe. Experimental results on 14 multimodal\nbenchmarks confirm the competitive performance of NaViL against existing MLLMs.\nBesides that, our findings and results provide in-depth insights for the future\nstudy of native MLLMs.",
        "url": "http://arxiv.org/abs/2510.08565v1",
        "published_date": "2025-10-09T17:59:37+00:00",
        "updated_date": "2025-10-09T17:59:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changyao Tian",
            "Hao Li",
            "Gen Luo",
            "Xizhou Zhu",
            "Weijie Su",
            "Hanming Deng",
            "Jinguo Zhu",
            "Jie Shao",
            "Ziran Zhu",
            "Yunpeng Liu",
            "Lewei Lu",
            "Wenhai Wang",
            "Hongsheng Li",
            "Jifeng Dai"
        ],
        "tldr": "This paper explores native, end-to-end training of Multimodal Large Language Models (MLLMs) under data constraints, identifying an optimal meta-architecture and scaling relationships to achieve competitive performance on multimodal benchmarks.",
        "tldr_zh": "本文探讨了在数据约束下，多模态大型语言模型（MLLM）的原生端到端训练，确定了最佳的元架构和缩放关系，从而在多模态基准测试中实现了具有竞争力的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning",
        "summary": "In web data, product images are central to boosting user engagement and\nadvertising efficacy on e-commerce platforms, yet the intrusive elements such\nas watermarks and promotional text remain major obstacles to delivering clear\nand appealing product visuals. Although diffusion-based inpainting methods have\nadvanced, they still face challenges in commercial settings due to unreliable\nobject removal and limited domain-specific adaptation. To tackle these\nchallenges, we propose Repainter, a reinforcement learning framework that\nintegrates spatial-matting trajectory refinement with Group Relative Policy\nOptimization (GRPO). Our approach modulates attention mechanisms to emphasize\nbackground context, generating higher-reward samples and reducing unwanted\nobject insertion. We also introduce a composite reward mechanism that balances\nglobal, local, and semantic constraints, effectively reducing visual artifacts\nand reward hacking. Additionally, we contribute EcomPaint-100K, a high-quality,\nlarge-scale e-commerce inpainting dataset, and a standardized benchmark\nEcomPaint-Bench for fair evaluation. Extensive experiments demonstrate that\nRepainter significantly outperforms state-of-the-art methods, especially in\nchallenging scenes with intricate compositions. We will release our code and\nweights upon acceptance.",
        "url": "http://arxiv.org/abs/2510.07721v1",
        "published_date": "2025-10-09T02:57:33+00:00",
        "updated_date": "2025-10-09T02:57:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zipeng Guo",
            "Lichen Ma",
            "Xiaolong Fu",
            "Gaojing Zhou",
            "Lan Yang",
            "Yuchen Zhou",
            "Linkai Liu",
            "Yu He",
            "Ximan Liu",
            "Shiping Dong",
            "Jingling Fu",
            "Zhen Chen",
            "Yu Shi",
            "Junshi Huang",
            "Jason Li",
            "Chao Gou"
        ],
        "tldr": "The paper introduces RePainter, a reinforcement learning framework for removing unwanted objects like watermarks from e-commerce product images, along with a new dataset and benchmark for this task.",
        "tldr_zh": "该论文介绍了 RePainter，一个用于从电子商务产品图像中移除水印等不需要对象的强化学习框架，并为此任务提供了一个新的数据集和基准测试。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes",
        "summary": "Gaussian Splatting (GS) enables immersive rendering, but realistic 3D\nobject-scene composition remains challenging. Baked appearance and shadow\ninformation in GS radiance fields cause inconsistencies when combining objects\nand scenes. Addressing this requires relightable object reconstruction and\nscene lighting estimation. For relightable object reconstruction, existing\nGaussian-based inverse rendering methods often rely on ray tracing, leading to\nlow efficiency. We introduce Surface Octahedral Probes (SOPs), which store\nlighting and occlusion information and allow efficient 3D querying via\ninterpolation, avoiding expensive ray tracing. SOPs provide at least a 2x\nspeedup in reconstruction and enable real-time shadow computation in Gaussian\nscenes. For lighting estimation, existing Gaussian-based inverse rendering\nmethods struggle to model intricate light transport and often fail in complex\nscenes, while learning-based methods predict lighting from a single image and\nare viewpoint-sensitive. We observe that 3D object-scene composition primarily\nconcerns the object's appearance and nearby shadows. Thus, we simplify the\nchallenging task of full scene lighting estimation by focusing on the\nenvironment lighting at the object's placement. Specifically, we capture a 360\ndegrees reconstructed radiance field of the scene at the location and fine-tune\na diffusion model to complete the lighting. Building on these advances, we\npropose ComGS, a novel 3D object-scene composition framework. Our method\nachieves high-quality, real-time rendering at around 28 FPS, produces visually\nharmonious results with vivid shadows, and requires only 36 seconds for\nediting. Code and dataset are available at\nhttps://nju-3dv.github.io/projects/ComGS/.",
        "url": "http://arxiv.org/abs/2510.07729v1",
        "published_date": "2025-10-09T03:10:41+00:00",
        "updated_date": "2025-10-09T03:10:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jian Gao",
            "Mengqi Yuan",
            "Yifei Zeng",
            "Chang Zeng",
            "Zhihao Li",
            "Zhenyu Chen",
            "Weichao Qiu",
            "Xiao-Xiao Long",
            "Hao Zhu",
            "Xun Cao",
            "Yao Yao"
        ],
        "tldr": "The paper introduces ComGS, a framework for efficient 3D object-scene composition using Surface Octahedral Probes (SOPs) for relightable object reconstruction and diffusion models for lighting estimation, enabling real-time rendering with realistic shadows.",
        "tldr_zh": "该论文介绍了ComGS，一个用于高效3D物体-场景合成的框架。它采用表面八面体探针（SOPs）进行可重新光照的物体重建，并使用扩散模型进行光照估计，从而实现具有真实阴影的实时渲染。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Hierarchical Spatial Algorithms for High-Resolution Image Quantization and Feature Extraction",
        "summary": "This study introduces a modular framework for spatial image processing,\nintegrating grayscale quantization, color and brightness enhancement, image\nsharpening, bidirectional transformation pipelines, and geometric feature\nextraction. A stepwise intensity transformation quantizes grayscale images into\neight discrete levels, producing a posterization effect that simplifies\nrepresentation while preserving structural detail. Color enhancement is\nachieved via histogram equalization in both RGB and YCrCb color spaces, with\nthe latter improving contrast while maintaining chrominance fidelity.\nBrightness adjustment is implemented through HSV value-channel manipulation,\nand image sharpening is performed using a 3 * 3 convolution kernel to enhance\nhigh-frequency details. A bidirectional transformation pipeline that integrates\nunsharp masking, gamma correction, and noise amplification achieved accuracy\nlevels of 76.10% and 74.80% for the forward and reverse processes,\nrespectively. Geometric feature extraction employed Canny edge detection,\nHough-based line estimation (e.g., 51.50{\\deg} for billiard cue alignment),\nHarris corner detection, and morphological window localization. Cue isolation\nfurther yielded 81.87\\% similarity against ground truth images. Experimental\nevaluation across diverse datasets demonstrates robust and deterministic\nperformance, highlighting its potential for real-time image analysis and\ncomputer vision.",
        "url": "http://arxiv.org/abs/2510.08449v1",
        "published_date": "2025-10-09T16:56:24+00:00",
        "updated_date": "2025-10-09T16:56:24+00:00",
        "categories": [
            "cs.CV",
            "68T45, 68U10",
            "I.4.8; I.2.10"
        ],
        "authors": [
            "Noor Islam S. Mohammad"
        ],
        "tldr": "The paper presents a modular image processing framework encompassing quantization, enhancement, sharpening, bidirectional transformations, and geometric feature extraction, demonstrating robust performance across diverse datasets. It focuses on traditional image processing techniques rather than generation.",
        "tldr_zh": "该论文提出了一个模块化的图像处理框架，包括量化、增强、锐化、双向变换和几何特征提取，并在不同的数据集上表现出稳健的性能。它侧重于传统的图像处理技术，而不是生成。",
        "relevance_score": 2,
        "novelty_claim_score": 4,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 3
    }
]