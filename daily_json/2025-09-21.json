[
    {
        "title": "Animalbooth: multimodal feature enhancement for animal subject personalization",
        "summary": "Personalized animal image generation is challenging due to rich appearance\ncues and large morphological variability. Existing approaches often exhibit\nfeature misalignment across domains, which leads to identity drift. We present\nAnimalBooth, a framework that strengthens identity preservation with an Animal\nNet and an adaptive attention module, mitigating cross domain alignment errors.\nWe further introduce a frequency controlled feature integration module that\napplies Discrete Cosine Transform filtering in the latent space to guide the\ndiffusion process, enabling a coarse to fine progression from global structure\nto detailed texture. To advance research in this area, we curate AnimalBench, a\nhigh resolution dataset for animal personalization. Extensive experiments show\nthat AnimalBooth consistently outperforms strong baselines on multiple\nbenchmarks and improves both identity fidelity and perceptual quality.",
        "url": "http://arxiv.org/abs/2509.16702v1",
        "published_date": "2025-09-20T14:09:48+00:00",
        "updated_date": "2025-09-20T14:09:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Liu",
            "Haitao Wu",
            "Kafeng Wang",
            "Xiaowang Zhang"
        ],
        "tldr": "AnimalBooth improves personalized animal image generation by addressing feature misalignment with a novel framework including Animal Net, attention mechanisms, and frequency-controlled feature integration, supported by a new high-resolution dataset. It outperforms existing methods in identity fidelity and perceptual quality.",
        "tldr_zh": "AnimalBooth通过一个新颖的框架，包含动物网络、注意力机制和频率控制的特征整合，解决了特征不对齐的问题，从而改进了个性化动物图像生成。 论文还提供了一个新的高分辨率数据集，并在身份保真度和感知质量方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation",
        "summary": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/.",
        "url": "http://arxiv.org/abs/2509.16630v1",
        "published_date": "2025-09-20T11:09:01+00:00",
        "updated_date": "2025-09-20T11:09:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yue Ma",
            "Zexuan Yan",
            "Hongyu Liu",
            "Hongfa Wang",
            "Heng Pan",
            "Yingqing He",
            "Junkun Yuan",
            "Ailing Zeng",
            "Chengfei Cai",
            "Heung-Yeung Shum",
            "Zhifeng Li",
            "Wei Liu",
            "Linfeng Zhang",
            "Qifeng Chen"
        ],
        "tldr": "The paper introduces Follow-Your-Emoji-Faster, a diffusion-based framework for efficient and controllable portrait animation using facial landmarks, addressing identity preservation, expression transfer, and temporal consistency with a novel progressive generation strategy and Taylor-interpolated cache for acceleration.",
        "tldr_zh": "该论文介绍了Follow-Your-Emoji-Faster，一个基于扩散模型的框架，利用面部特征点实现高效且可控的人像动画，通过渐进式生成策略和泰勒插值缓存加速，解决了身份保持、表情迁移和时间一致性问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ViTCAE: ViT-based Class-conditioned Autoencoder",
        "summary": "Vision Transformer (ViT) based autoencoders often underutilize the global\nClass token and employ static attention mechanisms, limiting both generative\ncontrol and optimization efficiency. This paper introduces ViTCAE, a framework\nthat addresses these issues by re-purposing the Class token into a generative\nlinchpin. In our architecture, the encoder maps the Class token to a global\nlatent variable that dictates the prior distribution for local, patch-level\nlatent variables, establishing a robust dependency where global semantics\ndirectly inform the synthesis of local details. Drawing inspiration from\nopinion dynamics, we treat each attention head as a dynamical system of\ninteracting tokens seeking consensus. This perspective motivates a\nconvergence-aware temperature scheduler that adaptively anneals each head's\ninfluence function based on its distributional stability. This process enables\na principled head-freezing mechanism, guided by theoretically-grounded\ndiagnostics like an attention evolution distance and a consensus/cluster\nfunctional. This technique prunes converged heads during training to\nsignificantly improve computational efficiency without sacrificing fidelity. By\nunifying a generative Class token with an adaptive attention mechanism rooted\nin multi-agent consensus theory, ViTCAE offers a more efficient and\ncontrollable approach to transformer-based generation.",
        "url": "http://arxiv.org/abs/2509.16554v1",
        "published_date": "2025-09-20T06:48:45+00:00",
        "updated_date": "2025-09-20T06:48:45+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Vahid Jebraeeli",
            "Hamid Krim",
            "Derya Cansever"
        ],
        "tldr": "The paper introduces ViTCAE, a Vision Transformer autoencoder architecture that improves generative control and efficiency by repurposing the Class token and implementing an adaptive, consensus-based attention mechanism. It achieves computational efficiency through a head-freezing mechanism guided by theoretical diagnostics.",
        "tldr_zh": "这篇论文介绍了ViTCAE，一种基于视觉Transformer的自编码器架构，通过重新利用类别token并实施基于共识的自适应注意力机制，提高了生成控制和效率。它通过理论诊断指导的头部冻结机制实现计算效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers",
        "summary": "Generating realistic videos with diffusion transformers demands significant\ncomputation, with attention layers the central bottleneck; even producing a\nshort clip requires running a transformer over a very long sequence of\nembeddings, e.g., more than 30K embeddings for a 5-second video, incurring\nsignificant latency. Prior work aims to mitigate this bottleneck by exploiting\nsparsity in the attention layers to reduce computation. However, these works\ntypically rely on block-sparse attention, which skips score computation only\nwhen all entries in a block of attention scores (corresponding to M queries and\nM keys, with M = 64 typically) are zero. This coarse-granular skipping of\nattention scores does not fully exploit sparsity in the attention map and\nleaves room for improvement. In this work, we propose FG-Attn, a sparse\nattention mechanism for long-context diffusion transformers that leverages\nsparsity at a fine granularity. Unlike block-sparse attention, which skips\nentire MxM blocks, our approach skips computations at the granularity of Mx1\nslices of the attention map. Each slice is produced by query-key dot products\nbetween a block of query vectors and a single key. To implement our proposed\nsparse attention mechanism, we develop a new efficient bulk-load operation\ncalled asynchronous-gather load. This load operation gathers a sparse set of\nrelevant key-value vectors from memory and arranges them into packed tiles in\nthe GPU's shared memory. Only a sparse set of keys relevant to those queries\nare loaded into shared memory when computing attention for a block of queries,\nin contrast to loading full blocks of key tokens in block-sparse attention. Our\nfine-grained sparse attention, applied to video diffusion models, achieves an\naverage 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average\n1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.",
        "url": "http://arxiv.org/abs/2509.16518v1",
        "published_date": "2025-09-20T03:48:32+00:00",
        "updated_date": "2025-09-20T03:48:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AR"
        ],
        "authors": [
            "Sankeerth Durvasula",
            "Kavya Sreedhar",
            "Zain Moustafa",
            "Suraj Kothawade",
            "Ashish Gondimalla",
            "Suvinay Subramanian",
            "Narges Shahidi",
            "Nandita Vijaykumar"
        ],
        "tldr": "The paper introduces FG-Attn, a fine-grained sparse attention mechanism for diffusion transformers used in video generation, achieving significant speedups compared to block-sparse attention by selectively loading relevant key-value vectors into GPU shared memory.",
        "tldr_zh": "该论文介绍了FG-Attn，一种用于视频生成中扩散Transformer的细粒度稀疏注意力机制。通过选择性地将相关的键值向量加载到GPU共享内存中，相比于块稀疏注意力，它实现了显著的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Introducing Resizable Region Packing Problem in Image Generation, with a Heuristic Solution",
        "summary": "The problem of image data generation in computer vision has traditionally\nbeen a harder problem to solve, than discriminative problems. Such data\ngeneration entails placing relevant objects of appropriate sizes each, at\nmeaningful location in a scene canvas. There have been two classes of popular\napproaches to such generation: graphics based, and generative models-based.\nOptimization problems are known to lurk in the background for both these\nclasses of approaches. In this paper, we introduce a novel, practically useful\nmanifestation of the classical Bin Packing problem in the context of generation\nof synthetic image data. We conjecture that the newly introduced problem,\nResizable Anchored Region Packing(RARP) Problem, is NP-hard, and provide\ndetailed arguments about our conjecture. As a first solution, we present a\nnovel heuristic algorithm that is generic enough and therefore scales and packs\narbitrary number of arbitrary-shaped regions at arbitrary locations, into an\nimage canvas. The algorithm follows greedy approach to iteratively pack region\npairs in a careful way, while obeying the optimization constraints. The\nalgorithm is validated by an implementation that was used to generate a\nlarge-scale synthetic anomaly detection dataset, with highly varying degree of\nbin packing parameters per image sample i.e. RARP instance. Visual inspection\nof such data and checking of the correctness of each solution proves the\neffectiveness of our algorithm. With generative modeling being on rise in deep\nlearning, and synthetic data generation poised to become mainstream, we expect\nthat the newly introduced problem will be valued in the imaging scientific\ncommunity.",
        "url": "http://arxiv.org/abs/2509.16363v1",
        "published_date": "2025-09-19T19:11:31+00:00",
        "updated_date": "2025-09-19T19:11:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hrishikesh Sharma"
        ],
        "tldr": "This paper introduces a novel optimization problem called Resizable Anchored Region Packing (RARP) in image generation, proposes a heuristic solution, and validates it by generating a synthetic anomaly detection dataset.",
        "tldr_zh": "本文介绍了一种新的图像生成优化问题，称为可调整大小的锚定区域 packing (RARP)，提出了一种启发式解决方案，并通过生成合成异常检测数据集对其进行验证。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Neural Atlas Graphs for Dynamic Scene Decomposition and Editing",
        "summary": "Learning editable high-resolution scene representations for dynamic scenes is\nan open problem with applications across the domains from autonomous driving to\ncreative editing - the most successful approaches today make a trade-off\nbetween editability and supporting scene complexity: neural atlases represent\ndynamic scenes as two deforming image layers, foreground and background, which\nare editable in 2D, but break down when multiple objects occlude and interact.\nIn contrast, scene graph models make use of annotated data such as masks and\nbounding boxes from autonomous-driving datasets to capture complex 3D spatial\nrelationships, but their implicit volumetric node representations are\nchallenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a\nhybrid high-resolution scene representation, where every graph node is a\nview-dependent neural atlas, facilitating both 2D appearance editing and 3D\nordering and positioning of scene elements. Fit at test-time, NAGs achieve\nstate-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR\nincrease compared to existing methods - and make environmental editing possible\nin high resolution and visual quality - creating counterfactual driving\nscenarios with new backgrounds and edited vehicle appearance. We find that the\nmethod also generalizes beyond driving scenes and compares favorably - by more\nthan 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS\nvideo dataset with a diverse set of human and animal-centric scenes.",
        "url": "http://arxiv.org/abs/2509.16336v1",
        "published_date": "2025-09-19T18:24:41+00:00",
        "updated_date": "2025-09-19T18:24:41+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jan Philipp Schneider",
            "Pratik Singh Bisht",
            "Ilya Chugunov",
            "Andreas Kolb",
            "Michael Moeller",
            "Felix Heide"
        ],
        "tldr": "The paper introduces Neural Atlas Graphs (NAGs), a novel hybrid scene representation combining neural atlases and scene graphs for editable dynamic scenes, achieving state-of-the-art results on driving and video datasets.",
        "tldr_zh": "本文介绍了神经图谱图（NAGs），一种新的混合场景表示方法，结合了神经图谱和场景图，用于可编辑的动态场景，并在驾驶和视频数据集上取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "V-CECE: Visual Counterfactual Explanations via Conceptual Edits",
        "summary": "Recent black-box counterfactual generation frameworks fail to take into\naccount the semantic content of the proposed edits, while relying heavily on\ntraining to guide the generation process. We propose a novel, plug-and-play\nblack-box counterfactual generation framework, which suggests step-by-step\nedits based on theoretical guarantees of optimal edits to produce human-level\ncounterfactual explanations with zero training. Our framework utilizes a\npre-trained image editing diffusion model, and operates without access to the\ninternals of the classifier, leading to an explainable counterfactual\ngeneration process. Throughout our experimentation, we showcase the explanatory\ngap between human reasoning and neural model behavior by utilizing both\nConvolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision\nLanguage Model (LVLM) classifiers, substantiated through a comprehensive human\nevaluation.",
        "url": "http://arxiv.org/abs/2509.16567v1",
        "published_date": "2025-09-20T07:53:06+00:00",
        "updated_date": "2025-09-20T07:53:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nikolaos Spanos",
            "Maria Lymperaiou",
            "Giorgos Filandrianos",
            "Konstantinos Thomas",
            "Athanasios Voulodimos",
            "Giorgos Stamou"
        ],
        "tldr": "The paper introduces V-CECE, a training-free, plug-and-play framework for generating visual counterfactual explanations by suggesting semantically meaningful image edits using a pre-trained diffusion model, effectively explaining black-box classifier decisions.",
        "tldr_zh": "该论文介绍 V-CECE，一个无需训练的即插即用框架，用于通过使用预训练的扩散模型生成语义上有意义的图像编辑来生成视觉反事实解释，从而有效地解释黑盒分类器的决策。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "The Iconicity of the Generated Image",
        "summary": "How humans interpret and produce images is influenced by the images we have\nbeen exposed to. Similarly, visual generative AI models are exposed to many\ntraining images and learn to generate new images based on this. Given the\nimportance of iconic images in human visual communication, as they are widely\nseen, reproduced, and used as inspiration, we may expect that they may\nsimilarly have a proportionally large influence within the generative AI\nprocess. In this work we explore this question through a three-part analysis,\ninvolving data attribution, semantic similarity analysis, and a user-study. Our\nfindings indicate that iconic images do not have an obvious influence on the\ngenerative process, and that for many icons it is challenging to reproduce an\nimage which resembles it closely. This highlights an important difference in\nhow humans and visual generative AI models draw on and learn from prior visual\ncommunication.",
        "url": "http://arxiv.org/abs/2509.16473v1",
        "published_date": "2025-09-19T23:59:43+00:00",
        "updated_date": "2025-09-19T23:59:43+00:00",
        "categories": [
            "cs.CY",
            "cs.CV"
        ],
        "authors": [
            "Nanne van Noord",
            "Noa Garcia"
        ],
        "tldr": "This paper investigates the influence of iconic images on visual generative AI models, finding that these images do not have a proportionally large influence and are difficult to reproduce closely compared to human perception.",
        "tldr_zh": "本文研究了标志性图像对视觉生成AI模型的影响，发现这些图像并没有产生相应的巨大影响，并且与人类的感知相比，很难精确地复制。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR",
        "summary": "The 3D structure of living and non-living components in ecosystems plays a\ncritical role in determining ecological processes and feedbacks from both\nnatural and human-driven disturbances. Anticipating the effects of wildfire,\ndrought, disease, or atmospheric deposition depends on accurate\ncharacterization of 3D vegetation structure, yet widespread measurement remains\nprohibitively expensive and often infeasible. We introduce ForestGen3D, a novel\ngenerative modeling framework that synthesizes high-fidelity 3D forest\nstructure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on\nconditional denoising diffusion probabilistic models (DDPMs) trained on\nco-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate\nTLS-like 3D point clouds conditioned on sparse ALS observations, effectively\nreconstructing occluded sub-canopy detail at scale. To ensure ecological\nplausibility, we introduce a geometric containment prior based on the convex\nhull of ALS observations and provide theoretical and empirical guarantees that\ngenerated structures remain spatially consistent. We evaluate ForestGen3D at\ntree, plot, and landscape scales using real-world data from mixed conifer\necosystems, and show that it produces high-fidelity reconstructions that\nclosely match TLS references in terms of geometric similarity and biophysical\nmetrics, such as tree height, DBH, crown diameter and crown volume.\nAdditionally, we demonstrate that the containment property can serve as a\npractical proxy for generation quality in settings where TLS ground truth is\nunavailable. Our results position ForestGen3D as a scalable tool for ecological\nmodeling, wildfire simulation, and structural fuel characterization in ALS-only\nenvironments.",
        "url": "http://arxiv.org/abs/2509.16346v1",
        "published_date": "2025-09-19T18:39:50+00:00",
        "updated_date": "2025-09-19T18:39:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Juan Castorena",
            "E. Louise Loudermilk",
            "Scott Pokswinski",
            "Rodman Linn"
        ],
        "tldr": "The paper introduces ForestGen3D, a novel generative model using conditional denoising diffusion probabilistic models (DDPMs) to reconstruct high-fidelity 3D forest structures from aerial LiDAR data, addressing the need for scalable 3D forest characterization for ecological modeling and related applications.",
        "tldr_zh": "该论文介绍了ForestGen3D，一种新型生成模型，它使用条件去噪扩散概率模型（DDPM）从航空激光雷达数据重建高保真3D森林结构，旨在解决生态建模及相关应用中可扩展3D森林表征的需求。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]