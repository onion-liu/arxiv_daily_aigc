[
    {
        "title": "VUGEN: Visual Understanding priors for GENeration",
        "summary": "Recent advances in Vision-Language Models (VLMs) have enabled unified\nunderstanding across text and images, yet equipping these models with robust\nimage generation capabilities remains challenging. Existing approaches often\nrely on reconstruction-oriented autoencoders or complex bridging mechanisms,\nleading to misalignment between understanding and generation representations,\nor architectural complexity. In this work, we propose VUGEN, a novel framework\nthat explicitly leverages VLM's pretrained visual understanding priors for\nefficient and high-quality image generation. Our approach first transforms the\nhigh-dimensional latent space of the VLM's native vision encoder into a\nlower-dimensional, tractable distribution that maximally preserves visual\ninformation. The VLM is then trained to sample within this reduced latent\nspace, ensuring alignment with its visual understanding capabilities. Finally,\na dedicated pixel decoder maps these generated latents back to the image space.\nWe find that a VAE-free pixel diffusion decoder to be on par or better than\ncommonly used complex latent diffusion decoders that internally rely on VAE\nlatents. Extensive experiments demonstrate that VUGEN achieves superior image\ngeneration performance, improving DPG Bench from 71.17 to 74.32 and FID from\n11.86 to 9.06 on COCO, while fully preserving the VLM's original understanding\ncapabilities.",
        "url": "http://arxiv.org/abs/2510.06529v1",
        "published_date": "2025-10-08T00:04:47+00:00",
        "updated_date": "2025-10-08T00:04:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangyi Chen",
            "Théophane Vallaeys",
            "Maha Elbayad",
            "John Nguyen",
            "Jakob Verbeek"
        ],
        "tldr": "VUGEN is a novel framework that leverages pre-trained VLMs' visual understanding priors for efficient image generation by transforming and reducing the latent space, leading to improved image generation performance.",
        "tldr_zh": "VUGEN 是一种新颖的框架，它利用预训练的视觉语言模型 (VLM) 的视觉理解先验进行高效的图像生成，通过转换和减少潜在空间，从而提高图像生成性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation",
        "summary": "Wrist-view observations are crucial for VLA models as they capture\nfine-grained hand-object interactions that directly enhance manipulation\nperformance. Yet large-scale datasets rarely include such recordings, resulting\nin a substantial gap between abundant anchor views and scarce wrist views.\nExisting world models cannot bridge this gap, as they require a wrist-view\nfirst frame and thus fail to generate wrist-view videos from anchor views\nalone. Amid this gap, recent visual geometry models such as VGGT emerge with\ngeometric and cross-view priors that make it possible to address extreme\nviewpoint shifts. Inspired by these insights, we propose WristWorld, the first\n4D world model that generates wrist-view videos solely from anchor views.\nWristWorld operates in two stages: (i) Reconstruction, which extends VGGT and\nincorporates our Spatial Projection Consistency (SPC) Loss to estimate\ngeometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,\nwhich employs our video generation model to synthesize temporally coherent\nwrist-view videos from the reconstructed perspective. Experiments on Droid,\nCalvin, and Franka Panda demonstrate state-of-the-art video generation with\nsuperior spatial consistency, while also improving VLA performance, raising the\naverage task completion length on Calvin by 3.81% and closing 42.4% of the\nanchor-wrist view gap.",
        "url": "http://arxiv.org/abs/2510.07313v1",
        "published_date": "2025-10-08T17:59:08+00:00",
        "updated_date": "2025-10-08T17:59:08+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zezhong Qian",
            "Xiaowei Chi",
            "Yuming Li",
            "Shizun Wang",
            "Zhiyuan Qin",
            "Xiaozhu Ju",
            "Sirui Han",
            "Shanghang Zhang"
        ],
        "tldr": "The paper introduces WristWorld, a 4D world model that generates wrist-view videos from anchor views for robotic manipulation, achieving state-of-the-art video generation and improved VLA performance.",
        "tldr_zh": "该论文介绍了WristWorld，一个4D世界模型，它从锚视图生成用于机器人操作的手腕视图视频，实现了最先进的视频生成并提高了VLA性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation",
        "summary": "Video DiTs have advanced video generation, yet they still struggle to model\nmulti-instance or subject-object interactions. This raises a key question: How\ndo these models internally represent interactions? To answer this, we curate\nMATRIX-11K, a video dataset with interaction-aware captions and multi-instance\nmask tracks. Using this dataset, we conduct a systematic analysis that\nformalizes two perspectives of video DiTs: semantic grounding, via\nvideo-to-text attention, which evaluates whether noun and verb tokens capture\ninstances and their relations; and semantic propagation, via video-to-video\nattention, which assesses whether instance bindings persist across frames. We\nfind both effects concentrate in a small subset of interaction-dominant layers.\nMotivated by this, we introduce MATRIX, a simple and effective regularization\nthat aligns attention in specific layers of video DiTs with multi-instance mask\ntracks from the MATRIX-11K dataset, enhancing both grounding and propagation.\nWe further propose InterGenEval, an evaluation protocol for interaction-aware\nvideo generation. In experiments, MATRIX improves both interaction fidelity and\nsemantic alignment while reducing drift and hallucination. Extensive ablations\nvalidate our design choices. Codes and weights will be released.",
        "url": "http://arxiv.org/abs/2510.07310v1",
        "published_date": "2025-10-08T17:57:38+00:00",
        "updated_date": "2025-10-08T17:57:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siyoon Jin",
            "Seongchan Kim",
            "Dahyun Chung",
            "Jaeho Lee",
            "Hyunwook Choi",
            "Jisu Nam",
            "Jiyoung Kim",
            "Seungryong Kim"
        ],
        "tldr": "The paper introduces MATRIX, a regularization method that aligns attention in video diffusion transformers with multi-instance mask tracks to improve interaction fidelity and semantic alignment in video generation, along with a new dataset and evaluation protocol.",
        "tldr_zh": "该论文介绍了MATRIX，一种正则化方法，通过将视频扩散Transformer中的注意力与多实例掩码轨迹对齐，来提高视频生成中的交互保真度和语义对齐，同时还提出了一个新的数据集和评估协议。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation",
        "summary": "In this work, we present TalkCuts, a large-scale dataset designed to\nfacilitate the study of multi-shot human speech video generation. Unlike\nexisting datasets that focus on single-shot, static viewpoints, TalkCuts offers\n164k clips totaling over 500 hours of high-quality human speech videos with\ndiverse camera shots, including close-up, half-body, and full-body views. The\ndataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X\nmotion annotations, covering over 10k identities, enabling multimodal learning\nand evaluation. As a first attempt to showcase the value of the dataset, we\npresent Orator, an LLM-guided multi-modal generation framework as a simple\nbaseline, where the language model functions as a multi-faceted director,\norchestrating detailed specifications for camera transitions, speaker\ngesticulations, and vocal modulation. This architecture enables the synthesis\nof coherent long-form videos through our integrated multi-modal video\ngeneration module. Extensive experiments in both pose-guided and audio-driven\nsettings show that training on TalkCuts significantly enhances the\ncinematographic coherence and visual appeal of generated multi-shot speech\nvideos. We believe TalkCuts provides a strong foundation for future work in\ncontrollable, multi-shot speech video generation and broader multimodal\nlearning.",
        "url": "http://arxiv.org/abs/2510.07249v1",
        "published_date": "2025-10-08T17:16:09+00:00",
        "updated_date": "2025-10-08T17:16:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaben Chen",
            "Zixin Wang",
            "Ailing Zeng",
            "Yang Fu",
            "Xueyang Yu",
            "Siyuan Cen",
            "Julian Tanke",
            "Yihang Chen",
            "Koichi Saito",
            "Yuki Mitsufuji",
            "Chuang Gan"
        ],
        "tldr": "The paper introduces TalkCuts, a large-scale dataset of human speech videos with diverse camera shots and annotations, and presents Orator, an LLM-guided framework for multi-shot video generation using this dataset.",
        "tldr_zh": "该论文介绍了TalkCuts，一个大规模的人类语音视频数据集，具有多样的相机视角和标注，并提出了Orator，一个基于LLM的框架，用于使用该数据集生成多镜头视频。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation",
        "summary": "Text-to-image synthesis has made remarkable progress, yet accurately\ninterpreting complex and lengthy prompts remains challenging, often resulting\nin semantic inconsistencies and missing details. Existing solutions, such as\nfine-tuning, are model-specific and require training, while prior automatic\nprompt optimization (APO) approaches typically lack systematic error analysis\nand refinement strategies, resulting in limited reliability and effectiveness.\nMeanwhile, test-time scaling methods operate on fixed prompts and on noise or\nsample numbers, limiting their interpretability and adaptability. To solve\nthese, we introduce a flexible and efficient test-time prompt optimization\nstrategy that operates directly on the input text. We propose a plug-and-play\nmulti-agent system called GenPilot, integrating error analysis,\nclustering-based adaptive exploration, fine-grained verification, and a memory\nmodule for iterative optimization. Our approach is model-agnostic,\ninterpretable, and well-suited for handling long and complex prompts.\nSimultaneously, we summarize the common patterns of errors and the refinement\nstrategy, offering more experience and encouraging further exploration.\nExperiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%\ndemonstrate the strong capability of our methods in enhancing the text and\nimage consistency and structural coherence of generated images, revealing the\neffectiveness of our test-time prompt optimization strategy. The code is\navailable at https://github.com/27yw/GenPilot.",
        "url": "http://arxiv.org/abs/2510.07217v1",
        "published_date": "2025-10-08T16:51:52+00:00",
        "updated_date": "2025-10-08T16:51:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wen Ye",
            "Zhaocheng Liu",
            "Yuwei Gui",
            "Tingyu Yuan",
            "Yunyue Su",
            "Bowen Fang",
            "Chaoyang Zhao",
            "Qiang Liu",
            "Liang Wang"
        ],
        "tldr": "The paper introduces GenPilot, a model-agnostic multi-agent system for test-time prompt optimization in image generation, addressing the challenge of accurately interpreting complex prompts and improving text-image consistency.",
        "tldr_zh": "该论文介绍了GenPilot，一个与模型无关的多智能体系统，用于图像生成中的测试时提示优化，旨在解决准确解释复杂提示和提高文本-图像一致性的问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Graph Conditioned Diffusion for Controllable Histopathology Image Generation",
        "summary": "Recent advances in Diffusion Probabilistic Models (DPMs) have set new\nstandards in high-quality image synthesis. Yet, controlled generation remains\nchallenging, particularly in sensitive areas such as medical imaging. Medical\nimages feature inherent structure such as consistent spatial arrangement, shape\nor texture, all of which are critical for diagnosis. However, existing DPMs\noperate in noisy latent spaces that lack semantic structure and strong priors,\nmaking it difficult to ensure meaningful control over generated content. To\naddress this, we propose graph-based object-level representations for\nGraph-Conditioned-Diffusion. Our approach generates graph nodes corresponding\nto each major structure in the image, encapsulating their individual features\nand relationships. These graph representations are processed by a transformer\nmodule and integrated into a diffusion model via the text-conditioning\nmechanism, enabling fine-grained control over generation. We evaluate this\napproach using a real-world histopathology use case, demonstrating that our\ngenerated data can reliably substitute for annotated patient data in downstream\nsegmentation tasks. The code is available here.",
        "url": "http://arxiv.org/abs/2510.07129v1",
        "published_date": "2025-10-08T15:26:08+00:00",
        "updated_date": "2025-10-08T15:26:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sarah Cechnicka",
            "Matthew Baugh",
            "Weitong Zhang",
            "Mischa Dombrowski",
            "Zhe Li",
            "Johannes C. Paetzold",
            "Candice Roufosse",
            "Bernhard Kainz"
        ],
        "tldr": "The paper proposes a graph-conditioned diffusion model for controllable histopathology image generation, enabling fine-grained control using graph-based object-level representations and demonstrating its utility in downstream segmentation tasks.",
        "tldr_zh": "该论文提出了一种图条件扩散模型，用于可控的组织病理学图像生成，通过使用基于图的对象级表示实现细粒度控制，并证明了其在下游分割任务中的效用。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generating Surface for Text-to-3D using 2D Gaussian Splatting",
        "summary": "Recent advancements in Text-to-3D modeling have shown significant potential\nfor the creation of 3D content. However, due to the complex geometric shapes of\nobjects in the natural world, generating 3D content remains a challenging task.\nCurrent methods either leverage 2D diffusion priors to recover 3D geometry, or\ntrain the model directly based on specific 3D representations. In this paper,\nwe propose a novel method named DirectGaussian, which focuses on generating the\nsurfaces of 3D objects represented by surfels. In DirectGaussian, we utilize\nconditional text generation models and the surface of a 3D object is rendered\nby 2D Gaussian splatting with multi-view normal and texture priors. For\nmulti-view geometric consistency problems, DirectGaussian incorporates\ncurvature constraints on the generated surface during optimization process.\nThrough extensive experiments, we demonstrate that our framework is capable of\nachieving diverse and high-fidelity 3D content creation.",
        "url": "http://arxiv.org/abs/2510.06967v1",
        "published_date": "2025-10-08T12:54:57+00:00",
        "updated_date": "2025-10-08T12:54:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Huanning Dong",
            "Fan Li",
            "Ping Kuang",
            "Jianwen Min"
        ],
        "tldr": "This paper introduces DirectGaussian, a novel text-to-3D method utilizing 2D Gaussian splatting with multi-view priors and curvature constraints to generate high-fidelity 3D object surfaces.",
        "tldr_zh": "本文介绍了一种名为DirectGaussian的新型文本到3D方法，该方法利用2D高斯溅射与多视角先验和曲率约束来生成高保真3D对象表面。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IAR2: Improving Autoregressive Visual Generation with Semantic-Detail Associated Token Prediction",
        "summary": "Autoregressive models have emerged as a powerful paradigm for visual content\ncreation, but often overlook the intrinsic structural properties of visual\ndata. Our prior work, IAR, initiated a direction to address this by\nreorganizing the visual codebook based on embedding similarity, thereby\nimproving generation robustness. However, it is constrained by the rigidity of\npre-trained codebooks and the inaccuracies of hard, uniform clustering. To\novercome these limitations, we propose IAR2, an advanced autoregressive\nframework that enables a hierarchical semantic-detail synthesis process. At the\ncore of IAR2 is a novel Semantic-Detail Associated Dual Codebook, which\ndecouples image representations into a semantic codebook for global semantic\ninformation and a detail codebook for fine-grained refinements. It expands the\nquantization capacity from a linear to a polynomial scale, significantly\nenhancing expressiveness. To accommodate this dual representation, we propose a\nSemantic-Detail Autoregressive Prediction scheme coupled with a Local-Context\nEnhanced Autoregressive Head, which performs hierarchical prediction-first the\nsemantic token, then the detail token-while leveraging a local context window\nto enhance spatial coherence. Furthermore, for conditional generation, we\nintroduce a Progressive Attention-Guided Adaptive CFG mechanism that\ndynamically modulates the guidance scale for each token based on its relevance\nto the condition and its temporal position in the generation sequence,\nimproving conditional alignment without sacrificing realism. Extensive\nexperiments demonstrate that IAR2 sets a new state-of-the-art for\nautoregressive image generation, achieving a FID of 1.50 on ImageNet. Our model\nnot only surpasses previous methods in performance but also demonstrates\nsuperior computational efficiency, highlighting the effectiveness of our\nstructured, coarse-to-fine generation strategy.",
        "url": "http://arxiv.org/abs/2510.06928v1",
        "published_date": "2025-10-08T12:08:21+00:00",
        "updated_date": "2025-10-08T12:08:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ran Yi",
            "Teng Hu",
            "Zihan Su",
            "Lizhuang Ma"
        ],
        "tldr": "IAR2 introduces a novel autoregressive image generation framework with a semantic-detail associated dual codebook and prediction scheme, achieving state-of-the-art FID scores on ImageNet with improved efficiency.",
        "tldr_zh": "IAR2 提出了一种新的自回归图像生成框架，该框架具有语义细节相关的双重密码本和预测方案，在 ImageNet 上实现了最先进的 FID 分数，并提高了效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DreamOmni2: Multimodal Instruction-based Editing and Generation",
        "summary": "Recent advancements in instruction-based image editing and subject-driven\ngeneration have garnered significant attention, yet both tasks still face\nlimitations in meeting practical user needs. Instruction-based editing relies\nsolely on language instructions, which often fail to capture specific editing\ndetails, making reference images necessary. Meanwhile, subject-driven\ngeneration is limited to combining concrete objects or people, overlooking\nbroader, abstract concepts. To address these challenges, we propose two novel\ntasks: multimodal instruction-based editing and generation. These tasks support\nboth text and image instructions and extend the scope to include both concrete\nand abstract concepts, greatly enhancing their practical applications. We\nintroduce DreamOmni2, tackling two primary challenges: data creation and model\nframework design. Our data synthesis pipeline consists of three steps: (1)\nusing a feature mixing method to create extraction data for both abstract and\nconcrete concepts, (2) generating multimodal instruction-based editing training\ndata using the editing and extraction models, and (3) further applying the\nextraction model to create training data for multimodal instruction-based\nediting. For the framework, to handle multi-image input, we propose an index\nencoding and position encoding shift scheme, which helps the model distinguish\nimages and avoid pixel confusion. Additionally, we introduce joint training\nwith the VLM and our generation/editing model to better process complex\ninstructions. In addition, we have proposed comprehensive benchmarks for these\ntwo new tasks to drive their development. Experiments show that DreamOmni2 has\nachieved impressive results. Models and codes will be released.",
        "url": "http://arxiv.org/abs/2510.06679v1",
        "published_date": "2025-10-08T06:07:14+00:00",
        "updated_date": "2025-10-08T06:07:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bin Xia",
            "Bohao Peng",
            "Yuechen Zhang",
            "Junjia Huang",
            "Jiyang Liu",
            "Jingyao Li",
            "Haoru Tan",
            "Sitong Wu",
            "Chengyao Wang",
            "Yitong Wang",
            "Xinglong Wu",
            "Bei Yu",
            "Jiaya Jia"
        ],
        "tldr": "The paper introduces DreamOmni2, a framework tackling multimodal instruction-based image editing and generation with novel data synthesis and model architecture, addressing limitations in existing instruction-based editing and subject-driven generation tasks by handling both concrete and abstract concepts with multi-image inputs.",
        "tldr_zh": "该论文介绍了DreamOmni2，一个用于多模态指令驱动图像编辑和生成的框架。它通过新颖的数据合成和模型架构，解决了现有指令驱动编辑和主题驱动生成任务的局限性，能够处理具体和抽象概念，并支持多图像输入。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Heptapod: Language Modeling on Visual Signals",
        "summary": "We introduce Heptapod, an image autoregressive model that adheres to the\nfoundational principles of language modeling. Heptapod employs \\textbf{causal\nattention}, \\textbf{eliminates reliance on CFG}, and \\textbf{eschews the trend\nof semantic tokenizers}. Our key innovation is \\textit{next 2D distribution\nprediction}: a causal Transformer with reconstruction-focused visual tokenizer,\nlearns to predict the distribution over the entire 2D spatial grid of images at\neach timestep. This learning objective unifies the sequential modeling of\nautoregressive framework with the holistic self-supervised learning of masked\nautoencoding, enabling the model to capture comprehensive image semantics via\ngenerative training. On the ImageNet generation benchmark, Heptapod achieves an\nFID of $2.70$, significantly outperforming previous causal autoregressive\napproaches. We hope our work inspires a principled rethinking of language\nmodeling on visual signals and beyond.",
        "url": "http://arxiv.org/abs/2510.06673v1",
        "published_date": "2025-10-08T05:54:46+00:00",
        "updated_date": "2025-10-08T05:54:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yongxin Zhu",
            "Jiawei Chen",
            "Yuanzhe Chen",
            "Zhuo Chen",
            "Dongya Jia",
            "Jian Cong",
            "Xiaobin Zhuang",
            "Yuping Wang",
            "Yuxuan Wang"
        ],
        "tldr": "Heptapod is a novel image autoregressive model that predicts the distribution over the entire 2D spatial grid of images at each timestep, achieving state-of-the-art FID on ImageNet generation by unifying autoregressive and masked autoencoding approaches.",
        "tldr_zh": "Heptapod 是一种新型图像自回归模型，通过在每个时间步预测整个图像 2D 空间网格上的分布，将自回归和掩码式自编码方法相结合，在 ImageNet 生成上实现了最先进的 FID。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer",
        "summary": "Visual tokenization remains a core challenge in unifying visual understanding\nand generation within the autoregressive paradigm. Existing methods typically\nemploy tokenizers in discrete latent spaces to align with the tokens from large\nlanguage models, where the quantization errors can limit semantic\nexpressiveness and degrade the capability of vision-language understanding. To\naddress this, we introduce MingTok, a new family of visual tokenizers with a\ncontinuous latent space, for unified autoregressive generation and\nunderstanding. While understanding tasks favor discriminative high-dimensional\nfeatures, generation tasks prefer compact low-level codes. Thus, to reconcile\nthese competing demands, MingTok adopts a three-stage sequential architecture\ninvolving low-level encoding, semantic expansion, and visual reconstruction.\nBuilt on top of it, Ming-UniVision eliminates the need for task-specific visual\nrepresentations, and unifies diverse vision-language tasks under a single\nautoregrsssive prediction paradigm. By formulating both understanding and\ngeneration as next-token prediction in a shared continuous space, it seamlessly\nsupports multi-round, in-context tasks such as iterative understanding,\ngeneration and editing. Empirically, we find that using a unified continuous\nvisual representation reconciles the competing requirements on the tokenizers\nby the understanding and generation tasks, thereby leading to state-of-the-art\nlevel performance across both domains. We hope our findings will facilitate\nunified visual tokenization in the continuous domain. Inference code and model\nweights are released to benefit community.",
        "url": "http://arxiv.org/abs/2510.06590v1",
        "published_date": "2025-10-08T02:50:14+00:00",
        "updated_date": "2025-10-08T02:50:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyuan Huang",
            "DanDan Zheng",
            "Cheng Zou",
            "Rui Liu",
            "Xiaolong Wang",
            "Kaixiang Ji",
            "Weilong Chai",
            "Jianxin Sun",
            "Libin Wang",
            "Yongjie Lv",
            "Taozhi Huang",
            "Jiajia Liu",
            "Qingpei Guo",
            "Ming Yang",
            "Jingdong Chen",
            "Jun Zhou"
        ],
        "tldr": "The paper introduces Ming-UniVision, a novel approach using a continuous visual tokenizer (MingTok) to unify image understanding and generation tasks under a single autoregressive framework, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了 Ming-UniVision，一种新颖的方法，使用连续视觉 tokenizer (MingTok) 将图像理解和生成任务统一到一个自回归框架下，实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Text2Interact: High-Fidelity and Diverse Text-to-Two-Person Interaction Generation",
        "summary": "Modeling human-human interactions from text remains challenging because it\nrequires not only realistic individual dynamics but also precise,\ntext-consistent spatiotemporal coupling between agents. Currently, progress is\nhindered by 1) limited two-person training data, inadequate to capture the\ndiverse intricacies of two-person interactions; and 2) insufficiently\nfine-grained text-to-interaction modeling, where language conditioning\ncollapses rich, structured prompts into a single sentence embedding. To address\nthese limitations, we propose our Text2Interact framework, designed to generate\nrealistic, text-aligned human-human interactions through a scalable\nhigh-fidelity interaction data synthesizer and an effective spatiotemporal\ncoordination pipeline. First, we present InterCompose, a scalable\nsynthesis-by-composition pipeline that aligns LLM-generated interaction\ndescriptions with strong single-person motion priors. Given a prompt and a\nmotion for an agent, InterCompose retrieves candidate single-person motions,\ntrains a conditional reaction generator for another agent, and uses a neural\nmotion evaluator to filter weak or misaligned samples-expanding interaction\ncoverage without extra capture. Second, we propose InterActor, a\ntext-to-interaction model with word-level conditioning that preserves\ntoken-level cues (initiation, response, contact ordering) and an adaptive\ninteraction loss that emphasizes contextually relevant inter-person joint\npairs, improving coupling and physical plausibility for fine-grained\ninteraction modeling. Extensive experiments show consistent gains in motion\ndiversity, fidelity, and generalization, including out-of-distribution\nscenarios and user studies. We will release code and models to facilitate\nreproducibility.",
        "url": "http://arxiv.org/abs/2510.06504v1",
        "published_date": "2025-10-07T22:41:23+00:00",
        "updated_date": "2025-10-07T22:41:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qingxuan Wu",
            "Zhiyang Dou",
            "Chuan Guo",
            "Yiming Huang",
            "Qiao Feng",
            "Bing Zhou",
            "Jian Wang",
            "Lingjie Liu"
        ],
        "tldr": "This paper introduces Text2Interact, a framework for generating realistic and text-aligned two-person interactions from text descriptions, using a scalable data synthesis pipeline and a fine-grained text-to-interaction model. It addresses limitations in existing methods regarding data scarcity and fine-grained control.",
        "tldr_zh": "本文介绍了Text2Interact，一个通过文本描述生成逼真且文本对齐的双人互动框架，它使用可扩展的数据合成管道和一个细粒度的文本到互动模型。它解决了现有方法在数据稀缺和细粒度控制方面的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SIGMA-GEN: Structure and Identity Guided Multi-subject Assembly for Image Generation",
        "summary": "We present SIGMA-GEN, a unified framework for multi-identity preserving image\ngeneration. Unlike prior approaches, SIGMA-GEN is the first to enable\nsingle-pass multi-subject identity-preserved generation guided by both\nstructural and spatial constraints. A key strength of our method is its ability\nto support user guidance at various levels of precision -- from coarse 2D or 3D\nboxes to pixel-level segmentations and depth -- with a single model. To enable\nthis, we introduce SIGMA-SET27K, a novel synthetic dataset that provides\nidentity, structure, and spatial information for over 100k unique subjects\nacross 27k images. Through extensive evaluation we demonstrate that SIGMA-GEN\nachieves state-of-the-art performance in identity preservation, image\ngeneration quality, and speed. Code and visualizations at\nhttps://oindrilasaha.github.io/SIGMA-Gen/",
        "url": "http://arxiv.org/abs/2510.06469v1",
        "published_date": "2025-10-07T21:12:02+00:00",
        "updated_date": "2025-10-07T21:12:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Oindrila Saha",
            "Vojtech Krs",
            "Radomir Mech",
            "Subhransu Maji",
            "Kevin Blackburn-Matzen",
            "Matheus Gadelha"
        ],
        "tldr": "SIGMA-GEN is a novel framework for multi-subject, identity-preserving image generation guided by structural and spatial constraints, leveraging a new synthetic dataset SIGMA-SET27K to achieve state-of-the-art performance.",
        "tldr_zh": "SIGMA-GEN是一个新颖的多主体、身份保持的图像生成框架，它通过结构和空间约束进行引导，并利用新的合成数据集SIGMA-SET27K来实现最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]