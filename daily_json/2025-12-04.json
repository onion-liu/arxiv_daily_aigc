[
    {
        "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
        "summary": "Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.",
        "url": "http://arxiv.org/abs/2512.03534v1",
        "published_date": "2025-12-03T07:54:05+00:00",
        "updated_date": "2025-12-03T07:54:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Subin Kim",
            "Sangwoo Mo",
            "Mamshad Nayeem Rizve",
            "Yiran Xu",
            "Difan Liu",
            "Jinwoo Shin",
            "Tobias Hinz"
        ],
        "tldr": "The paper introduces PRIS, a framework that adaptively revises prompts during inference, based on feedback from generated visuals, to improve alignment in text-to-visual generation, achieving better performance than scaling visual generation alone.",
        "tldr_zh": "该论文介绍了PRIS，一个在推理过程中自适应修改提示的框架，该框架基于生成的可视化结果，以提高文本到视觉生成中的对齐效果，并实现了优于仅扩展视觉生成的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers",
        "summary": "Diffusion models have revolutionized video generation, becoming essential tools in creative content generation and physical simulation. Transformer-based architectures (DiTs) and classifier-free guidance (CFG) are two cornerstones of this success, enabling strong prompt adherence and realistic video quality. Despite their versatility and superior performance, these models require intensive computation. Each video generation requires dozens of iterative steps, and CFG doubles the required compute. This inefficiency hinders broader adoption in downstream applications.\n  We introduce GalaxyDiT, a training-free method to accelerate video generation with guidance alignment and systematic proxy selection for reuse metrics. Through rank-order correlation analysis, our technique identifies the optimal proxy for each video model, across model families and parameter scales, thereby ensuring optimal computational reuse. We achieve $1.87\\times$ and $2.37\\times$ speedup on Wan2.1-1.3B and Wan2.1-14B with only 0.97% and 0.72% drops on the VBench-2.0 benchmark. At high speedup rates, our approach maintains superior fidelity to the base model, exceeding prior state-of-the-art approaches by 5 to 10 dB in peak signal-to-noise ratio (PSNR).",
        "url": "http://arxiv.org/abs/2512.03451v1",
        "published_date": "2025-12-03T05:08:18+00:00",
        "updated_date": "2025-12-03T05:08:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Zhiye Song",
            "Steve Dai",
            "Ben Keller",
            "Brucek Khailany"
        ],
        "tldr": "GalaxyDiT proposes a training-free method to accelerate video generation in Diffusion Transformers by using guidance alignment and adaptive proxy selection, achieving significant speedups with minimal performance drop.",
        "tldr_zh": "GalaxyDiT 提出了一种免训练的方法，通过在扩散 Transformer 中使用引导对齐和自适应代理选择来加速视频生成，在性能损失最小的情况下实现了显著的加速。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework",
        "summary": "Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.",
        "url": "http://arxiv.org/abs/2512.03041v1",
        "published_date": "2025-12-02T18:59:48+00:00",
        "updated_date": "2025-12-02T18:59:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qinghe Wang",
            "Xiaoyu Shi",
            "Baolu Li",
            "Weikang Bian",
            "Quande Liu",
            "Huchuan Lu",
            "Xintao Wang",
            "Pengfei Wan",
            "Kun Gai",
            "Xu Jia"
        ],
        "tldr": "The paper introduces MultiShotMaster, a controllable framework for generating narrative multi-shot videos using novel RoPE variants to enable flexible shot arrangement, coherent narrative, and spatiotemporal control, addressing the limitations of current single-shot video generation techniques.",
        "tldr_zh": "该论文介绍了MultiShotMaster，一个可控的框架，用于生成叙事性的多镜头视频，通过使用新型RoPE变体来实现灵活的镜头排列、连贯的叙事以及时空控制，解决了当前单镜头视频生成技术的局限性。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation",
        "summary": "Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at \\href{https://github.com/xuhang07/TEP-Diffusion}{https://github.com/xuhang07/TEP-Diffusion}.",
        "url": "http://arxiv.org/abs/2512.03996v1",
        "published_date": "2025-12-03T17:27:53+00:00",
        "updated_date": "2025-12-03T17:27:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hang Xu",
            "Linjiang Huang",
            "Feng Zhao"
        ],
        "tldr": "This paper introduces a novel Test-Time Scaling (TTS) method for text-to-image diffusion models that leverages text embedding perturbation, guided by frequency-domain analysis, to enhance generative diversity and quality with minimal added computation.",
        "tldr_zh": "本文提出了一种新颖的文本到图像扩散模型的测试时缩放（TTS）方法，该方法利用文本嵌入扰动，并以频域分析为指导，以增强生成多样性和质量，同时几乎不增加额外的计算。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment",
        "summary": "Drag-based image editing using generative models provides intuitive control over image structures. However, existing methods rely heavily on manually provided masks and textual prompts to preserve semantic fidelity and motion precision. Removing these constraints creates a fundamental trade-off: visual artifacts without masks and poor spatial control without prompts. To address these limitations, we propose DirectDrag, a novel mask- and prompt-free editing framework. DirectDrag enables precise and efficient manipulation with minimal user input while maintaining high image fidelity and accurate point alignment. DirectDrag introduces two key innovations. First, we design an Auto Soft Mask Generation module that intelligently infers editable regions from point displacement, automatically localizing deformation along movement paths while preserving contextual integrity through the generative model's inherent capacity. Second, we develop a Readout-Guided Feature Alignment mechanism that leverages intermediate diffusion activations to maintain structural consistency during point-based edits, substantially improving visual fidelity. Despite operating without manual mask or prompt, DirectDrag achieves superior image quality compared to existing methods while maintaining competitive drag accuracy. Extensive experiments on DragBench and real-world scenarios demonstrate the effectiveness and practicality of DirectDrag for high-quality, interactive image manipulation. Project Page: https://frakw.github.io/DirectDrag/. Code is available at: https://github.com/frakw/DirectDrag.",
        "url": "http://arxiv.org/abs/2512.03981v1",
        "published_date": "2025-12-03T17:12:00+00:00",
        "updated_date": "2025-12-03T17:12:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sheng-Hao Liao",
            "Shang-Fu Chen",
            "Tai-Ming Huang",
            "Wen-Huang Cheng",
            "Kai-Lung Hua"
        ],
        "tldr": "DirectDrag introduces a mask- and prompt-free drag-based image editing framework that uses auto-generated soft masks and readout-guided feature alignment to achieve high-fidelity and precise image manipulation.",
        "tldr_zh": "DirectDrag 提出了一个无需掩码和提示的基于拖拽的图像编辑框架，该框架使用自动生成的软掩码和读出引导的特征对齐来实现高质量且精确的图像操作。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework",
        "summary": "We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.",
        "url": "http://arxiv.org/abs/2512.03918v1",
        "published_date": "2025-12-03T16:03:18+00:00",
        "updated_date": "2025-12-03T16:03:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Youxin Pang",
            "Yong Zhang",
            "Ruizhi Shao",
            "Xiang Deng",
            "Feng Gao",
            "Xu Xiaoming",
            "Xiaoming Wei",
            "Yebin Liu"
        ],
        "tldr": "The paper introduces UniMo, a novel autoregressive model that unifies 2D video and 3D human motion generation and understanding within a single framework, using tokenization similar to LLMs and a specialized 3D motion tokenizer.",
        "tldr_zh": "该论文介绍了UniMo，一种新颖的自回归模型，它在一个统一的框架内实现了2D视频和3D人体运动的生成和理解，使用了类似于LLM的tokenization和一个专门的3D运动tokenizer。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence",
        "summary": "The remarkable success in text-to-image diffusion models has motivated extensive investigation of their potential for video applications. Zero-shot techniques aim to adapt image diffusion models for videos without requiring further model training. Recent methods largely emphasize integrating inter-frame correspondence into attention mechanisms. However, the soft constraint applied to identify the valid features to attend is insufficient, which could lead to temporal inconsistency. In this paper, we present FRESCO, which integrates intra-frame correspondence with inter-frame correspondence to formulate a more robust spatial-temporal constraint. This enhancement ensures a consistent transformation of semantically similar content between frames. Our method goes beyond attention guidance to explicitly optimize features, achieving high spatial-temporal consistency with the input video, significantly enhancing the visual coherence of manipulated videos. We verify FRESCO adaptations on two zero-shot tasks of video-to-video translation and text-guided video editing. Comprehensive experiments demonstrate the effectiveness of our framework in generating high-quality, coherent videos, highlighting a significant advance over current zero-shot methods.",
        "url": "http://arxiv.org/abs/2512.03905v1",
        "published_date": "2025-12-03T15:51:11+00:00",
        "updated_date": "2025-12-03T15:51:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuai Yang",
            "Junxin Lin",
            "Yifan Zhou",
            "Ziwei Liu",
            "Chen Change Loy"
        ],
        "tldr": "This paper introduces FRESCO, a zero-shot video translation and editing method that uses improved spatial-temporal constraints for better consistency between frames, outperforming existing methods.",
        "tldr_zh": "本文介绍了一种名为FRESCO的零-shot视频翻译和编辑方法，该方法使用改进的时空约束来提高帧之间的一致性，并优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LSRS: Latent Scale Rejection Sampling for Visual Autoregressive Modeling",
        "summary": "Visual Autoregressive (VAR) modeling approach for image generation proposes autoregressive processing across hierarchical scales, decoding multiple tokens per scale in parallel. This method achieves high-quality generation while accelerating synthesis. However, parallel token sampling within a scale may lead to structural errors, resulting in suboptimal generated images. To mitigate this, we propose Latent Scale Rejection Sampling (LSRS), a method that progressively refines token maps in the latent scale during inference to enhance VAR models. Our method uses a lightweight scoring model to evaluate multiple candidate token maps sampled at each scale, selecting the high-quality map to guide subsequent scale generation. By prioritizing early scales critical for structural coherence, LSRS effectively mitigates autoregressive error accumulation while maintaining computational efficiency. Experiments demonstrate that LSRS significantly improves VAR's generation quality with minimal additional computational overhead. For the VAR-d30 model, LSRS increases the inference time by merely 1% while reducing its FID score from 1.95 to 1.78. When the inference time is increased by 15%, the FID score can be further reduced to 1.66. LSRS offers an efficient test-time scaling solution for enhancing VAR-based generation.",
        "url": "http://arxiv.org/abs/2512.03796v1",
        "published_date": "2025-12-03T13:44:30+00:00",
        "updated_date": "2025-12-03T13:44:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hong-Kai Zheng",
            "Piji Li"
        ],
        "tldr": "The paper introduces Latent Scale Rejection Sampling (LSRS) to refine token maps in visual autoregressive (VAR) models, improving image generation quality with minimal computational overhead by using a lightweight scoring model to select high-quality token maps.",
        "tldr_zh": "本文介绍了潜在尺度拒绝采样（LSRS），它通过使用轻量级评分模型选择高质量的token map来改进视觉自回归（VAR）模型中的token map，从而在尽量不增加计算开销的情况下提高图像生成质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
        "summary": "We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.",
        "url": "http://arxiv.org/abs/2512.03621v1",
        "published_date": "2025-12-03T09:55:25+00:00",
        "updated_date": "2025-12-03T09:55:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaokun Li",
            "Shuaixian Wang",
            "Mantang Guo",
            "Jiehui Huang",
            "Taojun Ding",
            "Mu Hu",
            "Kaixuan Wang",
            "Shaojie Shen",
            "Guang Tan"
        ],
        "tldr": "ReCamDriving is a novel vision-based framework for camera-controlled video generation using 3D Gaussian Splatting, trained in two stages and leveraging a new large-scale dataset called ParaDrive to improve camera controllability and structural consistency.",
        "tldr_zh": "ReCamDriving是一个新型的基于视觉的框架，用于相机控制的视频生成，它使用3D高斯溅射，分两个阶段进行训练，并利用名为ParaDrive的大型数据集来提高相机可控性和结构一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LAMP: Language-Assisted Motion Planning for Controllable Video Generation",
        "summary": "Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion. Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited. We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras. LAMP defines a motion domain-specific language (DSL), inspired by cinematography conventions. By harnessing program synthesis capabilities of LLMs, LAMP generates structured motion programs from natural language, which are deterministically mapped to 3D trajectories. We construct a large-scale procedural dataset pairing natural text descriptions with corresponding motion programs and 3D trajectories. Experiments demonstrate LAMP's improved performance in motion controllability and alignment with user intent compared to state-of-the-art alternatives establishing the first framework for generating both object and camera motions directly from natural language specifications.",
        "url": "http://arxiv.org/abs/2512.03619v1",
        "published_date": "2025-12-03T09:51:13+00:00",
        "updated_date": "2025-12-03T09:51:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muhammed Burak Kizil",
            "Enes Sanli",
            "Niloy J. Mitra",
            "Erkut Erdem",
            "Aykut Erdem",
            "Duygu Ceylan"
        ],
        "tldr": "The paper introduces LAMP, a framework that uses LLMs to translate natural language descriptions into 3D trajectories for objects and cameras in video generation, improving motion controllability.",
        "tldr_zh": "该论文介绍了LAMP，一个利用大型语言模型将自然语言描述转换为视频生成中物体和相机的3D轨迹的框架，从而提高运动可控性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
        "summary": "Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.",
        "url": "http://arxiv.org/abs/2512.03540v1",
        "published_date": "2025-12-03T08:01:48+00:00",
        "updated_date": "2025-12-03T08:01:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ruoxuan Zhang",
            "Bin Wen",
            "Hongxia Xie",
            "Yi Yao",
            "Songhan Zuo",
            "Jian-Yu Jiang-Lin",
            "Hong-Han Shuai",
            "Wen-Huang Cheng"
        ],
        "tldr": "The paper introduces CookAnything, a diffusion-based framework for generating coherent image sequences from multi-step cooking instructions of varying lengths, featuring step-wise regional control, flexible positional encoding, and cross-step consistency control.",
        "tldr_zh": "该论文介绍了CookAnything，一个基于扩散模型的框架，用于从不同长度的多步骤烹饪说明中生成连贯的图像序列，具有逐步区域控制、灵活的位置编码和跨步骤一致性控制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GeoVideo: Introducing Geometric Regularization into Video Generation Model",
        "summary": "Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.",
        "url": "http://arxiv.org/abs/2512.03453v1",
        "published_date": "2025-12-03T05:11:57+00:00",
        "updated_date": "2025-12-03T05:11:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunpeng Bai",
            "Shaoheng Fang",
            "Chaohui Yu",
            "Fan Wang",
            "Qixing Huang"
        ],
        "tldr": "This paper introduces geometric regularization losses, specifically using depth prediction, into video generation latent diffusion models to improve spatio-temporal coherence and geometric consistency.",
        "tldr_zh": "该论文将几何正则化损失（特别是使用深度预测）引入视频生成潜在扩散模型中，以提高时空连贯性和几何一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SeeU: Seeing the Unseen World via 4D Dynamics-aware Generation",
        "summary": "Images and videos are discrete 2D projections of the 4D world (3D space + time). Most visual understanding, prediction, and generation operate directly on 2D observations, leading to suboptimal performance. We propose SeeU, a novel approach that learns the continuous 4D dynamics and generate the unseen visual contents. The principle behind SeeU is a new 2D$\\to$4D$\\to$2D learning framework. SeeU first reconstructs the 4D world from sparse and monocular 2D frames (2D$\\to$4D). It then learns the continuous 4D dynamics on a low-rank representation and physical constraints (discrete 4D$\\to$continuous 4D). Finally, SeeU rolls the world forward in time, re-projects it back to 2D at sampled times and viewpoints, and generates unseen regions based on spatial-temporal context awareness (4D$\\to$2D). By modeling dynamics in 4D, SeeU achieves continuous and physically-consistent novel visual generation, demonstrating strong potentials in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing.",
        "url": "http://arxiv.org/abs/2512.03350v1",
        "published_date": "2025-12-03T01:30:45+00:00",
        "updated_date": "2025-12-03T01:30:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Yuan",
            "Tharindu Wickremasinghe",
            "Zeeshan Nadir",
            "Xijun Wang",
            "Yiheng Chi",
            "Stanley H. Chan"
        ],
        "tldr": "The paper introduces SeeU, a novel 2D->4D->2D framework that learns continuous 4D dynamics from sparse 2D frames to generate unseen visual content with spatial-temporal consistency for tasks like novel view/time generation and video editing.",
        "tldr_zh": "该论文提出了SeeU，一种新颖的2D到4D到2D的框架，它从稀疏的2D帧中学习连续的4D动态，从而生成具有时空一致性的未见视觉内容，用于新视角/时间生成和视频编辑等任务。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues",
        "summary": "We propose MagicQuill V2, a novel system that introduces a \\textbf{layered composition} paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software. While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance. To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette. Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal. Extensive experiments validate that this layered approach effectively resolves the user intention gap, granting creators direct, intuitive control over the generative process.",
        "url": "http://arxiv.org/abs/2512.03046v1",
        "published_date": "2025-12-02T18:59:58+00:00",
        "updated_date": "2025-12-02T18:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zichen Liu",
            "Yue Yu",
            "Hao Ouyang",
            "Qiuyu Wang",
            "Shuailei Ma",
            "Ka Leong Cheng",
            "Wen Wang",
            "Qingyan Bai",
            "Yuxuan Zhang",
            "Yanhong Zeng",
            "Yixuan Li",
            "Xing Zhu",
            "Yujun Shen",
            "Qifeng Chen"
        ],
        "tldr": "MagicQuillV2 introduces a layered composition paradigm for generative image editing, offering more granular control over content, position, shape, and color compared to monolithic prompt approaches in diffusion models.",
        "tldr_zh": "MagicQuillV2 引入了一种分层组合范式用于生成式图像编辑，与扩散模型中整体提示方法相比，它可以更精细地控制内容、位置、形状和颜色。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models",
        "summary": "Multi-view diffusion models have recently emerged as a powerful paradigm for novel view synthesis, yet the underlying mechanism that enables their view-consistency remains unclear. In this work, we first verify that the attention maps of these models acquire geometric correspondence throughout training, attending to the geometrically corresponding regions across reference and target views for view-consistent generation. However, this correspondence signal remains incomplete, with its accuracy degrading under large viewpoint changes. Building on these findings, we introduce CAMEO, a simple yet effective training technique that directly supervises attention maps using geometric correspondence to enhance both the training efficiency and generation quality of multi-view diffusion models. Notably, supervising a single attention layer is sufficient to guide the model toward learning precise correspondences, thereby preserving the geometry and structure of reference images, accelerating convergence, and improving novel view synthesis performance. CAMEO reduces the number of training iterations required for convergence by half while achieving superior performance at the same iteration counts. We further demonstrate that CAMEO is model-agnostic and can be applied to any multi-view diffusion model.",
        "url": "http://arxiv.org/abs/2512.03045v1",
        "published_date": "2025-12-02T18:59:57+00:00",
        "updated_date": "2025-12-02T18:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minkyung Kwon",
            "Jinhyeok Choi",
            "Jiho Park",
            "Seonghu Jeon",
            "Jinhyuk Jang",
            "Junyoung Seo",
            "Minseop Kwak",
            "Jin-Hwa Kim",
            "Seungryong Kim"
        ],
        "tldr": "The paper introduces CAMEO, a training technique that supervises attention maps in multi-view diffusion models using geometric correspondence, improving training efficiency and novel view synthesis quality.",
        "tldr_zh": "该论文介绍了CAMEO，一种训练技术，利用几何对应关系监督多视图扩散模型中的注意力图，从而提高训练效率和新视点合成质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
        "summary": "Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.",
        "url": "http://arxiv.org/abs/2512.03043v2",
        "published_date": "2025-12-02T18:59:52+00:00",
        "updated_date": "2025-12-03T08:46:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaituo Feng",
            "Manyuan Zhang",
            "Hongyu Li",
            "Kaixuan Fan",
            "Shuang Chen",
            "Yilei Jiang",
            "Dian Zheng",
            "Peiwen Sun",
            "Yiyuan Zhang",
            "Haoze Sun",
            "Yan Feng",
            "Peng Pei",
            "Xunliang Cai",
            "Xiangyu Yue"
        ],
        "tldr": "The paper introduces OneThinker, an all-in-one multimodal reasoning model for image and video that unifies several visual understanding tasks using a large training corpus and a novel reward optimization method, achieving strong performance across diverse benchmarks.",
        "tldr_zh": "该论文介绍了OneThinker，一个用于图像和视频的all-in-one多模态推理模型，它使用大型训练语料库和新颖的奖励优化方法统一了多个视觉理解任务，并在各种基准测试中取得了强大的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
        "summary": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
        "url": "http://arxiv.org/abs/2512.03040v1",
        "published_date": "2025-12-02T18:59:44+00:00",
        "updated_date": "2025-12-02T18:59:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zeqi Xiao",
            "Yiwei Zhao",
            "Lingxiao Li",
            "Yushi Lan",
            "Yu Ning",
            "Rahul Garg",
            "Roshni Cooper",
            "Mohammad H. Taghavi",
            "Xingang Pan"
        ],
        "tldr": "The paper introduces Video4Spatial, a video diffusion framework that uses video-based scene context to perform complex spatial tasks like scene navigation and object grounding, demonstrating visuospatial reasoning capabilities without auxiliary modalities.",
        "tldr_zh": "该论文介绍了Video4Spatial，一个视频扩散框架，它仅使用基于视频的场景上下文来执行复杂的空间任务，如场景导航和物体定位，展示了无需额外模态的视觉空间推理能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation",
        "summary": "We propose MAViD, a novel Multimodal framework for Audio-Visual Dialogue understanding and generation. Existing approaches primarily focus on non-interactive systems and are limited to producing constrained and unnatural human speech.The primary challenge of this task lies in effectively integrating understanding and generation capabilities, as well as achieving seamless multimodal audio-video fusion. To solve these problems, we propose a Conductor-Creator architecture that divides the dialogue system into two primary components.The Conductor is tasked with understanding, reasoning, and generating instructions by breaking them down into motion and speech components, thereby enabling fine-grained control over interactions. The Creator then delivers interactive responses based on these instructions.Furthermore, to address the difficulty of generating long videos with consistent identity, timbre, and tone using dual DiT structures, the Creator adopts a structure that combines autoregressive (AR) and diffusion models. The AR model is responsible for audio generation, while the diffusion model ensures high-quality video generation.Additionally, we propose a novel fusion module to enhance connections between contextually consecutive clips and modalities, enabling synchronized long-duration audio-visual content generation.Extensive experiments demonstrate that our framework can generate vivid and contextually coherent long-duration dialogue interactions and accurately interpret users' multimodal queries.",
        "url": "http://arxiv.org/abs/2512.03034v1",
        "published_date": "2025-12-02T18:55:53+00:00",
        "updated_date": "2025-12-02T18:55:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Youxin Pang",
            "Jiajun Liu",
            "Lingfeng Tan",
            "Yong Zhang",
            "Feng Gao",
            "Xiang Deng",
            "Zhuoliang Kang",
            "Xiaoming Wei",
            "Yebin Liu"
        ],
        "tldr": "The paper introduces MAViD, a multimodal framework with a Conductor-Creator architecture, that aims to improve audio-visual dialogue understanding and generation, particularly for long-duration, contextually coherent interactions. It uses a combination of AR and diffusion models for generating high-quality, synchronized audio and video.",
        "tldr_zh": "该论文介绍了MAViD，一个具有指挥者-创造者架构的多模态框架，旨在改进视听对话的理解和生成，特别是在长时间、上下文连贯的交互方面。它结合使用AR和扩散模型来生成高质量、同步的音频和视频。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "In-Context Sync-LoRA for Portrait Video Editing",
        "summary": "Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.",
        "url": "http://arxiv.org/abs/2512.03013v1",
        "published_date": "2025-12-02T18:40:35+00:00",
        "updated_date": "2025-12-02T18:40:35+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.GR"
        ],
        "authors": [
            "Sagi Polaczek",
            "Or Patashnik",
            "Ali Mahdavi-Amiri",
            "Daniel Cohen-Or"
        ],
        "tldr": "The paper introduces Sync-LoRA, a method for portrait video editing that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency, using in-context learning with automatically generated and curated video pairs.",
        "tldr_zh": "该论文介绍了Sync-LoRA，一种用于人像视频编辑的方法，它通过使用自动生成和管理视频对的上下文学习，实现了高质量的视觉修改，同时保持了帧精确的同步和身份一致。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation",
        "summary": "We present FloodDiffusion, a new framework for text-driven, streaming human motion generation. Given time-varying text prompts, FloodDiffusion generates text-aligned, seamless motion sequences with real-time latency. Unlike existing methods that rely on chunk-by-chunk or auto-regressive model with diffusion head, we adopt a diffusion forcing framework to model this time-series generation task under time-varying control events. We find that a straightforward implementation of vanilla diffusion forcing (as proposed for video models) fails to model real motion distributions. We demonstrate that to guarantee modeling the output distribution, the vanilla diffusion forcing must be tailored to: (i) train with a bi-directional attention instead of casual attention; (ii) implement a lower triangular time scheduler instead of a random one; (iii) utilize a continues time-varying way to introduce text conditioning. With these improvements, we demonstrate in the first time that the diffusion forcing-based framework achieves state-of-the-art performance on the streaming motion generation task, reaching an FID of 0.057 on the HumanML3D benchmark. Models, code, and weights are available. https://shandaai.github.io/FloodDiffusion/",
        "url": "http://arxiv.org/abs/2512.03520v1",
        "published_date": "2025-12-03T07:23:47+00:00",
        "updated_date": "2025-12-03T07:23:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiyi Cai",
            "Yuhan Wu",
            "Kunhang Li",
            "You Zhou",
            "Bo Zheng",
            "Haiyang Liu"
        ],
        "tldr": "FloodDiffusion is a new framework for text-driven, streaming human motion generation using a tailored diffusion forcing approach, achieving state-of-the-art performance. It addresses limitations of vanilla diffusion forcing by incorporating bidirectional attention, a lower triangular time scheduler, and continuous time-varying text conditioning.",
        "tldr_zh": "FloodDiffusion是一个新的文本驱动的流式人体运动生成框架，它使用了定制的扩散强制方法，并实现了最先进的性能。它通过结合双向注意力、下三角时间调度器和连续时变文本条件来解决普通扩散强制的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]