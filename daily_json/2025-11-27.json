[
    {
        "title": "Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy",
        "summary": "The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.",
        "url": "http://arxiv.org/abs/2511.21579v1",
        "published_date": "2025-11-26T16:53:05+00:00",
        "updated_date": "2025-11-26T16:53:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Teng Hu",
            "Zhentao Yu",
            "Guozhen Zhang",
            "Zihan Su",
            "Zhengguang Zhou",
            "Youliang Zhang",
            "Yuan Zhou",
            "Qinglin Lu",
            "Ran Yi"
        ],
        "tldr": "The paper introduces Harmony, a novel framework for synchronized audio-visual content generation that addresses challenges in audio-video alignment using Cross-Task Synergy training, a Global-Local Decoupled Interaction Module, and Synchronization-Enhanced CFG, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了一种名为 Harmony 的新型框架，用于同步音视频内容生成。该框架通过交叉任务协同训练、全局-局部解耦交互模块和同步增强的 CFG，解决了音视频对齐方面的挑战，并取得了最先进的结果。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MobileI2V: Fast and High-Resolution Image-to-Video on Mobile Devices",
        "summary": "Recently, video generation has witnessed rapid advancements, drawing increasing attention to image-to-video (I2V) synthesis on mobile devices. However, the substantial computational complexity and slow generation speed of diffusion models pose significant challenges for real-time, high-resolution video generation on resource-constrained mobile devices. In this work, we propose MobileI2V, a 270M lightweight diffusion model for real-time image-to-video generation on mobile devices. The core lies in: (1) We analyzed the performance of linear attention modules and softmax attention modules on mobile devices, and proposed a linear hybrid architecture denoiser that balances generation efficiency and quality. (2) We design a time-step distillation strategy that compresses the I2V sampling steps from more than 20 to only two without significant quality loss, resulting in a 10-fold increase in generation speed. (3) We apply mobile-specific attention optimizations that yield a 2-fold speed-up for attention operations during on-device inference. MobileI2V enables, for the first time, fast 720p image-to-video generation on mobile devices, with quality comparable to existing models. Under one-step conditions, the generation speed of each frame of 720p video is less than 100 ms. Our code is available at: https://github.com/hustvl/MobileI2V.",
        "url": "http://arxiv.org/abs/2511.21475v1",
        "published_date": "2025-11-26T15:09:02+00:00",
        "updated_date": "2025-11-26T15:09:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuai Zhang",
            "Bao Tang",
            "Siyuan Yu",
            "Yueting Zhu",
            "Jingfeng Yao",
            "Ya Zou",
            "Shanglin Yuan",
            "Li Yu",
            "Wenyu Liu",
            "Xinggang Wang"
        ],
        "tldr": "The paper introduces MobileI2V, a lightweight diffusion model with optimizations for real-time, high-resolution image-to-video generation on mobile devices, achieving fast 720p video generation with comparable quality to existing models.",
        "tldr_zh": "本文介绍了MobileI2V，这是一种轻量级扩散模型，针对移动设备上的实时、高分辨率图像到视频生成进行了优化，实现了快速的720p视频生成，其质量与现有模型相当。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "CtrlVDiff: Controllable Video Generation via Unified Multimodal Video Diffusion",
        "summary": "We tackle the dual challenges of video understanding and controllable video generation within a unified diffusion framework. Our key insights are two-fold: geometry-only cues (e.g., depth, edges) are insufficient: they specify layout but under-constrain appearance, materials, and illumination, limiting physically meaningful edits such as relighting or material swaps and often causing temporal drift. Enriching the model with additional graphics-based modalities (intrinsics and semantics) provides complementary constraints that both disambiguate understanding and enable precise, predictable control during generation.\n  However, building a single model that uses many heterogeneous cues introduces two core difficulties. Architecturally, the model must accept any subset of modalities, remain robust to missing inputs, and inject control signals without sacrificing temporal consistency. Data-wise, training demands large-scale, temporally aligned supervision that ties real videos to per-pixel multimodal annotations.\n  We then propose CtrlVDiff, a unified diffusion model trained with a Hybrid Modality Control Strategy (HMCS) that routes and fuses features from depth, normals, segmentation, edges, and graphics-based intrinsics (albedo, roughness, metallic), and re-renders videos from any chosen subset with strong temporal coherence. To enable this, we build MMVideo, a hybrid real-and-synthetic dataset aligned across modalities and captions. Across understanding and generation benchmarks, CtrlVDiff delivers superior controllability and fidelity, enabling layer-wise edits (relighting, material adjustment, object insertion) and surpassing state-of-the-art baselines while remaining robust when some modalities are unavailable.",
        "url": "http://arxiv.org/abs/2511.21129v1",
        "published_date": "2025-11-26T07:27:11+00:00",
        "updated_date": "2025-11-26T07:27:11+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Dianbing Xi",
            "Jiepeng Wang",
            "Yuanzhi Liang",
            "Xi Qiu",
            "Jialun Liu",
            "Hao Pan",
            "Yuchi Huo",
            "Rui Wang",
            "Haibin Huang",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "tldr": "The paper introduces CtrlVDiff, a unified multimodal video diffusion model leveraging geometry and graphics-based modalities for controllable and high-fidelity video generation, trained on a new dataset MMVideo.",
        "tldr_zh": "该论文介绍了CtrlVDiff，一个统一的多模态视频扩散模型，利用几何和基于图形的模态进行可控和高保真的视频生成，并在一个新的数据集MMVideo上进行训练。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
        "summary": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
        "url": "http://arxiv.org/abs/2511.20649v1",
        "published_date": "2025-11-25T18:59:46+00:00",
        "updated_date": "2025-11-25T18:59:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hidir Yesiltepe",
            "Tuna Han Salih Meral",
            "Adil Kaan Akan",
            "Kaan Oktay",
            "Pinar Yanardag"
        ],
        "tldr": "The paper introduces Infinity-RoPE ($\\infty$-RoPE), a training-free framework enabling infinite-horizon, controllable, and cinematic video diffusion by addressing limitations in 3D-RoPE based models through Block-Relativistic RoPE, KV Flush, and RoPE Cut.",
        "tldr_zh": "该论文介绍了 Infinity-RoPE ($\\infty$-RoPE)，一个无需训练的框架，通过 Block-Relativistic RoPE、KV Flush 和 RoPE Cut 解决了基于 3D-RoPE 模型的局限性，从而实现无限视野、可控且电影化的视频扩散。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization",
        "summary": "While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.",
        "url": "http://arxiv.org/abs/2511.20647v1",
        "published_date": "2025-11-25T18:59:45+00:00",
        "updated_date": "2025-11-25T18:59:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tahira Kazimi",
            "Connor Dunlop",
            "Pinar Yanardag"
        ],
        "tldr": "This paper introduces DPP-GRPO, a novel plug-and-play framework for diverse text-to-video generation that uses Determinantal Point Processes and Group Relative Policy Optimization to improve video diversity without sacrificing prompt fidelity.",
        "tldr_zh": "本文介绍了一种名为DPP-GRPO的新型即插即用框架，用于实现多样化的文本到视频生成。该框架利用行列式点过程和群相对策略优化来提高视频的多样性，同时不牺牲提示的保真度。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "summary": "Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.",
        "url": "http://arxiv.org/abs/2511.20645v1",
        "published_date": "2025-11-25T18:59:25+00:00",
        "updated_date": "2025-11-25T18:59:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongsheng Yu",
            "Wei Xiong",
            "Weili Nie",
            "Yichen Sheng",
            "Shiqiu Liu",
            "Jiebo Luo"
        ],
        "tldr": "PixelDiT is a single-stage, end-to-end diffusion transformer model operating directly in pixel space, eliminating the need for autoencoders and achieving state-of-the-art results in image generation.",
        "tldr_zh": "PixelDiT是一个单阶段、端到端的扩散Transformer模型，直接在像素空间中运行，消除了对自动编码器的需求，并在图像生成方面取得了最先进的效果。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models",
        "summary": "Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.",
        "url": "http://arxiv.org/abs/2511.20629v1",
        "published_date": "2025-11-25T18:49:21+00:00",
        "updated_date": "2025-11-25T18:49:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Chieh-Yun Chen",
            "Zhonghao Wang",
            "Qi Chen",
            "Zhifan Ye",
            "Min Shi",
            "Yue Zhao",
            "Yinan Zhao",
            "Hui Qu",
            "Wei-An Lin",
            "Yiru Shen",
            "Ajinkya Kale",
            "Irfan Essa",
            "Humphrey Shi"
        ],
        "tldr": "The paper introduces MapReduce LoRA and Reward-aware Token Embedding (RaTE) to improve multi-preference optimization in generative models, showing significant improvements in text-to-image, text-to-video, and language tasks.",
        "tldr_zh": "该论文介绍了MapReduce LoRA和Reward-aware Token Embedding (RaTE)两种方法，旨在改进生成模型中的多偏好优化，并在文本到图像、文本到视频和语言任务中显示出显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "VQ-VA World: Towards High-Quality Visual Question-Visual Answering",
        "summary": "This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.",
        "url": "http://arxiv.org/abs/2511.20573v1",
        "published_date": "2025-11-25T18:06:22+00:00",
        "updated_date": "2025-11-25T18:06:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenhui Gou",
            "Zilong Chen",
            "Zeyu Wang",
            "Feng Li",
            "Deyao Zhu",
            "Zicheng Duan",
            "Kunchang Li",
            "Chaorui Deng",
            "Hongyi Yuan",
            "Haoqi Fan",
            "Cihang Xie",
            "Jianfei Cai",
            "Hamid Rezatofighi"
        ],
        "tldr": "The paper introduces VQ-VA World, a framework and large-scale dataset for Visual Question-Visual Answering (VQ-VA), achieving significant performance improvements over open-source baselines on a newly introduced benchmark, IntelligentBench.",
        "tldr_zh": "该论文介绍了VQ-VA World，一个用于视觉问题-视觉回答(VQ-VA)的框架和大规模数据集，并在新推出的基准测试IntelligentBench上实现了优于开源基线的显著性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 10,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Video Generation Models Are Good Latent Reward Models",
        "summary": "Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.",
        "url": "http://arxiv.org/abs/2511.21541v1",
        "published_date": "2025-11-26T16:14:18+00:00",
        "updated_date": "2025-11-26T16:14:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyue Mi",
            "Wenqing Yu",
            "Jiesong Lian",
            "Shibo Jie",
            "Ruizhe Zhong",
            "Zijun Liu",
            "Guozhen Zhang",
            "Zixiang Zhou",
            "Zhiyong Xu",
            "Yuan Zhou",
            "Qinglin Lu",
            "Fan Tang"
        ],
        "tldr": "The paper introduces Process Reward Feedback Learning (PRFL), a method for aligning video generation with human preferences by performing reward modeling directly in the latent space of pre-trained video generation models, leading to improved efficiency and performance compared to pixel-space reward feedback learning.",
        "tldr_zh": "本文介绍了一种名为过程奖励反馈学习 (PRFL) 的方法，该方法通过直接在预训练视频生成模型的潜在空间中执行奖励建模，来使视频生成与人类偏好对齐，与像素空间奖励反馈学习相比，提高了效率和性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiverseVAR: Balancing Diversity and Quality of Next-Scale Visual Autoregressive Models",
        "summary": "We introduce DiverseVAR, a framework that enhances the diversity of text-conditioned visual autoregressive models (VAR) at test time without requiring retraining, fine-tuning, or substantial computational overhead. While VAR models have recently emerged as strong competitors to diffusion and flow models for image generation, they suffer from a critical limitation in diversity, often producing nearly identical images even for simple prompts. This issue has largely gone unnoticed amid the predominant focus on image quality. We address this limitation at test time in two stages. First, inspired by diversity enhancement techniques in diffusion models, we propose injecting noise into the text embedding. This introduces a trade-off between diversity and image quality: as diversity increases, the image quality sharply declines. To preserve quality, we propose scale-travel: a novel latent refinement technique inspired by time-travel strategies in diffusion models. Specifically, we use a multi-scale autoencoder to extract coarse-scale tokens that enable us to resume generation at intermediate stages. Extensive experiments show that combining text-embedding noise injection with our scale-travel refinement significantly enhances diversity while minimizing image-quality degradation, achieving a new Pareto frontier in the diversity-quality trade-off.",
        "url": "http://arxiv.org/abs/2511.21415v1",
        "published_date": "2025-11-26T14:06:52+00:00",
        "updated_date": "2025-11-26T14:06:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingue Park",
            "Prin Phunyaphibarn",
            "Phillip Y. Lee",
            "Minhyuk Sung"
        ],
        "tldr": "The paper introduces DiverseVAR, a novel test-time method to improve the diversity of text-conditioned visual autoregressive models without retraining, using noise injection and a multi-scale refinement technique called scale-travel to balance diversity and image quality.",
        "tldr_zh": "该论文介绍了DiverseVAR，一种新颖的测试时方法，通过注入噪声和一种名为scale-travel的多尺度优化技术来提高文本条件视觉自回归模型的多样性，无需重新训练，从而平衡了多样性和图像质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AV-Edit: Multimodal Generative Sound Effect Editing via Audio-Visual Semantic Joint Control",
        "summary": "Sound effect editing-modifying audio by adding, removing, or replacing elements-remains constrained by existing approaches that rely solely on low-level signal processing or coarse text prompts, often resulting in limited flexibility and suboptimal audio quality. To address this, we propose AV-Edit, a generative sound effect editing framework that enables fine-grained editing of existing audio tracks in videos by jointly leveraging visual, audio, and text semantics. Specifically, the proposed method employs a specially designed contrastive audio-visual masking autoencoder (CAV-MAE-Edit) for multimodal pre-training, learning aligned cross-modal representations. These representations are then used to train an editorial Multimodal Diffusion Transformer (MM-DiT) capable of removing visually irrelevant sounds and generating missing audio elements consistent with video content through a correlation-based feature gating training strategy. Furthermore, we construct a dedicated video-based sound editing dataset as an evaluation benchmark. Experiments demonstrate that the proposed AV-Edit generates high-quality audio with precise modifications based on visual content, achieving state-of-the-art performance in the field of sound effect editing and exhibiting strong competitiveness in the domain of audio generation.",
        "url": "http://arxiv.org/abs/2511.21146v1",
        "published_date": "2025-11-26T07:59:53+00:00",
        "updated_date": "2025-11-26T07:59:53+00:00",
        "categories": [
            "cs.MM",
            "cs.CV",
            "cs.SD"
        ],
        "authors": [
            "Xinyue Guo",
            "Xiaoran Yang",
            "Lipan Zhang",
            "Jianxuan Yang",
            "Zhao Wang",
            "Jian Luan"
        ],
        "tldr": "The paper introduces AV-Edit, a multimodal generative framework for fine-grained sound effect editing in videos using visual, audio, and text semantics, achieving state-of-the-art performance by leveraging a contrastive audio-visual masking autoencoder and a multimodal diffusion transformer.",
        "tldr_zh": "该论文介绍了AV-Edit，一个多模态生成框架，通过联合利用视觉、音频和文本语义，对视频中的声音效果进行细粒度的编辑。它利用对比音频-视觉掩蔽自编码器和多模态扩散变换器，实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Training for Human Video Generation with Entropy-Guided Prioritized Progressive Learning",
        "summary": "Human video generation has advanced rapidly with the development of diffusion models, but the high computational cost and substantial memory consumption associated with training these models on high-resolution, multi-frame data pose significant challenges. In this paper, we propose Entropy-Guided Prioritized Progressive Learning (Ent-Prog), an efficient training framework tailored for diffusion models on human video generation. First, we introduce Conditional Entropy Inflation (CEI) to assess the importance of different model components on the target conditional generation task, enabling prioritized training of the most critical components. Second, we introduce an adaptive progressive schedule that adaptively increases computational complexity during training by measuring the convergence efficiency. Ent-Prog reduces both training time and GPU memory consumption while maintaining model performance. Extensive experiments across three datasets, demonstrate the effectiveness of Ent-Prog, achieving up to 2.2$\\times$ training speedup and 2.4$\\times$ GPU memory reduction without compromising generative performance.",
        "url": "http://arxiv.org/abs/2511.21136v1",
        "published_date": "2025-11-26T07:36:37+00:00",
        "updated_date": "2025-11-26T07:36:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Changlin Li",
            "Jiawei Zhang",
            "Shuhao Liu",
            "Sihao Lin",
            "Zeyi Shi",
            "Zhihui Li",
            "Xiaojun Chang"
        ],
        "tldr": "This paper introduces an efficient training framework (Ent-Prog) for human video generation using diffusion models, which prioritizes training based on component importance and adapts computational complexity, achieving speedup and memory reduction.",
        "tldr_zh": "本文提出了一种高效的扩散模型人体视频生成训练框架（Ent-Prog），该框架通过基于组件重要性的优先级训练和自适应计算复杂度，实现了加速和内存减少。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MIRA: Multimodal Iterative Reasoning Agent for Image Editing",
        "summary": "Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.",
        "url": "http://arxiv.org/abs/2511.21087v1",
        "published_date": "2025-11-26T06:13:32+00:00",
        "updated_date": "2025-11-26T06:13:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyun Zeng",
            "Hang Hua",
            "Jiebo Luo"
        ],
        "tldr": "The paper introduces MIRA, a multimodal iterative reasoning agent that enhances instruction-guided image editing by using visual feedback to refine edits step-by-step, achieving performance comparable to proprietary systems.",
        "tldr_zh": "这篇论文介绍了MIRA，一个多模态迭代推理代理，通过使用视觉反馈逐步改进编辑，从而增强了指令引导的图像编辑，实现了与专有系统相当的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MUSE: Manipulating Unified Framework for Synthesizing Emotions in Images via Test-Time Optimization",
        "summary": "Images evoke emotions that profoundly influence perception, often prioritized over content. Current Image Emotional Synthesis (IES) approaches artificially separate generation and editing tasks, creating inefficiencies and limiting applications where these tasks naturally intertwine, such as therapeutic interventions or storytelling. In this work, we introduce MUSE, the first unified framework capable of both emotional generation and editing. By adopting a strategy conceptually aligned with Test-Time Scaling (TTS) that widely used in both LLM and diffusion model communities, it avoids the requirement for additional updating diffusion model and specialized emotional synthesis datasets. More specifically, MUSE addresses three key questions in emotional synthesis: (1) HOW to stably guide synthesis by leveraging an off-the-shelf emotion classifier with gradient-based optimization of emotional tokens; (2) WHEN to introduce emotional guidance by identifying the optimal timing using semantic similarity as a supervisory signal; and (3) WHICH emotion to guide synthesis through a multi-emotion loss that reduces interference from inherent and similar emotions. Experimental results show that MUSE performs favorably against all methods for both generation and editing, improving emotional accuracy and semantic diversity while maintaining an optimal balance between desired content, adherence to text prompts, and realistic emotional expression. It establishes a new paradigm for emotion synthesis.",
        "url": "http://arxiv.org/abs/2511.21051v1",
        "published_date": "2025-11-26T04:39:25+00:00",
        "updated_date": "2025-11-26T04:39:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingjie Xia",
            "Xi Wang",
            "Jinglei Shi",
            "Vicky Kalogeiton",
            "Jian Yang"
        ],
        "tldr": "The paper introduces MUSE, a unified framework for emotional image generation and editing using test-time optimization with an off-the-shelf emotion classifier, claiming state-of-the-art performance.",
        "tldr_zh": "该论文介绍了MUSE，一个统一的情感图像生成和编辑框架，它使用测试时优化和一个现成的的情感分类器，并声称达到了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Deep Parameter Interpolation for Scalar Conditioning",
        "summary": "We propose deep parameter interpolation (DPI), a general-purpose method for transforming an existing deep neural network architecture into one that accepts an additional scalar input. Recent deep generative models, including diffusion models and flow matching, employ a single neural network to learn a time- or noise level-dependent vector field. Designing a network architecture to accurately represent this vector field is challenging because the network must integrate information from two different sources: a high-dimensional vector (usually an image) and a scalar. Common approaches either encode the scalar as an additional image input or combine scalar and vector information in specific network components, which restricts architecture choices. Instead, we propose to maintain two learnable parameter sets within a single network and to introduce the scalar dependency by dynamically interpolating between the parameter sets based on the scalar value during training and sampling. DPI is a simple, architecture-agnostic method for adding scalar dependence to a neural network. We demonstrate that our method improves denoising performance and enhances sample quality for both diffusion and flow matching models, while achieving computational efficiency comparable to standard scalar conditioning techniques. Code is available at https://github.com/wustl-cig/parameter_interpolation.",
        "url": "http://arxiv.org/abs/2511.21028v1",
        "published_date": "2025-11-26T03:52:12+00:00",
        "updated_date": "2025-11-26T03:52:12+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Chicago Y. Park",
            "Michael T. McCann",
            "Cristina Garcia-Cardona",
            "Brendt Wohlberg",
            "Ulugbek S. Kamilov"
        ],
        "tldr": "This paper introduces Deep Parameter Interpolation (DPI) for incorporating scalar conditioning into deep neural networks, especially for generative models like diffusion models, improving performance and sample quality without architectural restrictions.",
        "tldr_zh": "该论文介绍了深度参数插值（DPI），用于将标量调节融入深度神经网络，特别是在扩散模型等生成模型中，从而提高性能和样本质量，且不限制架构选择。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation",
        "summary": "Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.",
        "url": "http://arxiv.org/abs/2511.20889v1",
        "published_date": "2025-11-25T22:11:51+00:00",
        "updated_date": "2025-11-25T22:11:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Taehoon Kim",
            "Henry Gouk",
            "Timothy Hospedales"
        ],
        "tldr": "The paper introduces Null-TTA, a novel test-time alignment method for text-to-image diffusion models that optimizes the unconditional embedding in classifier-free guidance to steer the generative distribution towards a target reward while preventing reward hacking.",
        "tldr_zh": "该论文介绍了一种名为Null-TTA的新型测试时对齐方法，用于文本到图像扩散模型。该方法通过优化无分类器引导中的无条件嵌入来引导生成分布趋向于目标奖励，同时防止奖励利用。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion",
        "summary": "Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and data-free alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective, achieving quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, indicating that the idea merits further investigation. The code will be publicly available upon acceptance.",
        "url": "http://arxiv.org/abs/2511.20821v1",
        "published_date": "2025-11-25T20:20:21+00:00",
        "updated_date": "2025-11-25T20:20:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Samuele Dell'Erba",
            "Andrew D. Bagdanov"
        ],
        "tldr": "This paper presents a training-free alternative to diffusion priors in text-to-image generation using optimization-based visual inversion (OVI) and shows that existing benchmarks may be flawed. OVI with novel constraints achieves competitive performance compared to trained priors.",
        "tldr_zh": "本文提出了一种基于优化视觉反演(OVI)的无训练扩散先验，用于文本到图像生成，并表明现有的基准可能存在缺陷。具有新约束的OVI实现了与训练先验相当的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Layer-Aware Video Composition via Split-then-Merge",
        "summary": "We present Split-then-Merge (StM), a novel framework designed to enhance control in generative video composition and address its data scarcity problem. Unlike conventional methods relying on annotated datasets or handcrafted rules, StM splits a large corpus of unlabeled videos into dynamic foreground and background layers, then self-composes them to learn how dynamic subjects interact with diverse scenes. This process enables the model to learn the complex compositional dynamics required for realistic video generation. StM introduces a novel transformation-aware training pipeline that utilizes a multi-layer fusion and augmentation to achieve affordance-aware composition, alongside an identity-preservation loss that maintains foreground fidelity during blending. Experiments show StM outperforms SoTA methods in both quantitative benchmarks and in humans/VLLM-based qualitative evaluations. More details are available at our project page: https://split-then-merge.github.io",
        "url": "http://arxiv.org/abs/2511.20809v1",
        "published_date": "2025-11-25T19:53:15+00:00",
        "updated_date": "2025-11-25T19:53:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ozgur Kara",
            "Yujia Chen",
            "Ming-Hsuan Yang",
            "James M. Rehg",
            "Wen-Sheng Chu",
            "Du Tran"
        ],
        "tldr": "The paper introduces Split-then-Merge (StM), a novel framework for generative video composition that addresses data scarcity by self-composing foreground and background layers from unlabeled videos, achieving state-of-the-art performance.",
        "tldr_zh": "本文介绍了一种新的生成视频合成框架Split-then-Merge (StM)，通过从无标签视频中自我合成前景和背景层来解决数据稀缺问题，并实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RubricRL: Simple Generalizable Rewards for Text-to-Image Generation",
        "summary": "Reinforcement learning (RL) has recently emerged as a promising approach for aligning text-to-image generative models with human preferences. A key challenge, however, lies in designing effective and interpretable rewards. Existing methods often rely on either composite metrics (e.g., CLIP, OCR, and realism scores) with fixed weights or a single scalar reward distilled from human preference models, which can limit interpretability and flexibility. We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control. Instead of using a black-box scalar signal, RubricRL dynamically constructs a structured rubric for each prompt--a decomposable checklist of fine-grained visual criteria such as object correctness, attribute accuracy, OCR fidelity, and realism--tailored to the input text. Each criterion is independently evaluated by a multimodal judge (e.g., o4-mini), and a prompt-adaptive weighting mechanism emphasizes the most relevant dimensions. This design not only produces interpretable and modular supervision signals for policy optimization (e.g., GRPO or PPO), but also enables users to directly adjust which aspects to reward or penalize. Experiments with an autoregressive text-to-image model demonstrate that RubricRL improves prompt faithfulness, visual detail, and generalizability, while offering a flexible and extensible foundation for interpretable RL alignment across text-to-image architectures.",
        "url": "http://arxiv.org/abs/2511.20651v1",
        "published_date": "2025-11-25T18:59:55+00:00",
        "updated_date": "2025-11-25T18:59:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuelu Feng",
            "Yunsheng Li",
            "Ziyu Wan",
            "Zixuan Gao",
            "Junsong Yuan",
            "Dongdong Chen",
            "Chunming Qiao"
        ],
        "tldr": "RubricRL introduces a rubric-based reward system for RL fine-tuning of text-to-image models, providing interpretable and controllable feedback based on fine-grained visual criteria assessed by multimodal judges.",
        "tldr_zh": "RubricRL 引入了一种基于规则的奖励系统，用于对文本到图像模型的强化学习微调，基于多模态裁判评估的细粒度视觉标准提供可解释和可控的反馈。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation",
        "summary": "Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.",
        "url": "http://arxiv.org/abs/2511.20635v1",
        "published_date": "2025-11-25T18:54:16+00:00",
        "updated_date": "2025-11-25T18:54:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhoujie Fu",
            "Xianfang Zeng",
            "Jinghong Lan",
            "Xinyao Liao",
            "Cheng Chen",
            "Junyi Chen",
            "Jiacheng Wei",
            "Wei Cheng",
            "Shiyu Liu",
            "Yunuo Chen",
            "Gang Yu",
            "Guosheng Lin"
        ],
        "tldr": "The paper introduces iMontage, a unified framework that repurposes a video model for versatile many-to-many image generation, achieving both contextual consistency and expansive dynamics.",
        "tldr_zh": "该论文介绍了iMontage，一个统一的框架，它将视频模型重新用于多功能的多对多图像生成，实现了上下文一致性和广泛的动态性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ShapeGen: Towards High-Quality 3D Shape Synthesis",
        "summary": "Inspired by generative paradigms in image and video, 3D shape generation has made notable progress, enabling the rapid synthesis of high-fidelity 3D assets from a single image. However, current methods still face challenges, including the lack of intricate details, overly smoothed surfaces, and fragmented thin-shell structures. These limitations leave the generated 3D assets still one step short of meeting the standards favored by artists. In this paper, we present ShapeGen, which achieves high-quality image-to-3D shape generation through 3D representation and supervision improvements, resolution scaling up, and the advantages of linear transformers. These advancements allow the generated assets to be seamlessly integrated into 3D pipelines, facilitating their widespread adoption across various applications. Through extensive experiments, we validate the impact of these improvements on overall performance. Ultimately, thanks to the synergistic effects of these enhancements, ShapeGen achieves a significant leap in image-to-3D generation, establishing a new state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2511.20624v1",
        "published_date": "2025-11-25T18:47:27+00:00",
        "updated_date": "2025-11-25T18:47:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yangguang Li",
            "Xianglong He",
            "Zi-Xin Zou",
            "Zexiang Liu",
            "Wanli Ouyang",
            "Ding Liang",
            "Yan-Pei Cao"
        ],
        "tldr": "ShapeGen presents a new approach to image-to-3D shape generation that overcomes limitations of previous methods related to detail, smoothness, and thin structures, achieving state-of-the-art performance.",
        "tldr_zh": "ShapeGen提出了一种新的图像到3D形状生成方法，克服了先前方法在细节、平滑度和薄结构方面的局限性，实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment",
        "summary": "Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods.",
        "url": "http://arxiv.org/abs/2511.20614v1",
        "published_date": "2025-11-25T18:40:25+00:00",
        "updated_date": "2025-11-25T18:40:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziheng Ouyang",
            "Yiren Song",
            "Yaoli Liu",
            "Shihao Zhu",
            "Qibin Hou",
            "Ming-Ming Cheng",
            "Mike Zheng Shou"
        ],
        "tldr": "The paper introduces ImageCritic, a reference-guided post-editing approach to correct inconsistencies in generated images, using a VLM-based dataset construction and attention alignment loss.",
        "tldr_zh": "本文介绍了一种名为ImageCritic的参考引导后编辑方法，通过基于VLM的数据集构建和注意力对齐损失来纠正生成图像中的不一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DINO-Tok: Adapting DINO for Visual Tokenizers",
        "summary": "Recent advances in visual generation have highlighted the rise of Latent Generative Models (LGMs), which rely on effective visual tokenizers to bridge pixels and semantics. However, existing tokenizers are typically trained from scratch and struggle to balance semantic representation and reconstruction fidelity, particularly in high-dimensional latent spaces. In this work, we introduce DINO-Tok, a DINO-based visual tokenizer that unifies hierarchical representations into an information-complete latent space. By integrating shallow features that retain fine-grained details with deep features encoding global semantics, DINO-Tok effectively bridges pretrained representations and visual generation. We further analyze the challenges of vector quantization (VQ) in this high-dimensional space, where key information is often lost and codebook collapse occurs. We thus propose a global PCA reweighting mechanism to stabilize VQ and preserve essential information across dimensions. On ImageNet 256$\\times$256, DINO-Tok achieves state-of-the-art reconstruction performance, reaching 28.54 PSNR for autoencoding and 23.98 PSNR for VQ-based modeling, significantly outperforming prior tokenizers and comparable to billion-level data trained models (such as Hunyuan and Wan). These results demonstrate that adapting powerful pretrained vision models like DINO for tokenization enables semantically aligned and high-fidelity latent representations, enabling next-generation visual generative models. Code will be publicly available at https://github.com/MKJia/DINO-Tok.",
        "url": "http://arxiv.org/abs/2511.20565v1",
        "published_date": "2025-11-25T18:00:00+00:00",
        "updated_date": "2025-11-25T18:00:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingkai Jia",
            "Mingxiao Li",
            "Liaoyuan Fan",
            "Tianxing Shi",
            "Jiaxin Guo",
            "Zeming Li",
            "Xiaoyang Guo",
            "Xiao-Xiao Long",
            "Qian Zhang",
            "Ping Tan",
            "Wei Yin"
        ],
        "tldr": "DINO-Tok adapts the DINO vision model for visual tokenization in latent generative models, achieving state-of-the-art reconstruction performance by unifying hierarchical representations and stabilizing vector quantization with a global PCA reweighting mechanism.",
        "tldr_zh": "DINO-Tok 采用 DINO 视觉模型进行潜在生成模型中的视觉令牌化，通过统一分层表示和使用全局 PCA 重新加权机制稳定矢量量化，实现了最先进的重建性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CameraMaster: Unified Camera Semantic-Parameter Control for Photography Retouching",
        "summary": "Text-guided diffusion models have greatly advanced image editing and generation. However, achieving physically consistent image retouching with precise parameter control (e.g., exposure, white balance, zoom) remains challenging. Existing methods either rely solely on ambiguous and entangled text prompts, which hinders precise camera control, or train separate heads/weights for parameter adjustment, which compromises scalability, multi-parameter composition, and sensitivity to subtle variations. To address these limitations, we propose CameraMaster, a unified camera-aware framework for image retouching. The key idea is to explicitly decouple the camera directive and then coherently integrate two critical information streams: a directive representation that captures the photographer's intent, and a parameter embedding that encodes precise camera settings. CameraMaster first uses the camera parameter embedding to modulate both the camera directive and the content semantics. The modulated directive is then injected into the content features via cross-attention, yielding a strongly camera-sensitive semantic context. In addition, the directive and camera embeddings are injected as conditioning and gating signals into the time embedding, enabling unified, layer-wise modulation throughout the denoising process and enforcing tight semantic-parameter alignment. To train and evaluate CameraMaster, we construct a large-scale dataset of 78K image-prompt pairs annotated with camera parameters. Extensive experiments show that CameraMaster produces monotonic and near-linear responses to parameter variations, supports seamless multi-parameter composition, and significantly outperforms existing methods.",
        "url": "http://arxiv.org/abs/2511.21024v1",
        "published_date": "2025-11-26T03:42:34+00:00",
        "updated_date": "2025-11-26T03:42:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qirui Yang",
            "Yang Yang",
            "Ying Zeng",
            "Xiaobin Hu",
            "Bo Li",
            "Huanjing Yue",
            "Jingyu Yang",
            "Peng-Tao Jiang"
        ],
        "tldr": "The paper introduces CameraMaster, a unified framework for text-guided image retouching with precise camera parameter control, achieving superior performance in monotonic responses and multi-parameter composition compared to existing methods.",
        "tldr_zh": "该论文介绍了CameraMaster，一个统一的框架，用于通过精确的相机参数控制进行文本引导的图像修饰，与现有方法相比，在单调响应和多参数组合方面实现了卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Beyond Realism: Learning the Art of Expressive Composition with StickerNet",
        "summary": "As a widely used operation in image editing workflows, image composition has traditionally been studied with a focus on achieving visual realism and semantic plausibility. However, in practical editing scenarios of the modern content creation landscape, many compositions are not intended to preserve realism. Instead, users of online platforms motivated by gaining community recognition often aim to create content that is more artistic, playful, or socially engaging. Taking inspiration from this observation, we define the expressive composition task, a new formulation of image composition that embraces stylistic diversity and looser placement logic, reflecting how users edit images on real-world creative platforms. To address this underexplored problem, we present StickerNet, a two-stage framework that first determines the composition type, then predicts placement parameters such as opacity, mask, location, and scale accordingly. Unlike prior work that constructs datasets by simulating object placements on real images, we directly build our dataset from 1.8 million editing actions collected on an anonymous online visual creation and editing platform, each reflecting user-community validated placement decisions. This grounding in authentic editing behavior ensures strong alignment between task definition and training supervision. User studies and quantitative evaluations show that StickerNet outperforms common baselines and closely matches human placement behavior, demonstrating the effectiveness of learning from real-world editing patterns despite the inherent ambiguity of the task. This work introduces a new direction in visual understanding that emphasizes expressiveness and user intent over realism.",
        "url": "http://arxiv.org/abs/2511.20957v1",
        "published_date": "2025-11-26T01:24:30+00:00",
        "updated_date": "2025-11-26T01:24:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoming Lu",
            "David Kocharian",
            "Humphrey Shi"
        ],
        "tldr": "The paper introduces \"expressive composition,\" a task reflecting artistic image editing, and presents StickerNet, a framework trained on real-world user editing data to predict sticker placement parameters beyond realism.",
        "tldr_zh": "该论文介绍了“表达性组合”，这是一个反映艺术图像编辑的任务，并提出了StickerNet，一个在真实用户编辑数据上训练的框架，用于预测超越现实主义的贴纸放置参数。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GaINeR: Geometry-Aware Implicit Network Representation",
        "summary": "Implicit Neural Representations (INRs) have become an essential tool for modeling continuous 2D images, enabling high-fidelity reconstruction, super-resolution, and compression. Popular architectures such as SIREN, WIRE, and FINER demonstrate the potential of INR for capturing fine-grained image details. However, traditional INRs often lack explicit geometric structure and have limited capabilities for local editing or integration with physical simulation, restricting their applicability in dynamic or interactive settings. To address these limitations, we propose GaINeR: Geometry-Aware Implicit Network Representation, a novel framework for 2D images that combines trainable Gaussian distributions with a neural network-based INR. For a given image coordinate, the model retrieves the K nearest Gaussians, aggregates distance-weighted embeddings, and predicts the RGB value via a neural network. This design enables continuous image representation, interpretable geometric structure, and flexible local editing, providing a foundation for physically aware and interactive image manipulation. The official implementation of our method is publicly available at https://github.com/WJakubowska/GaINeR.",
        "url": "http://arxiv.org/abs/2511.20924v1",
        "published_date": "2025-11-25T23:37:54+00:00",
        "updated_date": "2025-11-25T23:37:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weronika Jakubowska",
            "Mikołaj Zieliński",
            "Rafał Tobiasz",
            "Krzysztof Byrski",
            "Maciej Zięba",
            "Dominik Belter",
            "Przemysław Spurek"
        ],
        "tldr": "GaINeR introduces a novel Geometry-Aware Implicit Network Representation that combines Gaussian distributions with neural network-based INRs for 2D image modeling, enabling local editing and integration with physical simulation.",
        "tldr_zh": "GaINeR 提出了一种新的几何感知隐式网络表示方法，将高斯分布与基于神经网络的 INRs 结合用于 2D 图像建模，从而实现局部编辑和与物理模拟的集成。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]