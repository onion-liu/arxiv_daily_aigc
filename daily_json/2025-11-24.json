[
    {
        "title": "ViMix-14M: A Curated Multi-Source Video-Text Dataset with Long-Form, High-Quality Captions and Crawl-Free Access",
        "summary": "Text-to-video generation has surged in interest since Sora, yet open-source models still face a data bottleneck: there is no large, high-quality, easily obtainable video-text corpus. Existing public datasets typically require manual YouTube crawling, which yields low usable volume due to link rot and access limits, and raises licensing uncertainty. This work addresses this challenge by introducing ViMix-14M, a curated multi-source video-text dataset of around 14 million pairs that provides crawl-free, download-ready access and long-form, high-quality captions tightly aligned to video. ViMix-14M is built by merging diverse open video sources, followed by unified de-duplication and quality filtering, and a multi-granularity, ground-truth-guided re-captioning pipeline that refines descriptions to better match actions, scenes, and temporal structure. We evaluate the dataset by multimodal retrieval, text-to-video generation, and video question answering tasks, observing consistent improvements over counterpart datasets. We hope this work can help removing the key barrier to training and fine-tuning open-source video foundation models, and provide insights of building high-quality and generalizable video-text datasets.",
        "url": "http://arxiv.org/abs/2511.18382v1",
        "published_date": "2025-11-23T10:19:56+00:00",
        "updated_date": "2025-11-23T10:19:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Timing Yang",
            "Sucheng Ren",
            "Alan Yuille",
            "Feng Wang"
        ],
        "tldr": "The paper introduces ViMix-14M, a large, crawl-free, high-quality video-text dataset designed to address the data bottleneck in open-source text-to-video generation, featuring 14 million video-text pairs with long-form, aligned captions.",
        "tldr_zh": "该论文介绍了 ViMix-14M，一个大型、免爬取、高质量的视频文本数据集，旨在解决开源文本到视频生成中的数据瓶颈问题，包含 1400 万个视频文本对，具有长文本、对齐的字幕。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Beyond Words and Pixels: A Benchmark for Implicit World Knowledge Reasoning in Generative Models",
        "summary": "Text-to-image (T2I) models today are capable of producing photorealistic, instruction-following images, yet they still frequently fail on prompts that require implicit world knowledge. Existing evaluation protocols either emphasize compositional alignment or rely on single-round VQA-based scoring, leaving critical dimensions such as knowledge grounding, multi-physics interactions, and auditable evidence-substantially undertested. To address these limitations, we introduce PicWorld, the first comprehensive benchmark that assesses the grasp of implicit world knowledge and physical causal reasoning of T2I models. This benchmark consists of 1,100 prompts across three core categories. To facilitate fine-grained evaluation, we propose PW-Agent, an evidence-grounded multi-agent evaluator to hierarchically assess images on their physical realism and logical consistency by decomposing prompts into verifiable visual evidence. We conduct a thorough analysis of 17 mainstream T2I models on PicWorld, illustrating that they universally exhibit a fundamental limitation in their capacity for implicit world knowledge and physical causal reasoning to varying degrees. The findings highlight the need for reasoning-aware, knowledge-integrative architectures in future T2I systems.",
        "url": "http://arxiv.org/abs/2511.18271v1",
        "published_date": "2025-11-23T03:44:54+00:00",
        "updated_date": "2025-11-23T03:44:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tianyang Han",
            "Junhao Su",
            "Junjie Hu",
            "Peizhen Yang",
            "Hengyu Shi",
            "Junfeng Luo",
            "Jialin Gao"
        ],
        "tldr": "The paper introduces PicWorld, a new benchmark for evaluating implicit world knowledge and physical reasoning in text-to-image models, revealing limitations in existing models and advocating for knowledge-integrative architectures.",
        "tldr_zh": "该论文介绍了一个新的基准测试PicWorld，用于评估文本到图像模型中隐含的世界知识和物理推理能力，揭示了现有模型的局限性，并提倡知识整合架构。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Synthetic Curriculum Reinforces Compositional Text-to-Image Generation",
        "summary": "Text-to-Image (T2I) generation has long been an open problem, with compositional synthesis remaining particularly challenging. This task requires accurate rendering of complex scenes containing multiple objects that exhibit diverse attributes as well as intricate spatial and semantic relationships, demanding both precise object placement and coherent inter-object interactions. In this paper, we propose a novel compositional curriculum reinforcement learning framework named CompGen that addresses compositional weakness in existing T2I models. Specifically, we leverage scene graphs to establish a novel difficulty criterion for compositional ability and develop a corresponding adaptive Markov Chain Monte Carlo graph sampling algorithm. This difficulty-aware approach enables the synthesis of training curriculum data that progressively optimize T2I models through reinforcement learning. We integrate our curriculum learning approach into Group Relative Policy Optimization (GRPO) and investigate different curriculum scheduling strategies. Our experiments reveal that CompGen exhibits distinct scaling curves under different curriculum scheduling strategies, with easy-to-hard and Gaussian sampling strategies yielding superior scaling performance compared to random sampling. Extensive experiments demonstrate that CompGen significantly enhances compositional generation capabilities for both diffusion-based and auto-regressive T2I models, highlighting its effectiveness in improving the compositional T2I generation systems.",
        "url": "http://arxiv.org/abs/2511.18378v1",
        "published_date": "2025-11-23T09:56:24+00:00",
        "updated_date": "2025-11-23T09:56:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shijian Wang",
            "Runhao Fu",
            "Siyi Zhao",
            "Qingqin Zhan",
            "Xingjian Wang",
            "Jiarui Jin",
            "Yuan Lu",
            "Hanqian Wu",
            "Cunjian Chen"
        ],
        "tldr": "This paper introduces CompGen, a novel curriculum reinforcement learning framework that leverages scene graphs and adaptive sampling to improve compositional text-to-image generation in existing T2I models.",
        "tldr_zh": "该论文介绍了一种名为CompGen的新型课程强化学习框架，该框架利用场景图和自适应采样来改进现有T2I模型中的组合文本到图像的生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TRANSPORTER: Transferring Visual Semantics from VLM Manifolds",
        "summary": "How do video understanding models acquire their answers? Although current Vision Language Models (VLMs) reason over complex scenes with diverse objects, action performances, and scene dynamics, understanding and controlling their internal processes remains an open challenge. Motivated by recent advancements in text-to-video (T2V) generative models, this paper introduces a logits-to-video (L2V) task alongside a model-independent approach, TRANSPORTER, to generate videos that capture the underlying rules behind VLMs' predictions. Given the high-visual-fidelity produced by T2V models, TRANSPORTER learns an optimal transport coupling to VLM's high-semantic embedding spaces. In turn, logit scores define embedding directions for conditional video generation. TRANSPORTER generates videos that reflect caption changes over diverse object attributes, action adverbs, and scene context. Quantitative and qualitative evaluations across VLMs demonstrate that L2V can provide a fidelity-rich, novel direction for model interpretability that has not been previously explored.",
        "url": "http://arxiv.org/abs/2511.18359v1",
        "published_date": "2025-11-23T09:12:48+00:00",
        "updated_date": "2025-11-23T09:12:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alexandros Stergiou"
        ],
        "tldr": "The paper introduces TRANSPORTER, a model-independent approach for video generation from VLM logits to interpret VLM predictions, leveraging optimal transport and T2V models.",
        "tldr_zh": "该论文介绍了TRANSPORTER，一种独立于模型的方法，通过 VLM logits 生成视频以解释 VLM 预测，利用最优传输和 T2V 模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MagicWand: A Universal Agent for Generation and Evaluation Aligned with User Preference",
        "summary": "Recent advances in AIGC (Artificial Intelligence Generated Content) models have enabled significant progress in image and video generation. However, users still struggle to obtain content that aligns with their preferences due to the difficulty of crafting detailed prompts and the lack of mechanisms to retain their preferences. To address these challenges, we construct \\textbf{UniPrefer-100K}, a large-scale dataset comprising images, videos, and associated text that describes the styles users tend to prefer. Based on UniPrefer-100K, we propose \\textbf{MagicWand}, a universal generation and evaluation agent that enhances prompts based on user preferences, leverages advanced generation models for high-quality content, and applies preference-aligned evaluation and refinement. In addition, we introduce \\textbf{UniPreferBench}, the first large-scale benchmark with over 120K annotations for assessing user preference alignment across diverse AIGC tasks. Experiments on UniPreferBench demonstrate that MagicWand consistently generates content and evaluations that are well aligned with user preferences across a wide range of scenarios.",
        "url": "http://arxiv.org/abs/2511.18352v1",
        "published_date": "2025-11-23T08:59:47+00:00",
        "updated_date": "2025-11-23T08:59:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zitong Xu",
            "Dake Shen",
            "Yaosong Du",
            "Kexiang Hao",
            "Jinghan Huang",
            "Xiande Huang"
        ],
        "tldr": "The paper introduces MagicWand, a universal agent leveraging a new dataset (UniPrefer-100K) and benchmark (UniPreferBench) to generate and evaluate AI-generated content aligned with user preferences across image and video domains.",
        "tldr_zh": "该论文介绍了MagicWand，一个通用代理，它利用一个新的数据集 (UniPrefer-100K) 和基准 (UniPreferBench) 来生成和评估与用户偏好一致的AI生成内容，涵盖图像和视频领域。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ConsistCompose: Unified Multimodal Layout Control for Image Composition",
        "summary": "Unified multimodal models that couple visual understanding with image generation have advanced rapidly, yet most systems still focus on visual grounding-aligning language with image regions-while their generative counterpart, linguistic-embedded layout-grounded generation (LELG) for layout-controllable multi-instance generation, remains underexplored and limits precise compositional control. We present ConsistCompose, a unified multimodal framework that embeds layout coordinates directly into language prompts, enabling layout-controlled multi-instance image generation from Interleaved Image-Text within a single generative interface. We further construct ConsistCompose3M, a 3.4M multi-instance generation dataset with layout and identity annotations (2.6M text-guided and 0.8M image-guided data pairs) that provides large-scale supervision for layout-conditioned generation. Within this framework, LELG is instantiated through instance-coordinate binding prompts and coordinate-aware classifier-free guidance, which translate linguistic layout cues into precise spatial control without task-specific branches. Experiments on COCO-Position and MS-Bench show that ConsistCompose substantially improves spatial accuracy over layout-controlled baselines while preserving identity fidelity and competitive general multimodal understanding, establishing a unified paradigm for layout-controllable multimodal image generation.",
        "url": "http://arxiv.org/abs/2511.18333v1",
        "published_date": "2025-11-23T08:14:53+00:00",
        "updated_date": "2025-11-23T08:14:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuanke Shi",
            "Boxuan Li",
            "Xiaoyang Han",
            "Zhongang Cai",
            "Lei Yang",
            "Dahua Lin",
            "Quan Wang"
        ],
        "tldr": "ConsistCompose introduces a unified multimodal framework for layout-controlled multi-instance image generation by embedding layout coordinates into language prompts and provides a large-scale dataset for training.",
        "tldr_zh": "ConsistCompose 提出了一种统一的多模态框架，通过将布局坐标嵌入到语言提示中来实现布局控制的多实例图像生成，并提供了一个大规模数据集用于训练。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uni-DAD: Unified Distillation and Adaptation of Diffusion Models for Few-step Few-shot Image Generation",
        "summary": "Diffusion models (DMs) produce high-quality images, yet their sampling remains costly when adapted to new domains. Distilled DMs are faster but typically remain confined within their teacher's domain. Thus, fast and high-quality generation for novel domains relies on two-stage training pipelines: Adapt-then-Distill or Distill-then-Adapt. However, both add design complexity and suffer from degraded quality or diversity. We introduce Uni-DAD, a single-stage pipeline that unifies distillation and adaptation of DMs. It couples two signals during training: (i) a dual-domain distribution-matching distillation objective that guides the student toward the distributions of the source teacher and a target teacher, and (ii) a multi-head generative adversarial network (GAN) loss that encourages target realism across multiple feature scales. The source domain distillation preserves diverse source knowledge, while the multi-head GAN stabilizes training and reduces overfitting, especially in few-shot regimes. The inclusion of a target teacher facilitates adaptation to more structurally distant domains. We perform evaluations on a variety of datasets for few-shot image generation (FSIG) and subject-driven personalization (SDP). Uni-DAD delivers higher quality than state-of-the-art (SoTA) adaptation methods even with less than 4 sampling steps, and outperforms two-stage training pipelines in both quality and diversity.",
        "url": "http://arxiv.org/abs/2511.18281v1",
        "published_date": "2025-11-23T04:22:42+00:00",
        "updated_date": "2025-11-23T04:22:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yara Bahram",
            "Melodie Desbos",
            "Mohammadhadi Shateri",
            "Eric Granger"
        ],
        "tldr": "The paper introduces Uni-DAD, a single-stage pipeline for few-shot image generation which unifies distillation and adaptation of diffusion models, achieving higher quality and diversity than existing two-stage methods with fewer sampling steps.",
        "tldr_zh": "该论文介绍了一种名为Uni-DAD的单阶段流程，用于少样本图像生成，它统一了扩散模型的蒸馏和适配，在更少的采样步骤下实现了比现有两阶段方法更高的质量和多样性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Point-to-Point: Sparse Motion Guidance for Controllable Video Editing",
        "summary": "Accurately preserving motion while editing a subject remains a core challenge in video editing tasks. Existing methods often face a trade-off between edit and motion fidelity, as they rely on motion representations that are either overfitted to the layout or only implicitly defined. To overcome this limitation, we revisit point-based motion representation. However, identifying meaningful points remains challenging without human input, especially across diverse video scenarios. To address this, we propose a novel motion representation, anchor tokens, that capture the most essential motion patterns by leveraging the rich prior of a video diffusion model. Anchor tokens encode video dynamics compactly through a small number of informative point trajectories and can be flexibly relocated to align with new subjects. This allows our method, Point-to-Point, to generalize across diverse scenarios. Extensive experiments demonstrate that anchor tokens lead to more controllable and semantically aligned video edits, achieving superior performance in terms of edit and motion fidelity.",
        "url": "http://arxiv.org/abs/2511.18277v1",
        "published_date": "2025-11-23T03:59:59+00:00",
        "updated_date": "2025-11-23T03:59:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yeji Song",
            "Jaehyun Lee",
            "Mijin Koo",
            "JunHoo Lee",
            "Nojun Kwak"
        ],
        "tldr": "The paper introduces \"anchor tokens,\" a novel point-based motion representation learned from video diffusion models, for controllable video editing that improves both edit and motion fidelity.",
        "tldr_zh": "该论文提出了一种名为“锚点标记”的新型基于点的运动表示方法，该方法从视频扩散模型中学习，用于可控的视频编辑，从而提高了编辑和运动的保真度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation",
        "summary": "Unified multimodal models aim to integrate understanding and generation within a single framework, yet bridging the gap between discrete semantic reasoning and high-fidelity visual synthesis remains challenging. We present MammothModa2 (Mammoth2), a unified autoregressive-diffusion (AR-Diffusion) framework designed to effectively couple autoregressive semantic planning with diffusion-based generation. Mammoth2 adopts a serial design: an AR path equipped with generation experts performs global semantic modeling over discrete tokens, while a single-stream Diffusion Transformer (DiT) decoder handles high-fidelity image synthesis. A carefully designed AR-Diffusion feature alignment module combines multi-layer feature aggregation, unified condition encoding, and in-context conditioning to stably align AR's representations with the diffusion decoder's continuous latents. Mammoth2 is trained end-to-end with joint Next-Token Prediction and Flow Matching objectives, followed by supervised fine-tuning and reinforcement learning over both generation and editing. With roughly 60M supervised generation samples and no reliance on pre-trained generators, Mammoth2 delivers strong text-to-image and instruction-based editing performance on public benchmarks, achieving 0.87 on GenEval, 87.2 on DPGBench, and 4.06 on ImgEdit, while remaining competitive with understanding-only backbones (e.g., Qwen3-VL-8B) on multimodal understanding tasks. These results suggest that a carefully coupled AR-Diffusion architecture can provide high-fidelity generation and editing while maintaining strong multimodal comprehension within a single, parameter- and data-efficient model.",
        "url": "http://arxiv.org/abs/2511.18262v1",
        "published_date": "2025-11-23T03:25:39+00:00",
        "updated_date": "2025-11-23T03:25:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tao Shen",
            "Xin Wan",
            "Taicai Chen",
            "Rui Zhang",
            "Junwen Pan",
            "Dawei Lu",
            "Fanding Lei",
            "Zhilin Lu",
            "Yunfei Yang",
            "Chen Cheng",
            "Qi She",
            "Chang Liu",
            "Zhenbang Sun"
        ],
        "tldr": "MammothModa2 is a unified AR-Diffusion framework that combines autoregressive semantic planning with diffusion-based generation for multimodal understanding and high-fidelity image synthesis, achieving strong performance in text-to-image generation, image editing, and multimodal comprehension.",
        "tldr_zh": "MammothModa2是一个统一的AR-Diffusion框架，它将自回归语义规划与基于扩散的生成相结合，用于多模态理解和高保真图像合成，在文本到图像生成、图像编辑和多模态理解方面取得了强大的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Sequence-Adaptive Video Prediction in Continuous Streams using Diffusion Noise Optimization",
        "summary": "In this work, we investigate diffusion-based video prediction models, which forecast future video frames, for continuous video streams. In this context, the models observe continuously new training samples, and we aim to leverage this to improve their predictions. We thus propose an approach that continuously adapts a pre-trained diffusion model to a video stream. Since fine-tuning the parameters of a large diffusion model is too expensive, we refine the diffusion noise during inference while keeping the model parameters frozen, allowing the model to adaptively determine suitable sampling noise. We term the approach Sequence Adaptive Video Prediction with Diffusion Noise Optimization (SAVi-DNO). To validate our approach, we introduce a new evaluation setting on the Ego4D dataset, focusing on simultaneous adaptation and evaluation on long continuous videos. Empirical results demonstrate improved performance based on FVD, SSIM, and PSNR metrics on long videos of Ego4D and OpenDV-YouTube, as well as videos of UCF-101 and SkyTimelapse, showcasing SAVi-DNO's effectiveness.",
        "url": "http://arxiv.org/abs/2511.18255v1",
        "published_date": "2025-11-23T02:58:10+00:00",
        "updated_date": "2025-11-23T02:58:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sina Mokhtarzadeh Azar",
            "Emad Bahrami",
            "Enrico Pallotta",
            "Gianpiero Francesca",
            "Radu Timofte",
            "Juergen Gall"
        ],
        "tldr": "The paper introduces SAVi-DNO, a method for adapting diffusion-based video prediction models to continuous video streams by refining the diffusion noise during inference. It demonstrates improved performance on long videos from multiple datasets without fine-tuning model parameters.",
        "tldr_zh": "该论文介绍了SAVi-DNO，一种通过在推理过程中优化扩散噪声，使基于扩散的视频预测模型适应连续视频流的方法。实验表明，该方法在多个数据集的长视频上实现了性能提升，且无需微调模型参数。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Early Lung Cancer Diagnosis from Virtual Follow-up LDCT Generation via Correlational Autoencoder and Latent Flow Matching",
        "summary": "Lung cancer is one of the most commonly diagnosed cancers, and early diagnosis is critical because the survival rate declines sharply once the disease progresses to advanced stages. However, achieving an early diagnosis remains challenging, particularly in distinguishing subtle early signals of malignancy from those of benign conditions. In clinical practice, a patient with a high risk may need to undergo an initial baseline and several annual follow-up examinations (e.g., CT scans) before receiving a definitive diagnosis, which can result in missing the optimal treatment. Recently, Artificial Intelligence (AI) methods have been increasingly used for early diagnosis of lung cancer, but most existing algorithms focus on radiomic features extraction from single early-stage CT scans. Inspired by recent advances in diffusion models for image generation, this paper proposes a generative method, named CorrFlowNet, which creates a virtual, one-year follow-up CT scan after the initial baseline scan. This virtual follow-up would allow for an early detection of malignant/benign nodules, reducing the need to wait for clinical follow-ups. During training, our approach employs a correlational autoencoder to encode both early baseline and follow-up CT images into a latent space that captures the dynamics of nodule progression as well as the correlations between them, followed by a flow matching algorithm on the latent space with a neural ordinary differential equation. An auxiliary classifier is used to further enhance the diagnostic accuracy. Evaluations on a real clinical dataset show our method can significantly improve downstream lung nodule risk assessment compared with existing baseline models. Moreover, its diagnostic accuracy is comparable with real clinical CT follow-ups, highlighting its potential to improve cancer diagnosis.",
        "url": "http://arxiv.org/abs/2511.18185v1",
        "published_date": "2025-11-22T20:40:05+00:00",
        "updated_date": "2025-11-22T20:40:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yutong Wu",
            "Yifan Wang",
            "Qining Zhang",
            "Chuan Zhou",
            "Lei Ying"
        ],
        "tldr": "The paper proposes CorrFlowNet, a generative model using correlational autoencoders and latent flow matching to generate virtual follow-up CT scans from initial baseline scans for early lung cancer diagnosis, achieving comparable accuracy to real clinical follow-ups.",
        "tldr_zh": "该论文提出了 CorrFlowNet，一种使用相关自编码器和潜在流匹配的生成模型，用于从初始基线扫描生成虚拟的后续 CT 扫描，以进行早期肺癌诊断，其准确性与真实的临床随访相当。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses",
        "summary": "Egocentric video generation with fine-grained control through body motion is a key requirement towards embodied AI agents that can simulate, predict, and plan actions. In this work, we propose EgoControl, a pose-controllable video diffusion model trained on egocentric data. We train a video prediction model to condition future frame generation on explicit 3D body pose sequences. To achieve precise motion control, we introduce a novel pose representation that captures both global camera dynamics and articulated body movements, and integrate it through a dedicated control mechanism within the diffusion process. Given a short sequence of observed frames and a sequence of target poses, EgoControl generates temporally coherent and visually realistic future frames that align with the provided pose control. Experimental results demonstrate that EgoControl produces high-quality, pose-consistent egocentric videos, paving the way toward controllable embodied video simulation and understanding.",
        "url": "http://arxiv.org/abs/2511.18173v1",
        "published_date": "2025-11-22T19:56:39+00:00",
        "updated_date": "2025-11-22T19:56:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Enrico Pallotta",
            "Sina Mokhtarzadeh Azar",
            "Lars Doorenbos",
            "Serdar Ozsoy",
            "Umar Iqbal",
            "Juergen Gall"
        ],
        "tldr": "EgoControl introduces a pose-controllable video diffusion model for egocentric video generation, enabling control over future frames through 3D full-body pose sequences. It achieves this with a novel pose representation and control mechanism integrated into the diffusion process.",
        "tldr_zh": "EgoControl 提出了一种姿态可控的自中心视频生成扩散模型，通过 3D 全身姿势序列实现对未来帧的控制。它通过一种新颖的姿态表示和集成到扩散过程中的控制机制来实现这一目标。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FlowPortal: Residual-Corrected Flow for Training-Free Video Relighting and Background Replacement",
        "summary": "Video relighting with background replacement is a challenging task critical for applications in film production and creative media. Existing methods struggle to balance temporal consistency, spatial fidelity, and illumination naturalness. To address these issues, we introduce FlowPortal, a novel training-free flow-based video relighting framework. Our core innovation is a Residual-Corrected Flow mechanism that transforms a standard flow-based model into an editing model, guaranteeing perfect reconstruction when input conditions are identical and enabling faithful relighting when they differ, resulting in high structural consistency. This is further enhanced by a Decoupled Condition Design for precise lighting control and a High-Frequency Transfer mechanism for detail preservation. Additionally, a masking strategy isolates foreground relighting from background pure generation process. Experiments demonstrate that FlowPortal achieves superior performance in temporal coherence, structural preservation, and lighting realism, while maintaining high efficiency. Project Page: https://gaowenshuo.github.io/FlowPortalProject/.",
        "url": "http://arxiv.org/abs/2511.18346v1",
        "published_date": "2025-11-23T08:45:17+00:00",
        "updated_date": "2025-11-23T08:45:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenshuo Gao",
            "Junyi Fan",
            "Jiangyue Zeng",
            "Shuai Yang"
        ],
        "tldr": "The paper introduces FlowPortal, a training-free flow-based video relighting and background replacement framework with a Residual-Corrected Flow mechanism for improved consistency, lighting control, and detail preservation.",
        "tldr_zh": "该论文介绍了一种名为FlowPortal的无需训练的基于流的视频重新打光和背景替换框架，该框架具有残差校正流机制，可提高一致性，光照控制和细节保留。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]