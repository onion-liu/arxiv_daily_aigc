[
    {
        "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
        "summary": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.",
        "url": "http://arxiv.org/abs/2512.17909v1",
        "published_date": "2025-12-19T18:59:57+00:00",
        "updated_date": "2025-12-19T18:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shilong Zhang",
            "He Zhang",
            "Zhifei Zhang",
            "Chongjian Ge",
            "Shuchen Xue",
            "Shaoteng Liu",
            "Mengwei Ren",
            "Soo Ye Kim",
            "Yuqian Zhou",
            "Qing Liu",
            "Daniil Pakhomov",
            "Kai Zhang",
            "Zhe Lin",
            "Ping Luo"
        ],
        "tldr": "This paper introduces a novel framework to adapt representation encoders for text-to-image generation and editing by incorporating a semantic-pixel reconstruction objective, achieving state-of-the-art performance in reconstruction, convergence speed, and generation quality.",
        "tldr_zh": "该论文提出了一种新颖的框架，通过结合语义-像素重建目标来调整表示编码器，以用于文本到图像的生成和编辑，从而在重建、收敛速度和生成质量方面实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion Forcing for Multi-Agent Interaction Sequence Modeling",
        "summary": "Understanding and generating multi-person interactions is a fundamental challenge with broad implications for robotics and social computing. While humans naturally coordinate in groups, modeling such interactions remains difficult due to long temporal horizons, strong inter-agent dependencies, and variable group sizes. Existing motion generation methods are largely task-specific and do not generalize to flexible multi-agent generation. We introduce MAGNet (Multi-Agent Diffusion Forcing Transformer), a unified autoregressive diffusion framework for multi-agent motion generation that supports a wide range of interaction tasks through flexible conditioning and sampling. MAGNet performs dyadic prediction, partner inpainting, and full multi-agent motion generation within a single model, and can autoregressively generate ultra-long sequences spanning hundreds of v. Building on Diffusion Forcing, we introduce key modifications that explicitly model inter-agent coupling during autoregressive denoising, enabling coherent coordination across agents. As a result, MAGNet captures both tightly synchronized activities (e.g, dancing, boxing) and loosely structured social interactions. Our approach performs on par with specialized methods on dyadic benchmarks while naturally extending to polyadic scenarios involving three or more interacting people, enabled by a scalable architecture that is agnostic to the number of agents. We refer readers to the supplemental video, where the temporal dynamics and spatial coordination of generated interactions are best appreciated. Project page: https://von31.github.io/MAGNet/",
        "url": "http://arxiv.org/abs/2512.17900v1",
        "published_date": "2025-12-19T18:59:02+00:00",
        "updated_date": "2025-12-19T18:59:02+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Vongani H. Maluleke",
            "Kie Horiuchi",
            "Lea Wilken",
            "Evonne Ng",
            "Jitendra Malik",
            "Angjoo Kanazawa"
        ],
        "tldr": "The paper introduces MAGNet, a diffusion-based framework for generating coherent multi-agent interaction sequences, outperforming specialized methods and generalizing to larger groups.",
        "tldr_zh": "该论文介绍了MAGNet，一个基于扩散的框架，用于生成连贯的多智能体交互序列，优于专门的方法，并可推广到更大的群体。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InSPECT: Invariant Spectral Features Preservation of Diffusion Models",
        "summary": "Modern diffusion models (DMs) have achieved state-of-the-art image generation. However, the fundamental design choice of diffusing data all the way to white noise and then reconstructing it leads to an extremely difficult and computationally intractable prediction task. To overcome this limitation, we propose InSPECT (Invariant Spectral Feature-Preserving Diffusion Model), a novel diffusion model that keeps invariant spectral features during both the forward and backward processes. At the end of the forward process, the Fourier coefficients smoothly converge to a specified random noise, enabling features preservation while maintaining diversity and randomness. By preserving invariant features, InSPECT demonstrates enhanced visual diversity, faster convergence rate, and a smoother diffusion process. Experiments on CIFAR-10, Celeb-A, and LSUN demonstrate that InSPECT achieves on average a 39.23% reduction in FID and 45.80% improvement in IS against DDPM for 10K iterations under specified parameter settings, which demonstrates the significant advantages of preserving invariant features: achieving superior generation quality and diversity, while enhancing computational efficiency and enabling faster convergence rate. To the best of our knowledge, this is the first attempt to analyze and preserve invariant spectral features in diffusion models.",
        "url": "http://arxiv.org/abs/2512.17873v1",
        "published_date": "2025-12-19T18:24:02+00:00",
        "updated_date": "2025-12-19T18:24:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Baohua Yan",
            "Qingyuan Liu",
            "Jennifer Kava",
            "Xuan Di"
        ],
        "tldr": "The paper proposes InSPECT, a diffusion model that preserves invariant spectral features during forward and backward processes to improve image generation quality, diversity, and convergence rate compared to DDPM.",
        "tldr_zh": "该论文提出了 InSPECT，一种扩散模型，它在正向和反向过程中保留不变的频谱特征，从而提高了图像生成质量、多样性和收敛速度，优于 DDPM。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]