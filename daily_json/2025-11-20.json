[
    {
        "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
        "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
        "url": "http://arxiv.org/abs/2511.14993v1",
        "published_date": "2025-11-19T00:23:22+00:00",
        "updated_date": "2025-11-19T00:23:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Vladimir Arkhipkin",
            "Vladimir Korviakov",
            "Nikolai Gerasimenko",
            "Denis Parkhomenko",
            "Viacheslav Vasilev",
            "Alexey Letunovskiy",
            "Maria Kovaleva",
            "Nikolai Vaulin",
            "Ivan Kirillov",
            "Lev Novitskiy",
            "Denis Koposov",
            "Nikita Kiselev",
            "Alexander Varlamov",
            "Dmitrii Mikhailov",
            "Vladimir Polovnikov",
            "Andrey Shutkin",
            "Ilya Vasiliev",
            "Julia Agafonova",
            "Anastasiia Kargapoltseva",
            "Anna Dmitrienko",
            "Anastasia Maltseva",
            "Anna Averchenkova",
            "Olga Kim",
            "Tatiana Nikulina",
            "Denis Dimitrov"
        ],
        "tldr": "Kandinsky 5.0 introduces a family of high-resolution image and video generation models, including Lite and Pro versions, with detailed data curation and training procedures, alongside open-source code and checkpoints for broad research use.",
        "tldr_zh": "Kandinsky 5.0 推出了一系列高分辨率图像和视频生成模型，包括 Lite 和 Pro 版本，提供了详细的数据整理和训练过程，以及开源代码和检查点，供广泛研究使用。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Learning to Expand Images for Efficient Visual Autoregressive Modeling",
        "summary": "Autoregressive models have recently shown great promise in visual generation by leveraging discrete token sequences akin to language modeling. However, existing approaches often suffer from inefficiency, either due to token-by-token decoding or the complexity of multi-scale representations. In this work, we introduce Expanding Autoregressive Representation (EAR), a novel generation paradigm that emulates the human visual system's center-outward perception pattern. EAR unfolds image tokens in a spiral order from the center and progressively expands outward, preserving spatial continuity and enabling efficient parallel decoding. To further enhance flexibility and speed, we propose a length-adaptive decoding strategy that dynamically adjusts the number of tokens predicted at each step. This biologically inspired design not only reduces computational cost but also improves generation quality by aligning the generation order with perceptual relevance. Extensive experiments on ImageNet demonstrate that EAR achieves state-of-the-art trade-offs between fidelity and efficiency on single-scale autoregressive models, setting a new direction for scalable and cognitively aligned autoregressive image generation.",
        "url": "http://arxiv.org/abs/2511.15499v1",
        "published_date": "2025-11-19T14:55:07+00:00",
        "updated_date": "2025-11-19T14:55:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruiqing Yang",
            "Kaixin Zhang",
            "Zheng Zhang",
            "Shan You",
            "Tao Huang"
        ],
        "tldr": "The paper introduces Expanding Autoregressive Representation (EAR), a novel center-outward image generation method with length-adaptive decoding for improved efficiency and quality in visual autoregressive modeling.",
        "tldr_zh": "该论文介绍了一种新的扩展自回归表示 (EAR) 方法，该方法采用由中心向外的图像生成方式和长度自适应解码，以提高视觉自回归建模的效率和质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Jointly Conditioned Diffusion Model for Multi-View Pose-Guided Person Image Synthesis",
        "summary": "Pose-guided human image generation is limited by incomplete textures from single reference views and the absence of explicit cross-view interaction. We present jointly conditioned diffusion model (JCDM), a jointly conditioned diffusion framework that exploits multi-view priors. The appearance prior module (APM) infers a holistic identity preserving prior from incomplete references, and the joint conditional injection (JCI) mechanism fuses multi-view cues and injects shared conditioning into the denoising backbone to align identity, color, and texture across poses. JCDM supports a variable number of reference views and integrates with standard diffusion backbones with minimal and targeted architectural modifications. Experiments demonstrate state of the art fidelity and cross-view consistency.",
        "url": "http://arxiv.org/abs/2511.15092v1",
        "published_date": "2025-11-19T04:05:39+00:00",
        "updated_date": "2025-11-19T04:05:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengyu Xie",
            "Zhi Gong",
            "Junchi Ren",
            "Linkun Yu",
            "Si Shen",
            "Fei Shen",
            "Xiaoyu Du"
        ],
        "tldr": "This paper introduces a jointly conditioned diffusion model (JCDM) for pose-guided person image generation that leverages multi-view priors to improve fidelity and cross-view consistency, addressing limitations of single reference view methods.",
        "tldr_zh": "本文提出了一个联合条件扩散模型（JCDM），用于姿势引导的人像生成，该模型利用多视角先验来提高保真度和跨视角一致性，解决了单参考视角方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization",
        "summary": "We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.",
        "url": "http://arxiv.org/abs/2511.14899v1",
        "published_date": "2025-11-18T20:37:52+00:00",
        "updated_date": "2025-11-18T20:37:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daniel Gilo",
            "Or Litany"
        ],
        "tldr": "InstructMix2Mix improves multi-view image editing consistency by distilling 2D diffusion model editing capabilities into a multi-view diffusion model, using novel adaptations within a Score Distillation Sampling framework.",
        "tldr_zh": "InstructMix2Mix通过将2D扩散模型的编辑能力提炼到多视角扩散模型中，改善了多视角图像编辑的一致性; 该方法在Score Distillation Sampling框架中进行了创新性适配。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning",
        "summary": "We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.",
        "url": "http://arxiv.org/abs/2511.14760v1",
        "published_date": "2025-11-18T18:59:30+00:00",
        "updated_date": "2025-11-18T18:59:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rui Tian",
            "Mingfei Gao",
            "Haiming Gang",
            "Jiasen Lu",
            "Zhe Gan",
            "Yinfei Yang",
            "Zuxuan Wu",
            "Afshin Dehghan"
        ],
        "tldr": "UniGen-1.5 is presented as an improved multimodal LLM excelling in image understanding, generation, and editing by employing a unified Reinforcement Learning strategy and Edit Instruction Alignment stage, achieving state-of-the-art or comparable performance to leading models.",
        "tldr_zh": "UniGen-1.5是一个改进的多模态LLM，通过采用统一的强化学习策略和编辑指令对齐阶段，在图像理解、生成和编辑方面表现出色，实现了最先进或与领先模型相当的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation",
        "summary": "Autoregressive models can generate high-quality 3D meshes by sequentially producing vertices and faces, but their token-by-token decoding results in slow inference, limiting practical use in interactive and large-scale applications. We present FlashMesh, a fast and high-fidelity mesh generation framework that rethinks autoregressive decoding through a predict-correct-verify paradigm. The key insight is that mesh tokens exhibit strong structural and geometric correlations that enable confident multi-token speculation. FlashMesh leverages this by introducing a speculative decoding scheme tailored to the commonly used hourglass transformer architecture, enabling parallel prediction across face, point, and coordinate levels. Extensive experiments show that FlashMesh achieves up to a 2 x speedup over standard autoregressive models while also improving generation fidelity. Our results demonstrate that structural priors in mesh data can be systematically harnessed to accelerate and enhance autoregressive generation.",
        "url": "http://arxiv.org/abs/2511.15618v1",
        "published_date": "2025-11-19T17:03:49+00:00",
        "updated_date": "2025-11-19T17:03:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tingrui Shen",
            "Yiheng Zhang",
            "Chen Tang",
            "Chuan Ping",
            "Zixing Zhao",
            "Le Wan",
            "Yuwang Wang",
            "Ronggang Wang",
            "Shengfeng He"
        ],
        "tldr": "FlashMesh accelerates autoregressive 3D mesh generation by leveraging structural and geometric correlations for parallel decoding, achieving a 2x speedup and improved fidelity.",
        "tldr_zh": "FlashMesh通过利用结构和几何相关性进行并行解码，加速了自回归3D网格生成，实现了2倍的加速并提高了保真度。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Taming Generative Synthetic Data for X-ray Prohibited Item Detection",
        "summary": "Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.",
        "url": "http://arxiv.org/abs/2511.15299v1",
        "published_date": "2025-11-19T10:07:11+00:00",
        "updated_date": "2025-11-19T10:07:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jialong Sun",
            "Hongguang Zhu",
            "Weizhe Liu",
            "Yunda Sun",
            "Renshuai Tao",
            "Yunchao Wei"
        ],
        "tldr": "The paper introduces Xsyn, a one-stage text-to-image pipeline for generating synthetic X-ray security images, improving prohibited item detection by incorporating cross-attention refinement and background occlusion modeling without extra labor cost.",
        "tldr_zh": "该论文介绍了一种名为Xsyn的单阶段文本到图像生成流程，用于生成合成的X射线安全图像，通过结合交叉注意力优化和背景遮挡建模，在无需额外人工成本的情况下提高了违禁物品的检测效果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space",
        "summary": "In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.",
        "url": "http://arxiv.org/abs/2511.15046v1",
        "published_date": "2025-11-19T02:37:03+00:00",
        "updated_date": "2025-11-19T02:37:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Panqi Yang",
            "Haodong Jing",
            "Nanning Zheng",
            "Yongqiang Ma"
        ],
        "tldr": "The paper introduces UniHOI, a unified framework for jointly modeling HOI detection and generation using a unified token space, achieving state-of-the-art results in both tasks.",
        "tldr_zh": "该论文介绍了UniHOI，一个统一的框架，通过统一的令牌空间联合建模HOI检测和生成，并在两个任务中都取得了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GeoSceneGraph: Geometric Scene Graph Diffusion Model for Text-guided 3D Indoor Scene Synthesis",
        "summary": "Methods that synthesize indoor 3D scenes from text prompts have wide-ranging applications in film production, interior design, video games, virtual reality, and synthetic data generation for training embodied agents. Existing approaches typically either train generative models from scratch or leverage vision-language models (VLMs). While VLMs achieve strong performance, particularly for complex or open-ended prompts, smaller task-specific models remain necessary for deployment on resource-constrained devices such as extended reality (XR) glasses or mobile phones. However, many generative approaches that train from scratch overlook the inherent graph structure of indoor scenes, which can limit scene coherence and realism. Conversely, methods that incorporate scene graphs either demand a user-provided semantic graph, which is generally inconvenient and restrictive, or rely on ground-truth relationship annotations, limiting their capacity to capture more varied object interactions. To address these challenges, we introduce GeoSceneGraph, a method that synthesizes 3D scenes from text prompts by leveraging the graph structure and geometric symmetries of 3D scenes, without relying on predefined relationship classes. Despite not using ground-truth relationships, GeoSceneGraph achieves performance comparable to methods that do. Our model is built on equivariant graph neural networks (EGNNs), but existing EGNN approaches are typically limited to low-dimensional conditioning and are not designed to handle complex modalities such as text. We propose a simple and effective strategy for conditioning EGNNs on text features, and we validate our design through ablation studies.",
        "url": "http://arxiv.org/abs/2511.14884v1",
        "published_date": "2025-11-18T20:06:49+00:00",
        "updated_date": "2025-11-18T20:06:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Antonio Ruiz",
            "Tao Wu",
            "Andrew Melnik",
            "Qing Cheng",
            "Xuqin Wang",
            "Lu Liu",
            "Yongliang Wang",
            "Yanfeng Zhang",
            "Helge Ritter"
        ],
        "tldr": "GeoSceneGraph synthesizes 3D indoor scenes from text prompts by leveraging scene graph structure and geometric symmetries, offering a performant, resource-efficient solution for XR and mobile devices without relying on predefined relationship classes.",
        "tldr_zh": "GeoSceneGraph 通过利用场景图结构和几何对称性，从文本提示合成 3D 室内场景，为 XR 和移动设备提供了一种高性能、资源高效的解决方案，而且不依赖于预定义的类间关系。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]