[
    {
        "title": "V-Shuffle: Zero-Shot Style Transfer via Value Shuffle",
        "summary": "Attention injection-based style transfer has achieved remarkable progress in\nrecent years. However, existing methods often suffer from content leakage,\nwhere the undesired semantic content of the style image mistakenly appears in\nthe stylized output. In this paper, we propose V-Shuffle, a zero-shot style\ntransfer method that leverages multiple style images from the same style domain\nto effectively navigate the trade-off between content preservation and style\nfidelity. V-Shuffle implicitly disrupts the semantic content of the style\nimages by shuffling the value features within the self-attention layers of the\ndiffusion model, thereby preserving low-level style representations. We further\nintroduce a Hybrid Style Regularization that complements these low-level\nrepresentations with high-level style textures to enhance style fidelity.\nEmpirical results demonstrate that V-Shuffle achieves excellent performance\nwhen utilizing multiple style images. Moreover, when applied to a single style\nimage, V-Shuffle outperforms previous state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2511.06365v1",
        "published_date": "2025-11-09T13:07:23+00:00",
        "updated_date": "2025-11-09T13:07:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haojun Tang",
            "Qiwei Lin",
            "Tongda Xu",
            "Lida Huang",
            "Yan Wang"
        ],
        "tldr": "The paper introduces V-Shuffle, a zero-shot style transfer method that shuffles value features within self-attention layers of a diffusion model and uses hybrid style regularization to achieve state-of-the-art style transfer, even with single style images.",
        "tldr_zh": "该论文介绍了 V-Shuffle，一种零样本风格迁移方法，它通过在扩散模型的自注意力层中混洗值特征并使用混合风格正则化来实现最先进的风格迁移，即使使用单张风格图像也能表现出色。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Test-Time Iterative Error Correction for Efficient Diffusion Models",
        "summary": "With the growing demand for high-quality image generation on\nresource-constrained devices, efficient diffusion models have received\nincreasing attention. However, such models suffer from approximation errors\nintroduced by efficiency techniques, which significantly degrade generation\nquality. Once deployed, these errors are difficult to correct, as modifying the\nmodel is typically infeasible in deployment environments. Through an analysis\nof error propagation across diffusion timesteps, we reveal that these\napproximation errors can accumulate exponentially, severely impairing output\nquality. Motivated by this insight, we propose Iterative Error Correction\n(IEC), a novel test-time method that mitigates inference-time errors by\niteratively refining the model's output. IEC is theoretically proven to reduce\nerror propagation from exponential to linear growth, without requiring any\nretraining or architectural changes. IEC can seamlessly integrate into the\ninference process of existing diffusion models, enabling a flexible trade-off\nbetween performance and efficiency. Extensive experiments show that IEC\nconsistently improves generation quality across various datasets, efficiency\ntechniques, and model architectures, establishing it as a practical and\ngeneralizable solution for test-time enhancement of efficient diffusion models.",
        "url": "http://arxiv.org/abs/2511.06250v1",
        "published_date": "2025-11-09T06:29:22+00:00",
        "updated_date": "2025-11-09T06:29:22+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Yunshan Zhong",
            "Yanwei Qi",
            "Yuxin Zhang"
        ],
        "tldr": "This paper introduces Iterative Error Correction (IEC), a test-time method to reduce error propagation in efficient diffusion models, improving generation quality without retraining.",
        "tldr_zh": "本文介绍了一种迭代误差校正（IEC）方法，这是一种测试时方法，旨在减少高效扩散模型中的误差传播，从而在无需重新训练的情况下提高生成质量。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Inpaint360GS: Efficient Object-Aware 3D Inpainting via Gaussian Splatting for 360° Scenes",
        "summary": "Despite recent advances in single-object front-facing inpainting using NeRF\nand 3D Gaussian Splatting (3DGS), inpainting in complex 360{\\deg} scenes\nremains largely underexplored. This is primarily due to three key challenges:\n(i) identifying target objects in the 3D field of 360{\\deg} environments, (ii)\ndealing with severe occlusions in multi-object scenes, which makes it hard to\ndefine regions to inpaint, and (iii) maintaining consistent and high-quality\nappearance across views effectively. To tackle these challenges, we propose\nInpaint360GS, a flexible 360{\\deg} editing framework based on 3DGS that\nsupports multi-object removal and high-fidelity inpainting in 3D space. By\ndistilling 2D segmentation into 3D and leveraging virtual camera views for\ncontextual guidance, our method enables accurate object-level editing and\nconsistent scene completion. We further introduce a new dataset tailored for\n360{\\deg} inpainting, addressing the lack of ground truth object-free scenes.\nExperiments demonstrate that Inpaint360GS outperforms existing baselines and\nachieves state-of-the-art performance. Project page:\nhttps://dfki-av.github.io/inpaint360gs/",
        "url": "http://arxiv.org/abs/2511.06457v1",
        "published_date": "2025-11-09T16:47:30+00:00",
        "updated_date": "2025-11-09T16:47:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shaoxiang Wang",
            "Shihong Zhang",
            "Christen Millerdurai",
            "Rüdiger Westermann",
            "Didier Stricker",
            "Alain Pagani"
        ],
        "tldr": "The paper introduces Inpaint360GS, a 3D Gaussian Splatting based framework for object-aware inpainting in 360° scenes, addressing challenges like object identification, occlusion handling, and view consistency, along with a new 360° inpainting dataset.",
        "tldr_zh": "该论文介绍了Inpaint360GS，一个基于3D高斯溅射的框架，用于360°场景中的对象感知修复，解决了对象识别、遮挡处理和视图一致性等挑战，并提出了一个新的360°修复数据集。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]