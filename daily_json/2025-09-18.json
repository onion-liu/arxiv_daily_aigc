[
    {
        "title": "GenExam: A Multidisciplinary Text-to-Image Exam",
        "summary": "Exams are a fundamental test of expert-level intelligence and require\nintegrated understanding, reasoning, and generation. Existing exam-style\nbenchmarks mainly focus on understanding and reasoning tasks, and current\ngeneration benchmarks emphasize the illustration of world knowledge and visual\nconcepts, neglecting the evaluation of rigorous drawing exams. We introduce\nGenExam, the first benchmark for multidisciplinary text-to-image exams,\nfeaturing 1,000 samples across 10 subjects with exam-style prompts organized\nunder a four-level taxonomy. Each problem is equipped with ground-truth images\nand fine-grained scoring points to enable a precise evaluation of semantic\ncorrectness and visual plausibility. Experiments show that even\nstate-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve\nless than 15% strict scores, and most models yield almost 0%, suggesting the\ngreat challenge of our benchmark. By framing image generation as an exam,\nGenExam offers a rigorous assessment of models' ability to integrate knowledge,\nreasoning, and generation, providing insights on the path to general AGI.",
        "url": "http://arxiv.org/abs/2509.14232v1",
        "published_date": "2025-09-17T17:59:14+00:00",
        "updated_date": "2025-09-17T17:59:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaokai Wang",
            "Penghao Yin",
            "Xiangyu Zhao",
            "Changyao Tian",
            "Yu Qiao",
            "Wenhai Wang",
            "Jifeng Dai",
            "Gen Luo"
        ],
        "tldr": "The paper introduces GenExam, a new multidisciplinary text-to-image exam benchmark for evaluating image generation models on their ability to integrate knowledge, reason, and generate exam-style images. The results show that even state-of-the-art models perform poorly, indicating the benchmark's challenging nature and its potential for advancing general AGI.",
        "tldr_zh": "本文介绍GenExam，这是一个新的多学科文本到图像考试基准，用于评估图像生成模型在整合知识、推理和生成考试式图像方面的能力。结果表明，即使是最先进的模型也表现不佳，表明该基准的挑战性及其促进通用人工智能的潜力。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 10,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching",
        "summary": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality.",
        "url": "http://arxiv.org/abs/2509.13789v1",
        "published_date": "2025-09-17T07:58:36+00:00",
        "updated_date": "2025-09-17T07:58:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hanshuai Cui",
            "Zhiqing Tang",
            "Zhifei Xu",
            "Zhi Yao",
            "Wenyi Zeng",
            "Weijia Jia"
        ],
        "tldr": "The paper introduces BWCache, a training-free method that accelerates Diffusion Transformer (DiT)-based video generation by caching and reusing features from DiT blocks across diffusion timesteps, achieving up to 2.24x speedup with comparable visual quality.",
        "tldr_zh": "该论文介绍了一种名为BWCache的免训练方法，通过缓存和重用Diffusion Transformer (DiT)中DiT块在扩散时间步中的特征，来加速基于DiT的视频生成，实现了高达2.24倍的加速，同时保持了可比的视觉质量。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "SAIL-VL2 Technical Report",
        "summary": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Three\ncore innovations drive its effectiveness. First, a large-scale data curation\npipeline with scoring and filtering strategies enhances both quality and\ndistribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community.",
        "url": "http://arxiv.org/abs/2509.14033v1",
        "published_date": "2025-09-17T14:34:02+00:00",
        "updated_date": "2025-09-17T14:34:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weijie Yin",
            "Yongjie Ye",
            "Fangxun Shu",
            "Yue Liao",
            "Zijian Kang",
            "Hongyuan Dong",
            "Haiyang Yu",
            "Dingkang Yang",
            "Jiacong Wang",
            "Han Wang",
            "Wenzhuo Liu",
            "Xiao Liang",
            "Shuicheng Yan",
            "Chao Feng"
        ],
        "tldr": "SAIL-VL2 is a new open-source vision-language foundation model achieving state-of-the-art results on various multimodal benchmarks using data curation, a progressive training framework, and efficient sparse MoE architectures.",
        "tldr_zh": "SAIL-VL2是一个新的开源视觉语言基础模型，它通过数据管理、渐进式训练框架和高效的稀疏MoE架构，在各种多模态基准测试中取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Noise-Level Diffusion Guidance: Well Begun is Half Done",
        "summary": "Diffusion models have achieved state-of-the-art image generation. However,\nthe random Gaussian noise used to start the diffusion process influences the\nfinal output, causing variations in image quality and prompt adherence.\nExisting noise-level optimization approaches generally rely on extra dataset\nconstruction, additional networks, or backpropagation-based optimization,\nlimiting their practicality. In this paper, we propose Noise Level Guidance\n(NLG), a simple, efficient, and general noise-level optimization approach that\nrefines initial noise by increasing the likelihood of its alignment with\ngeneral guidance - requiring no additional training data, auxiliary networks,\nor backpropagation. The proposed NLG approach provides a unified framework\ngeneralizable to both conditional and unconditional diffusion models,\naccommodating various forms of diffusion-level guidance. Extensive experiments\non five standard benchmarks demonstrate that our approach enhances output\ngeneration quality and input condition adherence. By seamlessly integrating\nwith existing guidance methods while maintaining computational efficiency, our\nmethod establishes NLG as a practical and scalable enhancement to diffusion\nmodels. Code can be found at\nhttps://github.com/harveymannering/NoiseLevelGuidance.",
        "url": "http://arxiv.org/abs/2509.13936v1",
        "published_date": "2025-09-17T13:05:59+00:00",
        "updated_date": "2025-09-17T13:05:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Harvey Mannering",
            "Zhiwu Huang",
            "Adam Prugel-Bennett"
        ],
        "tldr": "The paper introduces Noise Level Guidance (NLG), a simple and efficient method to refine initial noise in diffusion models, improving image quality and prompt adherence without requiring extra data or networks.",
        "tldr_zh": "本文介绍了一种名为噪声水平引导（NLG）的简单高效方法，用于优化扩散模型中的初始噪声，从而在不额外需要数据或网络的情况下，提高图像质量和提示词遵循度。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Controllable-Continuous Color Editing in Diffusion Model via Color Mapping",
        "summary": "In recent years, text-driven image editing has made significant progress.\nHowever, due to the inherent ambiguity and discreteness of natural language,\ncolor editing still faces challenges such as insufficient precision and\ndifficulty in achieving continuous control. Although linearly interpolating the\nembedding vectors of different textual descriptions can guide the model to\ngenerate a sequence of images with varying colors, this approach lacks precise\ncontrol over the range of color changes in the output images. Moreover, the\nrelationship between the interpolation coefficient and the resulting image\ncolor is unknown and uncontrollable. To address these issues, we introduce a\ncolor mapping module that explicitly models the correspondence between the text\nembedding space and image RGB values. This module predicts the corresponding\nembedding vector based on a given RGB value, enabling precise color control of\nthe generated images while maintaining semantic consistency. Users can specify\na target RGB range to generate images with continuous color variations within\nthe desired range, thereby achieving finer-grained, continuous, and\ncontrollable color editing. Experimental results demonstrate that our method\nperforms well in terms of color continuity and controllability.",
        "url": "http://arxiv.org/abs/2509.13756v1",
        "published_date": "2025-09-17T07:12:51+00:00",
        "updated_date": "2025-09-17T07:12:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuqi Yang",
            "Dongliang Chang",
            "Yuanchen Fang",
            "Yi-Zhe SonG",
            "Zhanyu Ma",
            "Jun Guo"
        ],
        "tldr": "The paper introduces a color mapping module for diffusion models to enable precise and continuous color editing in text-to-image generation, addressing the limitations of existing methods in controlling the range and relationship of color changes.",
        "tldr_zh": "该论文介绍了一种用于扩散模型的颜色映射模块，可以在文本到图像的生成中实现精确和连续的颜色编辑，解决了现有方法在控制颜色变化的范围和关系方面的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform",
        "summary": "Diffusion models enable high-quality virtual try-on (VTO) with their\nestablished image synthesis abilities. Despite the extensive end-to-end\ntraining of large pre-trained models involved in current VTO methods,\nreal-world applications often prioritize limited training and inference,\nserving, and deployment budgets for VTO. To solve this obstacle, we apply\nDoob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained\nunconditional models for downstream image-conditioned VTO abilities. DEFT\nfreezes the pre-trained model's parameters and trains a small h-transform\nnetwork to learn a conditional h-transform. The h-transform network allows\ntraining only 1.42 percent of the frozen parameters, compared to a baseline of\n5.52 percent in traditional parameter-efficient fine-tuning (PEFT).\n  To further improve DEFT's performance and decrease existing models' inference\ntime, we additionally propose an adaptive consistency loss. Consistency\ntraining distills slow but high-performing diffusion models into a fast one\nwhile retaining performance by enforcing consistencies along the inference\npath. Inspired by constrained optimization, instead of distillation, we combine\nthe consistency loss and the denoising score matching loss in a data-adaptive\nmanner for fine-tuning existing VTO models at a low cost. Empirical results\nshow the proposed DEFT-VTON method achieves state-of-the-art performance on VTO\ntasks, with as few as 15 denoising steps, while maintaining competitive\nresults.",
        "url": "http://arxiv.org/abs/2509.13506v1",
        "published_date": "2025-09-16T20:11:48+00:00",
        "updated_date": "2025-09-16T20:11:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xingzi Xu",
            "Qi Li",
            "Shuwen Qiu",
            "Julien Han",
            "Karim Bouyarmane"
        ],
        "tldr": "The paper introduces DEFT-VTON, a method for efficient virtual try-on using Doob's h-transform efficient fine-tuning (DEFT) and an adaptive consistency loss, achieving state-of-the-art performance with minimal training and inference costs.",
        "tldr_zh": "该论文介绍了DEFT-VTON，一种高效的虚拟试穿方法，它利用Doob的h变换高效微调(DEFT)和自适应一致性损失，在最小的训练和推理成本下实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]