[
    {
        "title": "Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing",
        "summary": "Rectified flow (RF) models have recently demonstrated superior generative\nperformance compared to DDIM-based diffusion models. However, in real-world\napplications, they suffer from two major challenges: (1) low inversion accuracy\nthat hinders the consistency with the source image, and (2) entangled\nmultimodal attention in diffusion transformers, which hinders precise attention\ncontrol. To address the first challenge, we propose an efficient high-order\ninversion method for rectified flow models based on the Runge-Kutta solver of\ndifferential equations. To tackle the second challenge, we introduce Decoupled\nDiffusion Transformer Attention (DDTA), a novel mechanism that disentangles\ntext and image attention inside the multimodal diffusion transformers, enabling\nmore precise semantic control. Extensive experiments on image reconstruction\nand text-guided editing tasks demonstrate that our method achieves\nstate-of-the-art performance in terms of fidelity and editability. Code is\navailable at https://github.com/wmchen/RKSovler_DDTA.",
        "url": "http://arxiv.org/abs/2509.12888v1",
        "published_date": "2025-09-16T09:41:14+00:00",
        "updated_date": "2025-09-16T09:41:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Weiming Chen",
            "Zhihan Zhu",
            "Yijia Wang",
            "Zhihai He"
        ],
        "tldr": "This paper presents a Runge-Kutta solver for improved inversion accuracy in rectified flow models and a decoupled attention mechanism for enhanced semantic control in multimodal diffusion transformers, leading to state-of-the-art image reconstruction and editing.",
        "tldr_zh": "本文提出了一种基于Runge-Kutta求解器的修正流模型的高阶反演方法，以提高反演精度，并提出了一种解耦注意力机制，用于增强多模态扩散转换器中的语义控制，从而实现最先进的图像重建和编辑。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Lego-Edit: A General Image Editing Framework with Model-Level Bricks and MLLM Builder",
        "summary": "Instruction-based image editing has garnered significant attention due to its\ndirect interaction with users. However, real-world user instructions are\nimmensely diverse, and existing methods often fail to generalize effectively to\ninstructions outside their training domain, limiting their practical\napplication. To address this, we propose Lego-Edit, which leverages the\ngeneralization capability of Multi-modal Large Language Model (MLLM) to\norganize a suite of model-level editing tools to tackle this challenge.\nLego-Edit incorporates two key designs: (1) a model-level toolkit comprising\ndiverse models efficiently trained on limited data and several image\nmanipulation functions, enabling fine-grained composition of editing actions by\nthe MLLM; and (2) a three-stage progressive reinforcement learning approach\nthat uses feedback on unannotated, open-domain instructions to train the MLLM,\nequipping it with generalized reasoning capabilities for handling real-world\ninstructions. Experiments demonstrate that Lego-Edit achieves state-of-the-art\nperformance on GEdit-Bench and ImgBench. It exhibits robust reasoning\ncapabilities for open-domain instructions and can utilize newly introduced\nediting tools without additional fine-tuning.\n  Code is available: https://github.com/xiaomi-research/lego-edit.",
        "url": "http://arxiv.org/abs/2509.12883v1",
        "published_date": "2025-09-16T09:36:17+00:00",
        "updated_date": "2025-09-16T09:36:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qifei Jia",
            "Yu Liu",
            "Yajie Chai",
            "Xintong Yao",
            "Qiming Lu",
            "Yasen Zhang",
            "Runyu Shi",
            "Ying Huang",
            "Guoquan Zhang"
        ],
        "tldr": "Lego-Edit is a new instruction-based image editing framework that uses an MLLM to orchestrate a suite of specialized editing models, achieving SOTA performance and strong generalization to open-domain instructions.",
        "tldr_zh": "Lego-Edit是一个新的基于指令的图像编辑框架，它使用MLLM来协调一套专门的编辑模型，实现了SOTA的性能，并对未知的指令具有强大的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Leveraging Large Language Models to Effectively Generate Visual Data for Canine Musculoskeletal Diagnoses",
        "summary": "It is well-established that more data generally improves AI model\nperformance. However, data collection can be challenging for certain tasks due\nto the rarity of occurrences or high costs. These challenges are evident in our\nuse case, where we apply AI models to a novel approach for visually documenting\nthe musculoskeletal condition of dogs. Here, abnormalities are marked as\ncolored strokes on a body map of a dog. Since these strokes correspond to\ndistinct muscles or joints, they can be mapped to the textual domain in which\nlarge language models (LLMs) operate. LLMs have demonstrated impressive\ncapabilities across a wide range of tasks, including medical applications,\noffering promising potential for generating synthetic training data. In this\nwork, we investigate whether LLMs can effectively generate synthetic visual\ntraining data for canine musculoskeletal diagnoses. For this, we developed a\nmapping that segments visual documentations into over 200 labeled regions\nrepresenting muscles or joints. Using techniques like guided decoding,\nchain-of-thought reasoning, and few-shot prompting, we generated 1,000\nsynthetic visual documentations for patellar luxation (kneecap dislocation)\ndiagnosis, the diagnosis for which we have the most real-world data. Our\nanalysis shows that the generated documentations are sensitive to location and\nseverity of the diagnosis while remaining independent of the dog's sex. We\nfurther generated 1,000 visual documentations for various other diagnoses to\ncreate a binary classification dataset. A model trained solely on this\nsynthetic data achieved an F1 score of 88% on 70 real-world documentations.\nThese results demonstrate the potential of LLM-generated synthetic data, which\nis particularly valuable for addressing data scarcity in rare diseases. While\nour methodology is tailored to the medical domain, the insights and techniques\ncan be adapted to other fields.",
        "url": "http://arxiv.org/abs/2509.12866v1",
        "published_date": "2025-09-16T09:22:17+00:00",
        "updated_date": "2025-09-16T09:22:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Martin Thißen",
            "Thi Ngoc Diep Tran",
            "Barbara Esteve Ratsch",
            "Ben Joel Schönbein",
            "Ute Trapp",
            "Beate Egner",
            "Romana Piat",
            "Elke Hergenröther"
        ],
        "tldr": "This paper explores using LLMs to generate synthetic visual data for training AI models in canine musculoskeletal diagnoses, achieving promising results in data-scarce scenarios.",
        "tldr_zh": "本文探讨了使用大型语言模型（LLM）生成合成视觉数据，以训练用于犬类肌肉骨骼诊断的AI模型，并在数据稀缺情况下取得了可喜的成果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models",
        "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive performance\non multimodal tasks through scaled architectures and extensive training.\nHowever, existing Mixture of Experts (MoE) approaches face challenges due to\nthe asymmetry between visual and linguistic processing. Visual information is\nspatially complete, while language requires maintaining sequential context. As\na result, MoE models struggle to balance modality-specific features and\ncross-modal interactions. Through systematic analysis, we observe that language\nexperts in deeper layers progressively lose contextual grounding and rely more\non parametric knowledge rather than utilizing the provided visual and\nlinguistic information. To address this, we propose AsyMoE, a novel\narchitecture that models this asymmetry using three specialized expert groups.\nWe design intra-modality experts for modality-specific processing, hyperbolic\ninter-modality experts for hierarchical cross-modal interactions, and\nevidence-priority language experts to suppress parametric biases and maintain\ncontextual grounding. Extensive experiments demonstrate that AsyMoE achieves\n26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific\nMoE respectively, with 25.45% fewer activated parameters than dense models.",
        "url": "http://arxiv.org/abs/2509.12715v1",
        "published_date": "2025-09-16T06:16:05+00:00",
        "updated_date": "2025-09-16T06:16:05+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Heng Zhang",
            "Haichuan Hu",
            "Yaomin Shen",
            "Weihao Yu",
            "Yilei Yuan",
            "Haochen You",
            "Guo Cheng",
            "Zijian Zhang",
            "Lubin Gan",
            "Huihui Wei",
            "Hao Zhang",
            "Jin Huang"
        ],
        "tldr": "The paper introduces AsyMoE, a Mixture of Experts architecture for Large Vision-Language Models that leverages modal asymmetry to improve expert specialization and performance on multimodal tasks, showing significant accuracy improvements and parameter reduction compared to existing MoE approaches.",
        "tldr_zh": "该论文介绍了AsyMoE，一种用于大型视觉语言模型的混合专家架构，它利用模态不对称性来提高专家专业化程度，并提升多模态任务的性能。与现有的MoE方法相比，它在准确性方面有显著提升，并减少了参数量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Adaptive Sampling Scheduler",
        "summary": "Consistent distillation methods have evolved into effective techniques that\nsignificantly accelerate the sampling process of diffusion models. Although\nexisting methods have achieved remarkable results, the selection of target\ntimesteps during distillation mainly relies on deterministic or stochastic\nstrategies, which often require sampling schedulers to be designed specifically\nfor different distillation processes. Moreover, this pattern severely limits\nflexibility, thereby restricting the full sampling potential of diffusion\nmodels in practical applications. To overcome these limitations, this paper\nproposes an adaptive sampling scheduler that is applicable to various\nconsistency distillation frameworks. The scheduler introduces three innovative\nstrategies: (i) dynamic target timestep selection, which adapts to different\nconsistency distillation frameworks by selecting timesteps based on their\ncomputed importance; (ii) Optimized alternating sampling along the solution\ntrajectory by guiding forward denoising and backward noise addition based on\nthe proposed time step importance, enabling more effective exploration of the\nsolution space to enhance generation performance; and (iii) Utilization of\nsmoothing clipping and color balancing techniques to achieve stable and\nhigh-quality generation results at high guidance scales, thereby expanding the\napplicability of consistency distillation models in complex generation\nscenarios. We validated the effectiveness and flexibility of the adaptive\nsampling scheduler across various consistency distillation methods through\ncomprehensive experimental evaluations. Experimental results consistently\ndemonstrated significant improvements in generative performance, highlighting\nthe strong adaptability achieved by our method.",
        "url": "http://arxiv.org/abs/2509.12569v1",
        "published_date": "2025-09-16T01:51:16+00:00",
        "updated_date": "2025-09-16T01:51:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qi Wang",
            "Shuliang Zhu",
            "Jinjia Zhou"
        ],
        "tldr": "This paper introduces an adaptive sampling scheduler for consistency distillation in diffusion models, aiming to improve flexibility and generation performance across various frameworks by dynamically selecting timesteps and employing optimized alternating sampling.",
        "tldr_zh": "本文介绍了一种扩散模型中一致性蒸馏的自适应采样调度器，旨在通过动态选择时间步长和采用优化的交替采样，提高各种框架的灵活性和生成性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Image Tokenizer Needs Post-Training",
        "summary": "Recent image generative models typically capture the image distribution in a\npre-constructed latent space, relying on a frozen image tokenizer. However,\nthere exists a significant discrepancy between the reconstruction and\ngeneration distribution, where current tokenizers only prioritize the\nreconstruction task that happens before generative training without considering\nthe generation errors during sampling. In this paper, we comprehensively\nanalyze the reason for this discrepancy in a discrete latent space, and, from\nwhich, we propose a novel tokenizer training scheme including both\nmain-training and post-training, focusing on improving latent space\nconstruction and decoding respectively. During the main training, a latent\nperturbation strategy is proposed to simulate sampling noises, \\ie, the\nunexpected tokens generated in generative inference. Specifically, we propose a\nplug-and-play tokenizer training scheme, which significantly enhances the\nrobustness of tokenizer, thus boosting the generation quality and convergence\nspeed, and a novel tokenizer evaluation metric, \\ie, pFID, which successfully\ncorrelates the tokenizer performance to generation quality. During\npost-training, we further optimize the tokenizer decoder regarding a\nwell-trained generative model to mitigate the distribution difference between\ngenerated and reconstructed tokens. With a $\\sim$400M generator, a discrete\ntokenizer trained with our proposed main training achieves a notable 1.60 gFID\nand further obtains 1.36 gFID with the additional post-training. Further\nexperiments are conducted to broadly validate the effectiveness of our\npost-training strategy on off-the-shelf discrete and continuous tokenizers,\ncoupled with autoregressive and diffusion-based generators.",
        "url": "http://arxiv.org/abs/2509.12474v1",
        "published_date": "2025-09-15T21:38:03+00:00",
        "updated_date": "2025-09-15T21:38:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kai Qiu",
            "Xiang Li",
            "Hao Chen",
            "Jason Kuen",
            "Xiaohao Xu",
            "Jiuxiang Gu",
            "Yinyi Luo",
            "Bhiksha Raj",
            "Zhe Lin",
            "Marios Savvides"
        ],
        "tldr": "The paper introduces a novel tokenizer training scheme with main-training and post-training stages to address the discrepancy between reconstruction and generation distributions in image generative models, achieving improved generation quality and convergence speed.",
        "tldr_zh": "该论文提出了一种新颖的tokenizer训练方案，包含主训练和后训练阶段，旨在解决图像生成模型中重建和生成分布之间的差异，从而提高生成质量和收敛速度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with Texture-Geometry Dual Guidance",
        "summary": "Creating 3D assets that follow the texture and geometry style of existing\nones is often desirable or even inevitable in practical applications like video\ngaming and virtual reality. While impressive progress has been made in\ngenerating 3D objects from text or images, creating style-controllable 3D\nassets remains a complex and challenging problem. In this work, we propose\nStyleSculptor, a novel training-free approach for generating style-guided 3D\nassets from a content image and one or more style images. Unlike previous\nworks, StyleSculptor achieves style-guided 3D generation in a zero-shot manner,\nenabling fine-grained 3D style control that captures the texture, geometry, or\nboth styles of user-provided style images. At the core of StyleSculptor is a\nnovel Style Disentangled Attention (SD-Attn) module, which establishes a\ndynamic interaction between the input content image and style image for\nstyle-guided 3D asset generation via a cross-3D attention mechanism, enabling\nstable feature fusion and effective style-guided generation. To alleviate\nsemantic content leakage, we also introduce a style-disentangled feature\nselection strategy within the SD-Attn module, which leverages the variance of\n3D feature patches to disentangle style- and content-significant channels,\nallowing selective feature injection within the attention framework. With\nSD-Attn, the network can dynamically compute texture-, geometry-, or\nboth-guided features to steer the 3D generation process. Built upon this, we\nfurther propose the Style Guided Control (SGC) mechanism, which enables\nexclusive geometry- or texture-only stylization, as well as adjustable style\nintensity control. Extensive experiments demonstrate that StyleSculptor\noutperforms existing baseline methods in producing high-fidelity 3D assets.",
        "url": "http://arxiv.org/abs/2509.13301v1",
        "published_date": "2025-09-16T17:55:20+00:00",
        "updated_date": "2025-09-16T17:55:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zefan Qu",
            "Zhenwei Wang",
            "Haoyuan Wang",
            "Ke Xu",
            "Gerhard Hancke",
            "Rynson W. H. Lau"
        ],
        "tldr": "StyleSculptor is a zero-shot, training-free method for generating style-controllable 3D assets from content and style images, using a Style Disentangled Attention module for fine-grained texture and geometry control.",
        "tldr_zh": "StyleSculptor 是一种零样本、免训练的方法，用于从内容图像和风格图像生成风格可控的 3D 资产，它使用风格解耦注意力模块进行细粒度的纹理和几何控制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SmokeBench: A Real-World Dataset for Surveillance Image Desmoking in Early-Stage Fire Scenes",
        "summary": "Early-stage fire scenes (0-15 minutes after ignition) represent a crucial\ntemporal window for emergency interventions. During this stage, the smoke\nproduced by combustion significantly reduces the visibility of surveillance\nsystems, severely impairing situational awareness and hindering effective\nemergency response and rescue operations. Consequently, there is an urgent need\nto remove smoke from images to obtain clear scene information. However, the\ndevelopment of smoke removal algorithms remains limited due to the lack of\nlarge-scale, real-world datasets comprising paired smoke-free and\nsmoke-degraded images. To address these limitations, we present a real-world\nsurveillance image desmoking benchmark dataset named SmokeBench, which contains\nimage pairs captured under diverse scenes setup and smoke concentration. The\ncurated dataset provides precisely aligned degraded and clean images, enabling\nsupervised learning and rigorous evaluation. We conduct comprehensive\nexperiments by benchmarking a variety of desmoking methods on our dataset. Our\ndataset provides a valuable foundation for advancing robust and practical image\ndesmoking in real-world fire scenes. This dataset has been released to the\npublic and can be downloaded from https://github.com/ncfjd/SmokeBench.",
        "url": "http://arxiv.org/abs/2509.12701v1",
        "published_date": "2025-09-16T05:51:11+00:00",
        "updated_date": "2025-09-16T05:51:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenzhuo Jin",
            "Qianfeng Yang",
            "Xianhao Wu",
            "Hongming Chen",
            "Pengpeng Li",
            "Xiang Chen"
        ],
        "tldr": "The paper introduces SmokeBench, a new real-world dataset of paired smoke-free and smoke-degraded images for early-stage fire scene desmoking, aimed at improving surveillance system visibility and emergency response.",
        "tldr_zh": "该论文介绍了SmokeBench，一个用于早期火灾场景去烟雾的真实世界数据集，包含烟雾退化和无烟图像对，旨在提高监控系统的可见性和应急响应能力。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4
    }
]