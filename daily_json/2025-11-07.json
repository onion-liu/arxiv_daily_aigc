[
    {
        "title": "Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm",
        "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.",
        "url": "http://arxiv.org/abs/2511.04570v1",
        "published_date": "2025-11-06T17:25:23+00:00",
        "updated_date": "2025-11-06T17:25:23+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Jingqi Tong",
            "Yurong Mou",
            "Hangcheng Li",
            "Mingzhe Li",
            "Yongzhuo Yang",
            "Ming Zhang",
            "Qiguang Chen",
            "Tianyi Liang",
            "Xiaomeng Hu",
            "Yining Zheng",
            "Xinchi Chen",
            "Jun Zhao",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "tldr": "This paper introduces \"Thinking with Video,\" a novel multimodal reasoning paradigm using video generation models like Sora-2, and presents the VideoThinkBench benchmark to demonstrate its effectiveness in both vision-centric and text-centric tasks.",
        "tldr_zh": "本文介绍了“用视频思考”这一新的多模态推理范式，该范式使用Sora-2等视频生成模型，并提出了VideoThinkBench基准来证明其在以视觉为中心和以文本为中心的任务中的有效性。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation",
        "summary": "Most text-to-video(T2V) diffusion models depend on pre-trained text encoders\nfor semantic alignment, yet they often fail to maintain video quality when\nprovided with concise prompts rather than well-designed ones. The primary issue\nlies in their limited textual semantics understanding. Moreover, these text\nencoders cannot rephrase prompts online to better align with user intentions,\nwhich limits both the scalability and usability of the models, To address these\nchallenges, we introduce RISE-T2V, which uniquely integrates the processes of\nprompt rephrasing and semantic feature extraction into a single and seamless\nstep instead of two separate steps. RISE-T2V is universal and can be applied to\nvarious pre-trained LLMs and video diffusion models(VDMs), significantly\nenhancing their capabilities for T2V tasks. We propose an innovative module\ncalled the Rephrasing Adapter, enabling diffusion models to utilize text hidden\nstates during the next token prediction of the LLM as a condition for video\ngeneration. By employing a Rephrasing Adapter, the video generation model can\nimplicitly rephrase basic prompts into more comprehensive representations that\nbetter match the user's intent. Furthermore, we leverage the powerful\ncapabilities of LLMs to enable video generation models to accomplish a broader\nrange of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a\nversatile framework applicable to different video diffusion model\narchitectures, significantly enhancing the ability of T2V models to generate\nhigh-quality videos that align with user intent. Visual results are available\non the webpage at https://rise-t2v.github.io.",
        "url": "http://arxiv.org/abs/2511.04317v1",
        "published_date": "2025-11-06T12:42:03+00:00",
        "updated_date": "2025-11-06T12:42:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangjun Zhang",
            "Litong Gong",
            "Yinglin Zheng",
            "Yansong Liu",
            "Wentao Jiang",
            "Mingyi Xu",
            "Biao Wang",
            "Tiezheng Ge",
            "Ming Zeng"
        ],
        "tldr": "The paper introduces RISE-T2V, a framework integrating prompt rephrasing and semantic feature extraction into a single step for enhanced text-to-video generation by leveraging LLMs and a Rephrasing Adapter.",
        "tldr_zh": "该论文介绍了RISE-T2V，一个框架，通过利用LLM和一个重述适配器，将提示语改述和语义特征提取集成到一个步骤中，以增强文本到视频的生成效果。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Text to Sketch Generation with Multi-Styles",
        "summary": "Recent advances in vision-language models have facilitated progress in sketch\ngeneration. However, existing specialized methods primarily focus on generic\nsynthesis and lack mechanisms for precise control over sketch styles. In this\nwork, we propose a training-free framework based on diffusion models that\nenables explicit style guidance via textual prompts and referenced style\nsketches. Unlike previous style transfer methods that overwrite key and value\nmatrices in self-attention, we incorporate the reference features as auxiliary\ninformation with linear smoothing and leverage a style-content guidance\nmechanism. This design effectively reduces content leakage from reference\nsketches and enhances synthesis quality, especially in cases with low\nstructural similarity between reference and target sketches. Furthermore, we\nextend our framework to support controllable multi-style generation by\nintegrating features from multiple reference sketches, coordinated via a joint\nAdaIN module. Extensive experiments demonstrate that our approach achieves\nhigh-quality sketch generation with accurate style alignment and improved\nflexibility in style control. The official implementation of M3S is available\nat https://github.com/CMACH508/M3S.",
        "url": "http://arxiv.org/abs/2511.04123v1",
        "published_date": "2025-11-06T07:13:56+00:00",
        "updated_date": "2025-11-06T07:13:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tengjie Li",
            "Shikui Tu",
            "Lei Xu"
        ],
        "tldr": "This paper introduces a training-free diffusion model framework (M3S) for text-to-sketch generation, enabling explicit style control via textual prompts and reference sketches, even with low structural similarity.",
        "tldr_zh": "这篇论文介绍了一个无需训练的扩散模型框架 (M3S)，用于文本到草图的生成，能通过文本提示和参考草图实现显式的风格控制，即使在结构相似度较低的情况下。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration",
        "summary": "In this paper, we propose Tortoise and Hare Guidance (THG), a training-free\nstrategy that accelerates diffusion sampling while maintaining high-fidelity\ngeneration. We demonstrate that the noise estimate and the additional guidance\nterm exhibit markedly different sensitivity to numerical error by reformulating\nthe classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our\nerror-bound analysis shows that the additional guidance branch is more robust\nto approximation, revealing substantial redundancy that conventional solvers\nfail to exploit. Building on this insight, THG significantly reduces the\ncomputation of the additional guidance: the noise estimate is integrated with\nthe tortoise equation on the original, fine-grained timestep grid, while the\nadditional guidance is integrated with the hare equation only on a coarse grid.\nWe also introduce (i) an error-bound-aware timestep sampler that adaptively\nselects step sizes and (ii) a guidance-scale scheduler that stabilizes large\nextrapolation spans. THG reduces the number of function evaluations (NFE) by up\nto 30% with virtually no loss in generation fidelity ($\\Delta$ImageReward\n$\\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free\naccelerators under identical computation budgets. Our findings highlight the\npotential of multirate formulations for diffusion solvers, paving the way for\nreal-time high-quality image synthesis without any model retraining. The source\ncode is available at https://github.com/yhlee-add/THG.",
        "url": "http://arxiv.org/abs/2511.04117v1",
        "published_date": "2025-11-06T07:08:58+00:00",
        "updated_date": "2025-11-06T07:08:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunghee Lee",
            "Byeonghyun Pak",
            "Junwha Hong",
            "Hoseong Kim"
        ],
        "tldr": "The paper introduces Tortoise and Hare Guidance (THG), a training-free method based on multirate ODE integration, to accelerate diffusion model sampling by exploiting differential sensitivity to numerical error in classifier-free guidance, achieving up to 30% NFE reduction with minimal fidelity loss.",
        "tldr_zh": "该论文介绍了一种名为 Tortoise and Hare Guidance (THG) 的免训练方法，该方法基于多速率 ODE 积分，通过利用无分类器引导中对数值误差的不同敏感性来加速扩散模型采样，实现了高达 30% 的 NFE 减少，同时保持了最小的保真度损失。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]