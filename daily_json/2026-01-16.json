[
    {
        "title": "CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos",
        "summary": "In this paper, we find that the generation of 3D human motions and 2D human videos is intrinsically coupled. 3D motions provide the structural prior for plausibility and consistency in videos, while pre-trained video models offer strong generalization capabilities for motions, which necessitate coupling their generation processes. Based on this, we present CoMoVi, a co-generative framework that couples two video diffusion models (VDMs) to generate 3D human motions and videos synchronously within a single diffusion denoising loop. To achieve this, we first propose an effective 2D human motion representation that can inherit the powerful prior of pre-trained VDMs. Then, we design a dual-branch diffusion model to couple human motion and video generation process with mutual feature interaction and 3D-2D cross attentions. Moreover, we curate CoMoVi Dataset, a large-scale real-world human video dataset with text and motion annotations, covering diverse and challenging human motions. Extensive experiments demonstrate the effectiveness of our method in both 3D human motion and video generation tasks.",
        "url": "http://arxiv.org/abs/2601.10632v1",
        "published_date": "2026-01-15T17:52:29+00:00",
        "updated_date": "2026-01-15T17:52:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengfeng Zhao",
            "Jiazhi Shu",
            "Yubo Zhao",
            "Tianyu Huang",
            "Jiahao Lu",
            "Zekai Gu",
            "Chengwei Ren",
            "Zhiyang Dou",
            "Qing Shuai",
            "Yuan Liu"
        ],
        "tldr": "The paper introduces CoMoVi, a co-generative framework that uses coupled video diffusion models to synchronously generate 3D human motions and realistic videos, leveraging 3D motion priors and pre-trained video model generalizability. They also introduce a new dataset for this task.",
        "tldr_zh": "该论文介绍了CoMoVi，一个协同生成框架，它使用耦合的视频扩散模型同步生成3D人体运动和逼真的视频，利用3D运动先验和预训练的视频模型泛化能力。他们还为此任务引入了一个新的数据集。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
        "summary": "Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.",
        "url": "http://arxiv.org/abs/2601.10332v1",
        "published_date": "2026-01-15T12:19:05+00:00",
        "updated_date": "2026-01-15T12:19:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siqi Kou",
            "Jiachun Jin",
            "Zetong Zhou",
            "Ye Ma",
            "Yugang Wang",
            "Quan Chen",
            "Peng Jiang",
            "Xiao Yang",
            "Jun Zhu",
            "Kai Yu",
            "Zhijie Deng"
        ],
        "tldr": "The paper introduces a \"think-then-generate\" (T2G) paradigm for text-to-image diffusion models, encouraging LLMs to reason about and rewrite prompts before image generation, improving factual consistency and semantic alignment.",
        "tldr_zh": "该论文提出了一个“先思考后生成”(T2G)的文本到图像扩散模型范例，鼓励大型语言模型在图像生成之前推理和重写提示，从而提高事实一致性和语义对齐。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
        "summary": "State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.",
        "url": "http://arxiv.org/abs/2601.10553v1",
        "published_date": "2026-01-15T16:18:00+00:00",
        "updated_date": "2026-01-15T16:18:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianhao Yuan",
            "Xiaofeng Zhang",
            "Felix Friedrich",
            "Nicolas Beltran-Velez",
            "Melissa Hall",
            "Reyhane Askari-Hemmat",
            "Xiaochuang Han",
            "Nicolas Ballas",
            "Michal Drozdzal",
            "Adriana Romero-Soriano"
        ],
        "tldr": "This paper introduces WMReward, an inference-time method that uses a latent world model to improve the physics plausibility of video generation, achieving state-of-the-art results in a physics IQ challenge.",
        "tldr_zh": "本文介绍了一种名为WMReward的推理时方法，该方法利用潜在世界模型来提高视频生成的物理合理性，并在物理智商挑战赛中取得了最先进的成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
        "summary": "Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.",
        "url": "http://arxiv.org/abs/2601.10103v1",
        "published_date": "2026-01-15T06:16:22+00:00",
        "updated_date": "2026-01-15T06:16:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lizhen Wang",
            "Yongming Zhu",
            "Zhipeng Ge",
            "Youwei Zheng",
            "Longhao Zhang",
            "Tianshu Hu",
            "Shiyang Qin",
            "Mingshuang Luo",
            "Jiaxu Zhang",
            "Xin Chen",
            "Yulong Wang",
            "Zerong Zheng",
            "Jianwen Jiang",
            "Chao Liang",
            "Weifeng Chen",
            "Xing Wang",
            "Yuan Zhang",
            "Mingyuan Gao"
        ],
        "tldr": "FlowAct-R1 is a framework for real-time interactive humanoid video generation, achieving high-fidelity synthesis and low-latency responsiveness through chunkwise diffusion forcing and system optimizations.",
        "tldr_zh": "FlowAct-R1 是一个用于实时交互式人形视频生成的框架，通过分块扩散强制和系统优化，实现了高保真合成和低延迟响应。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
        "summary": "Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.",
        "url": "http://arxiv.org/abs/2601.10061v1",
        "published_date": "2026-01-15T04:33:06+00:00",
        "updated_date": "2026-01-15T04:33:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chengzhuo Tong",
            "Mingkun Chang",
            "Shenglong Zhang",
            "Yuran Wang",
            "Cheng Liang",
            "Zhizheng Zhao",
            "Ruichuan An",
            "Bohan Zeng",
            "Yang Shi",
            "Yifan Dai",
            "Ziming Zhao",
            "Guanbin Li",
            "Pengfei Wan",
            "Yuanxing Zhang",
            "Wentao Zhang"
        ],
        "tldr": "The paper introduces CoF-T2I, a text-to-image generation model that leverages Chain-of-Frame reasoning from video models for progressive visual refinement, and also introduces a new dataset CoF-Evol-Instruct, demonstrating competitive performance on challenging benchmarks.",
        "tldr_zh": "该论文介绍了CoF-T2I，一个利用视频模型的帧链推理进行渐进式视觉改进的文本到图像生成模型，并且引入了一个新的数据集CoF-Evol-Instruct，在具有挑战性的基准测试中表现出具有竞争力的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Transition Matching Distillation for Fast Video Generation",
        "summary": "Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd",
        "url": "http://arxiv.org/abs/2601.09881v1",
        "published_date": "2026-01-14T21:30:03+00:00",
        "updated_date": "2026-01-14T21:30:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Weili Nie",
            "Julius Berner",
            "Nanye Ma",
            "Chao Liu",
            "Saining Xie",
            "Arash Vahdat"
        ],
        "tldr": "This paper introduces Transition Matching Distillation (TMD), a method for distilling large video diffusion models into efficient few-step generators, achieving a good balance between generation speed and visual quality.",
        "tldr_zh": "该论文介绍了过渡匹配蒸馏（TMD），一种将大型视频扩散模型提炼为高效少步生成器的方法，从而在生成速度和视觉质量之间取得了良好的平衡。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering",
        "summary": "Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.",
        "url": "http://arxiv.org/abs/2601.09697v1",
        "published_date": "2026-01-14T18:50:06+00:00",
        "updated_date": "2026-01-14T18:50:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jieying Chen",
            "Jeffrey Hu",
            "Joan Lasenby",
            "Ayush Tewari"
        ],
        "tldr": "The paper introduces SRENDER, a method for efficient camera-controlled video generation from static scenes using sparse keyframe diffusion and 3D rendering, achieving a 40x speedup compared to diffusion baselines.",
        "tldr_zh": "该论文介绍了SRENDER，一种利用稀疏关键帧扩散和3D渲染，从静态场景高效生成相机控制视频的方法，与扩散基线相比，速度提高了40倍。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation",
        "summary": "Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero",
        "url": "http://arxiv.org/abs/2601.10094v1",
        "published_date": "2026-01-15T05:47:43+00:00",
        "updated_date": "2026-01-15T05:47:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Han Wang",
            "Yi Yang",
            "Jingyuan Hu",
            "Minfeng Zhu",
            "Wei Chen"
        ],
        "tldr": "V-Zero is a post-training framework for vision-language models that enables self-improvement using only unlabeled images through a co-evolutionary questioner-solver loop, achieving performance gains without human annotations.",
        "tldr_zh": "V-Zero是一个视觉语言模型的后训练框架，通过使用无标签图像和一个共同进化的提问者-解答者循环来实现自我改进，无需人工标注即可获得性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]