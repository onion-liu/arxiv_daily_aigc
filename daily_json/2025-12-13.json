[
    {
        "title": "OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis",
        "summary": "Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\\% in multiview NVS LLFF dataset, 60\\% in dynamic NVS Neural 3D Video benchmark, 20\\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/",
        "url": "http://arxiv.org/abs/2512.10940v1",
        "published_date": "2025-12-11T18:59:05+00:00",
        "updated_date": "2025-12-11T18:59:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiang Fan",
            "Sharath Girish",
            "Vivek Ramanujan",
            "Chaoyang Wang",
            "Ashkan Mirzaei",
            "Petr Sushko",
            "Aliaksandr Siarohin",
            "Sergey Tulyakov",
            "Ranjay Krishna"
        ],
        "tldr": "OmniView is a unified diffusion model for 3D and 4D view synthesis that generalizes across diverse tasks like novel view synthesis and text-to-video generation with camera control, achieving state-of-the-art results.",
        "tldr_zh": "OmniView 是一个统一的扩散模型，用于 3D 和 4D 视图合成，它可以推广到各种任务，如新视角合成和具有相机控制的文本到视频生成，并取得最先进的结果。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
        "summary": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.",
        "url": "http://arxiv.org/abs/2512.10955v1",
        "published_date": "2025-12-11T18:59:56+00:00",
        "updated_date": "2025-12-11T18:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tsai-Shien Chen",
            "Aliaksandr Siarohin",
            "Guocheng Gordon Qian",
            "Kuan-Chieh Jackson Wang",
            "Egor Nemchinov",
            "Moayed Haji-Ali",
            "Riza Alp Guler",
            "Willi Menapace",
            "Ivan Skorokhodov",
            "Anil Kag",
            "Jun-Yan Zhu",
            "Sergey Tulyakov"
        ],
        "tldr": "The paper introduces Omni-Attribute, a novel open-vocabulary image attribute encoder that uses paired data and a dual-objective training scheme to generate attribute-specific representations for improved visual concept personalization and compositional generation.",
        "tldr_zh": "本文介绍了一种名为Omni-Attribute的新型开放词汇图像属性编码器，它使用配对数据和双目标训练方案来生成特定于属性的表示，从而改进视觉概念的个性化和组合生成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
        "summary": "Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation (\"1-NFE\") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.",
        "url": "http://arxiv.org/abs/2512.10953v1",
        "published_date": "2025-12-11T18:59:55+00:00",
        "updated_date": "2025-12-11T18:59:55+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Yiyang Lu",
            "Qiao Sun",
            "Xianbang Wang",
            "Zhicheng Jiang",
            "Hanhong Zhao",
            "Kaiming He"
        ],
        "tldr": "The paper introduces Bidirectional Normalizing Flow (BiFlow), a novel framework that removes the exact invertibility constraint in traditional Normalizing Flows, leading to improved generation quality and faster sampling, achieving state-of-the-art results on ImageNet.",
        "tldr_zh": "该论文介绍了一种双向归一化流（BiFlow）的新框架，它消除了传统归一化流中精确可逆性的约束，从而提高了生成质量和采样速度，并在ImageNet上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration",
        "summary": "In this work, we explore an untapped signal in diffusion model inference. While all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. This enables images to be jointly denoised at inference time, learning both intra and inter-image correspondence. We observe a clear scaling effect - larger group sizes yield stronger cross-sample attention and better generation quality. Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID. Built on standard diffusion transformers, our GroupDiff achieves up to 32.2% FID improvement on ImageNet-256x256. Our work reveals cross-sample inference as an effective, previously unexplored mechanism for generative modeling.",
        "url": "http://arxiv.org/abs/2512.10954v1",
        "published_date": "2025-12-11T18:59:55+00:00",
        "updated_date": "2025-12-11T18:59:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sicheng Mo",
            "Thao Nguyen",
            "Richard Zhang",
            "Nick Kolkin",
            "Siddharth Srinivasan Iyer",
            "Eli Shechtman",
            "Krishna Kumar Singh",
            "Yong Jae Lee",
            "Bolei Zhou",
            "Yuheng Li"
        ],
        "tldr": "The paper introduces Group Diffusion, a novel approach that enhances image generation by enabling cross-sample collaboration through a shared attention mechanism during the diffusion process, resulting in significant FID improvements.",
        "tldr_zh": "该论文介绍了Group Diffusion，一种通过在扩散过程中启用共享注意力机制来实现跨样本协作，从而提高图像生成效果的新方法，并显著提升了FID。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language",
        "summary": "We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.",
        "url": "http://arxiv.org/abs/2512.10942v1",
        "published_date": "2025-12-11T18:59:22+00:00",
        "updated_date": "2025-12-11T18:59:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Delong Chen",
            "Mustafa Shukor",
            "Theo Moutakanni",
            "Willy Chung",
            "Jade Yu",
            "Tejaswi Kasarla",
            "Allen Bolourchi",
            "Yann LeCun",
            "Pascale Fung"
        ],
        "tldr": "VL-JEPA, a new vision-language model, predicts continuous embeddings instead of tokens, achieving better performance with fewer parameters and enabling efficient selective decoding and strong results on various video and multimodal tasks.",
        "tldr_zh": "VL-JEPA是一种新的视觉语言模型，它预测连续嵌入而不是token，以更少的参数实现了更好的性能，并支持高效的选择性解码以及在各种视频和多模态任务上的强大结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
        "summary": "Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.",
        "url": "http://arxiv.org/abs/2512.10894v1",
        "published_date": "2025-12-11T18:23:03+00:00",
        "updated_date": "2025-12-11T18:23:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peiying Zhang",
            "Nanxuan Zhao",
            "Matthew Fisher",
            "Yiran Xu",
            "Jing Liao",
            "Difan Liu"
        ],
        "tldr": "DuetSVG is a multimodal model that generates both image and SVG tokens, leveraging visual predictions as guidance to improve the quality of generated SVGs, outperforming existing methods.",
        "tldr_zh": "DuetSVG是一个多模态模型，可以生成图像和SVG tokens。该模型利用视觉预测作为指导，提升SVG的生成质量，并且优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
        "summary": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
        "url": "http://arxiv.org/abs/2512.10959v1",
        "published_date": "2025-12-11T18:59:59+00:00",
        "updated_date": "2025-12-11T18:59:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tjark Behrens",
            "Anton Obukhov",
            "Bingxin Ke",
            "Fabio Tosi",
            "Matteo Poggi",
            "Konrad Schindler"
        ],
        "tldr": "StereoSpace introduces a diffusion-based method for monocular-to-stereo synthesis that avoids explicit depth estimation by directly conditioning on viewpoint in a canonical space, achieving superior performance in perceptual comfort and geometric consistency.",
        "tldr_zh": "StereoSpace提出了一种基于扩散的单眼到立体视觉合成方法，该方法通过在规范空间中直接以视点为条件，避免了显式的深度估计，并在感知舒适度和几何一致性方面实现了卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
        "summary": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.",
        "url": "http://arxiv.org/abs/2512.10957v1",
        "published_date": "2025-12-11T18:59:56+00:00",
        "updated_date": "2025-12-11T18:59:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yukai Shi",
            "Weiyu Li",
            "Zihao Wang",
            "Hongyang Li",
            "Xingyu Chen",
            "Ping Tan",
            "Lei Zhang"
        ],
        "tldr": "SceneMaker is a decoupled 3D scene generation framework that tackles open-set de-occlusion and pose estimation challenges by using dedicated models and a custom dataset to improve generation quality and pose accuracy in diverse scenes.",
        "tldr_zh": "SceneMaker是一个解耦的3D场景生成框架，通过使用专用模型和自定义数据集来解决开放集去遮挡和姿态估计的挑战，从而提高各种场景中的生成质量和姿态准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models",
        "summary": "Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.",
        "url": "http://arxiv.org/abs/2512.10932v1",
        "published_date": "2025-12-11T18:57:05+00:00",
        "updated_date": "2025-12-11T18:57:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shengao Wang",
            "Wenqi Wang",
            "Zecheng Wang",
            "Max Whitton",
            "Michael Wakeham",
            "Arjun Chandra",
            "Joey Huang",
            "Pengyue Zhu",
            "Helen Chen",
            "David Li",
            "Jeffrey Li",
            "Shawn Li",
            "Andrew Zagula",
            "Amy Zhao",
            "Andrew Zhu",
            "Sayaka Nakamura",
            "Yuki Yamamoto",
            "Jerry Jun Yokono",
            "Aaron Mueller",
            "Bryan A. Plummer",
            "Kate Saenko",
            "Venkatesh Saligrama",
            "Boqing Gong"
        ],
        "tldr": "BabyVLM-V2 introduces a developmentally grounded pretraining framework for vision foundation models, using an infant-centric dataset and a cognitive evaluation toolbox (DevCV) benchmark, showing competitive performance even outperforming GPT-4o on some tasks.",
        "tldr_zh": "BabyVLM-V2 介绍了一个基于发展心理学的视觉基础模型预训练框架，它使用以婴儿为中心的数据集和一个认知评估工具箱（DevCV）基准，并显示出有竞争力的性能，在某些任务上甚至优于 GPT-4o。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]