[
    {
        "title": "EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer",
        "summary": "Video generation models have advanced significantly, yet they still struggle to synthesize complex human movements due to the high degrees of freedom in human articulation. This limitation stems from the intrinsic constraints of pixel-only training objectives, which inherently bias models toward appearance fidelity at the expense of learning underlying kinematic principles. To address this, we introduce EchoMotion, a framework designed to model the joint distribution of appearance and human motion, thereby improving the quality of complex human action video generation. EchoMotion extends the DiT (Diffusion Transformer) framework with a dual-branch architecture that jointly processes tokens concatenated from different modalities. Furthermore, we propose MVS-RoPE (Motion-Video Syncronized RoPE), which offers unified 3D positional encoding for both video and motion tokens. By providing a synchronized coordinate system for the dual-modal latent sequence, MVS-RoPE establishes an inductive bias that fosters temporal alignment between the two modalities. We also propose a Motion-Video Two-Stage Training Strategy. This strategy enables the model to perform both the joint generation of complex human action videos and their corresponding motion sequences, as well as versatile cross-modal conditional generation tasks. To facilitate the training of a model with these capabilities, we construct HuMoVe, a large-scale dataset of approximately 80,000 high-quality, human-centric video-motion pairs. Our findings reveal that explicitly representing human motion is complementary to appearance, significantly boosting the coherence and plausibility of human-centric video generation.",
        "url": "http://arxiv.org/abs/2512.18814v1",
        "published_date": "2025-12-21T17:08:14+00:00",
        "updated_date": "2025-12-21T17:08:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxiao Yang",
            "Hualian Sheng",
            "Sijia Cai",
            "Jing Lin",
            "Jiahao Wang",
            "Bing Deng",
            "Junzhe Lu",
            "Haoqian Wang",
            "Jieping Ye"
        ],
        "tldr": "The paper introduces EchoMotion, a dual-modality diffusion transformer framework for generating high-quality human action videos by explicitly modeling human motion alongside appearance, using a novel positional encoding scheme and a large-scale dataset.",
        "tldr_zh": "该论文介绍了EchoMotion，一种双模态扩散Transformer框架，通过显式地对人类运动和外观进行建模，并使用一种新的位置编码方案和一个大规模数据集，生成高质量的人类活动视频。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Revealing Perception and Generation Dynamics in LVLMs: Mitigating Hallucinations via Validated Dominance Correction",
        "summary": "Large Vision-Language Models (LVLMs) have shown remarkable capabilities, yet hallucinations remain a persistent challenge. This work presents a systematic analysis of the internal evolution of visual perception and token generation in LVLMs, revealing two key patterns. First, perception follows a three-stage GATE process: early layers perform a Global scan, intermediate layers Approach and Tighten on core content, and later layers Explore supplementary regions. Second, generation exhibits an SAD (Subdominant Accumulation to Dominant) pattern, where hallucinated tokens arise from the repeated accumulation of subdominant tokens lacking support from attention (visual perception) or feed-forward network (internal knowledge). Guided by these findings, we devise the VDC (Validated Dominance Correction) strategy, which detects unsupported tokens and replaces them with validated dominant ones to improve output reliability. Extensive experiments across multiple models and benchmarks confirm that VDC substantially mitigates hallucinations.",
        "url": "http://arxiv.org/abs/2512.18813v1",
        "published_date": "2025-12-21T17:05:42+00:00",
        "updated_date": "2025-12-21T17:05:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guangtao Lyu",
            "Xinyi Cheng",
            "Chenghao Xu",
            "Qi Liu",
            "Muli Yang",
            "Fen Fang",
            "Huilin Chen",
            "Jiexi Yan",
            "Xu Yang",
            "Cheng Deng"
        ],
        "tldr": "This paper analyzes the internal dynamics of LVLMs, identifies patterns leading to hallucinations, and proposes a method (VDC) to mitigate them by correcting unsupported tokens.",
        "tldr_zh": "本文分析了大型视觉语言模型（LVLMs）的内部动态，发现了导致幻觉的模式，并提出了一种通过校正不支持的tokens来减轻幻觉的方法（VDC）。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "In-Context Audio Control of Video Diffusion Transformers",
        "summary": "Recent advancements in video generation have seen a shift towards unified, transformer-based foundation models that can handle multiple conditional inputs in-context. However, these models have primarily focused on modalities like text, images, and depth maps, while strictly time-synchronous signals like audio have been underexplored. This paper introduces In-Context Audio Control of video diffusion transformers (ICAC), a framework that investigates the integration of audio signals for speech-driven video generation within a unified full-attention architecture, akin to FullDiT. We systematically explore three distinct mechanisms for injecting audio conditions: standard cross-attention, 2D self-attention, and unified 3D self-attention. Our findings reveal that while 3D attention offers the highest potential for capturing spatio-temporal audio-visual correlations, it presents significant training challenges. To overcome this, we propose a Masked 3D Attention mechanism that constrains the attention pattern to enforce temporal alignment, enabling stable training and superior performance. Our experiments demonstrate that this approach achieves strong lip synchronization and video quality, conditioned on an audio stream and reference images.",
        "url": "http://arxiv.org/abs/2512.18772v1",
        "published_date": "2025-12-21T15:22:28+00:00",
        "updated_date": "2025-12-21T15:22:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenze Liu",
            "Weicai Ye",
            "Minghong Cai",
            "Quande Liu",
            "Xintao Wang",
            "Xiangyu Yue"
        ],
        "tldr": "This paper introduces a framework, ICAC, for integrating audio signals into video diffusion transformers for speech-driven video generation, using masked 3D attention to achieve stable training and strong lip synchronization.",
        "tldr_zh": "本文介绍了一个框架ICAC，用于将音频信号集成到视频扩散 Transformer 中，以实现语音驱动的视频生成，并使用掩码 3D 注意力来实现稳定的训练和强大的唇形同步。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MaskFocus: Focusing Policy Optimization on Critical Steps for Masked Image Generation",
        "summary": "Reinforcement learning (RL) has demonstrated significant potential for post-training language models and autoregressive visual generative models, but adapting RL to masked generative models remains challenging. The core factor is that policy optimization requires accounting for the probability likelihood of each step due to its multi-step and iterative refinement process. This reliance on entire sampling trajectories introduces high computational cost, whereas natively optimizing random steps often yields suboptimal results. In this paper, we present MaskFocus, a novel RL framework that achieves effective policy optimization for masked generative models by focusing on critical steps. Specifically, we determine the step-level information gain by measuring the similarity between the intermediate images at each sampling step and the final generated image. Crucially, we leverage this to identify the most critical and valuable steps and execute focused policy optimization on them. Furthermore, we design a dynamic routing sampling mechanism based on entropy to encourage the model to explore more valuable masking strategies for samples with low entropy. Extensive experiments on multiple Text-to-Image benchmarks validate the effectiveness of our method.",
        "url": "http://arxiv.org/abs/2512.18766v1",
        "published_date": "2025-12-21T15:08:31+00:00",
        "updated_date": "2025-12-21T15:08:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guohui Zhang",
            "Hu Yu",
            "Xiaoxiao Ma",
            "Yaning Pan",
            "Hang Xu",
            "Feng Zhao"
        ],
        "tldr": "The paper introduces MaskFocus, a reinforcement learning framework that optimizes masked image generation by focusing policy optimization on critical steps identified through step-level information gain, demonstrating effectiveness on Text-to-Image benchmarks.",
        "tldr_zh": "本文介绍 MaskFocus，一个强化学习框架，通过关注关键步骤并将策略优化集中在这些步骤上，来优化掩码图像生成。该方法通过测量步骤级信息增益来识别关键步骤，并在文本到图像的基准测试中展示了有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation",
        "summary": "Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose \\textbf{Memorize-and-Generate (MAG)}, a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce \\textbf{MAG-Bench} to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks.",
        "url": "http://arxiv.org/abs/2512.18741v1",
        "published_date": "2025-12-21T14:02:53+00:00",
        "updated_date": "2025-12-21T14:02:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianrui Zhu",
            "Shiyi Zhang",
            "Zhirui Sun",
            "Jingqi Tian",
            "Yansong Tang"
        ],
        "tldr": "The paper introduces Memorize-and-Generate (MAG), a framework for long video generation that addresses the trade-off between memory cost and historical consistency by decoupling memory compression and frame generation, achieving better consistency performance.",
        "tldr_zh": "该论文提出了一个名为 Memorize-and-Generate (MAG) 的长视频生成框架，通过解耦记忆压缩和帧生成，解决内存成本和历史一致性之间的权衡，从而实现更好的一致性表现。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AsyncDiff: Asynchronous Timestep Conditioning for Enhanced Text-to-Image Diffusion Inference",
        "summary": "Text-to-image diffusion inference typically follows synchronized schedules, where the numerical integrator advances the latent state to the same timestep at which the denoiser is conditioned. We propose an asynchronous inference mechanism that decouples these two, allowing the denoiser to be conditioned at a different, learned timestep while keeping image update schedule unchanged. A lightweight timestep prediction module (TPM), trained with Group Relative Policy Optimization (GRPO), selects a more feasible conditioning timestep based on the current state, effectively choosing a desired noise level to control image detail and textural richness. At deployment, a scaling hyper-parameter can be used to interpolate between the original and de-synchronized timesteps, enabling conservative or aggressive adjustments. To keep the study computationally affordable, we cap the inference at 15 steps for SD3.5 and 10 steps for Flux. Evaluated on Stable Diffusion 3.5 Medium and Flux.1-dev across MS-COCO 2014 and T2I-CompBench datasets, our method optimizes a composite reward that averages Image Reward, HPSv2, CLIP Score and Pick Score, and shows consistent improvement.",
        "url": "http://arxiv.org/abs/2512.18675v1",
        "published_date": "2025-12-21T10:29:57+00:00",
        "updated_date": "2025-12-21T10:29:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Longhuan Xu",
            "Feng Yin",
            "Cunjian Chen"
        ],
        "tldr": "The paper introduces AsyncDiff, an asynchronous timestep conditioning method that improves text-to-image diffusion inference by decoupling the numerical integration timestep from the denoiser conditioning timestep, which allows for more flexible control over image detail and quality, tested on SD3.5 and Flux with multiple metrics showing improvement.",
        "tldr_zh": "该论文介绍了AsyncDiff，一种异步时间步调节方法，通过解耦数值积分时间步和去噪器调节时间步来改进文本到图像的扩散推理。这种方法允许更灵活地控制图像细节和质量，并在SD3.5和Flux上进行了测试，多项指标表明有所改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Uni-Neur2Img: Unified Neural Signal-Guided Image Generation, Editing, and Stylization via Diffusion Transformers",
        "summary": "Generating or editing images directly from Neural signals has immense potential at the intersection of neuroscience, vision, and Brain-computer interaction. In this paper, We present Uni-Neur2Img, a unified framework for neural signal-driven image generation and editing. The framework introduces a parameter-efficient LoRA-based neural signal injection module that independently processes each conditioning signal as a pluggable component, facilitating flexible multi-modal conditioning without altering base model parameters. Additionally, we employ a causal attention mechanism accommodate the long-sequence modeling demands of conditional generation tasks. Existing neural-driven generation research predominantly focuses on textual modalities as conditions or intermediate representations, resulting in limited exploration of visual modalities as direct conditioning signals. To bridge this research gap, we introduce the EEG-Style dataset. We conduct comprehensive evaluations across public benchmarks and self-collected neural signal datasets: (1) EEG-driven image generation on the public CVPR40 dataset; (2) neural signal-guided image editing on the public Loongx dataset for semantic-aware local modifications; and (3) EEG-driven style transfer on our self-collected EEG-Style dataset. Extensive experimental results demonstrate significant improvements in generation fidelity, editing consistency, and style transfer quality while maintaining low computational overhead and strong scalability to additional modalities. Thus, Uni-Neur2Img offers a unified, efficient, and extensible solution for bridging neural signals and visual content generation.",
        "url": "http://arxiv.org/abs/2512.18635v1",
        "published_date": "2025-12-21T08:12:38+00:00",
        "updated_date": "2025-12-21T08:12:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiyue Bai",
            "Ronghao Yu",
            "Jia Xiu",
            "Pengfei Zhou",
            "Jie Xia",
            "Peng Ji"
        ],
        "tldr": "Uni-Neur2Img is a unified framework leveraging diffusion transformers for generating, editing, and stylizing images directly from neural signals, introducing a novel neural signal injection module and EEG-Style dataset.",
        "tldr_zh": "Uni-Neur2Img是一个统一框架，利用扩散变换器直接从神经信号生成、编辑和风格化图像，引入了一种新的神经信号注入模块和EEG-Style数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PTTA: A Pure Text-to-Animation Framework for High-Quality Creation",
        "summary": "Traditional animation production involves complex pipelines and significant manual labor cost. While recent video generation models such as Sora, Kling, and CogVideoX achieve impressive results on natural video synthesis, they exhibit notable limitations when applied to animation generation. Recent efforts, such as AniSora, demonstrate promising performance by fine-tuning image-to-video models for animation styles, yet analogous exploration in the text-to-video setting remains limited.\n  In this work, we present PTTA, a pure text-to-animation framework for high-quality animation creation. We first construct a small-scale but high-quality paired dataset of animation videos and textual descriptions. Building upon the pretrained text-to-video model HunyuanVideo, we perform fine-tuning to adapt it to animation-style generation. Extensive visual evaluations across multiple dimensions show that the proposed approach consistently outperforms comparable baselines in animation video synthesis.",
        "url": "http://arxiv.org/abs/2512.18614v1",
        "published_date": "2025-12-21T06:17:28+00:00",
        "updated_date": "2025-12-21T06:17:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ruiqi Chen",
            "Kaitong Cai",
            "Yijia Fan",
            "Keze Wang"
        ],
        "tldr": "The paper introduces PTTA, a pure text-to-animation framework fine-tuned from HunyuanVideo using a newly created high-quality animation dataset, demonstrating superior performance compared to baselines.",
        "tldr_zh": "该论文介绍了一个名为PTTA的纯文本到动画框架，该框架通过使用新创建的高质量动画数据集对HunyuanVideo进行微调，并展示了优于基线的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]