[
    {
        "title": "IMAGEdit: Let Any Subject Transform",
        "summary": "In this paper, we present IMAGEdit, a training-free framework for any number\nof video subject editing that manipulates the appearances of multiple\ndesignated subjects while preserving non-target regions, without finetuning or\nretraining. We achieve this by providing robust multimodal conditioning and\nprecise mask sequences through a prompt-guided multimodal alignment module and\na prior-based mask retargeting module. We first leverage large models'\nunderstanding and generation capabilities to produce multimodal information and\nmask motion sequences for multiple subjects across various types. Then, the\nobtained prior mask sequences are fed into a pretrained mask-driven video\ngeneration model to synthesize the edited video. With strong generalization\ncapability, IMAGEdit remedies insufficient prompt-side multimodal conditioning\nand overcomes mask boundary entanglement in videos with any number of subjects,\nthereby significantly expanding the applicability of video editing. More\nimportantly, IMAGEdit is compatible with any mask-driven video generation\nmodel, significantly improving overall performance. Extensive experiments on\nour newly constructed multi-subject benchmark MSVBench verify that IMAGEdit\nconsistently surpasses state-of-the-art methods. Code, models, and datasets are\npublicly available at https://github.com/XWH-A/IMAGEdit.",
        "url": "http://arxiv.org/abs/2510.01186v1",
        "published_date": "2025-10-01T17:59:56+00:00",
        "updated_date": "2025-10-01T17:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fei Shen",
            "Weihao Xu",
            "Rui Yan",
            "Dong Zhang",
            "Xiangbo Shu",
            "Jinhui Tang"
        ],
        "tldr": "IMAGEdit is a training-free framework for multi-subject video editing that allows for appearance manipulation using multimodal conditioning and prior-based mask retargeting, achieving state-of-the-art results on a new benchmark.",
        "tldr_zh": "IMAGEdit是一个无需训练的多主体视频编辑框架，通过多模态条件约束和基于先验的掩码重定向实现外观操作，并在新的基准测试中取得了最先进的结果。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration",
        "summary": "Diffusion Transformer has shown remarkable abilities in generating\nhigh-fidelity videos, delivering visually coherent frames and rich details over\nextended durations. However, existing video generation models still fall short\nin subject-consistent video generation due to an inherent difficulty in parsing\nprompts that specify complex spatial relationships, temporal logic, and\ninteractions among multiple subjects. To address this issue, we propose\nBindWeave, a unified framework that handles a broad range of subject-to-video\nscenarios from single-subject cases to complex multi-subject scenes with\nheterogeneous entities. To bind complex prompt semantics to concrete visual\nsubjects, we introduce an MLLM-DiT framework in which a pretrained multimodal\nlarge language model performs deep cross-modal reasoning to ground entities and\ndisentangle roles, attributes, and interactions, yielding subject-aware hidden\nstates that condition the diffusion transformer for high-fidelity\nsubject-consistent video generation. Experiments on the OpenS2V benchmark\ndemonstrate that our method achieves superior performance across subject\nconsistency, naturalness, and text relevance in generated videos, outperforming\nexisting open-source and commercial models.",
        "url": "http://arxiv.org/abs/2510.00438v1",
        "published_date": "2025-10-01T02:41:11+00:00",
        "updated_date": "2025-10-01T02:41:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaoyang Li",
            "Dongjun Qian",
            "Kai Su",
            "Qishuai Diao",
            "Xiangyang Xia",
            "Chang Liu",
            "Wenfei Yang",
            "Tianzhu Zhang",
            "Zehuan Yuan"
        ],
        "tldr": "The paper introduces BindWeave, a framework using MLLMs and diffusion transformers to generate subject-consistent videos from complex prompts, outperforming existing methods.",
        "tldr_zh": "该论文提出了BindWeave，一个利用MLLM和扩散Transformer框架，从复杂提示词生成主体一致性视频的方法，性能优于现有方法。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation",
        "summary": "Audio-video generation has often relied on complex multi-stage architectures\nor sequential synthesis of sound and visuals. We introduce Ovi, a unified\nparadigm for audio-video generation that models the two modalities as a single\ngenerative process. By using blockwise cross-modal fusion of twin-DiT modules,\nOvi achieves natural synchronization and removes the need for separate\npipelines or post hoc alignment. To facilitate fine-grained multimodal fusion\nmodeling, we initialize an audio tower with an architecture identical to that\nof a strong pretrained video model. Trained from scratch on hundreds of\nthousands of hours of raw audio, the audio tower learns to generate realistic\nsound effects, as well as speech that conveys rich speaker identity and\nemotion. Fusion is obtained by jointly training the identical video and audio\ntowers via blockwise exchange of timing (via scaled-RoPE embeddings) and\nsemantics (through bidirectional cross-attention) on a vast video corpus. Our\nmodel enables cinematic storytelling with natural speech and accurate,\ncontext-matched sound effects, producing movie-grade video clips. All the\ndemos, code and model weights are published at https://aaxwaz.github.io/Ovi",
        "url": "http://arxiv.org/abs/2510.01284v1",
        "published_date": "2025-09-30T21:03:50+00:00",
        "updated_date": "2025-09-30T21:03:50+00:00",
        "categories": [
            "cs.MM",
            "cs.CV",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Chetwin Low",
            "Weimin Wang",
            "Calder Katyal"
        ],
        "tldr": "Ovi is a unified audio-video generation model employing twin-DiT modules with blockwise cross-modal fusion, achieving synchronized and realistic audio-video generation without separate pipelines. It utilizes a pre-trained video model architecture for its audio tower and is trained end-to-end on a large dataset.",
        "tldr_zh": "Ovi 是一种统一的音视频生成模型，它采用双 DiT 模块和分块跨模态融合，无需单独的流水线即可实现同步且逼真的音视频生成。它利用预训练的视频模型架构作为音频塔，并在大型数据集上进行端到端训练。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory",
        "summary": "Humans possess a remarkable ability to mentally explore and replay 3D\nenvironments they have previously experienced. Inspired by this mental process,\nwe present EvoWorld: a world model that bridges panoramic video generation with\nevolving 3D memory to enable spatially consistent long-horizon exploration.\nGiven a single panoramic image as input, EvoWorld first generates future video\nframes by leveraging a video generator with fine-grained view control, then\nevolves the scene's 3D reconstruction using a feedforward plug-and-play\ntransformer, and finally synthesizes futures by conditioning on geometric\nreprojections from this evolving explicit 3D memory. Unlike prior\nstate-of-the-arts that synthesize videos only, our key insight lies in\nexploiting this evolving 3D reconstruction as explicit spatial guidance for the\nvideo generation process, projecting the reconstructed geometry onto target\nviewpoints to provide rich spatial cues that significantly enhance both visual\nrealism and geometric consistency. To evaluate long-range exploration\ncapabilities, we introduce the first comprehensive benchmark spanning synthetic\noutdoor environments, Habitat indoor scenes, and challenging real-world\nscenarios, with particular emphasis on loop-closure detection and spatial\ncoherence over extended trajectories. Extensive experiments demonstrate that\nour evolving 3D memory substantially improves visual fidelity and maintains\nspatial scene coherence compared to existing approaches, representing a\nsignificant advance toward long-horizon spatially consistent world modeling.",
        "url": "http://arxiv.org/abs/2510.01183v1",
        "published_date": "2025-10-01T17:59:38+00:00",
        "updated_date": "2025-10-01T17:59:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Wang",
            "Luoxin Ye",
            "TaiMing Lu",
            "Junfei Xiao",
            "Jiahan Zhang",
            "Yuxiang Guo",
            "Xijun Liu",
            "Rama Chellappa",
            "Cheng Peng",
            "Alan Yuille",
            "Jieneng Chen"
        ],
        "tldr": "EvoWorld introduces a novel world model using evolving 3D memory to enhance panoramic video generation, leading to spatially consistent long-horizon exploration, and demonstrates improved visual fidelity and spatial coherence across diverse environments.",
        "tldr_zh": "EvoWorld 引入了一种新颖的世界模型，该模型使用不断演变的 3D 记忆来增强全景视频生成，从而实现空间一致的长程探索，并展示了在各种环境中改进的视觉保真度和空间连贯性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
        "summary": "While recent generative models advance pixel-space video synthesis, they\nremain limited in producing professional educational videos, which demand\ndisciplinary knowledge, precise visual structures, and coherent transitions,\nlimiting their applicability in educational scenarios. Intuitively, such\nrequirements are better addressed through the manipulation of a renderable\nenvironment, which can be explicitly controlled via logical commands (e.g.,\ncode). In this work, we propose Code2Video, a code-centric agent framework for\ngenerating educational videos via executable Python code. The framework\ncomprises three collaborative agents: (i) Planner, which structures lecture\ncontent into temporally coherent flows and prepares corresponding visual\nassets; (ii) Coder, which converts structured instructions into executable\nPython codes while incorporating scope-guided auto-fix to enhance efficiency;\nand (iii) Critic, which leverages vision-language models (VLM) with visual\nanchor prompts to refine spatial layout and ensure clarity. To support\nsystematic evaluation, we build MMMC, a benchmark of professionally produced,\ndiscipline-specific educational videos. We evaluate MMMC across diverse\ndimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and\nparticularly, TeachQuiz, a novel end-to-end metric that quantifies how well a\nVLM, after unlearning, can recover knowledge by watching the generated videos.\nOur results demonstrate the potential of Code2Video as a scalable,\ninterpretable, and controllable approach, achieving 40% improvement over direct\ncode generation and producing videos comparable to human-crafted tutorials. The\ncode and datasets are available at https://github.com/showlab/Code2Video.",
        "url": "http://arxiv.org/abs/2510.01174v1",
        "published_date": "2025-10-01T17:56:48+00:00",
        "updated_date": "2025-10-01T17:56:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.HC",
            "cs.MM"
        ],
        "authors": [
            "Yanzhe Chen",
            "Kevin Qinghong Lin",
            "Mike Zheng Shou"
        ],
        "tldr": "The paper introduces Code2Video, a code-centric framework for generating educational videos using Python code, consisting of a Planner, Coder, and Critic agent, and evaluated on a newly created MMMC benchmark, achieving significant improvements over direct code generation.",
        "tldr_zh": "该论文介绍了Code2Video，一个以代码为中心的框架，用于使用Python代码生成教育视频，它由Planner、Coder和Critic三个代理组成，并在新创建的MMMC基准上进行了评估，与直接代码生成相比取得了显著的改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Authentic Discrete Diffusion Model",
        "summary": "We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally\nredefines prior pseudo-discrete approaches by preserving core diffusion\ncharacteristics directly in the one-hot space through a suite of coordinated\nmechanisms. Unlike conventional \"pseudo\" discrete diffusion (PDD) methods, ADD\nreformulates the diffusion input by directly using float-encoded one-hot class\ndata, without relying on diffusing in the continuous latent spaces or masking\npolicies. At its core, a timestep-conditioned cross-entropy loss is introduced\nbetween the diffusion model's outputs and the original one-hot labels. This\nsynergistic design establishes a bridge between discriminative and generative\nlearning. Our experiments demonstrate that ADD not only achieves superior\nperformance on classification tasks compared to the baseline, but also exhibits\nexcellent text generation capabilities on Image captioning. Extensive ablations\nvalidate the measurable gains of each component.",
        "url": "http://arxiv.org/abs/2510.01047v1",
        "published_date": "2025-10-01T15:51:10+00:00",
        "updated_date": "2025-10-01T15:51:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Xiao Li",
            "Jiaqi Zhang",
            "Shuxiang Zhang",
            "Tianshui Chen",
            "Liang Lin",
            "Guangrun Wang"
        ],
        "tldr": "The paper introduces Authentic Discrete Diffusion (ADD), a novel diffusion framework operating directly in the one-hot space, achieving improvements in classification and image captioning compared to existing methods.",
        "tldr_zh": "该论文介绍了真实的离散扩散（ADD），一种新颖的直接在one-hot空间中运行的扩散框架，与现有方法相比，在分类和图像描述方面取得了改进。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SoftCFG: Uncertainty-guided Stable Guidance for Visual Autoregressive Model",
        "summary": "Autoregressive (AR) models have emerged as powerful tools for image\ngeneration by modeling images as sequences of discrete tokens. While\nClassifier-Free Guidance (CFG) has been adopted to improve conditional\ngeneration, its application in AR models faces two key issues: guidance\ndiminishing, where the conditional-unconditional gap quickly vanishes as\ndecoding progresses, and over-guidance, where strong conditions distort visual\ncoherence. To address these challenges, we propose SoftCFG, an\nuncertainty-guided inference method that distributes adaptive perturbations\nacross all tokens in the sequence. The key idea behind SoftCFG is to let each\ngenerated token contribute certainty-weighted guidance, ensuring that the\nsignal persists across steps while resolving conflicts between text guidance\nand visual context. To further stabilize long-sequence generation, we introduce\nStep Normalization, which bounds cumulative perturbations of SoftCFG. Our\nmethod is training-free, model-agnostic, and seamlessly integrates with\nexisting AR pipelines. Experiments show that SoftCFG significantly improves\nimage quality over standard CFG and achieves state-of-the-art FID on ImageNet\n256*256 among autoregressive models.",
        "url": "http://arxiv.org/abs/2510.00996v2",
        "published_date": "2025-10-01T15:04:00+00:00",
        "updated_date": "2025-10-02T09:32:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dongli Xu",
            "Aleksei Tiulpin",
            "Matthew B. Blaschko"
        ],
        "tldr": "The paper introduces SoftCFG, a training-free and model-agnostic uncertainty-guided inference method for autoregressive image generation that addresses the issues of guidance diminishing and over-guidance in Classifier-Free Guidance (CFG). It achieves state-of-the-art FID on ImageNet 256*256 among autoregressive models.",
        "tldr_zh": "该论文介绍了一种名为SoftCFG的、无需训练且模型无关的、基于不确定性引导的推理方法，用于自回归图像生成，旨在解决分类器无关引导（CFG）中出现的引导衰减和过度引导问题。该方法在ImageNet 256*256上实现了自回归模型的最先进的FID。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Self-Refinement for Autoregressive Models",
        "summary": "Autoregressive models excel in sequential modeling and have proven to be\neffective for vision-language data. However, the spatial nature of visual\nsignals conflicts with the sequential dependencies of next-token prediction,\nleading to suboptimal results. This work proposes a plug-and-play refinement\nmodule to enhance the complex spatial correspondence modeling within the\ngenerated visual sequence. This module operates as a post-pretraining step to\njointly refine all generated tokens of autoregressive model, enhancing\nvision-language modeling under a shared sequential prediction framework. By\nleveraging global context and relationship across the tokens, our method\nmitigates the error accumulation issue within the sequential generation.\nExperiments demonstrate that the proposed method improves the generation\nquality, enhancing the model's ability to produce semantically consistent\nresults.",
        "url": "http://arxiv.org/abs/2510.00993v1",
        "published_date": "2025-10-01T15:03:32+00:00",
        "updated_date": "2025-10-01T15:03:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiamian Wang",
            "Ziqi Zhou",
            "Chaithanya Kumar Mummadi",
            "Sohail Dianat",
            "Majid Rabbani",
            "Raghuveer Rao",
            "Chen Qiu",
            "Zhiqiang Tao"
        ],
        "tldr": "This paper introduces a plug-and-play refinement module for autoregressive models to improve the spatial consistency of generated visual sequences in vision-language tasks, addressing the conflict between spatial visual data and sequential prediction.",
        "tldr_zh": "该论文介绍了一种即插即用的细化模块，用于自回归模型，以提高视觉语言任务中生成视觉序列的空间一致性，解决了空间视觉数据和顺序预测之间的冲突。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation",
        "summary": "Modern Text-to-Image (T2I) generation increasingly relies on token-centric\narchitectures that are trained with self-supervision, yet effectively fusing\ntext with visual tokens remains a challenge. We propose \\textbf{JEPA-T}, a\nunified multimodal framework that encodes images and captions into discrete\nvisual and textual tokens, processed by a joint-embedding predictive\nTransformer. To enhance fusion, we incorporate cross-attention after the\nfeature predictor for conditional denoising while maintaining a task-agnostic\nbackbone. Additionally, raw texts embeddings are injected prior to the flow\nmatching loss to improve alignment during training. During inference, the same\nnetwork performs both class-conditional and free-text image generation by\niteratively denoising visual tokens conditioned on text. Evaluations on\nImageNet-1K demonstrate that JEPA-T achieves strong data efficiency,\nopen-vocabulary generalization, and consistently outperforms non-fusion and\nlate-fusion baselines. Our approach shows that late architectural fusion\ncombined with objective-level alignment offers an effective balance between\nconditioning strength and backbone generality in token-based T2I.The code is\nnow available: https://github.com/justin-herry/JEPA-T.git",
        "url": "http://arxiv.org/abs/2510.00974v1",
        "published_date": "2025-10-01T14:51:10+00:00",
        "updated_date": "2025-10-01T14:51:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siheng Wan",
            "Zhengtao Yao",
            "Zhengdao Li",
            "Junhao Dong",
            "Yanshu Li",
            "Yikai Li",
            "Linshan Li",
            "Haoyan Xu",
            "Yijiang Li",
            "Zhikang Dong",
            "Huacan Wang",
            "Jifeng Shen"
        ],
        "tldr": "JEPA-T is a novel text-to-image generation framework that uses a joint-embedding predictive Transformer with cross-attention and objective-level alignment to achieve strong data efficiency and generalization, outperforming existing fusion methods.",
        "tldr_zh": "JEPA-T是一种新型的文本到图像生成框架，它使用联合嵌入预测Transformer，结合交叉注意力和目标层面对齐，实现了强大的数据效率和泛化能力，超越了现有的融合方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MorphGen: Controllable and Morphologically Plausible Generative Cell-Imaging",
        "summary": "Simulating in silico cellular responses to interventions is a promising\ndirection to accelerate high-content image-based assays, critical for advancing\ndrug discovery and gene editing. To support this, we introduce MorphGen, a\nstate-of-the-art diffusion-based generative model for fluorescent microscopy\nthat enables controllable generation across multiple cell types and\nperturbations. To capture biologically meaningful patterns consistent with\nknown cellular morphologies, MorphGen is trained with an alignment loss to\nmatch its representations to the phenotypic embeddings of OpenPhenom, a\nstate-of-the-art biological foundation model. Unlike prior approaches that\ncompress multichannel stains into RGB images -- thus sacrificing\norganelle-specific detail -- MorphGen generates the complete set of fluorescent\nchannels jointly, preserving per-organelle structures and enabling a\nfine-grained morphological analysis that is essential for biological\ninterpretation. We demonstrate biological consistency with real images via\nCellProfiler features, and MorphGen attains an FID score over $35\\%$ lower than\nthe prior state-of-the-art MorphoDiff, which only generates RGB images for a\nsingle cell type. Code is available at https://github.com/czi-ai/MorphGen.",
        "url": "http://arxiv.org/abs/2510.01298v1",
        "published_date": "2025-10-01T13:34:29+00:00",
        "updated_date": "2025-10-01T13:34:29+00:00",
        "categories": [
            "q-bio.QM",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Berker Demirel",
            "Marco Fumero",
            "Theofanis Karaletsos",
            "Francesco Locatello"
        ],
        "tldr": "MorphGen is a diffusion-based generative model for fluorescent microscopy that generates multichannel cell images with morphological plausibility, outperforming prior work by aligning representations with a biological foundation model.",
        "tldr_zh": "MorphGen是一个基于扩散的荧光显微镜细胞图像生成模型，它通过将表示与生物学基础模型对齐，生成具有形态学合理性的多通道细胞图像，性能优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Can World Models Benefit VLMs for World Dynamics?",
        "summary": "Trained on internet-scale video data, generative world models are\nincreasingly recognized as powerful world simulators that can generate\nconsistent and plausible dynamics over structure, motion, and physics. This\nraises a natural question: with the advent of strong video foundational models,\nmight they supplant conventional vision encoder paradigms for general-purpose\nmultimodal understanding? While recent studies have begun to explore the\npotential of world models on common vision tasks, these explorations typically\nlack a systematic investigation of generic, multimodal tasks. In this work, we\nstrive to investigate the capabilities when world model priors are transferred\ninto Vision-Language Models: we re-purpose a video diffusion model as a\ngenerative encoder to perform a single denoising step and treat the resulting\nlatents as a set of visual embedding. We empirically investigate this class of\nmodels, which we refer to as World-Language Models (WorldLMs), and we find that\ngenerative encoders can capture latents useful for downstream understanding\nthat show distinctions from conventional encoders. Naming our best-performing\nvariant Dynamic Vision Aligner (DyVA), we further discover that this method\nsignificantly enhances spatial reasoning abilities and enables single-image\nmodels to perform multi-frame reasoning. Through the curation of a suite of\nvisual reasoning tasks, we find DyVA to surpass both open-source and\nproprietary baselines, achieving state-of-the-art or comparable performance. We\nattribute these gains to WorldLM's inherited motion-consistency internalization\nfrom video pre-training. Finally, we systematically explore extensive model\ndesigns to highlight promising directions for future work. We hope our study\ncan pave the way for a new family of VLMs that leverage priors from world\nmodels and are on a promising path towards generalist vision learners.",
        "url": "http://arxiv.org/abs/2510.00855v1",
        "published_date": "2025-10-01T13:07:05+00:00",
        "updated_date": "2025-10-01T13:07:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Kevin Zhang",
            "Kuangzhi Ge",
            "Xiaowei Chi",
            "Renrui Zhang",
            "Shaojun Shi",
            "Zhen Dong",
            "Sirui Han",
            "Shanghang Zhang"
        ],
        "tldr": "The paper investigates using world models as generative encoders within Vision-Language Models (VLMs), finding improved spatial and multi-frame reasoning capabilities with their proposed Dynamic Vision Aligner (DyVA) model.",
        "tldr_zh": "本文研究了将世界模型用作视觉语言模型（VLM）中的生成式编码器，发现其提出的动态视觉对齐器（DyVA）模型在空间和多帧推理能力方面有所提高。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "From Seeing to Predicting: A Vision-Language Framework for Trajectory Forecasting and Controlled Video Generation",
        "summary": "Current video generation models produce physically inconsistent motion that\nviolates real-world dynamics. We propose TrajVLM-Gen, a two-stage framework for\nphysics-aware image-to-video generation. First, we employ a Vision Language\nModel to predict coarse-grained motion trajectories that maintain consistency\nwith real-world physics. Second, these trajectories guide video generation\nthrough attention-based mechanisms for fine-grained motion refinement. We build\na trajectory prediction dataset based on video tracking data with realistic\nmotion patterns. Experiments on UCF-101 and MSR-VTT demonstrate that\nTrajVLM-Gen outperforms existing methods, achieving competitive FVD scores of\n545 on UCF-101 and 539 on MSR-VTT.",
        "url": "http://arxiv.org/abs/2510.00806v1",
        "published_date": "2025-10-01T12:11:36+00:00",
        "updated_date": "2025-10-01T12:11:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fan Yang",
            "Zhiyang Chen",
            "Yousong Zhu",
            "Xin Li",
            "Jinqiao Wang"
        ],
        "tldr": "This paper introduces TrajVLM-Gen, a two-stage framework for physics-aware video generation using a Vision Language Model to predict motion trajectories and guide video generation, demonstrating improved performance on UCF-101 and MSR-VTT.",
        "tldr_zh": "本文介绍了 TrajVLM-Gen，一种用于物理感知视频生成的两阶段框架，该框架使用视觉语言模型来预测运动轨迹并指导视频生成，并在 UCF-101 和 MSR-VTT 上表现出改进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models",
        "summary": "The foundational premise of generative AI for images is the assumption that\nimages are inherently low-dimensional objects embedded within a\nhigh-dimensional space. Additionally, it is often implicitly assumed that\nthematic image datasets form smooth or piecewise smooth manifolds. Common\napproaches overlook the geometric structure and focus solely on probabilistic\nmethods, approximating the probability distribution through universal\napproximation techniques such as the kernel method. In some generative models,\nthe low dimensional nature of the data manifest itself by the introduction of a\nlower dimensional latent space. Yet, the probability distribution in the latent\nor the manifold coordinate space is considered uninteresting and is predefined\nor considered uniform. This study unifies the geometric and probabilistic\nperspectives by providing a geometric framework and a kernel-based\nprobabilistic method simultaneously. The resulting framework demystifies\ndiffusion models by interpreting them as a projection mechanism onto the\nmanifold of ``good images''. This interpretation leads to the construction of a\nnew deterministic model, the Manifold-Probabilistic Projection Model (MPPM),\nwhich operates in both the representation (pixel) space and the latent space.\nWe demonstrate that the Latent MPPM (LMPPM) outperforms the Latent Diffusion\nModel (LDM) across various datasets, achieving superior results in terms of\nimage restoration and generation.",
        "url": "http://arxiv.org/abs/2510.00666v1",
        "published_date": "2025-10-01T08:50:30+00:00",
        "updated_date": "2025-10-01T08:50:30+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Leah Bar",
            "Liron Mor Yosef",
            "Shai Zucker",
            "Neta Shoham",
            "Inbar Seroussi",
            "Nir Sochen"
        ],
        "tldr": "This paper proposes a geometric framework unifying geometric and probabilistic perspectives in generative AI, introducing the Manifold-Probabilistic Projection Model (MPPM) which outperforms Latent Diffusion Models in image restoration and generation.",
        "tldr_zh": "本文提出了一个几何框架，统一了生成式人工智能中的几何和概率视角，引入了流形概率投影模型（MPPM），该模型在图像恢复和生成方面优于潜在扩散模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents",
        "summary": "With diffusion and flow matching models achieving state-of-the-art generating\nperformance, the interest of the community now turned to reducing the inference\ntime without sacrificing sample quality. Consistency Models (CMs), which are\ntrained to be consistent on diffusion or probability flow ordinary differential\nequation (PF-ODE) trajectories, enable one or two-step flow or diffusion\nsampling. However, CMs typically require prolonged training with large batch\nsizes to obtain competitive sample quality. In this paper, we examine the\ntraining dynamics of CMs near convergence and discover that CM tangents -- CM\noutput update directions -- are quite oscillatory, in the sense that they move\nparallel to the data manifold, not towards the manifold. To mitigate\noscillatory tangents, we propose a new loss function, called the manifold\nfeature distance (MFD), which provides manifold-aligned tangents that point\ntoward the data manifold. Consequently, our method -- dubbed Align Your Tangent\n(AYT) -- can accelerate CM training by orders of magnitude and even out-perform\nthe learned perceptual image patch similarity metric (LPIPS). Furthermore, we\nfind that our loss enables training with extremely small batch sizes without\ncompromising sample quality. Code: https://github.com/1202kbs/AYT",
        "url": "http://arxiv.org/abs/2510.00658v1",
        "published_date": "2025-10-01T08:35:18+00:00",
        "updated_date": "2025-10-01T08:35:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Beomsu Kim",
            "Byunghee Cha",
            "Jong Chul Ye"
        ],
        "tldr": "The paper introduces a new loss function, MFD, for Consistency Models (CMs) that aligns CM tangents to the data manifold, leading to faster training and improved sample quality, even with small batch sizes.",
        "tldr_zh": "该论文提出了一种新的损失函数MFD，用于一致性模型（CMs），它可以将CM切线与数据流形对齐，从而加快训练速度并提高样本质量，即使使用小批量大小也是如此。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UCD: Unconditional Discriminator Promotes Nash Equilibrium in GANs",
        "summary": "Adversarial training turns out to be the key to one-step generation,\nespecially for Generative Adversarial Network (GAN) and diffusion model\ndistillation. Yet in practice, GAN training hardly converges properly and\nstruggles in mode collapse. In this work, we quantitatively analyze the extent\nof Nash equilibrium in GAN training, and conclude that redundant shortcuts by\ninputting condition in $D$ disables meaningful knowledge extraction. We thereby\npropose to employ an unconditional discriminator (UCD), in which $D$ is\nenforced to extract more comprehensive and robust features with no condition\ninjection. In this way, $D$ is able to leverage better knowledge to supervise\n$G$, which promotes Nash equilibrium in GAN literature. Theoretical guarantee\non compatibility with vanilla GAN theory indicates that UCD can be implemented\nin a plug-in manner. Extensive experiments confirm the significant performance\nimprovements with high efficiency. For instance, we achieved \\textbf{1.47 FID}\non the ImageNet-64 dataset, surpassing StyleGAN-XL and several state-of-the-art\none-step diffusion models. The code will be made publicly available.",
        "url": "http://arxiv.org/abs/2510.00624v1",
        "published_date": "2025-10-01T07:58:33+00:00",
        "updated_date": "2025-10-01T07:58:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mengfei Xia",
            "Nan Xue",
            "Jiapeng Zhu",
            "Yujun Shen"
        ],
        "tldr": "This paper proposes using an unconditional discriminator (UCD) in GANs, arguing it promotes Nash equilibrium and improves performance, achieving state-of-the-art FID scores on ImageNet-64.",
        "tldr_zh": "该论文提出在GAN中使用无条件判别器(UCD)，认为它能促进纳什均衡并提高性能，在ImageNet-64上实现了最先进的FID分数。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Arbitrary Generative Video Interpolation",
        "summary": "Video frame interpolation (VFI), which generates intermediate frames from\ngiven start and end frames, has become a fundamental function in video\ngeneration applications. However, existing generative VFI methods are\nconstrained to synthesize a fixed number of intermediate frames, lacking the\nflexibility to adjust generated frame rates or total sequence duration. In this\nwork, we present ArbInterp, a novel generative VFI framework that enables\nefficient interpolation at any timestamp and of any length. Specifically, to\nsupport interpolation at any timestamp, we propose the Timestamp-aware Rotary\nPosition Embedding (TaRoPE), which modulates positions in temporal RoPE to\nalign generated frames with target normalized timestamps. This design enables\nfine-grained control over frame timestamps, addressing the inflexibility of\nfixed-position paradigms in prior work. For any-length interpolation, we\ndecompose long-sequence generation into segment-wise frame synthesis. We\nfurther design a novel appearance-motion decoupled conditioning strategy: it\nleverages prior segment endpoints to enforce appearance consistency and\ntemporal semantics to maintain motion coherence, ensuring seamless\nspatiotemporal transitions across segments. Experimentally, we build\ncomprehensive benchmarks for multi-scale frame interpolation (2x to 32x) to\nassess generalizability across arbitrary interpolation factors. Results show\nthat ArbInterp outperforms prior methods across all scenarios with higher\nfidelity and more seamless spatiotemporal continuity. Project website:\nhttps://mcg-nju.github.io/ArbInterp-Web/.",
        "url": "http://arxiv.org/abs/2510.00578v1",
        "published_date": "2025-10-01T06:57:10+00:00",
        "updated_date": "2025-10-01T06:57:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guozhen Zhang",
            "Haiguang Wang",
            "Chunyu Wang",
            "Yuan Zhou",
            "Qinglin Lu",
            "Limin Wang"
        ],
        "tldr": "The paper introduces ArbInterp, a novel generative video frame interpolation framework that supports interpolation at any timestamp and length, addressing limitations of existing methods with a Timestamp-aware Rotary Position Embedding and appearance-motion decoupled conditioning strategy.",
        "tldr_zh": "该论文介绍了一种新型的生成式视频帧插值框架ArbInterp，它支持在任意时间戳和长度上进行插值，通过时间戳感知旋转位置嵌入和外观-运动解耦条件策略，克服了现有方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning Energy-based Variational Latent Prior for VAEs",
        "summary": "Variational Auto-Encoders (VAEs) are known to generate blurry and\ninconsistent samples. One reason for this is the \"prior hole\" problem. A prior\nhole refers to regions that have high probability under the VAE's prior but low\nprobability under the VAE's posterior. This means that during data generation,\nhigh probability samples from the prior could have low probability under the\nposterior, resulting in poor quality data. Ideally, a prior needs to be\nflexible enough to match the posterior while retaining the ability to generate\nsamples fast. Generative models continue to address this tradeoff. This paper\nproposes to model the prior as an energy-based model (EBM). While EBMs are\nknown to offer the flexibility to match posteriors (and also improving the\nELBO), they are traditionally slow in sample generation due to their dependency\non MCMC methods. Our key idea is to bring a variational approach to tackle the\nnormalization constant in EBMs, thus bypassing the expensive MCMC approaches.\nThe variational form can be approximated with a sampler network, and we show\nthat such an approach to training priors can be formulated as an alternating\noptimization problem. Moreover, the same sampler reduces to an implicit\nvariational prior during generation, providing efficient and fast sampling. We\ncompare our Energy-based Variational Latent Prior (EVaLP) method to multiple\nSOTA baselines and show improvements in image generation quality, reduced prior\nholes, and better sampling efficiency.",
        "url": "http://arxiv.org/abs/2510.00260v1",
        "published_date": "2025-09-30T20:32:00+00:00",
        "updated_date": "2025-09-30T20:32:00+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Debottam Dutta",
            "Chaitanya Amballa",
            "Zhongweiyang Xu",
            "Yu-Lin Wei",
            "Romit Roy Choudhury"
        ],
        "tldr": "This paper introduces Energy-based Variational Latent Prior (EVaLP) to address the 'prior hole' problem in VAEs, enabling faster and better image generation by using a variational approach within EBMs to bypass the need for MCMC sampling.",
        "tldr_zh": "本文提出了基于能量的变分隐变量先验(EVaLP)方法，旨在解决VAE中的“先验空洞”问题。该方法通过在EBM中使用变分方法绕过MCMC采样，从而实现更快、更好的图像生成效果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EditTrack: Detecting and Attributing AI-assisted Image Editing",
        "summary": "In this work, we formulate and study the problem of image-editing detection\nand attribution: given a base image and a suspicious image, detection seeks to\ndetermine whether the suspicious image was derived from the base image using an\nAI editing model, while attribution further identifies the specific editing\nmodel responsible. Existing methods for detecting and attributing AI-generated\nimages are insufficient for this problem, as they focus on determining whether\nan image was AI-generated/edited rather than whether it was edited from a\nparticular base image. To bridge this gap, we propose EditTrack, the first\nframework for this image-editing detection and attribution problem. Building on\nfour key observations about the editing process, EditTrack introduces a novel\nre-editing strategy and leverages carefully designed similarity metrics to\ndetermine whether a suspicious image originates from a base image and, if so,\nby which model. We evaluate EditTrack on five state-of-the-art editing models\nacross six datasets, demonstrating that it consistently achieves accurate\ndetection and attribution, significantly outperforming five baselines.",
        "url": "http://arxiv.org/abs/2510.01173v1",
        "published_date": "2025-10-01T17:56:35+00:00",
        "updated_date": "2025-10-01T17:56:35+00:00",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhengyuan Jiang",
            "Yuyang Zhang",
            "Moyang Guo",
            "Neil Zhenqiang Gong"
        ],
        "tldr": "EditTrack is a new framework for detecting and attributing AI-assisted image editing, determining if a suspicious image originates from a base image and which model edited it, outperforming existing methods.",
        "tldr_zh": "EditTrack是一个新的框架，用于检测和归因AI辅助的图像编辑，确定可疑图像是否源自基本图像以及哪个模型对其进行了编辑，性能优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack",
        "summary": "Recent advances in text-to-image (T2I) diffusion models have enabled\nimpressive generative capabilities, but they also raise significant safety\nconcerns due to the potential to produce harmful or undesirable content. While\nconcept erasure has been explored as a mitigation strategy, most existing\napproaches and corresponding attack evaluations are tailored to Stable\nDiffusion (SD) and exhibit limited effectiveness when transferred to\nnext-generation rectified flow transformers such as Flux. In this work, we\npresent ReFlux, the first concept attack method specifically designed to assess\nthe robustness of concept erasure in the latest rectified flow-based T2I\nframework. Our approach is motivated by the observation that existing concept\nerasure techniques, when applied to Flux, fundamentally rely on a phenomenon\nknown as attention localization. Building on this insight, we propose a simple\nyet effective attack strategy that specifically targets this property. At its\ncore, a reverse-attention optimization strategy is introduced to effectively\nreactivate suppressed signals while stabilizing attention. This is further\nreinforced by a velocity-guided dynamic that enhances the robustness of concept\nreactivation by steering the flow matching process, and a\nconsistency-preserving objective that maintains the global layout and preserves\nunrelated content. Extensive experiments consistently demonstrate the\neffectiveness and efficiency of the proposed attack method, establishing a\nreliable benchmark for evaluating the robustness of concept erasure strategies\nin rectified flow transformers.",
        "url": "http://arxiv.org/abs/2510.00635v1",
        "published_date": "2025-10-01T08:12:07+00:00",
        "updated_date": "2025-10-01T08:12:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nanxiang Jiang",
            "Zhaoxin Fan",
            "Enhan Kang",
            "Daiheng Gao",
            "Yun Zhou",
            "Yanxia Chang",
            "Zheng Zhu",
            "Yeying Jin",
            "Wenjun Wu"
        ],
        "tldr": "This paper introduces ReFlux, a novel concept attack method to evaluate the robustness of concept erasure techniques in rectified flow transformer-based text-to-image models, highlighting vulnerabilities in attention localization.",
        "tldr_zh": "本文介绍了 ReFlux，一种新型概念攻击方法，用于评估基于修正流 Transformer 的文本到图像模型中概念擦除技术的鲁棒性，突出了注意力定位的漏洞。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multi-level Dynamic Style Transfer for NeRFs",
        "summary": "As the application of neural radiance fields (NeRFs) in various 3D vision\ntasks continues to expand, numerous NeRF-based style transfer techniques have\nbeen developed. However, existing methods typically integrate style statistics\ninto the original NeRF pipeline, often leading to suboptimal results in both\ncontent preservation and artistic stylization. In this paper, we present\nmulti-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that\nreengineers the NeRF pipeline specifically for stylization and incorporates an\ninnovative dynamic style injection module. Particularly, we propose a\nmulti-level feature adaptor that helps generate a multi-level feature grid\nrepresentation from the content radiance field, effectively capturing the\nmulti-scale spatial structure of the scene. In addition, we present a dynamic\nstyle injection module that learns to extract relevant style features and\nadaptively integrates them into the content patterns. The stylized multi-level\nfeatures are then transformed into the final stylized view through our proposed\nmulti-level cascade decoder. Furthermore, we extend our 3D style transfer\nmethod to support omni-view style transfer using 3D style references. Extensive\nexperiments demonstrate that MDS-NeRF achieves outstanding performance for 3D\nstyle transfer, preserving multi-scale spatial structures while effectively\ntransferring stylistic characteristics.",
        "url": "http://arxiv.org/abs/2510.00592v1",
        "published_date": "2025-10-01T07:19:27+00:00",
        "updated_date": "2025-10-01T07:19:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zesheng Li",
            "Shuaibo Li",
            "Wei Ma",
            "Jianwei Guo",
            "Hongbin Zha"
        ],
        "tldr": "The paper introduces MDS-NeRF, a novel NeRF architecture for 3D style transfer that uses multi-level feature adaptation and dynamic style injection to improve content preservation and stylization compared to existing methods, even supporting omni-view style transfer.",
        "tldr_zh": "该论文介绍了MDS-NeRF，一种用于 3D 风格迁移的新型 NeRF 架构，它使用多级特征适应和动态风格注入来改进内容保留和风格化，与现有方法相比，甚至支持全方位风格迁移。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]