[
    {
        "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model",
        "summary": "SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.",
        "url": "http://arxiv.org/abs/2602.21818v1",
        "published_date": "2026-02-25T11:47:00+00:00",
        "updated_date": "2026-02-25T11:47:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guibin Chen",
            "Dixuan Lin",
            "Jiangping Yang",
            "Youqiang Zhang",
            "Zhengcong Fei",
            "Debang Li",
            "Sheng Chen",
            "Chaofeng Ao",
            "Nuo Pang",
            "Yiming Wang",
            "Yikun Dou",
            "Zheng Chen",
            "Mingyuan Fan",
            "Tuanhui Li",
            "Mingshan Chang",
            "Hao Zhang",
            "Xiaopeng Sun",
            "Jingtao Xu",
            "Yuqiang Xie",
            "Jiahua Wang",
            "Zhiheng Xu",
            "Weiming Xiong",
            "Yuzhe Jin",
            "Baoxuan Gu",
            "Binjie Mao",
            "Yunjie Yu",
            "Jujie He",
            "Yuhao Feng",
            "Shiwen Tu",
            "Chaojie Wang",
            "Rui Yan",
            "Wei Shen",
            "Jingchen Wu",
            "Peng Zhao",
            "Xuanyue Zhong",
            "Zhuangzhuang Liu",
            "Kaifei Wang",
            "Fuxiang Zhang",
            "Weikai Xu",
            "Wenyan Liu",
            "Binglu Zhang",
            "Yu Shen",
            "Tianhui Xiong",
            "Bin Peng",
            "Liang Zeng",
            "Xuchen Song",
            "Haoxiang Guo",
            "Peiyu Wang",
            "Yahui Zhou"
        ],
        "tldr": "SkyReels V4 is a novel multi-modal video foundation model capable of high-resolution, long-duration video and audio generation, inpainting, and editing based on diverse inputs, employing efficient generation strategies.",
        "tldr_zh": "SkyReels V4是一个新型多模态视频基础模型，能够根据各种输入生成高分辨率、长视频和音频，进行修复和编辑，并采用高效的生成策略。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "CoLoGen: Progressive Learning of Concept`-`Localization Duality for Unified Image Generation",
        "summary": "Unified conditional image generation remains difficult because different tasks depend on fundamentally different internal representations. Some require conceptual understanding for semantic synthesis, while others rely on localization cues for spatial precision. Forcing these heterogeneous tasks to share a single representation leads to concept`-`localization representational conflict. To address this issue, we propose CoLoGen, a unified diffusion framework that progressively learns and reconciles this concept`-`localization duality. CoLoGen uses a staged curriculum that first builds core conceptual and localization abilities, then adapts them to diverse visual conditions, and finally refines their synergy for complex instruction`-`driven tasks. Central to this process is the Progressive Representation Weaving (PRW) module, which dynamically routes features to specialized experts and stably integrates their outputs across stages. Experiments on editing, controllable generation, and customized generation show that CoLoGen achieves competitive or superior performance, offering a principled representational perspective for unified image generation.",
        "url": "http://arxiv.org/abs/2602.22150v1",
        "published_date": "2026-02-25T17:59:29+00:00",
        "updated_date": "2026-02-25T17:59:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "YuXin Song",
            "Yu Lu",
            "Haoyuan Sun",
            "Huanjin Yao",
            "Fanglong Liu",
            "Yifan Sun",
            "Haocheng Feng",
            "Hang Zhou",
            "Jingdong Wang"
        ],
        "tldr": "CoLoGen, a diffusion-based framework, addresses the concept-localization conflict in unified image generation by progressively learning and reconciling these dual representations through a staged curriculum and a Progressive Representation Weaving module.",
        "tldr_zh": "CoLoGen是一个基于扩散模型的框架，通过分阶段课程和渐进式表征编织模块，逐步学习和协调概念与定位的双重表征，从而解决统一图像生成中的概念-定位冲突问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Geometry-as-context: Modulating Explicit 3D in Scene-consistent Video Generation to Geometry Context",
        "summary": "Scene-consistent video generation aims to create videos that explore 3D scenes based on a camera trajectory. Previous methods rely on video generation models with external memory for consistency, or iterative 3D reconstruction and inpainting, which accumulate errors during inference due to incorrect intermediary outputs, non-differentiable processes, and separate models. To overcome these limitations, we introduce ``geometry-as-context\". It iteratively completes the following steps using an autoregressive camera-controlled video generation model: (1) estimates the geometry of the current view necessary for 3D reconstruction, and (2) simulates and restores novel view images rendered by the 3D scene. Under this multi-task framework, we develop the camera gated attention module to enhance the model's capability to effectively leverage camera poses. During the training phase, text contexts are utilized to ascertain whether geometric or RGB images should be generated. To ensure that the model can generate RGB-only outputs during inference, the geometry context is randomly dropped from the interleaved text-image-geometry training sequence. The method has been tested on scene video generation with one-direction and forth-and-back trajectories. The results show its superiority over previous approaches in maintaining scene consistency and camera control.",
        "url": "http://arxiv.org/abs/2602.21929v1",
        "published_date": "2026-02-25T14:09:03+00:00",
        "updated_date": "2026-02-25T14:09:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "JiaKui Hu",
            "Jialun Liu",
            "Liying Yang",
            "Xinliang Zhang",
            "Kaiwen Li",
            "Shuang Zeng",
            "Yuanwei Li",
            "Haibin Huang",
            "Chi Zhang",
            "Yanye Lu"
        ],
        "tldr": "This paper introduces a novel \"geometry-as-context\" approach for scene-consistent video generation by iteratively estimating scene geometry and rendering novel views, using a camera-gated attention module and a multi-task framework with random geometry context dropout during training.",
        "tldr_zh": "本文提出了一种新颖的“几何作为上下文”方法，用于生成场景一致的视频。该方法通过迭代地估计场景几何结构和渲染新视角，利用相机门控注意力模块和一个多任务框架，并在训练期间随机丢弃几何上下文。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors",
        "summary": "Instruction-based image editing has achieved remarkable success in semantic alignment, yet state-of-the-art models frequently fail to render physically plausible results when editing involves complex causal dynamics, such as refraction or material deformation. We attribute this limitation to the dominant paradigm that treats editing as a discrete mapping between image pairs, which provides only boundary conditions and leaves transition dynamics underspecified. To address this, we reformulate physics-aware editing as predictive physical state transitions and introduce PhysicTran38K, a large-scale video-based dataset comprising 38K transition trajectories across five physical domains, constructed via a two-stage filtering and constraint-aware annotation pipeline. Building on this supervision, we propose PhysicEdit, an end-to-end framework equipped with a textual-visual dual-thinking mechanism. It combines a frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries that provide timestep-adaptive visual guidance to a diffusion backbone. Experiments show that PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, setting a new state-of-the-art for open-source methods, while remaining competitive with leading proprietary models.",
        "url": "http://arxiv.org/abs/2602.21778v1",
        "published_date": "2026-02-25T10:54:46+00:00",
        "updated_date": "2026-02-25T10:54:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liangbing Zhao",
            "Le Zhuo",
            "Sayak Paul",
            "Hongsheng Li",
            "Mohamed Elhoseiny"
        ],
        "tldr": "The paper introduces PhysicEdit, a framework for physics-aware image editing using a new video dataset (PhysicTran38K) representing physical state transitions, which improves physical realism and knowledge grounding in image editing compared to existing methods.",
        "tldr_zh": "该论文介绍了一个名为PhysicEdit的物理感知图像编辑框架，并使用了一个新的视频数据集(PhysicTran38K)来表示物理状态转换。相比现有方法，该框架在图像编辑中提高了物理真实感和知识基础。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling",
        "summary": "Diffusion models have achieved remarkable progress in high-fidelity image, video, and audio generation, yet inference remains computationally expensive. Nevertheless, current diffusion acceleration methods based on distributed parallelism suffer from noticeable generation artifacts and fail to achieve substantial acceleration proportional to the number of GPUs. Therefore, we propose a hybrid parallelism framework that combines a novel data parallel strategy, condition-based partitioning, with an optimal pipeline scheduling method, adaptive parallelism switching, to reduce generation latency and achieve high generation quality in conditional diffusion models. The key ideas are to (i) leverage the conditional and unconditional denoising paths as a new data-partitioning perspective and (ii) adaptively enable optimal pipeline parallelism according to the denoising discrepancy between these two paths. Our framework achieves $2.31\\times$ and $2.07\\times$ latency reductions on SDXL and SD3, respectively, using two NVIDIA RTX~3090 GPUs, while preserving image quality. This result confirms the generality of our approach across U-Net-based diffusion models and DiT-based flow-matching architectures. Our approach also outperforms existing methods in acceleration under high-resolution synthesis settings. Code is available at https://github.com/kaist-dmlab/Hybridiff.",
        "url": "http://arxiv.org/abs/2602.21760v1",
        "published_date": "2026-02-25T10:23:07+00:00",
        "updated_date": "2026-02-25T10:23:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Euisoo Jung",
            "Byunghyun Kim",
            "Hyunjin Kim",
            "Seonghye Cho",
            "Jae-Gil Lee"
        ],
        "tldr": "This paper presents a hybrid data-pipeline parallelism framework for accelerating conditional diffusion models, achieving significant latency reductions on SDXL and SD3 while preserving image quality. It uses condition-based partitioning and adaptive pipeline scheduling.",
        "tldr_zh": "本文提出了一种混合数据流水线并行框架，用于加速条件扩散模型，在 SDXL 和 SD3 上实现了显著的延迟降低，同时保持了图像质量。它利用了基于条件的划分和自适应流水线调度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Hidden Semantic Bottleneck in Conditional Embeddings of Diffusion Transformers",
        "summary": "Diffusion Transformers have achieved state-of-the-art performance in class-conditional and multimodal generation, yet the structure of their learned conditional embeddings remains poorly understood. In this work, we present the first systematic study of these embeddings and uncover a notable redundancy: class-conditioned embeddings exhibit extreme angular similarity, exceeding 99\\% on ImageNet-1K, while continuous-condition tasks such as pose-guided image generation and video-to-audio generation reach over 99.9\\%. We further find that semantic information is concentrated in a small subset of dimensions, with head dimensions carrying the dominant signal and tail dimensions contributing minimally. By pruning low-magnitude dimensions--removing up to two-thirds of the embedding space--we show that generation quality and fidelity remain largely unaffected, and in some cases improve. These results reveal a semantic bottleneck in Transformer-based diffusion models, providing new insights into how semantics are encoded and suggesting opportunities for more efficient conditioning mechanisms.",
        "url": "http://arxiv.org/abs/2602.21596v1",
        "published_date": "2026-02-25T05:46:40+00:00",
        "updated_date": "2026-02-25T05:46:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Trung X. Pham",
            "Kang Zhang",
            "Ji Woo Hong",
            "Chang D. Yoo"
        ],
        "tldr": "This paper uncovers a significant redundancy in the conditional embeddings of Diffusion Transformers, showing that semantic information is concentrated in a small subset of dimensions, which can be pruned with minimal impact on generation quality.",
        "tldr_zh": "本文揭示了扩散Transformer条件嵌入中的显著冗余，表明语义信息集中在少数维度中，这些维度可以在对生成质量影响最小的情况下进行修剪。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Synergizing Understanding and Generation with Interleaved Analyzing-Drafting Thinking",
        "summary": "Unified Vision-Language Models (UVLMs) aim to advance multimodal learning by supporting both understanding and generation within a single framework. However, existing approaches largely focus on architectural unification while overlooking the need for explicit interaction between the two capabilities during task solving. As a result, current models treat understanding and generation as parallel skills rather than synergistic processes. To achieve real synergy, we introduce the interleaved Analyzing-Drafting problem-solving loop (AD-Loop), a new think paradigm that dynamically alternates between analytic and drafting operations. By interleaving textual thoughts with visual thoughts, AD-Loop enables models to iteratively refine both comprehension and outputs, fostering genuine synergy. To train this mechanism, we design a two-stage strategy: supervised learning on interleaved thought data to initialize alternation, followed by reinforcement learning to promote adaptive and autonomous control. Extensive experiments demonstrate that AD-Loop consistently improves performance across standard benchmarks for both understanding and generation, with strong transferability to various UVLMs architectures. Visual analyses further validate the effectiveness of implicit visual thoughts. These results highlight AD-Loop as a principled and broadly applicable strategy for synergizing comprehension and creation. The project page is at https://sqwu.top/AD-Loop.",
        "url": "http://arxiv.org/abs/2602.21435v1",
        "published_date": "2026-02-24T23:26:09+00:00",
        "updated_date": "2026-02-24T23:26:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shengqiong Wu",
            "Bobo Li",
            "Xinkai Wang",
            "Xiangtai Li",
            "Lei Cui",
            "Furu Wei",
            "Shuicheng Yan",
            "Hao Fei",
            "Tat-seng Chua"
        ],
        "tldr": "The paper introduces an interleaved Analyzing-Drafting (AD-Loop) approach for Unified Vision-Language Models (UVLMs) that alternates between analytic and drafting operations to improve both visual understanding and generation, showing strong performance gains on standard benchmarks.",
        "tldr_zh": "本文提出一种用于统一视觉-语言模型 (UVLM) 的交错式分析-起草 (AD-Loop) 方法，该方法在分析和起草操作之间交替进行，以提高视觉理解和生成能力，并在标准基准测试中表现出强大的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Controllable Video Synthesis of Routine and Rare OR Events",
        "summary": "Purpose: Curating large-scale datasets of operating room (OR) workflow, encompassing rare, safety-critical, or atypical events, remains operationally and ethically challenging. This data bottleneck complicates the development of ambient intelligence for detecting, understanding, and mitigating rare or safety-critical events in the OR.\n  Methods: This work presents an OR video diffusion framework that enables controlled synthesis of rare and safety-critical events. The framework integrates a geometric abstraction module, a conditioning module, and a fine-tuned diffusion model to first transform OR scenes into abstract geometric representations, then condition the synthesis process, and finally generate realistic OR event videos. Using this framework, we also curate a synthetic dataset to train and validate AI models for detecting near-misses of sterile-field violations.\n  Results: In synthesizing routine OR events, our method outperforms off-the-shelf video diffusion baselines, achieving lower FVD/LPIPS and higher SSIM/PSNR in both in- and out-of-domain datasets. Through qualitative results, we illustrate its ability for controlled video synthesis of counterfactual events. An AI model trained and validated on the generated synthetic data achieved a RECALL of 70.13% in detecting near safety-critical events. Finally, we conduct an ablation study to quantify performance gains from key design choices.\n  Conclusion: Our solution enables controlled synthesis of routine and rare OR events from abstract geometric representations. Beyond demonstrating its capability to generate rare and safety-critical scenarios, we show its potential to support the development of ambient intelligence models.",
        "url": "http://arxiv.org/abs/2602.21365v1",
        "published_date": "2026-02-24T20:56:15+00:00",
        "updated_date": "2026-02-24T20:56:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Dominik Schneider",
            "Lalithkumar Seenivasan",
            "Sampath Rapuri",
            "Vishalroshan Anil",
            "Aiza Maksutova",
            "Yiqing Shen",
            "Jan Emily Mangulabnan",
            "Hao Ding",
            "Jose L. Porras",
            "Masaru Ishii",
            "Mathias Unberath"
        ],
        "tldr": "This paper introduces a framework for controlled video synthesis of operating room events (including rare and safety-critical ones) using geometric abstraction and diffusion models, demonstrating potential for training AI models to detect near-misses.",
        "tldr_zh": "本文介绍了一个框架，通过几何抽象和扩散模型来控制合成手术室事件的视频（包括罕见和对安全至关重要的事件），展示了训练人工智能模型来检测险兆的可能性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HorizonForge: Driving Scene Editing with Any Trajectories and Any Vehicles",
        "summary": "Controllable driving scene generation is critical for realistic and scalable autonomous driving simulation, yet existing approaches struggle to jointly achieve photorealism and precise control. We introduce HorizonForge, a unified framework that reconstructs scenes as editable Gaussian Splats and Meshes, enabling fine-grained 3D manipulation and language-driven vehicle insertion. Edits are rendered through a noise-aware video diffusion process that enforces spatial and temporal consistency, producing diverse scene variations in a single feed-forward pass without per-trajectory optimization. To standardize evaluation, we further propose HorizonSuite, a comprehensive benchmark spanning ego- and agent-level editing tasks such as trajectory modifications and object manipulation. Extensive experiments show that Gaussian-Mesh representation delivers substantially higher fidelity than alternative 3D representations, and that temporal priors from video diffusion are essential for coherent synthesis. Combining these findings, HorizonForge establishes a simple yet powerful paradigm for photorealistic, controllable driving simulation, achieving an 83.4% user-preference gain and a 25.19% FID improvement over the second best state-of-the-art method. Project page: https://horizonforge.github.io/ .",
        "url": "http://arxiv.org/abs/2602.21333v1",
        "published_date": "2026-02-24T20:03:47+00:00",
        "updated_date": "2026-02-24T20:03:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifan Wang",
            "Francesco Pittaluga",
            "Zaid Tasneem",
            "Chenyu You",
            "Manmohan Chandraker",
            "Ziyu Jiang"
        ],
        "tldr": "HorizonForge introduces a novel framework for controllable and photorealistic driving scene generation using Gaussian Splats and Meshes, with language-driven vehicle insertion and noise-aware video diffusion, achieving significant improvements over existing methods.",
        "tldr_zh": "HorizonForge 提出了一个新颖的框架，利用高斯溅射和网格，结合语言驱动的车辆插入和噪声感知视频扩散，实现了可控且逼真的驾驶场景生成，相比现有方法取得了显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Human Video Generation from a Single Image with 3D Pose and View Control",
        "summary": "Recent diffusion methods have made significant progress in generating videos from single images due to their powerful visual generation capabilities. However, challenges persist in image-to-video synthesis, particularly in human video generation, where inferring view-consistent, motion-dependent clothing wrinkles from a single image remains a formidable problem. In this paper, we present Human Video Generation in 4D (HVG), a latent video diffusion model capable of generating high-quality, multi-view, spatiotemporally coherent human videos from a single image with 3D pose and view control. HVG achieves this through three key designs: (i) Articulated Pose Modulation, which captures the anatomical relationships of 3D joints via a novel dual-dimensional bone map and resolves self-occlusions across views by introducing 3D information; (ii) View and Temporal Alignment, which ensures multi-view consistency and alignment between a reference image and pose sequences for frame-to-frame stability; and (iii) Progressive Spatio-Temporal Sampling with temporal alignment to maintain smooth transitions in long multi-view animations. Extensive experiments on image-to-video tasks demonstrate that HVG outperforms existing methods in generating high-quality 4D human videos from diverse human images and pose inputs.",
        "url": "http://arxiv.org/abs/2602.21188v1",
        "published_date": "2026-02-24T18:42:20+00:00",
        "updated_date": "2026-02-24T18:42:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tiantian Wang",
            "Chun-Han Yao",
            "Tao Hu",
            "Mallikarjun Byrasandra Ramalinga Reddy",
            "Ming-Hsuan Yang",
            "Varun Jampani"
        ],
        "tldr": "The paper introduces HVG, a novel latent video diffusion model capable of generating high-quality, multi-view, spatiotemporally coherent human videos from a single image with 3D pose and view control, addressing the challenge of generating realistic human video from a single image.",
        "tldr_zh": "本文介绍了一种新的潜在视频扩散模型HVG，它能够从带有3D姿势和视角控制的单个图像生成高质量、多视角、时空连贯的人体视频，解决了从单个图像生成逼真人体视频的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StoryMovie: A Dataset for Semantic Alignment of Visual Stories with Movie Scripts and Subtitles",
        "summary": "Visual storytelling models that correctly ground entities in images may still hallucinate semantic relationships, generating incorrect dialogue attribution, character interactions, or emotional states. We introduce StoryMovie, a dataset of 1,757 stories aligned with movie scripts and subtitles through LCS matching. Our alignment pipeline synchronizes screenplay dialogue with subtitle timestamps, enabling dialogue attribution by linking character names from scripts to temporal positions from subtitles. Using this aligned content, we generate stories that maintain visual grounding tags while incorporating authentic character names, dialogue, and relationship dynamics. We fine-tune Qwen Storyteller3 on this dataset, building on prior work in visual grounding and entity re-identification. Evaluation using DeepSeek V3 as judge shows that Storyteller3 achieves an 89.9% win rate against base Qwen2.5-VL 7B on subtitle alignment. Compared to Storyteller, trained without script grounding,\n  Storyteller3 achieves 48.5% versus 38.0%, confirming that semantic alignment progressively improves dialogue attribution beyond visual grounding alone.",
        "url": "http://arxiv.org/abs/2602.21829v1",
        "published_date": "2026-02-25T12:01:05+00:00",
        "updated_date": "2026-02-25T12:01:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Daniel Oliveira",
            "David Martins de Matos"
        ],
        "tldr": "The paper introduces StoryMovie, a dataset aligning visual stories with movie scripts and subtitles, and demonstrates its effectiveness in improving dialogue attribution and semantic understanding in visual storytelling models through fine-tuning Qwen Storyteller3.",
        "tldr_zh": "该论文介绍了 StoryMovie，一个将视觉故事与电影剧本和字幕对齐的数据集。 通过微调 Qwen Storyteller3，证明了该数据集在提升视觉故事模型中的对话归属和语义理解方面的有效性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]