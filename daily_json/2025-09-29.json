[
    {
        "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention",
        "summary": "In Diffusion Transformer (DiT) models, particularly for video generation,\nattention latency is a major bottleneck due to the long sequence length and the\nquadratic complexity. We find that attention weights can be separated into two\nparts: a small fraction of large weights with high rank and the remaining\nweights with very low rank. This naturally suggests applying sparse\nacceleration to the first part and low-rank acceleration to the second. Based\non this finding, we propose SLA (Sparse-Linear Attention), a trainable\nattention method that fuses sparse and linear attention to accelerate diffusion\nmodels. SLA classifies attention weights into critical, marginal, and\nnegligible categories, applying O(N^2) attention to critical weights, O(N)\nattention to marginal weights, and skipping negligible ones. SLA combines these\ncomputations into a single GPU kernel and supports both forward and backward\npasses. With only a few fine-tuning steps using SLA, DiT models achieve a 20x\nreduction in attention computation, resulting in significant acceleration\nwithout loss of generation quality. Experiments show that SLA reduces attention\ncomputation by 95% without degrading end-to-end generation quality,\noutperforming baseline methods. In addition, we implement an efficient GPU\nkernel for SLA, which yields a 13.7x speedup in attention computation and a\n2.2x end-to-end speedup in video generation on Wan2.1-1.3B.",
        "url": "http://arxiv.org/abs/2509.24006v1",
        "published_date": "2025-09-28T17:58:59+00:00",
        "updated_date": "2025-09-28T17:58:59+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jintao Zhang",
            "Haoxu Wang",
            "Kai Jiang",
            "Shuo Yang",
            "Kaiwen Zheng",
            "Haocheng Xi",
            "Ziteng Wang",
            "Hongzhou Zhu",
            "Min Zhao",
            "Ion Stoica",
            "Joseph E. Gonzalez",
            "Jun Zhu",
            "Jianfei Chen"
        ],
        "tldr": "The paper introduces Sparse-Linear Attention (SLA), a method to accelerate Diffusion Transformer models by selectively applying sparse and linear attention based on the significance of attention weights, achieving significant speedups in video generation without quality loss.",
        "tldr_zh": "该论文介绍了稀疏线性注意力（SLA），一种通过根据注意力权重的显着性选择性地应用稀疏和线性注意力来加速扩散Transformer模型的方法，从而在视频生成中实现显著加速且不损失质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "HunyuanImage 3.0 Technical Report",
        "summary": "We present HunyuanImage 3.0, a native multimodal model that unifies\nmultimodal understanding and generation within an autoregressive framework,\nwith its image generation module publicly available. The achievement of\nHunyuanImage 3.0 relies on several key components, including meticulous data\ncuration, advanced architecture design, a native Chain-of-Thoughts schema,\nprogressive model pre-training, aggressive model post-training, and an\nefficient infrastructure that enables large-scale training and inference. With\nthese advancements, we successfully trained a Mixture-of-Experts (MoE) model\ncomprising over 80 billion parameters in total, with 13 billion parameters\nactivated per token during inference, making it the largest and most powerful\nopen-source image generative model to date. We conducted extensive experiments\nand the results of automatic and human evaluation of text-image alignment and\nvisual quality demonstrate that HunyuanImage 3.0 rivals previous\nstate-of-the-art models. By releasing the code and weights of HunyuanImage 3.0,\nwe aim to enable the community to explore new ideas with a state-of-the-art\nfoundation model, fostering a dynamic and vibrant multimodal ecosystem. All\nopen source assets are publicly available at\nhttps://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
        "url": "http://arxiv.org/abs/2509.23951v1",
        "published_date": "2025-09-28T16:14:10+00:00",
        "updated_date": "2025-09-28T16:14:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siyu Cao",
            "Hangting Chen",
            "Peng Chen",
            "Yiji Cheng",
            "Yutao Cui",
            "Xinchi Deng",
            "Ying Dong",
            "Kipper Gong",
            "Tianpeng Gu",
            "Xiusen Gu",
            "Tiankai Hang",
            "Duojun Huang",
            "Jie Jiang",
            "Zhengkai Jiang",
            "Weijie Kong",
            "Changlin Li",
            "Donghao Li",
            "Junzhe Li",
            "Xin Li",
            "Yang Li",
            "Zhenxi Li",
            "Zhimin Li",
            "Jiaxin Lin",
            "Linus",
            "Lucaz Liu",
            "Shu Liu",
            "Songtao Liu",
            "Yu Liu",
            "Yuhong Liu",
            "Yanxin Long",
            "Fanbin Lu",
            "Qinglin Lu",
            "Yuyang Peng",
            "Yuanbo Peng",
            "Xiangwei Shen",
            "Yixuan Shi",
            "Jiale Tao",
            "Yangyu Tao",
            "Qi Tian",
            "Pengfei Wan",
            "Chunyu Wang",
            "Kai Wang",
            "Lei Wang",
            "Linqing Wang",
            "Lucas Wang",
            "Qixun Wang",
            "Weiyan Wang",
            "Hao Wen",
            "Bing Wu",
            "Jianbing Wu",
            "Yue Wu",
            "Senhao Xie",
            "Fang Yang",
            "Miles Yang",
            "Xiaofeng Yang",
            "Xuan Yang",
            "Zhantao Yang",
            "Jingmiao Yu",
            "Zheng Yuan",
            "Chao Zhang",
            "Jian-Wei Zhang",
            "Peizhen Zhang",
            "Shi-Xue Zhang",
            "Tao Zhang",
            "Weigang Zhang",
            "Yepeng Zhang",
            "Yingfang Zhang",
            "Zihao Zhang",
            "Zijian Zhang",
            "Penghao Zhao",
            "Zhiyuan Zhao",
            "Xuefei Zhe",
            "Jianchen Zhu",
            "Zhao Zhong"
        ],
        "tldr": "HunyuanImage 3.0 is a large-scale, open-source multimodal model for image generation trained with an autoregressive framework and MoE architecture, achieving state-of-the-art results and publicly released to foster research.",
        "tldr_zh": "HunyuanImage 3.0 是一个大规模的开源多模态图像生成模型，它采用自回归框架和 MoE 架构进行训练，实现了最先进的成果，并且公开发布以促进研究。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Uni4D-LLM: A Unified SpatioTemporal-Aware VLM for 4D Understanding and Generation",
        "summary": "Vision-language models (VLMs) have demonstrated strong performance in 2D\nscene understanding and generation, but extending this unification to the\nphysical world remains an open challenge. Existing 3D and 4D approaches\ntypically embed scene geometry into autoregressive model for semantic\nunderstanding and diffusion model for content generation. This paradigm gap\nprevents a single model from jointly handling both tasks, especially in dynamic\n4D settings where spatiotemporal modeling is critical. We propose Uni4D-LLM,\nthe first unified VLM framework with spatiotemporal awareness for 4D scene\nunderstanding and generation. Our design is guided by two key insights: 1)\nUnification requires a shared representation. We extract semantic features for\nunderstanding and noisy-injected appearance features for generation,\nincorporate 4D geometric cues, and fuse them into a spatiotemporal-aware visual\nrepresentation through adaptive cross-attention. 2) Unification requires a\nshared architecture. Both autoregression and diffusion are built on Transformer\nbackbones, and this enables integration into a single LLM with task-specific\nheads. By aligning visual and linguistic representations, our Uni4D-LLM\nproduces predictions for both understanding and generation within one\nTransformer-based framework. We further apply instruction fine-tuning on\ndiverse 4D vision-language datasets to improve generalization across tasks.\nExtensive experiments on multiple benchmarks demonstrate that Uni4D-LLM\nachieves competitive or superior results compared to state-of-the-art models\nand offers the first true unification of 4D scene understanding and generation.",
        "url": "http://arxiv.org/abs/2509.23828v1",
        "published_date": "2025-09-28T12:06:54+00:00",
        "updated_date": "2025-09-28T12:06:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanyu Zhou",
            "Gim Hee Lee"
        ],
        "tldr": "Uni4D-LLM is a novel unified VLM framework for 4D scene understanding and generation, utilizing a shared spatio-temporal-aware representation and a Transformer-based architecture with task-specific heads.",
        "tldr_zh": "Uni4D-LLM 是一种新颖的统一 VLM 框架，用于 4D 场景理解和生成，它利用共享的时空感知表示和基于 Transformer 的架构，并具有特定于任务的头部。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "UniAlignment: Semantic Alignment for Unified Image Generation, Understanding, Manipulation and Perception",
        "summary": "The remarkable success of diffusion models in text-to-image generation has\nsparked growing interest in expanding their capabilities to a variety of\nmulti-modal tasks, including image understanding, manipulation, and perception.\nThese tasks require advanced semantic comprehension across both visual and\ntextual modalities, especially in scenarios involving complex semantic\ninstructions. However, existing approaches often rely heavily on\nvision-language models (VLMs) or modular designs for semantic guidance, leading\nto fragmented architectures and computational inefficiency. To address these\nchallenges, we propose UniAlignment, a unified multimodal generation framework\nwithin a single diffusion transformer. UniAlignment introduces a dual-stream\ndiffusion training strategy that incorporates both intrinsic-modal semantic\nalignment and cross-modal semantic alignment, thereby enhancing the model's\ncross-modal consistency and instruction-following robustness. Additionally, we\npresent SemGen-Bench, a new benchmark specifically designed to evaluate\nmultimodal semantic consistency under complex textual instructions. Extensive\nexperiments across multiple tasks and benchmarks demonstrate that UniAlignment\noutperforms existing baselines, underscoring the significant potential of\ndiffusion models in unified multimodal generation.",
        "url": "http://arxiv.org/abs/2509.23760v1",
        "published_date": "2025-09-28T09:11:30+00:00",
        "updated_date": "2025-09-28T09:11:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyang Song",
            "Libin Wang",
            "Weining Wang",
            "Shaozhen Liu",
            "Dandan Zheng",
            "Jingdong Chen",
            "Qi Li",
            "Zhenan Sun"
        ],
        "tldr": "The paper introduces UniAlignment, a unified diffusion-based framework for multimodal generation, understanding, manipulation, and perception, which uses a dual-stream training strategy to enhance cross-modal consistency and instruction-following. They also introduce a new benchmark called SemGen-Bench for evaluating multimodal semantic consistency.",
        "tldr_zh": "该论文介绍了 UniAlignment，一个用于多模态生成、理解、操作和感知的统一扩散框架，它使用双流训练策略来增强跨模态一致性和指令遵循。他们还引入了一个名为 SemGen-Bench 的新基准，用于评估多模态语义一致性。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Token Painter: Training-Free Text-Guided Image Inpainting via Mask Autoregressive Models",
        "summary": "Text-guided image inpainting aims to inpaint masked image regions based on a\ntextual prompt while preserving the background. Although diffusion-based\nmethods have become dominant, their property of modeling the entire image in\nlatent space makes it challenging for the results to align well with prompt\ndetails and maintain a consistent background. To address these issues, we\nexplore Mask AutoRegressive (MAR) models for this task. MAR naturally supports\nimage inpainting by generating latent tokens corresponding to mask regions,\nenabling better local controllability without altering the background. However,\ndirectly applying MAR to this task makes the inpainting content either ignore\nthe prompts or be disharmonious with the background context. Through analysis\nof the attention maps from the inpainting images, we identify the impact of\nbackground tokens on text tokens during the MAR generation, and leverage this\nto design \\textbf{Token Painter}, a training-free text-guided image inpainting\nmethod based on MAR. Our approach introduces two key components: (1)\nDual-Stream Encoder Information Fusion (DEIF), which fuses the semantic and\ncontext information from text and background in frequency domain to produce\nnovel guidance tokens, allowing MAR to generate text-faithful inpainting\ncontent while keeping harmonious with background context. (2) Adaptive Decoder\nAttention Score Enhancing (ADAE), which adaptively enhances attention scores on\nguidance tokens and inpainting tokens to further enhance the alignment of\nprompt details and the content visual quality. Extensive experiments\ndemonstrate that our training-free method outperforms prior state-of-the-art\nmethods across almost all metrics and delivers superior visual results. Codes\nwill be released.",
        "url": "http://arxiv.org/abs/2509.23919v1",
        "published_date": "2025-09-28T14:48:52+00:00",
        "updated_date": "2025-09-28T14:48:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Longtao Jiang",
            "Mingfei Han",
            "Lei Chen",
            "Yongqiang Yu",
            "Feng Zhao",
            "Xiaojun Chang",
            "Zhihui Li"
        ],
        "tldr": "The paper introduces Token Painter, a training-free text-guided image inpainting method using Mask AutoRegressive models and frequency domain fusion to improve consistency and adherence to text prompts.",
        "tldr_zh": "该论文介绍了一种名为Token Painter的免训练文本引导图像修复方法，该方法使用掩码自回归模型和频域融合来提高图像修复的一致性和对文本提示的遵循度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling",
        "summary": "Instruction-guided image editing has achieved remarkable progress, yet\ncurrent models still face challenges with complex instructions and often\nrequire multiple samples to produce a desired result. Reinforcement Learning\n(RL) offers a promising solution, but its adoption in image editing has been\nseverely hindered by the lack of a high-fidelity, efficient reward signal. In\nthis work, we present a comprehensive methodology to overcome this barrier,\ncentered on the development of a state-of-the-art, specialized reward model. We\nfirst introduce EditReward-Bench, a comprehensive benchmark to systematically\nevaluate reward models on editing quality. Building on this benchmark, we\ndevelop EditScore, a series of reward models (7B-72B) for evaluating the\nquality of instruction-guided image editing. Through meticulous data curation\nand filtering, EditScore effectively matches the performance of learning\nproprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy\ntailored for the generative nature of EditScore, our largest variant even\nsurpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity\nreward model is the key to unlocking online RL for image editing. Our\nexperiments show that, while even the largest open-source VLMs fail to provide\nan effective learning signal, EditScore enables efficient and robust policy\noptimization. Applying our framework to a strong base model, OmniGen2, results\nin a final model that shows a substantial and consistent performance uplift.\nOverall, this work provides the first systematic path from benchmarking to\nreward modeling to RL training in image editing, showing that a high-fidelity,\ndomain-specialized reward model is the key to unlocking the full potential of\nRL in this domain.",
        "url": "http://arxiv.org/abs/2509.23909v1",
        "published_date": "2025-09-28T14:28:24+00:00",
        "updated_date": "2025-09-28T14:28:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Luo",
            "Jiahao Wang",
            "Chenyuan Wu",
            "Shitao Xiao",
            "Xiyan Jiang",
            "Defu Lian",
            "Jiajun Zhang",
            "Dong Liu",
            "Zheng liu"
        ],
        "tldr": "This paper introduces EditScore, a novel reward model for instruction-guided image editing, that surpasses existing models, and enables effective online reinforcement learning for improved image editing results.",
        "tldr_zh": "本文介绍了EditScore，一种用于指令引导图像编辑的新型奖励模型，该模型超越了现有模型，并实现了有效的在线强化学习，从而改进了图像编辑结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Not All Tokens are Guided Equal: Improving Guidance in Visual Autoregressive Models",
        "summary": "Autoregressive (AR) models based on next-scale prediction are rapidly\nemerging as a powerful tool for image generation, but they face a critical\nweakness: information inconsistencies between patches across timesteps\nintroduced by progressive resolution scaling. These inconsistencies scatter\nguidance signals, causing them to drift away from conditioning information and\nleaving behind ambiguous, unfaithful features. We tackle this challenge with\nInformation-Grounding Guidance (IGG), a novel mechanism that anchors guidance\nto semantically important regions through attention. By adaptively reinforcing\ninformative patches during sampling, IGG ensures that guidance and content\nremain tightly aligned. Across both class-conditioned and text-to-image\ngeneration tasks, IGG delivers sharper, more coherent, and semantically\ngrounded images, setting a new benchmark for AR-based methods.",
        "url": "http://arxiv.org/abs/2509.23876v1",
        "published_date": "2025-09-28T13:33:49+00:00",
        "updated_date": "2025-09-28T13:33:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ky Dan Nguyen",
            "Hoang Lam Tran",
            "Anh-Dung Dinh",
            "Daochang Liu",
            "Weidong Cai",
            "Xiuying Wang",
            "Chang Xu"
        ],
        "tldr": "The paper introduces Information-Grounding Guidance (IGG), a novel attention-based mechanism for autoregressive image generation models that improves coherence and semantic grounding by reinforcing informative patches during sampling, leading to sharper and more faithful images.",
        "tldr_zh": "该论文介绍了一种名为信息接地引导（IGG）的新型基于注意力的机制，用于自回归图像生成模型，通过在采样过程中强化信息丰富的补丁来提高连贯性和语义接地，从而生成更清晰、更忠实的图像。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ReLumix: Extending Image Relighting to Video via Video Diffusion Models",
        "summary": "Controlling illumination during video post-production is a crucial yet\nelusive goal in computational photography. Existing methods often lack\nflexibility, restricting users to certain relighting models. This paper\nintroduces ReLumix, a novel framework that decouples the relighting algorithm\nfrom temporal synthesis, thereby enabling any image relighting technique to be\nseamlessly applied to video. Our approach reformulates video relighting into a\nsimple yet effective two-stage process: (1) an artist relights a single\nreference frame using any preferred image-based technique (e.g., Diffusion\nModels, physics-based renderers); and (2) a fine-tuned stable video diffusion\n(SVD) model seamlessly propagates this target illumination throughout the\nsequence. To ensure temporal coherence and prevent artifacts, we introduce a\ngated cross-attention mechanism for smooth feature blending and a temporal\nbootstrapping strategy that harnesses SVD's powerful motion priors. Although\ntrained on synthetic data, ReLumix shows competitive generalization to\nreal-world videos. The method demonstrates significant improvements in visual\nfidelity, offering a scalable and versatile solution for dynamic lighting\ncontrol.",
        "url": "http://arxiv.org/abs/2509.23769v1",
        "published_date": "2025-09-28T09:35:33+00:00",
        "updated_date": "2025-09-28T09:35:33+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Lezhong Wang",
            "Shutong Jin",
            "Ruiqi Cui",
            "Anders Bjorholm Dahl",
            "Jeppe Revall Frisvad",
            "Siavash Bigdeli"
        ],
        "tldr": "ReLumix enables video relighting by decoupling the relighting algorithm from temporal synthesis, allowing any image relighting technique to be applied to video through a fine-tuned stable video diffusion model.",
        "tldr_zh": "ReLumix通过分离光照重构算法和时间合成，实现了视频光照重构，允许将任何图像光照重构技术通过微调的稳定视频扩散模型应用于视频。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and Generation",
        "summary": "In this work, we present HieraTok, a novel multi-scale Vision Transformer\n(ViT)-based tokenizer that overcomes the inherent limitation of modeling\nsingle-scale representations. This is realized through two key designs: (1)\nmulti-scale downsampling applied to the token map generated by the tokenizer\nencoder, producing a sequence of multi-scale tokens, and (2) a scale-causal\nattention mechanism that enables the progressive flow of information from\nlow-resolution global semantic features to high-resolution structural details.\nCoupling these designs, HieraTok achieves significant improvements in both\nimage reconstruction and generation tasks. Under identical settings, the\nmulti-scale visual tokenizer outperforms its single-scale counterpart by a\n27.2\\% improvement in rFID ($1.47 \\rightarrow 1.07$). When integrated into\ndownstream generation frameworks, it achieves a $1.38\\times$ faster convergence\nrate and an 18.9\\% boost in gFID ($16.4 \\rightarrow 13.3$), which may be\nattributed to the smoother and more uniformly distributed latent space.\nFurthermore, by scaling up the tokenizer's training, we demonstrate its\npotential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To\nthe best of our knowledge, we are the first to introduce multi-scale ViT-based\ntokenizer in image reconstruction and image generation. We hope our findings\nand designs advance the ViT-based tokenizers in visual generation tasks.",
        "url": "http://arxiv.org/abs/2509.23736v1",
        "published_date": "2025-09-28T08:30:26+00:00",
        "updated_date": "2025-09-28T08:30:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Cong Chen",
            "Ziyuan Huang",
            "Cheng Zou",
            "Muzhi Zhu",
            "Kaixiang Ji",
            "Jiajia Liu",
            "Jingdong Chen",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "tldr": "The paper introduces HieraTok, a multi-scale Vision Transformer tokenizer for image reconstruction and generation, showing significant improvements in rFID, gFID, and convergence rate compared to single-scale tokenizers. It claims to be the first multi-scale ViT-based tokenizer in this domain.",
        "tldr_zh": "该论文介绍了HieraTok，一种用于图像重建和生成的基于多尺度视觉Transformer的tokenizer，与单尺度tokenizer相比，在rFID、gFID和收敛速度方面表现出显著改进。论文声称是该领域首个多尺度ViT tokenizer。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "M3DLayout: A Multi-Source Dataset of 3D Indoor Layouts and Structured Descriptions for 3D Generation",
        "summary": "In text-driven 3D scene generation, object layout serves as a crucial\nintermediate representation that bridges high-level language instructions with\ndetailed geometric output. It not only provides a structural blueprint for\nensuring physical plausibility but also supports semantic controllability and\ninteractive editing. However, the learning capabilities of current 3D indoor\nlayout generation models are constrained by the limited scale, diversity, and\nannotation quality of existing datasets. To address this, we introduce\nM3DLayout, a large-scale, multi-source dataset for 3D indoor layout generation.\nM3DLayout comprises 15,080 layouts and over 258k object instances, integrating\nthree distinct sources: real-world scans, professional CAD designs, and\nprocedurally generated scenes. Each layout is paired with detailed structured\ntext describing global scene summaries, relational placements of large\nfurniture, and fine-grained arrangements of smaller items. This diverse and\nrichly annotated resource enables models to learn complex spatial and semantic\npatterns across a wide variety of indoor environments. To assess the potential\nof M3DLayout, we establish a benchmark using a text-conditioned diffusion\nmodel. Experimental results demonstrate that our dataset provides a solid\nfoundation for training layout generation models. Its multi-source composition\nenhances diversity, notably through the Inf3DLayout subset which provides rich\nsmall-object information, enabling the generation of more complex and detailed\nscenes. We hope that M3DLayout can serve as a valuable resource for advancing\nresearch in text-driven 3D scene synthesis.",
        "url": "http://arxiv.org/abs/2509.23728v1",
        "published_date": "2025-09-28T08:16:08+00:00",
        "updated_date": "2025-09-28T08:16:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yiheng Zhang",
            "Zhuojiang Cai",
            "Mingdao Wang",
            "Meitong Guo",
            "Tianxiao Li",
            "Li Lin",
            "Yuwang Wang"
        ],
        "tldr": "The paper introduces M3DLayout, a large-scale, multi-source dataset of 3D indoor layouts with structured descriptions for text-driven 3D scene generation, aimed at improving the learning capabilities of layout generation models.",
        "tldr_zh": "该论文介绍了M3DLayout，一个大规模、多来源的3D室内布局数据集，包含结构化描述，用于文本驱动的3D场景生成，旨在提高布局生成模型的学习能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "QuantSparse: Comprehensively Compressing Video Diffusion Transformer with Model Quantization and Attention Sparsification",
        "summary": "Diffusion transformers exhibit remarkable video generation capability, yet\ntheir prohibitive computational and memory costs hinder practical deployment.\nModel quantization and attention sparsification are two promising directions\nfor compression, but each alone suffers severe performance degradation under\naggressive compression. Combining them promises compounded efficiency gains,\nbut naive integration is ineffective. The sparsity-induced information loss\nexacerbates quantization noise, leading to amplified attention shifts. To\naddress this, we propose \\textbf{QuantSparse}, a unified framework that\nintegrates model quantization with attention sparsification. Specifically, we\nintroduce \\textit{Multi-Scale Salient Attention Distillation}, which leverages\nboth global structural guidance and local salient supervision to mitigate\nquantization-induced bias. In addition, we develop \\textit{Second-Order Sparse\nAttention Reparameterization}, which exploits the temporal stability of\nsecond-order residuals to efficiently recover information lost under sparsity.\nExperiments on HunyuanVideo-13B demonstrate that QuantSparse achieves 20.88\nPSNR, substantially outperforming the state-of-the-art quantization baseline\nQ-VDiT (16.85 PSNR), while simultaneously delivering a \\textbf{3.68$\\times$}\nreduction in storage and \\textbf{1.88$\\times$} acceleration in end-to-end\ninference. Our code will be released in\nhttps://github.com/wlfeng0509/QuantSparse.",
        "url": "http://arxiv.org/abs/2509.23681v1",
        "published_date": "2025-09-28T06:49:44+00:00",
        "updated_date": "2025-09-28T06:49:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weilun Feng",
            "Chuanguang Yang",
            "Haotong Qin",
            "Mingqiang Wu",
            "Yuqi Li",
            "Xiangqi Li",
            "Zhulin An",
            "Libo Huang",
            "Yulun Zhang",
            "Michele Magno",
            "Yongjun Xu"
        ],
        "tldr": "The paper introduces QuantSparse, a method that integrates model quantization and attention sparsification for compressing video diffusion transformers, achieving significant improvements in PSNR, storage reduction, and inference speed compared to existing quantization baselines.",
        "tldr_zh": "该论文介绍了 QuantSparse，这是一种结合模型量化和注意力稀疏化的方法，用于压缩视频扩散 Transformer。与现有的量化基线相比，该方法在 PSNR、存储减少和推理速度方面取得了显著提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
        "summary": "We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models\n(LMMs) that achieve state-of-the-art performance with significantly reduced\ncomputational and financial costs. Different from the existing works,\nLLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for\nbuilding high-quality vision-language models entirely from scratch. The\nLLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale\nCurated Datasets: We construct an 85M concept-balanced pretraining dataset\nLLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 26M instruction\ndataset LLaVA-OneVision-1.5-Instruct, collectively encompassing 64B compressed\nmultimodal tokens. (2) Efficient Training Framework: We develop a complete\nend-to-end efficient training framework leveraging an offline parallel data\npacking strategy to facilitate the training of LLaVA-OneVision-1.5 within a\n$16,000 budget. (3) State-of-the-art Performance: Experimental results\ndemonstrate that LLaVA-OneVision1.5 yields exceptionally competitive\nperformance across a broad range of downstream tasks. Specifically,\nLLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and\nLLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. We\nanticipate releasing LLaVA-OneVision-1.5-RL shortly and encourage the community\nto await further updates.",
        "url": "http://arxiv.org/abs/2509.23661v1",
        "published_date": "2025-09-28T05:52:55+00:00",
        "updated_date": "2025-09-28T05:52:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiang An",
            "Yin Xie",
            "Kaicheng Yang",
            "Wenkang Zhang",
            "Xiuwei Zhao",
            "Zheng Cheng",
            "Yirui Wang",
            "Songcen Xu",
            "Changrui Chen",
            "Chunsheng Wu",
            "Huajie Tan",
            "Chunyuan Li",
            "Jing Yang",
            "Jie Yu",
            "Xiyao Wang",
            "Bin Qin",
            "Yumeng Wang",
            "Zizhen Yan",
            "Ziyong Feng",
            "Ziwei Liu",
            "Bo Li",
            "Jiankang Deng"
        ],
        "tldr": "LLaVA-OneVision-1.5 introduces a fully open and efficient framework for training large multimodal models, achieving state-of-the-art performance with significantly reduced costs and outperforming Qwen2.5-VL on many benchmarks.",
        "tldr_zh": "LLaVA-OneVision-1.5 提出了一个完全开放且高效的框架，用于训练大型多模态模型，以显著降低的成本实现了最先进的性能，并在多个基准测试中优于 Qwen2.5-VL。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders",
        "summary": "This paper explores a novel lightweight approach LightFair to achieve fair\ntext-to-image diffusion models (T2I DMs) by addressing the adverse effects of\nthe text encoder. Most existing methods either couple different parts of the\ndiffusion model for full-parameter training or rely on auxiliary networks for\ncorrection. They incur heavy training or sampling burden and unsatisfactory\nperformance. Since T2I DMs consist of multiple components, with the text\nencoder being the most fine-tunable and front-end module, this paper focuses on\nmitigating bias by fine-tuning text embeddings. To validate feasibility, we\nobserve that the text encoder's neutral embedding output shows substantial\nskewness across image embeddings of various attributes in the CLIP space. More\nimportantly, the noise prediction network further amplifies this imbalance. To\nfinetune the text embedding, we propose a collaborative distance-constrained\ndebiasing strategy that balances embedding distances to improve fairness\nwithout auxiliary references. However, mitigating bias can compromise the\noriginal generation quality. To address this, we introduce a two-stage\ntext-guided sampling strategy to limit when the debiased text encoder\nintervenes. Extensive experiments demonstrate that LightFair is effective and\nefficient. Notably, on Stable Diffusion v1.5, our method achieves SOTA\ndebiasing at just $1/4$ of the training burden, with virtually no increase in\nsampling burden. The code is available at https://github.com/boyuh/LightFair.",
        "url": "http://arxiv.org/abs/2509.23639v1",
        "published_date": "2025-09-28T04:46:39+00:00",
        "updated_date": "2025-09-28T04:46:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Boyu Han",
            "Qianqian Xu",
            "Shilong Bao",
            "Zhiyong Yang",
            "Kangli Zi",
            "Qingming Huang"
        ],
        "tldr": "LightFair proposes a lightweight and efficient method for debiasing text-to-image diffusion models by fine-tuning the text encoder, achieving state-of-the-art fairness with minimal training overhead and no increase in sampling burden.",
        "tldr_zh": "LightFair提出了一种轻量级且高效的方法，通过微调文本编码器来去偏见文本到图像的扩散模型，以最小的训练开销和不增加采样负担实现最先进的公平性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VMDiff: Visual Mixing Diffusion for Limitless Cross-Object Synthesis",
        "summary": "Creating novel images by fusing visual cues from multiple sources is a\nfundamental yet underexplored problem in image-to-image generation, with broad\napplications in artistic creation, virtual reality and visual media. Existing\nmethods often face two key challenges: coexistent generation, where multiple\nobjects are simply juxtaposed without true integration, and bias generation,\nwhere one object dominates the output due to semantic imbalance. To address\nthese issues, we propose Visual Mixing Diffusion (VMDiff), a simple yet\neffective diffusion-based framework that synthesizes a single, coherent object\nby integrating two input images at both noise and latent levels. Our approach\ncomprises: (1) a hybrid sampling process that combines guided denoising,\ninversion, and spherical interpolation with adjustable parameters to achieve\nstructure-aware fusion, mitigating coexistent generation; and (2) an efficient\nadaptive adjustment module, which introduces a novel similarity-based score to\nautomatically and adaptively search for optimal parameters, countering semantic\nbias. Experiments on a curated benchmark of 780 concept pairs demonstrate that\nour method outperforms strong baselines in visual quality, semantic\nconsistency, and human-rated creativity.",
        "url": "http://arxiv.org/abs/2509.23605v1",
        "published_date": "2025-09-28T03:17:58+00:00",
        "updated_date": "2025-09-28T03:17:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeren Xiong",
            "Yue Yu",
            "Zedong Zhang",
            "Shuo Chen",
            "Jian Yang",
            "Jun Li"
        ],
        "tldr": "The paper introduces VMDiff, a diffusion-based framework for synthesizing novel images by integrating visual cues from two input images, addressing challenges of coexistent generation and semantic bias.",
        "tldr_zh": "该论文介绍了VMDiff，一种基于扩散的框架，通过整合两张输入图像的视觉线索来合成新的图像，解决了共存生成和语义偏差的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization",
        "summary": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone\nfor image generation, demonstrating superior scalability and performance over\nU-Net architectures. However, their practical deployment is hindered by\nsubstantial computational and memory costs. While Quantization-Aware Training\n(QAT) has shown promise for U-Nets, its application to DiTs faces unique\nchallenges, primarily due to the sensitivity and distributional complexity of\nactivations. In this work, we identify activation quantization as the primary\nbottleneck for pushing DiTs to extremely low-bit settings. To address this, we\npropose a systematic QAT framework for DiTs, named RobuQ. We start by\nestablishing a strong ternary weight (W1.58A4) DiT baseline. Building upon\nthis, we propose RobustQuantizer to achieve robust activation quantization. Our\ntheoretical analyses show that the Hadamard transform can convert unknown\nper-token distributions into per-token normal distributions, providing a strong\nfoundation for this method. Furthermore, we propose AMPN, the first\nActivation-only Mixed-Precision Network pipeline for DiTs. This method applies\nternary weights across the entire network while allocating different activation\nprecisions to each layer to eliminate information bottlenecks. Through\nextensive experiments on unconditional and conditional image generation, our\nRobuQ framework achieves state-of-the-art performance for DiT quantization in\nsub-4-bit quantization configuration. To the best of our knowledge, RobuQ is\nthe first achieving stable and competitive image generation on large datasets\nlike ImageNet-1K with activations quantized to average 2 bits. The code and\nmodels will be available at https://github.com/racoonykc/RobuQ .",
        "url": "http://arxiv.org/abs/2509.23582v1",
        "published_date": "2025-09-28T02:35:12+00:00",
        "updated_date": "2025-09-28T02:35:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaicheng Yang",
            "Xun Zhang",
            "Haotong Qin",
            "Yucheng Lin",
            "Kaisen Yang",
            "Xianglong Yan",
            "Yulun Zhang"
        ],
        "tldr": "This paper proposes RobuQ, a novel quantization-aware training framework for Diffusion Transformers (DiTs) that enables stable and competitive image generation with extremely low-bit activation quantization (W1.58A2), achieving state-of-the-art performance on ImageNet-1K.",
        "tldr_zh": "本文提出了RobuQ，一种新颖的扩散Transformer (DiT) 量化感知训练框架，能够以极低比特激活量化 (W1.58A2) 实现稳定且具有竞争力的图像生成，并在ImageNet-1K上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FoR-SALE: Frame of Reference-guided Spatial Adjustment in LLM-based Diffusion Editing",
        "summary": "Frame of Reference (FoR) is a fundamental concept in spatial reasoning that\nhumans utilize to comprehend and describe space. With the rapid progress in\nMultimodal Language models, the moment has come to integrate this\nlong-overlooked dimension into these models. In particular, in text-to-image\n(T2I) generation, even state-of-the-art models exhibit a significant\nperformance gap when spatial descriptions are provided from perspectives other\nthan the camera. To address this limitation, we propose Frame of\nReference-guided Spatial Adjustment in LLM-based Diffusion Editing (FoR-SALE),\nan extension of the Self-correcting LLM-controlled Diffusion (SLD) framework\nfor T2I. For-Sale evaluates the alignment between a given text and an initially\ngenerated image, and refines the image based on the Frame of Reference\nspecified in the spatial expressions. It employs vision modules to extract the\nspatial configuration of the image, while simultaneously mapping the spatial\nexpression to a corresponding camera perspective. This unified perspective\nenables direct evaluation of alignment between language and vision. When\nmisalignment is detected, the required editing operations are generated and\napplied. FoR-SALE applies novel latent-space operations to adjust the facing\ndirection and depth of the generated images. We evaluate FoR-SALE on two\nbenchmarks specifically designed to assess spatial understanding with FoR. Our\nframework improves the performance of state-of-the-art T2I models by up to 5.3%\nusing only a single round of correction.",
        "url": "http://arxiv.org/abs/2509.23452v1",
        "published_date": "2025-09-27T18:42:04+00:00",
        "updated_date": "2025-09-27T18:42:04+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Tanawan Premsri",
            "Parisa Kordjamshidi"
        ],
        "tldr": "The paper introduces FoR-SALE, a framework that enhances text-to-image generation by incorporating Frame of Reference (FoR) to improve spatial understanding and address perspective-related errors, demonstrating a performance increase of 5.3% on spatial benchmarks.",
        "tldr_zh": "该论文介绍了FoR-SALE，一个通过结合参考系（FoR）来增强文本到图像生成的框架，以提高空间理解并解决与视角相关的错误，在空间基准测试中表现提升了5.3%。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SAR-KnowLIP: Towards Multimodal Foundation Models for Remote Sensing",
        "summary": "Cross-modal artificial intelligence has garnered widespread attention in\nrecent years, achieving significant progress in the study of natural images.\nHowever, existing methods are mostly designed for RGB imagery, leaving a\nsignificant gap in modeling synthetic aperture radar (SAR) imagery. SAR, with\nits all-day, all-weather imaging capabilities, plays an irreplaceable role in\nremote sensing scene understanding. To address this gap, this paper proposes\nSAR-KnowLIP, the first universal SAR multimodal foundational model, along with\nreusable data and evaluation baselines. Specifically: (1) This work introduces\nthe critical yet long-overlooked attribute of geographic information into\nremote sensing research, constructing SAR-GEOVL-1M (the first large-scale SAR\ndataset with complete geographic projection properties), covering multiple\nsatellite platforms, 120,000 images, and 135 cities. (2) Aligned structured\ntext is generated through a hierarchical cognitive chain-of-thought (HCoT),\nproviding more than one million multi-dimensional semantic annotations of\nlandforms, regional functions, target attributes, and spatial relationships.\n(3) We design a Self-Consistent Iterative Optimization mechanism that\ncontinuously enhances cross-modal alignment through a self-supervised closed\nloop of contrastive, matching, and reconstruction learning on a transferable\nmultimodal encoder. (4) A unified evaluation benchmark is established across 11\nrepresentative downstream vision and vision-language tasks, with comparisons\nagainst 14 leading foundation models, where SAR-KnowLIP demonstrates leading\nperformance, particularly in object counting and land-cover classification. We\nexpect that SAR-KnowLIP's large-scale multimodal data, transferable model\narchitecture, and comprehensive experimental benchmark will significantly\nadvance the development of SAR multimodal baseline models.",
        "url": "http://arxiv.org/abs/2509.23927v1",
        "published_date": "2025-09-28T15:03:25+00:00",
        "updated_date": "2025-09-28T15:03:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yi Yang",
            "Xiaokun Zhang",
            "Qingchen Fang",
            "Ziqi Ye",
            "Rui Li",
            "Li Liu",
            "Haipeng Wang"
        ],
        "tldr": "The paper introduces SAR-KnowLIP, a novel multimodal foundation model for SAR imagery, featuring a large-scale dataset (SAR-GEOVL-1M) with geographic information, aligned structured text annotations, and a self-consistent iterative optimization mechanism, demonstrating leading performance on various downstream tasks.",
        "tldr_zh": "该论文介绍了SAR-KnowLIP，一种用于SAR图像的新型多模态基础模型，具有一个大规模数据集(SAR-GEOVL-1M)，其中包含地理信息、对齐的结构化文本注释和一个自一致迭代优化机制，并在各种下游任务上表现出领先性能。",
        "relevance_score": 5,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "MoReact: Generating Reactive Motion from Textual Descriptions",
        "summary": "Modeling and generating human reactions poses a significant challenge with\nbroad applications for computer vision and human-computer interaction. Existing\nmethods either treat multiple individuals as a single entity, directly\ngenerating interactions, or rely solely on one person's motion to generate the\nother's reaction, failing to integrate the rich semantic information that\nunderpins human interactions. Yet, these methods often fall short in adaptive\nresponsiveness, i.e., the ability to accurately respond to diverse and dynamic\ninteraction scenarios. Recognizing this gap, our work introduces an approach\ntailored to address the limitations of existing models by focusing on\ntext-driven human reaction generation. Our model specifically generates\nrealistic motion sequences for individuals that responding to the other's\nactions based on a descriptive text of the interaction scenario. The goal is to\nproduce motion sequences that not only complement the opponent's movements but\nalso semantically fit the described interactions. To achieve this, we present\nMoReact, a diffusion-based method designed to disentangle the generation of\nglobal trajectories and local motions sequentially. This approach stems from\nthe observation that generating global trajectories first is crucial for\nguiding local motion, ensuring better alignment with given action and text.\nFurthermore, we introduce a novel interaction loss to enhance the realism of\ngenerated close interactions. Our experiments, utilizing data adapted from a\ntwo-person motion dataset, demonstrate the efficacy of our approach for this\nnovel task, which is capable of producing realistic, diverse, and controllable\nreactions that not only closely match the movements of the counterpart but also\nadhere to the textual guidance. Please find our webpage at\nhttps://xiyan-xu.github.io/MoReactWebPage.",
        "url": "http://arxiv.org/abs/2509.23911v1",
        "published_date": "2025-09-28T14:31:41+00:00",
        "updated_date": "2025-09-28T14:31:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiyan Xu",
            "Sirui Xu",
            "Yu-Xiong Wang",
            "Liang-Yan Gui"
        ],
        "tldr": "The paper introduces MoReact, a diffusion-based method for generating realistic human reaction motion sequences from textual descriptions of interaction scenarios, showing improved realism and controllability.",
        "tldr_zh": "该论文介绍了MoReact，一种基于扩散模型的方法，用于从交互场景的文本描述中生成逼真的人类反应运动序列，展示了改进的真实性和可控性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CrimEdit: Controllable Editing for Counterfactual Object Removal, Insertion, and Movement",
        "summary": "Recent works on object removal and insertion have enhanced their performance\nby handling object effects such as shadows and reflections, using diffusion\nmodels trained on counterfactual datasets. However, the performance impact of\napplying classifier-free guidance to handle object effects across removal and\ninsertion tasks within a unified model remains largely unexplored. To address\nthis gap and improve efficiency in composite editing, we propose CrimEdit,\nwhich jointly trains the task embeddings for removal and insertion within a\nsingle model and leverages them in a classifier-free guidance scheme --\nenhancing the removal of both objects and their effects, and enabling\ncontrollable synthesis of object effects during insertion. CrimEdit also\nextends these two task prompts to be applied to spatially distinct regions,\nenabling object movement (repositioning) within a single denoising step. By\nemploying both guidance techniques, extensive experiments show that CrimEdit\nachieves superior object removal, controllable effect insertion, and efficient\nobject movement without requiring additional training or separate removal and\ninsertion stages.",
        "url": "http://arxiv.org/abs/2509.23708v1",
        "published_date": "2025-09-28T07:41:25+00:00",
        "updated_date": "2025-09-28T07:41:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Boseong Jeon",
            "Junghyuk Lee",
            "Jimin Park",
            "Kwanyoung Kim",
            "Jingi Jung",
            "Sangwon Lee",
            "Hyunbo Shim"
        ],
        "tldr": "CrimEdit introduces a unified diffusion model with classifier-free guidance that jointly handles object removal, effect insertion, and movement in images, achieving superior performance without additional training.",
        "tldr_zh": "CrimEdit 引入了一个统一的扩散模型，该模型具有无分类器指导，可以共同处理图像中的物体移除、效果插入和移动，无需额外训练即可实现卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FlowLUT: Efficient Image Enhancement via Differentiable LUTs and Iterative Flow Matching",
        "summary": "Deep learning-based image enhancement methods face a fundamental trade-off\nbetween computational efficiency and representational capacity. For example,\nalthough a conventional three-dimensional Look-Up Table (3D LUT) can process a\ndegraded image in real time, it lacks representational flexibility and depends\nsolely on a fixed prior. To address this problem, we introduce FlowLUT, a novel\nend-to-end model that integrates the efficiency of LUTs, multiple priors, and\nthe parameter-independent characteristic of flow-matched reconstructed images.\nSpecifically, firstly, the input image is transformed in color space by a\ncollection of differentiable 3D LUTs (containing a large number of 3D LUTs with\ndifferent priors). Subsequently, a lightweight content-aware dynamically\npredicts fusion weights, enabling scene-adaptive color correction with\n$\\mathcal{O}(1)$ complexity. Next, a lightweight fusion prediction network runs\non multiple 3D LUTs, with $\\mathcal{O}(1)$ complexity for scene-adaptive color\ncorrection.Furthermore, to address the inherent representation limitations of\nLUTs, we design an innovative iterative flow matching method to restore local\nstructural details and eliminate artifacts. Finally, the entire model is\njointly optimized under a composite loss function enforcing perceptual and\nstructural fidelity. Extensive experimental results demonstrate the\neffectiveness of our method on three benchmarks.",
        "url": "http://arxiv.org/abs/2509.23608v1",
        "published_date": "2025-09-28T03:22:01+00:00",
        "updated_date": "2025-09-28T03:22:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liubing Hu",
            "Chen Wu",
            "Anrui Wang",
            "Dianjie Lu",
            "Guijuan Zhang",
            "Zhuoran Zheng"
        ],
        "tldr": "The paper introduces FlowLUT, a novel image enhancement method that combines the efficiency of 3D LUTs with iterative flow matching to achieve real-time, scene-adaptive color correction and detail restoration, outperforming existing methods on multiple benchmarks. It uses multiple 3D LUTs with a lightweight network to dynamically predict the fusion weights for scene-adaptive color correction.",
        "tldr_zh": "该论文介绍了一种名为FlowLUT的新型图像增强方法，它结合了3D LUT的效率和迭代流匹配，实现了实时、场景自适应的色彩校正和细节恢复，并在多个基准测试中优于现有方法。 FlowLUT使用多个含有不同先验的3D LUT，并通过轻量级网络动态预测融合权重以实现场景自适应色彩校正。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    }
]