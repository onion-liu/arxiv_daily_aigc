[
    {
        "title": "SemanticNVS: Improving Semantic Scene Understanding in Generative Novel View Synthesis",
        "summary": "We present SemanticNVS, a camera-conditioned multi-view diffusion model for novel view synthesis (NVS), which improves generation quality and consistency by integrating pre-trained semantic feature extractors. Existing NVS methods perform well for views near the input view, however, they tend to generate semantically implausible and distorted images under long-range camera motion, revealing severe degradation. We speculate that this degradation is due to current models failing to fully understand their conditioning or intermediate generated scene content. Here, we propose to integrate pre-trained semantic feature extractors to incorporate stronger scene semantics as conditioning to achieve high-quality generation even at distant viewpoints. We investigate two different strategies, (1) warped semantic features and (2) an alternating scheme of understanding and generation at each denoising step. Experimental results on multiple datasets demonstrate the clear qualitative and quantitative (4.69%-15.26% in FID) improvement over state-of-the-art alternatives.",
        "url": "http://arxiv.org/abs/2602.20079v1",
        "published_date": "2026-02-23T17:45:21+00:00",
        "updated_date": "2026-02-23T17:45:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinya Chen",
            "Christopher Wewer",
            "Jiahao Xie",
            "Xinting Hu",
            "Jan Eric Lenssen"
        ],
        "tldr": "SemanticNVS improves novel view synthesis by incorporating pre-trained semantic feature extractors to enhance semantic consistency and quality, particularly at distant viewpoints, showing improvements over existing methods.",
        "tldr_zh": "SemanticNVS 通过整合预训练的语义特征提取器来提升新视角合成效果，特别是在远距离视角下，从而增强语义一致性和质量，并优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ExpPortrait: Expressive Portrait Generation via Personalized Representation",
        "summary": "While diffusion models have shown great potential in portrait generation, generating expressive, coherent, and controllable cinematic portrait videos remains a significant challenge. Existing intermediate signals for portrait generation, such as 2D landmarks and parametric models, have limited disentanglement capabilities and cannot express personalized details due to their sparse or low-rank representation. Therefore, existing methods based on these models struggle to accurately preserve subject identity and expressions, hindering the generation of highly expressive portrait videos. To overcome these limitations, we propose a high-fidelity personalized head representation that more effectively disentangles expression and identity. This representation captures both static, subject-specific global geometry and dynamic, expression-related details. Furthermore, we introduce an expression transfer module to achieve personalized transfer of head pose and expression details between different identities. We use this sophisticated and highly expressive head model as a conditional signal to train a diffusion transformer (DiT)-based generator to synthesize richly detailed portrait videos. Extensive experiments on self- and cross-reenactment tasks demonstrate that our method outperforms previous models in terms of identity preservation, expression accuracy, and temporal stability, particularly in capturing fine-grained details of complex motion.",
        "url": "http://arxiv.org/abs/2602.19900v1",
        "published_date": "2026-02-23T14:41:35+00:00",
        "updated_date": "2026-02-23T14:41:35+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Junyi Wang",
            "Yudong Guo",
            "Boyang Guo",
            "Shengming Yang",
            "Juyong Zhang"
        ],
        "tldr": "The paper introduces a novel personalized head representation to improve the generation of expressive portrait videos via diffusion models, addressing limitations in existing methods regarding identity preservation and expression accuracy.",
        "tldr_zh": "该论文提出了一种新颖的个性化头部表示，旨在通过扩散模型改进富有表现力的人像视频生成，解决了现有方法在身份保留和表情准确性方面的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "One2Scene: Geometric Consistent Explorable 3D Scene Generation from a Single Image",
        "summary": "Generating explorable 3D scenes from a single image is a highly challenging problem in 3D vision. Existing methods struggle to support free exploration, often producing severe geometric distortions and noisy artifacts when the viewpoint moves far from the original perspective. We introduce \\textbf{One2Scene}, an effective framework that decomposes this ill-posed problem into three tractable sub-tasks to enable immersive explorable scene generation. We first use a panorama generator to produce anchor views from a single input image as initialization. Then, we lift these 2D anchors into an explicit 3D geometric scaffold via a generalizable, feed-forward Gaussian Splatting network. Instead of treating the panorama as a single image for reconstruction, we project it into multiple sparse anchor views and reformulate the reconstruction task as multi-view stereo matching, which allows us to leverage robust geometric priors learned from large-scale multi-view datasets. A bidirectional feature fusion module is used to enforce cross-view consistency, yielding an efficient and geometrically reliable scaffold. Finally, the scaffold serves as a strong prior for a novel view generator to produce photorealistic and geometrically accurate views at arbitrary cameras. By explicitly conditioning on a 3D-consistent scaffold to perform reconstruction, One2Scene works stably under large camera motions, supporting immersive scene exploration. Extensive experiments show that One2Scene substantially outperforms state-of-the-art methods in panorama depth estimation, feed-forward 360° reconstruction, and explorable 3D scene generation. Code and models will be released.",
        "url": "http://arxiv.org/abs/2602.19766v1",
        "published_date": "2026-02-23T12:15:54+00:00",
        "updated_date": "2026-02-23T12:15:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pengfei Wang",
            "Liyi Chen",
            "Zhiyuan Ma",
            "Yanjun Guo",
            "Guowen Zhang",
            "Lei Zhang"
        ],
        "tldr": "The paper introduces One2Scene, a framework for generating explorable 3D scenes from a single image by decomposing the problem into panorama generation, 3D geometric scaffold creation using Gaussian Splatting, and novel view generation, significantly outperforming existing methods.",
        "tldr_zh": "本文介绍了一种名为One2Scene的框架，该框架通过将问题分解为全景生成、使用高斯溅射创建3D几何支架和新视角生成，从单张图像生成可探索的3D场景，显著优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ConceptPrism: Concept Disentanglement in Personalized Diffusion Models via Residual Token Optimization",
        "summary": "Personalized text-to-image generation suffers from concept entanglement, where irrelevant residual information from reference images is captured, leading to a trade-off between concept fidelity and text alignment. Recent disentanglement approaches attempt to solve this utilizing manual guidance, such as linguistic cues or segmentation masks, which limits their applicability and fails to fully articulate the target concept. In this paper, we propose ConceptPrism, a novel framework that automatically disentangles the shared visual concept from image-specific residuals by comparing images within a set. Our method jointly optimizes a target token and image-wise residual tokens using two complementary objectives: a reconstruction loss to ensure fidelity, and a novel exclusion loss that compels residual tokens to discard the shared concept. This process allows the target token to capture the pure concept without direct supervision. Extensive experiments demonstrate that ConceptPrism effectively resolves concept entanglement, achieving a significantly improved trade-off between fidelity and alignment.",
        "url": "http://arxiv.org/abs/2602.19575v1",
        "published_date": "2026-02-23T07:46:19+00:00",
        "updated_date": "2026-02-23T07:46:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minseo Kim",
            "Minchan Kwon",
            "Dongyeun Lee",
            "Yunho Jeon",
            "Junmo Kim"
        ],
        "tldr": "ConceptPrism is a novel framework for disentangling shared visual concepts from image-specific residuals in personalized diffusion models, achieving a better trade-off between fidelity and text alignment without manual guidance.",
        "tldr_zh": "ConceptPrism 是一个新的框架，用于在个性化扩散模型中将共享视觉概念从图像特定的残差中解耦，无需人工指导即可在保真度和文本对齐之间实现更好的平衡。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Variational Trajectory Optimization of Anisotropic Diffusion Schedules",
        "summary": "We introduce a variational framework for diffusion models with anisotropic noise schedules parameterized by a matrix-valued path $M_t(θ)$ that allocates noise across subspaces. Central to our framework is a trajectory-level objective that jointly trains the score network and learns $M_t(θ)$, which encompasses general parameterization classes of matrix-valued noise schedules. We further derive an estimator for the derivative with respect to $θ$ of the score that enables efficient optimization of the $M_t(θ)$ schedule. For inference, we develop an efficiently-implementable reverse-ODE solver that is an anisotropic generalization of the second-order Heun discretization algorithm. Across CIFAR-10, AFHQv2, FFHQ, and ImageNet-64, our method consistently improves upon the baseline EDM model in all NFE regimes. Code is available at https://github.com/lizeyu090312/anisotropic-diffusion-paper.",
        "url": "http://arxiv.org/abs/2602.19512v1",
        "published_date": "2026-02-23T04:56:41+00:00",
        "updated_date": "2026-02-23T04:56:41+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Pengxi Liu",
            "Zeyu Michael Li",
            "Xiang Cheng"
        ],
        "tldr": "This paper introduces a variational framework for diffusion models using anisotropic noise schedules, achieving improved performance across several image datasets compared to the EDM baseline.",
        "tldr_zh": "本文介绍了一种使用各向异性噪声计划的扩散模型的变分框架，与 EDM 基线相比，在多个图像数据集中实现了更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MICON-Bench: Benchmarking and Enhancing Multi-Image Context Image Generation in Unified Multimodal Models",
        "summary": "Recent advancements in Unified Multimodal Models (UMMs) have enabled remarkable image understanding and generation capabilities. However, while models like Gemini-2.5-Flash-Image show emerging abilities to reason over multiple related images, existing benchmarks rarely address the challenges of multi-image context generation, focusing mainly on text-to-image or single-image editing tasks. In this work, we introduce \\textbf{MICON-Bench}, a comprehensive benchmark covering six tasks that evaluate cross-image composition, contextual reasoning, and identity preservation. We further propose an MLLM-driven Evaluation-by-Checkpoint framework for automatic verification of semantic and visual consistency, where multimodal large language model (MLLM) serves as a verifier. Additionally, we present \\textbf{Dynamic Attention Rebalancing (DAR)}, a training-free, plug-and-play mechanism that dynamically adjusts attention during inference to enhance coherence and reduce hallucinations. Extensive experiments on various state-of-the-art open-source models demonstrate both the rigor of MICON-Bench in exposing multi-image reasoning challenges and the efficacy of DAR in improving generation quality and cross-image coherence. Github: https://github.com/Angusliuuu/MICON-Bench.",
        "url": "http://arxiv.org/abs/2602.19497v1",
        "published_date": "2026-02-23T04:32:52+00:00",
        "updated_date": "2026-02-23T04:32:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingrui Wu",
            "Hang Liu",
            "Jiayi Ji",
            "Xiaoshuai Sun",
            "Rongrong Ji"
        ],
        "tldr": "The paper introduces MICON-Bench, a benchmark for evaluating multi-image context generation in multimodal models, and proposes Dynamic Attention Rebalancing (DAR) to improve generation quality and coherence.",
        "tldr_zh": "该论文介绍了MICON-Bench，一个用于评估多模态模型中多图像上下文生成的基准，并提出了动态注意力重平衡（DAR）来提高生成质量和连贯性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Laplacian Multi-scale Flow Matching for Generative Modeling",
        "summary": "In this paper, we present Laplacian multiscale flow matching (LapFlow), a novel framework that enhances flow matching by leveraging multi-scale representations for image generative modeling. Our approach decomposes images into Laplacian pyramid residuals and processes different scales in parallel through a mixture-of-transformers (MoT) architecture with causal attention mechanisms. Unlike previous cascaded approaches that require explicit renoising between scales, our model generates multi-scale representations in parallel, eliminating the need for bridging processes. The proposed multi-scale architecture not only improves generation quality but also accelerates the sampling process and promotes scaling flow matching methods. Through extensive experimentation on CelebA-HQ and ImageNet, we demonstrate that our method achieves superior sample quality with fewer GFLOPs and faster inference compared to single-scale and multi-scale flow matching baselines. The proposed model scales effectively to high-resolution generation (up to 1024$\\times$1024) while maintaining lower computational overhead.",
        "url": "http://arxiv.org/abs/2602.19461v1",
        "published_date": "2026-02-23T03:09:56+00:00",
        "updated_date": "2026-02-23T03:09:56+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zelin Zhao",
            "Petr Molodyk",
            "Haotian Xue",
            "Yongxin Chen"
        ],
        "tldr": "The paper introduces LapFlow, a novel flow matching framework that uses Laplacian pyramid decomposition and parallel processing of different scales with a mixture-of-transformers architecture to improve image generation quality, speed, and scalability while reducing computational overhead.",
        "tldr_zh": "该论文介绍了 LapFlow，一种新颖的流匹配框架，它使用拉普拉斯金字塔分解和混合Transformer架构并行处理不同尺度，以提高图像生成质量、速度和可扩展性，同时降低计算开销。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Referring Layer Decomposition",
        "summary": "Precise, object-aware control over visual content is essential for advanced image editing and compositional generation. Yet, most existing approaches operate on entire images holistically, limiting the ability to isolate and manipulate individual scene elements. In contrast, layered representations, where scenes are explicitly separated into objects, environmental context, and visual effects, provide a more intuitive and structured framework for interpreting and editing visual content. To bridge this gap and enable both compositional understanding and controllable editing, we introduce the Referring Layer Decomposition (RLD) task, which predicts complete RGBA layers from a single RGB image, conditioned on flexible user prompts, such as spatial inputs (e.g., points, boxes, masks), natural language descriptions, or combinations thereof. At the core is the RefLade, a large-scale dataset comprising 1.11M image-layer-prompt triplets produced by our scalable data engine, along with 100K manually curated, high-fidelity layers. Coupled with a perceptually grounded, human-preference-aligned automatic evaluation protocol, RefLade establishes RLD as a well-defined and benchmarkable research task. Building on this foundation, we present RefLayer, a simple baseline designed for prompt-conditioned layer decomposition, achieving high visual fidelity and semantic alignment. Extensive experiments show our approach enables effective training, reliable evaluation, and high-quality image decomposition, while exhibiting strong zero-shot generalization capabilities.",
        "url": "http://arxiv.org/abs/2602.19358v1",
        "published_date": "2026-02-22T22:05:17+00:00",
        "updated_date": "2026-02-22T22:05:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fangyi Chen",
            "Yaojie Shen",
            "Lu Xu",
            "Ye Yuan",
            "Shu Zhang",
            "Yulei Niu",
            "Longyin Wen"
        ],
        "tldr": "This paper introduces Referring Layer Decomposition (RLD), a new task for predicting RGBA layers from a single RGB image based on various user prompts. They also present a large-scale dataset (RefLade) and a baseline model (RefLayer).",
        "tldr_zh": "该论文介绍了 Referring Layer Decomposition (RLD)，这是一个新的任务，旨在使用各种用户提示从单个 RGB 图像中预测 RGBA 层。 他们还提出了一个大规模数据集 (RefLade) 和一个基线模型 (RefLayer)。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MultiDiffSense: Diffusion-Based Multi-Modal Visuo-Tactile Image Generation Conditioned on Object Shape and Contact Pose",
        "summary": "Acquiring aligned visuo-tactile datasets is slow and costly, requiring specialised hardware and large-scale data collection. Synthetic generation is promising, but prior methods are typically single-modality, limiting cross-modal learning. We present MultiDiffSense, a unified diffusion model that synthesises images for multiple vision-based tactile sensors (ViTac, TacTip, ViTacTip) within a single architecture. Our approach uses dual conditioning on CAD-derived, pose-aligned depth maps and structured prompts that encode sensor type and 4-DoF contact pose, enabling controllable, physically consistent multi-modal synthesis. Evaluating on 8 objects (5 seen, 3 novel) and unseen poses, MultiDiffSense outperforms a Pix2Pix cGAN baseline in SSIM by +36.3% (ViTac), +134.6% (ViTacTip), and +64.7% (TacTip). For downstream 3-DoF pose estimation, mixing 50% synthetic with 50% real halves the required real data while maintaining competitive performance. MultiDiffSense alleviates the data-collection bottleneck in tactile sensing and enables scalable, controllable multi-modal dataset generation for robotic applications.",
        "url": "http://arxiv.org/abs/2602.19348v1",
        "published_date": "2026-02-22T21:31:24+00:00",
        "updated_date": "2026-02-22T21:31:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sirine Bhouri",
            "Lan Wei",
            "Jian-Qing Zheng",
            "Dandan Zhang"
        ],
        "tldr": "The paper introduces MultiDiffSense, a diffusion model for generating multi-modal visuo-tactile images conditioned on object shape and contact pose, addressing the data scarcity issue in tactile sensing and enabling scalable dataset generation for robotics.",
        "tldr_zh": "该论文介绍了 MultiDiffSense，一个基于扩散模型的生成多模态视觉触觉图像的模型，以物体形状和接触姿势为条件，解决了触觉传感中的数据稀缺问题，并为机器人技术实现了可扩展的数据集生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ChimeraLoRA: Multi-Head LoRA-Guided Synthetic Datasets",
        "summary": "Beyond general recognition tasks, specialized domains including privacy-constrained medical applications and fine-grained settings often encounter data scarcity, especially for tail classes. To obtain less biased and more reliable models under such scarcity, practitioners leverage diffusion models to supplement underrepresented regions of real data. Specifically, recent studies fine-tune pretrained diffusion models with LoRA on few-shot real sets to synthesize additional images. While an image-wise LoRA trained on a single image captures fine-grained details yet offers limited diversity, a class-wise LoRA trained over all shots produces diverse images as it encodes class priors yet tends to overlook fine details. To combine both benefits, we separate the adapter into a class-shared LoRA~$A$ for class priors and per-image LoRAs~$\\mathcal{B}$ for image-specific characteristics. To expose coherent class semantics in the shared LoRA~$A$, we propose a semantic boosting by preserving class bounding boxes during training. For generation, we compose $A$ with a mixture of $\\mathcal{B}$ using coefficients drawn from a Dirichlet distribution. Across diverse datasets, our synthesized images are both diverse and detail-rich while closely aligning with the few-shot real distribution, yielding robust gains in downstream classification accuracy.",
        "url": "http://arxiv.org/abs/2602.19708v1",
        "published_date": "2026-02-23T10:59:41+00:00",
        "updated_date": "2026-02-23T10:59:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hoyoung Kim",
            "Minwoo Jang",
            "Jabin Koo",
            "Sangdoo Yun",
            "Jungseul Ok"
        ],
        "tldr": "The paper introduces ChimeraLoRA, a novel approach for generating synthetic data by mixing class-wise and image-wise LoRA adapters to enhance data scarcity scenarios, particularly for fine-grained classification tasks.",
        "tldr_zh": "该论文介绍了 ChimeraLoRA，一种通过混合类级别和图像级别的 LoRA 适配器来生成合成数据的新方法，以增强数据稀缺场景，尤其适用于细粒度分类任务。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]