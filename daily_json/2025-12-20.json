[
    {
        "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
        "summary": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.",
        "url": "http://arxiv.org/abs/2512.16924v1",
        "published_date": "2025-12-18T18:59:59+00:00",
        "updated_date": "2025-12-18T18:59:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanlin Wang",
            "Hao Ouyang",
            "Qiuyu Wang",
            "Yue Yu",
            "Yihao Meng",
            "Wen Wang",
            "Ka Leong Cheng",
            "Shuailei Ma",
            "Qingyan Bai",
            "Yixuan Li",
            "Cheng Chen",
            "Yanhong Zeng",
            "Xing Zhu",
            "Yujun Shen",
            "Qifeng Chen"
        ],
        "tldr": "The paper introduces WorldCanvas, a multimodal framework for generating controllable world events using text, trajectories, and reference images, enabling richer and more interactive simulations than text-only or trajectory-controlled methods.",
        "tldr_zh": "该论文介绍了WorldCanvas，一个使用文本、轨迹和参考图像生成可控世界事件的多模态框架，与仅使用文本或轨迹控制的方法相比，可实现更丰富、更具交互性的模拟。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Next-Embedding Prediction Makes Strong Vision Learners",
        "summary": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.",
        "url": "http://arxiv.org/abs/2512.16922v1",
        "published_date": "2025-12-18T18:59:58+00:00",
        "updated_date": "2025-12-18T18:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sihan Xu",
            "Ziqiao Ma",
            "Wenhao Chai",
            "Xuweiyi Chen",
            "Weiyang Jin",
            "Joyce Chai",
            "Saining Xie",
            "Stella X. Yu"
        ],
        "tldr": "This paper introduces Next-Embedding Predictive Autoregression (NEPA), a new self-supervised learning method for vision that predicts future patch embeddings, achieving strong ImageNet classification and ADE20K segmentation results. It eschews traditional methods like pixel reconstruction or contrastive learning.",
        "tldr_zh": "本文介绍了一种新的视觉自监督学习方法，即下一嵌入预测自回归 (NEPA)，它通过预测未来的图像块嵌入来实现，在 ImageNet 分类和 ADE20K 分割上取得了很好的效果。它避免了传统的像素重建或对比学习等方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
        "summary": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
        "url": "http://arxiv.org/abs/2512.16920v1",
        "published_date": "2025-12-18T18:59:57+00:00",
        "updated_date": "2025-12-18T18:59:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jinjie Mai",
            "Chaoyang Wang",
            "Guocheng Gordon Qian",
            "Willi Menapace",
            "Sergey Tulyakov",
            "Bernard Ghanem",
            "Peter Wonka",
            "Ashkan Mirzaei"
        ],
        "tldr": "EasyV2V is a new framework for instruction-based video editing that leverages diverse data composition and a simplified model design based on pre-trained text-to-video models, achieving state-of-the-art results.",
        "tldr_zh": "EasyV2V是一个新的基于指令的视频编辑框架，它利用多样的数据组成和一个基于预训练文本到视频模型的简化模型设计，实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SFTok: Bridging the Performance Gap in Discrete Tokenizers",
        "summary": "Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \\textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \\textbf{self-forcing guided visual reconstruction} and \\textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).",
        "url": "http://arxiv.org/abs/2512.16910v1",
        "published_date": "2025-12-18T18:59:04+00:00",
        "updated_date": "2025-12-18T18:59:04+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Qihang Rao",
            "Borui Zhang",
            "Wenzhao Zheng",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "tldr": "The paper introduces SFTok, a novel discrete image tokenizer with a multi-step iterative mechanism that achieves state-of-the-art image reconstruction quality and demonstrates exceptional performance in class-to-image generation, addressing the performance gap between discrete and continuous tokenizers.",
        "tldr_zh": "该论文介绍了 SFTok，一种新型离散图像标记器，具有多步迭代机制，实现了最先进的图像重建质量，并在类到图像生成任务中表现出色，解决了离散和连续标记器之间的性能差距。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
        "summary": "Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io",
        "url": "http://arxiv.org/abs/2512.16864v1",
        "published_date": "2025-12-18T18:34:23+00:00",
        "updated_date": "2025-12-18T18:34:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianyuan Qu",
            "Lei Ke",
            "Xiaohang Zhan",
            "Longxiang Tang",
            "Yuqi Liu",
            "Bohao Peng",
            "Bei Yu",
            "Dong Yu",
            "Jiaya Jia"
        ],
        "tldr": "The paper introduces RePlan, a plan-then-execute framework for instruction-based image editing that addresses challenges in complex scenarios by using a vision-language planner and a diffusion editor with region-specific modifications, outperforming existing methods on a new benchmark.",
        "tldr_zh": "本文介绍了一种名为RePlan的图像编辑框架，该框架采用“先规划后执行”的方式，通过视觉-语言规划器和扩散编辑器实现基于指令的图像编辑。RePlan着重解决复杂场景中的挑战，并通过区域特定的修改超越了现有方法，并在一个新的基准测试中表现出色。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Sceniris: A Fast Procedural Scene Generation Framework",
        "summary": "Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris",
        "url": "http://arxiv.org/abs/2512.16896v1",
        "published_date": "2025-12-18T18:55:03+00:00",
        "updated_date": "2025-12-18T18:55:03+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Jinghuan Shang",
            "Harsh Patel",
            "Ran Gong",
            "Karl Schmeckpeper"
        ],
        "tldr": "Sceniris is a new framework for fast procedural 3D scene generation, claiming a significant speedup over previous methods, making it suitable for large-scale dataset creation for Physical AI and generative models.",
        "tldr_zh": "Sceniris 是一个用于快速程序化 3D 场景生成的新框架，声称比以前的方法有显著的加速，使其适用于为物理人工智能和生成模型创建大规模数据集。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]