[
    {
        "title": "CHAI: CacHe Attention Inference for text2video",
        "summary": "Text-to-video diffusion models deliver impressive results but remain slow because of the sequential denoising of 3D latents. Existing approaches to speed up inference either require expensive model retraining or use heuristic-based step skipping, which struggles to maintain video quality as the number of denoising steps decreases. Our work, CHAI, aims to use cross-inference caching to reduce latency while maintaining video quality. We introduce Cache Attention as an effective method for attending to shared objects/scenes across cross-inference latents. This selective attention mechanism enables effective reuse of cached latents across semantically related prompts, yielding high cache hit rates. We show that it is possible to generate high-quality videos using Cache Attention with as few as 8 denoising steps. When integrated into the overall system, CHAI is 1.65x - 3.35x faster than baseline OpenSora 1.2 while maintaining video quality.",
        "url": "http://arxiv.org/abs/2602.16132v1",
        "published_date": "2026-02-18T01:53:29+00:00",
        "updated_date": "2026-02-18T01:53:29+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Joel Mathew Cherian",
            "Ashutosh Muralidhara Bharadwaj",
            "Vima Gupta",
            "Anand Padmanabha Iyer"
        ],
        "tldr": "The paper introduces CHAI, a cross-inference caching method with Cache Attention, to accelerate text-to-video generation by reusing latents from semantically similar prompts, achieving a speedup of 1.65x - 3.35x over OpenSora 1.2 while maintaining video quality.",
        "tldr_zh": "该论文介绍了CHAI，一种使用缓存注意力的跨推理缓存方法，通过重用语义相似提示的潜在特征来加速文本到视频的生成，与OpenSora 1.2相比速度提高了1.65倍 - 3.35倍，同时保持了视频质量。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "LGQ: Learning Discretization Geometry for Scalable and Stable Image Tokenization",
        "summary": "Discrete image tokenization is a key bottleneck for scalable visual generation: a tokenizer must remain compact for efficient latent-space priors while preserving semantic structure and using discrete capacity effectively. Existing quantizers face a trade-off: vector-quantized tokenizers learn flexible geometries but often suffer from biased straight-through optimization, codebook under-utilization, and representation collapse at large vocabularies. Structured scalar or implicit tokenizers ensure stable, near-complete utilization by design, yet rely on fixed discretization geometries that may allocate capacity inefficiently under heterogeneous latent statistics.\n  We introduce Learnable Geometric Quantization (LGQ), a discrete image tokenizer that learns discretization geometry end-to-end. LGQ replaces hard nearest-neighbor lookup with temperature-controlled soft assignments, enabling fully differentiable training while recovering hard assignments at inference. The assignments correspond to posterior responsibilities of an isotropic Gaussian mixture and minimize a variational free-energy objective, provably converging to nearest-neighbor quantization in the low-temperature limit. LGQ combines a token-level peakedness regularizer with a global usage regularizer to encourage confident yet balanced code utilization without imposing rigid grids.\n  Under a controlled VQGAN-style backbone on ImageNet across multiple vocabulary sizes, LGQ achieves stable optimization and balanced utilization. At 16K codebook size, LGQ improves rFID by 11.88% over FSQ while using 49.96% fewer active codes, and improves rFID by 6.06% over SimVQ with 49.45% lower effective representation rate, achieving comparable fidelity with substantially fewer active entries. Our GitHub repository is available at: https://github.com/KurbanIntelligenceLab/LGQ",
        "url": "http://arxiv.org/abs/2602.16086v1",
        "published_date": "2026-02-17T23:20:26+00:00",
        "updated_date": "2026-02-17T23:20:26+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Idil Bilge Altun",
            "Mert Onur Cakiroglu",
            "Elham Buxton",
            "Mehmet Dalkilic",
            "Hasan Kurban"
        ],
        "tldr": "The paper introduces Learnable Geometric Quantization (LGQ), a novel discrete image tokenizer that learns discretization geometry end-to-end, achieving stable optimization and balanced code utilization compared to existing quantization methods, leading to improved image generation.",
        "tldr_zh": "该论文介绍了一种新的离散图像标记器，即可学习几何量化（LGQ），它能够端到端地学习离散化几何结构，与现有的量化方法相比，实现了稳定的优化和平衡的代码利用率，从而改进了图像生成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "B-DENSE: Branching For Dense Ensemble Network Learning",
        "summary": "Inspired by non-equilibrium thermodynamics, diffusion models have achieved state-of-the-art performance in generative modeling. However, their iterative sampling nature results in high inference latency. While recent distillation techniques accelerate sampling, they discard intermediate trajectory steps. This sparse supervision leads to a loss of structural information and introduces significant discretization errors. To mitigate this, we propose B-DENSE, a novel framework that leverages multi-branch trajectory alignment. We modify the student architecture to output $K$-fold expanded channels, where each subset corresponds to a specific branch representing a discrete intermediate step in the teacher's trajectory. By training these branches to simultaneously map to the entire sequence of the teacher's target timesteps, we enforce dense intermediate trajectory alignment. Consequently, the student model learns to navigate the solution space from the earliest stages of training, demonstrating superior image generation quality compared to baseline distillation frameworks.",
        "url": "http://arxiv.org/abs/2602.15971v1",
        "published_date": "2026-02-17T19:40:58+00:00",
        "updated_date": "2026-02-17T19:40:58+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.NE"
        ],
        "authors": [
            "Cherish Puniani",
            "Tushar Kumar",
            "Arnav Bendre",
            "Gaurav Kumar",
            "Shree Singhi"
        ],
        "tldr": "The paper introduces B-DENSE, a novel diffusion model distillation framework that uses multi-branch trajectory alignment to enforce dense intermediate trajectory alignment, improving image generation quality.",
        "tldr_zh": "该论文介绍了 B-DENSE，一种新颖的扩散模型蒸馏框架，它利用多分支轨迹对齐来强制进行密集的中间轨迹对齐，从而提高图像生成质量。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation",
        "summary": "Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.",
        "url": "http://arxiv.org/abs/2602.15819v1",
        "published_date": "2026-02-17T18:55:03+00:00",
        "updated_date": "2026-02-17T18:55:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hui Ren",
            "Yuval Alaluf",
            "Omer Bar Tal",
            "Alexander Schwing",
            "Antonio Torralba",
            "Yael Vinker"
        ],
        "tldr": "The paper introduces VideoSketcher, a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models by representing sketches as short videos guided by text instructions for stroke ordering, and employs a two-stage fine-tuning strategy to decouple stroke ordering from visual appearance learning.",
        "tldr_zh": "该论文介绍了VideoSketcher，一种高效的数据方法，用于顺序草图生成，它通过将草图表示为短视频，并以文本指令引导笔画顺序，从而适应了预训练的文本到视频扩散模型，并采用两阶段微调策略来分离笔画顺序的学习和视觉外观的学习。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models",
        "summary": "Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of \"generate-understand-regenerate\". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.",
        "url": "http://arxiv.org/abs/2602.15772v1",
        "published_date": "2026-02-17T18:04:13+00:00",
        "updated_date": "2026-02-17T18:04:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sen Ye",
            "Mengde Xu",
            "Shuyang Gu",
            "Di He",
            "Liwei Wang",
            "Han Hu"
        ],
        "tldr": "This paper introduces the Reason-Reflect-Refine (R3) framework to address the trade-off between generation and understanding in multimodal models, by explicitly incorporating understanding into the generation process.",
        "tldr_zh": "该论文介绍了一种名为Reason-Reflect-Refine (R3)的框架，通过在生成过程中显式地纳入理解能力，以解决多模态模型中生成能力和理解能力之间的权衡。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]