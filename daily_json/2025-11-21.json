[
    {
        "title": "Progressive Supernet Training for Efficient Visual Autoregressive Modeling",
        "summary": "Visual Auto-Regressive (VAR) models significantly reduce inference steps through the \"next-scale\" prediction paradigm. However, progressive multi-scale generation incurs substantial memory overhead due to cumulative KV caching, limiting practical deployment.\n  We observe a scale-depth asymmetric dependency in VAR: early scales exhibit extreme sensitivity to network depth, while later scales remain robust to depth reduction. Inspired by this, we propose VARiant: by equidistant sampling, we select multiple subnets ranging from 16 to 2 layers from the original 30-layer VAR-d30 network. Early scales are processed by the full network, while later scales utilize subnet. Subnet and the full network share weights, enabling flexible depth adjustment within a single model.\n  However, weight sharing between subnet and the entire network can lead to optimization conflicts. To address this, we propose a progressive training strategy that breaks through the Pareto frontier of generation quality for both subnets and the full network under fixed-ratio training, achieving joint optimality.\n  Experiments on ImageNet demonstrate that, compared to the pretrained VAR-d30 (FID 1.95), VARiant-d16 and VARiant-d8 achieve nearly equivalent quality (FID 2.05/2.12) while reducing memory consumption by 40-65%. VARiant-d2 achieves 3.5 times speedup and 80% memory reduction at moderate quality cost (FID 2.97). In terms of deployment, VARiant's single-model architecture supports zero-cost runtime depth switching and provides flexible deployment options from high quality to extreme efficiency, catering to diverse application scenarios.",
        "url": "http://arxiv.org/abs/2511.16546v1",
        "published_date": "2025-11-20T16:59:24+00:00",
        "updated_date": "2025-11-20T16:59:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyue Chen",
            "Yuling Shi",
            "Kaiyuan Li",
            "Huandong Wang",
            "Yong Li",
            "Xiaodong Gu",
            "Xinlei Chen",
            "Mingbao Lin"
        ],
        "tldr": "The paper introduces VARiant, a method for efficient visual autoregressive modeling using progressive supernet training and weight sharing, achieving significant memory reduction and speedup with minimal quality loss.",
        "tldr_zh": "该论文介绍了VARiant，一种通过渐进式超网络训练和权重共享实现高效视觉自回归建模的方法，在质量损失最小的情况下，显著减少了内存消耗和加速了推理。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Pluggable Pruning with Contiguous Layer Distillation for Diffusion Transformers",
        "summary": "Diffusion Transformers (DiTs) have shown exceptional performance in image generation, yet their large parameter counts incur high computational costs, impeding deployment in resource-constrained settings. To address this, we propose Pluggable Pruning with Contiguous Layer Distillation (PPCL), a flexible structured pruning framework specifically designed for DiT architectures. First, we identify redundant layer intervals through a linear probing mechanism combined with the first-order differential trend analysis of similarity metrics. Subsequently, we propose a plug-and-play teacher-student alternating distillation scheme tailored to integrate depth-wise and width-wise pruning within a single training phase. This distillation framework enables flexible knowledge transfer across diverse pruning ratios, eliminating the need for per-configuration retraining. Extensive experiments on multiple Multi-Modal Diffusion Transformer architecture models demonstrate that PPCL achieves a 50\\% reduction in parameter count compared to the full model, with less than 3\\% degradation in key objective metrics. Notably, our method maintains high-quality image generation capabilities while achieving higher compression ratios, rendering it well-suited for resource-constrained environments. The open-source code, checkpoints for PPCL can be found at the following link: https://github.com/OPPO-Mente-Lab/Qwen-Image-Pruning.",
        "url": "http://arxiv.org/abs/2511.16156v1",
        "published_date": "2025-11-20T08:53:07+00:00",
        "updated_date": "2025-11-20T08:53:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jian Ma",
            "Qirong Peng",
            "Xujie Zhu",
            "Peixing Xie",
            "Chen Chen",
            "Haonan Lu"
        ],
        "tldr": "The paper introduces PPCL, a pruning framework for Diffusion Transformers that achieves significant parameter reduction with minimal performance degradation through a plug-and-play distillation scheme, making DiTs more suitable for resource-constrained environments.",
        "tldr_zh": "该论文提出了PPCL，一个用于扩散Transformer的剪枝框架，通过即插即用的蒸馏方案，在性能损失最小的情况下显著减少参数量，使DiT更适合资源受限环境。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Decoupling Complexity from Scale in Latent Diffusion Model",
        "summary": "Existing latent diffusion models typically couple scale with content complexity, using more latent tokens to represent higher-resolution images or higher-frame rate videos. However, the latent capacity required to represent visual data primarily depends on content complexity, with scale serving only as an upper bound. Motivated by this observation, we propose DCS-LDM, a novel paradigm for visual generation that decouples information complexity from scale. DCS-LDM constructs a hierarchical, scale-independent latent space that models sample complexity through multi-level tokens and supports decoding to arbitrary resolutions and frame rates within a fixed latent representation. This latent space enables DCS-LDM to achieve a flexible computation-quality tradeoff. Furthermore, by decomposing structural and detailed information across levels, DCS-LDM supports a progressive coarse-to-fine generation paradigm. Experimental results show that DCS-LDM delivers performance comparable to state-of-the-art methods while offering flexible generation across diverse scales and visual qualities.",
        "url": "http://arxiv.org/abs/2511.16117v1",
        "published_date": "2025-11-20T07:20:33+00:00",
        "updated_date": "2025-11-20T07:20:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianxiong Zhong",
            "Xingye Tian",
            "Xuebo Wang",
            "Boyuan Jiang",
            "Xin Tao",
            "Pengfei Wan"
        ],
        "tldr": "The paper introduces DCS-LDM, a Latent Diffusion Model that decouples content complexity from scale by using a hierarchical latent space, enabling flexible generation across resolutions and frame rates.",
        "tldr_zh": "该论文提出了DCS-LDM，一种解耦内容复杂度和尺度的潜在扩散模型，通过使用分层潜在空间，实现了在不同分辨率和帧率下的灵活生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers",
        "summary": "Visual autoregressive modeling (VAR) via next-scale prediction has emerged as a scalable image generation paradigm. While Key and Value (KV) caching in large language models (LLMs) has been extensively studied, next-scale prediction presents unique challenges, and KV caching design for next-scale based VAR transformers remains largely unexplored. A major bottleneck is the excessive KV memory growth with the increasing number of scales-severely limiting scalability. Our systematic investigation reveals that: (1) Attending to tokens from local scales significantly contributes to generation quality (2) Allocating a small amount of memory for the coarsest scales, termed as condensed scales, stabilizes multi-scale image generation (3) Strong KV similarity across finer scales is predominantly observed in cache-efficient layers, whereas cache-demanding layers exhibit weaker inter-scale similarity. Based on the observations, we introduce AMS-KV, a scale-adaptive KV caching policy for next-scale prediction in VAR models. AMS-KV prioritizes storing KVs from condensed and local scales, preserving the most relevant tokens to maintain generation quality. It further optimizes KV cache utilization and computational efficiency identifying cache-demanding layers through inter-scale similarity analysis. Compared to the vanilla next-scale prediction-based VAR models, AMS-KV reduces KV cache usage by up to 84.83% and self-attention latency by 60.48%. Moreover, when the baseline VAR-d30 model encounters out-of-memory failures at a batch size of 128, AMS-KV enables stable scaling to a batch size of 256 with improved throughput.",
        "url": "http://arxiv.org/abs/2511.16047v1",
        "published_date": "2025-11-20T05:10:12+00:00",
        "updated_date": "2025-11-20T05:10:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Boxun Xu",
            "Yu Wang",
            "Zihu Wang",
            "Peng Li"
        ],
        "tldr": "This paper introduces AMS-KV, an adaptive KV caching policy for visual autoregressive transformers using next-scale prediction. It addresses the memory bottleneck in multi-scale image generation by prioritizing KVs from condensed and local scales and optimizing cache utilization based on inter-scale similarity.",
        "tldr_zh": "本文介绍了一种自适应KV缓存策略AMS-KV，用于使用下一尺度预测的视觉自回归Transformer。它通过优先存储来自精简和局部尺度的KV，并基于尺度间相似性优化缓存利用率，从而解决了多尺度图像生成中的内存瓶颈。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "First Frame Is the Place to Go for Video Content Customization",
        "summary": "What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.",
        "url": "http://arxiv.org/abs/2511.15700v1",
        "published_date": "2025-11-19T18:56:50+00:00",
        "updated_date": "2025-11-19T18:56:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingxi Chen",
            "Zongxia Li",
            "Zhichao Liu",
            "Guangyao Shi",
            "Xiyang Wu",
            "Fuxiao Liu",
            "Cornelia Fermuller",
            "Brandon Y. Feng",
            "Yiannis Aloimonos"
        ],
        "tldr": "The paper reveals that video generation models implicitly use the first frame as a memory buffer for visual entities, enabling customization with only a few training examples without architectural changes.",
        "tldr_zh": "该论文揭示了视频生成模型隐式地将第一帧用作视觉实体的内存缓冲区，只需少量训练样本即可实现定制，而无需架构更改。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving",
        "summary": "Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: https://ocean-luna.github.io/LiSTAR.gitub.io.",
        "url": "http://arxiv.org/abs/2511.16049v1",
        "published_date": "2025-11-20T05:11:22+00:00",
        "updated_date": "2025-11-20T05:11:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pei Liu",
            "Songtao Wang",
            "Lang Zhang",
            "Xingyue Peng",
            "Yuandong Lyu",
            "Jiaxin Deng",
            "Songxin Lu",
            "Weiliang Ma",
            "Xueyang Zhang",
            "Yifei Zhan",
            "XianPeng Lang",
            "Jun Ma"
        ],
        "tldr": "LiSTAR introduces a novel generative world model for 4D LiDAR sequences using a Hybrid-Cylindrical-Spherical representation and Spatio-Temporal Attention to achieve state-of-the-art performance in reconstruction, prediction, and conditional generation of LiDAR data for autonomous driving simulation.",
        "tldr_zh": "LiSTAR 提出了一种新的生成式世界模型，用于 4D 激光雷达序列，它使用混合圆柱-球面表示和时空注意力机制，在自动驾驶模拟的激光雷达数据的重建、预测和条件生成方面实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs",
        "summary": "Realistic and smooth full-body tracking is crucial for immersive AR/VR applications. Existing systems primarily track head and hands via Head Mounted Devices (HMDs) and controllers, making the 3D full-body reconstruction in-complete. One potential approach is to generate the full-body motions from sparse inputs collected from limited sensors using a Neural Network (NN) model. In this paper, we propose a novel method based on a multi-layer perceptron (MLP) backbone that is enhanced with residual connections and a novel NN-component called Memory-Block. In particular, Memory-Block represents missing sensor data with trainable code-vectors, which are combined with the sparse signals from previous time instances to improve the temporal consistency. Furthermore, we formulate our solution as a multi-task learning problem, allowing our MLP-backbone to learn robust representations that boost accuracy. Our experiments show that our method outperforms state-of-the-art baselines by substantially reducing prediction errors. Moreover, it achieves 72 FPS on mobile HMDs that ultimately improves the accuracy-running time tradeoff.",
        "url": "http://arxiv.org/abs/2511.16264v1",
        "published_date": "2025-11-20T11:45:58+00:00",
        "updated_date": "2025-11-20T11:45:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sinan Mutlu",
            "Georgios F. Angelis",
            "Savas Ozkan",
            "Paul Wisbey",
            "Anastasios Drosou",
            "Mete Ozay"
        ],
        "tldr": "This paper introduces Mem-MLP, a novel MLP-based method with a Memory-Block component for real-time 3D human motion generation from sparse sensor inputs, designed to improve accuracy and temporal consistency for AR/VR applications, achieving 72 FPS on mobile HMDs.",
        "tldr_zh": "本文提出了一种名为Mem-MLP的新型MLP方法，该方法具有一个Memory-Block组件，可从稀疏传感器输入中实时生成3D人体运动，旨在提高AR/VR应用的准确性和时间一致性，并在移动HMD上实现72 FPS。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]