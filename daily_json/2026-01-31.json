[
    {
        "title": "Creative Image Generation with Diffusion Model",
        "summary": "Creative image generation has emerged as a compelling area of research, driven by the need to produce novel and high-quality images that expand the boundaries of imagination. In this work, we propose a novel framework for creative generation using diffusion models, where creativity is associated with the inverse probability of an image's existence in the CLIP embedding space. Unlike prior approaches that rely on a manual blending of concepts or exclusion of subcategories, our method calculates the probability distribution of generated images and drives it towards low-probability regions to produce rare, imaginative, and visually captivating outputs. We also introduce pullback mechanisms, achieving high creativity without sacrificing visual fidelity. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness and efficiency of our creative generation framework, showcasing its ability to produce unique, novel, and thought-provoking images. This work provides a new perspective on creativity in generative models, offering a principled method to foster innovation in visual content synthesis.",
        "url": "http://arxiv.org/abs/2601.22125v1",
        "published_date": "2026-01-29T18:48:48+00:00",
        "updated_date": "2026-01-29T18:48:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kunpeng Song",
            "Ahmed Elgammal"
        ],
        "tldr": "This paper introduces a novel diffusion model framework for creative image generation by targeting low-probability regions in CLIP embedding space, achieving high creativity while maintaining visual fidelity.",
        "tldr_zh": "本文提出了一种新颖的扩散模型框架，用于创造性的图像生成，通过瞄准CLIP嵌入空间中的低概率区域，从而在保持视觉逼真度的同时实现高度的创造性。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
        "summary": "Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.",
        "url": "http://arxiv.org/abs/2601.22158v1",
        "published_date": "2026-01-29T18:59:56+00:00",
        "updated_date": "2026-01-29T18:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiyang Lu",
            "Susie Lu",
            "Qiao Sun",
            "Hanhong Zhao",
            "Zhicheng Jiang",
            "Xianbang Wang",
            "Tianhong Li",
            "Zhengyang Geng",
            "Kaiming He"
        ],
        "tldr": "This paper introduces pixel MeanFlow (pMF), a one-step latent-free image generation method, achieving strong FID scores on ImageNet at 256x256 and 512x512 resolutions. It formulates the network output and loss spaces separately, using an image manifold target and MeanFlow loss in velocity space.",
        "tldr_zh": "该论文介绍了像素均值流 (pMF)，一种单步无潜在空间图像生成方法，在 ImageNet 256x256 和 512x512 分辨率下实现了较好的 FID 分数。它分别制定了网络输出和损失空间，使用图像流形目标和速度空间中的均值流损失。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "UEval: A Benchmark for Unified Multimodal Generation",
        "summary": "We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.",
        "url": "http://arxiv.org/abs/2601.22155v1",
        "published_date": "2026-01-29T18:59:52+00:00",
        "updated_date": "2026-01-29T18:59:52+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Bo Li",
            "Yida Yin",
            "Wenhao Chai",
            "Xingyu Fu",
            "Zhuang Liu"
        ],
        "tldr": "The paper introduces UEval, a new benchmark for evaluating unified multimodal generation models capable of producing both images and text, using rubric-based scoring refined by human experts. Current state-of-the-art models perform poorly on UEval, highlighting the need for improved multimodal reasoning capabilities.",
        "tldr_zh": "该论文介绍了一个新的基准测试UEval，用于评估能够生成图像和文本的统一多模态生成模型，它使用由人工专家改进的基于规则的评分系统。目前最先进的模型在UEval上的表现不佳，突显了提高多模态推理能力的需求。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers",
        "summary": "Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.",
        "url": "http://arxiv.org/abs/2601.22127v1",
        "published_date": "2026-01-29T18:49:27+00:00",
        "updated_date": "2026-01-29T18:49:27+00:00",
        "categories": [
            "cs.CV",
            "cs.GR",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "John Flynn",
            "Wolfgang Paier",
            "Dimitar Dinev",
            "Sam Nhut Nguyen",
            "Hayk Poghosyan",
            "Manuel Toribio",
            "Sandipan Banerjee",
            "Guy Gafni"
        ],
        "tldr": "EditYourself introduces a DiT-based framework for audio-driven talking head video editing, enabling transcript-based modifications like adding, removing, and retiming spoken content while maintaining visual fidelity and lip synchronization.",
        "tldr_zh": "EditYourself 介绍了一种基于 DiT 的音频驱动的说话人头部视频编辑框架，能够基于文本稿本进行修改，例如添加、删除和重新计时口语内容，同时保持视觉保真度和唇形同步。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation",
        "summary": "In this paper, we propose a 3D asset-referenced diffusion model for image generation, exploring how to integrate 3D assets into image diffusion models. Existing reference-based image generation methods leverage large-scale pretrained diffusion models and demonstrate strong capability in generating diverse images conditioned on a single reference image. However, these methods are limited to single-image references and cannot leverage 3D assets, constraining their practical versatility. To address this gap, we present a cross-domain diffusion model with dual-branch perception that leverages multi-view RGB images and point maps of 3D assets to jointly model their colors and canonical-space coordinates, achieving precise consistency between generated images and the 3D references. Our spatially aligned dual-branch generation architecture and domain-decoupled generation mechanism ensure the simultaneous generation of two spatially aligned but content-disentangled outputs, RGB images and point maps, linking 2D image attributes with 3D asset attributes. Experiments show that our approach effectively uses 3D assets as references to produce images consistent with the given assets, opening new possibilities for combining diffusion models with 3D content creation.",
        "url": "http://arxiv.org/abs/2601.22094v1",
        "published_date": "2026-01-29T18:30:10+00:00",
        "updated_date": "2026-01-29T18:30:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanzhuo Huang",
            "Qingyang Bao",
            "Zekai Gu",
            "Zhongshuo Du",
            "Cheng Lin",
            "Yuan Liu",
            "Sibei Yang"
        ],
        "tldr": "This paper introduces a 3D asset-referenced diffusion model for image generation, bridging the gap between single-image reference diffusion models and the ability to leverage 3D assets as references for more versatile and consistent image generation.",
        "tldr_zh": "本文提出了一种3D资产参考的扩散模型，用于图像生成，填补了单图像参考扩散模型和利用3D资产作为参考之间的空白，从而实现更通用和一致的图像生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "PI-Light: Physics-Inspired Diffusion for Full-Image Relighting",
        "summary": "Full-image relighting remains a challenging problem due to the difficulty of collecting large-scale structured paired data, the difficulty of maintaining physical plausibility, and the limited generalizability imposed by data-driven priors. Existing attempts to bridge the synthetic-to-real gap for full-scene relighting remain suboptimal. To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight ($π$-Light, or PI-Light), a two-stage framework that leverages physics-inspired diffusion models. Our design incorporates (i) batch-aware attention, which improves the consistency of intrinsic predictions across a collection of images, (ii) a physics-guided neural rendering module that enforces physically plausible light transport, (iii) physics-inspired losses that regularize training dynamics toward a physically meaningful landscape, thereby enhancing generalizability to real-world image editing, and (iv) a carefully curated dataset of diverse objects and scenes captured under controlled lighting conditions. Together, these components enable efficient finetuning of pretrained diffusion models while also providing a solid benchmark for downstream evaluation. Experiments demonstrate that $π$-Light synthesizes specular highlights and diffuse reflections across a wide variety of materials, achieving superior generalization to real-world scenes compared with prior approaches.",
        "url": "http://arxiv.org/abs/2601.22135v1",
        "published_date": "2026-01-29T18:55:36+00:00",
        "updated_date": "2026-01-29T18:55:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhexin Liang",
            "Zhaoxi Chen",
            "Yongwei Chen",
            "Tianyi Wei",
            "Tengfei Wang",
            "Xingang Pan"
        ],
        "tldr": "PI-Light is a two-stage physics-inspired diffusion framework for full-image relighting that improves upon existing methods by incorporating batch-aware attention, physics-guided rendering, physics-inspired losses, and a curated dataset to achieve superior generalization to real-world scenes.",
        "tldr_zh": "PI-Light是一个两阶段的、受物理启发的扩散框架，用于全图像重新照明。通过结合批量感知注意力机制、物理引导的渲染、受物理启发的损失函数以及精心策划的数据集，该框架在真实场景中实现了比现有方法更好的泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]