[
    {
        "title": "MonarchRT: Efficient Attention for Real-Time Video Generation",
        "summary": "Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.",
        "url": "http://arxiv.org/abs/2602.12271v1",
        "published_date": "2026-02-12T18:56:53+00:00",
        "updated_date": "2026-02-12T18:56:53+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Krish Agarwal",
            "Zhuoming Chen",
            "Cheng Luo",
            "Yongqi Chen",
            "Haizhong Zheng",
            "Xun Huang",
            "Atri Rudra",
            "Beidi Chen"
        ],
        "tldr": "The paper introduces Monarch-RT, a structured attention mechanism for video diffusion models that uses Monarch matrices to achieve efficient and high-quality real-time video generation, outperforming existing sparse attention methods and optimized kernels.",
        "tldr_zh": "该论文介绍了Monarch-RT，一种用于视频扩散模型的结构化注意力机制，它使用Monarch矩阵来实现高效且高质量的实时视频生成，优于现有的稀疏注意方法和优化的内核。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 10,
        "overall_priority_score": 10
    },
    {
        "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
        "summary": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.",
        "url": "http://arxiv.org/abs/2602.12279v1",
        "published_date": "2026-02-12T18:59:49+00:00",
        "updated_date": "2026-02-12T18:59:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Leon Liangyu Chen",
            "Haoyu Ma",
            "Zhipeng Fan",
            "Ziqi Huang",
            "Animesh Sinha",
            "Xiaoliang Dai",
            "Jialiang Wang",
            "Zecheng He",
            "Jianwei Yang",
            "Chunyuan Li",
            "Junzhe Sun",
            "Chu Wang",
            "Serena Yeung-Levy",
            "Felix Juefei-Xu"
        ],
        "tldr": "The paper introduces UniT, a framework enabling unified multimodal models to perform iterative reasoning and refinement via test-time scaling, improving both generation and understanding through agentic data synthesis and specific training strategies.",
        "tldr_zh": "该论文介绍了 UniT，一种使统一多模态模型能够通过测试时缩放进行迭代推理和细化的框架，通过主动数据合成和特定的训练策略，提高了生成和理解能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]