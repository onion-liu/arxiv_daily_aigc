[
    {
        "title": "LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence",
        "summary": "The reliance on implicit point matching via attention has become a core\nbottleneck in drag-based editing, resulting in a fundamental compromise on\nweakened inversion strength and costly test-time optimization (TTO). This\ncompromise severely limits the generative capabilities of diffusion models,\nsuppressing high-fidelity inpainting and text-guided creation. In this paper,\nwe introduce LazyDrag, the first drag-based image editing method for\nMulti-Modal Diffusion Transformers, which directly eliminates the reliance on\nimplicit point matching. In concrete terms, our method generates an explicit\ncorrespondence map from user drag inputs as a reliable reference to boost the\nattention control. This reliable reference opens the potential for a stable\nfull-strength inversion process, which is the first in the drag-based editing\ntask. It obviates the necessity for TTO and unlocks the generative capability\nof models. Therefore, LazyDrag naturally unifies precise geometric control with\ntext guidance, enabling complex edits that were previously out of reach:\nopening the mouth of a dog and inpainting its interior, generating new objects\nlike a ``tennis ball'', or for ambiguous drags, making context-aware changes\nlike moving a hand into a pocket. Additionally, LazyDrag supports multi-round\nworkflows with simultaneous move and scale operations. Evaluated on the\nDragBench, our method outperforms baselines in drag accuracy and perceptual\nquality, as validated by VIEScore and human evaluation. LazyDrag not only\nestablishes new state-of-the-art performance, but also paves a new way to\nediting paradigms.",
        "url": "http://arxiv.org/abs/2509.12203v1",
        "published_date": "2025-09-15T17:59:47+00:00",
        "updated_date": "2025-09-15T17:59:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixin Yin",
            "Xili Dai",
            "Duomin Wang",
            "Xianfang Zeng",
            "Lionel M. Ni",
            "Gang Yu",
            "Heung-Yeung Shum"
        ],
        "tldr": "LazyDrag introduces a novel drag-based image editing method for multi-modal diffusion transformers that uses explicit correspondence maps to improve control, fidelity, and text guidance without test-time optimization.",
        "tldr_zh": "LazyDrag 提出了一种用于多模态扩散变换器的新型基于拖拽的图像编辑方法，该方法使用显式对应图来提高控制、保真度和文本引导，而无需测试时优化。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling",
        "summary": "The field of 4D world modeling - aiming to jointly capture spatial geometry\nand temporal dynamics - has witnessed remarkable progress in recent years,\ndriven by advances in large-scale generative models and multimodal learning.\nHowever, the development of truly general 4D world models remains fundamentally\nconstrained by the availability of high-quality data. Existing datasets and\nbenchmarks often lack the dynamic complexity, multi-domain diversity, and\nspatial-temporal annotations required to support key tasks such as 4D geometric\nreconstruction, future prediction, and camera-control video generation. To\naddress this gap, we introduce OmniWorld, a large-scale, multi-domain,\nmulti-modal dataset specifically designed for 4D world modeling. OmniWorld\nconsists of a newly collected OmniWorld-Game dataset and several curated public\ndatasets spanning diverse domains. Compared with existing synthetic datasets,\nOmniWorld-Game provides richer modality coverage, larger scale, and more\nrealistic dynamic interactions. Based on this dataset, we establish a\nchallenging benchmark that exposes the limitations of current state-of-the-art\n(SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning\nexisting SOTA methods on OmniWorld leads to significant performance gains\nacross 4D reconstruction and video generation tasks, strongly validating\nOmniWorld as a powerful resource for training and evaluation. We envision\nOmniWorld as a catalyst for accelerating the development of general-purpose 4D\nworld models, ultimately advancing machines' holistic understanding of the\nphysical world.",
        "url": "http://arxiv.org/abs/2509.12201v1",
        "published_date": "2025-09-15T17:59:19+00:00",
        "updated_date": "2025-09-15T17:59:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Zhou",
            "Yifan Wang",
            "Jianjun Zhou",
            "Wenzheng Chang",
            "Haoyu Guo",
            "Zizun Li",
            "Kaijing Ma",
            "Xinyue Li",
            "Yating Wang",
            "Haoyi Zhu",
            "Mingyu Liu",
            "Dingning Liu",
            "Jiange Yang",
            "Zhoujie Fu",
            "Junyi Chen",
            "Chunhua Shen",
            "Jiangmiao Pang",
            "Kaipeng Zhang",
            "Tong He"
        ],
        "tldr": "The paper introduces OmniWorld, a large-scale, multi-domain, multi-modal dataset for 4D world modeling, designed to overcome the limitations of existing datasets and benchmarks in terms of dynamic complexity and multi-modal diversity, leading to performance improvements in 4D reconstruction and video generation.",
        "tldr_zh": "该论文介绍了OmniWorld，一个大规模，多领域，多模态的4D世界建模数据集，旨在克服现有数据集和基准在动态复杂性和多模态多样性方面的局限性，从而提高4D重建和视频生成方面的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching",
        "summary": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}",
        "url": "http://arxiv.org/abs/2509.11628v1",
        "published_date": "2025-09-15T06:46:22+00:00",
        "updated_date": "2025-09-15T06:46:22+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jiacheng Liu",
            "Chang Zou",
            "Yuanhuiyi Lyu",
            "Fei Ren",
            "Shaobo Wang",
            "Kaixin Li",
            "Linfeng Zhang"
        ],
        "tldr": "The paper introduces SpeCa, a novel acceleration framework for diffusion models that uses speculative sampling and a verification mechanism to achieve significant speedups in image and video generation with minimal quality loss.",
        "tldr_zh": "该论文介绍了一种名为SpeCa的新型扩散模型加速框架，它使用推测性采样和验证机制，在图像和视频生成方面实现了显著的加速，同时质量损失极小。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking",
        "summary": "While autoregressive (AR) models have demonstrated remarkable success in\nimage generation, extending them to layout-conditioned generation remains\nchallenging due to the sparse nature of layout conditions and the risk of\nfeature entanglement. We present Structured Masking for AR-based\nLayout-to-Image (SMARLI), a novel framework for layoutto-image generation that\neffectively integrates spatial layout constraints into AR-based image\ngeneration. To equip AR model with layout control, a specially designed\nstructured masking strategy is applied to attention computation to govern the\ninteraction among the global prompt, layout, and image tokens. This design\nprevents mis-association between different regions and their descriptions while\nenabling sufficient injection of layout constraints into the generation\nprocess. To further enhance generation quality and layout accuracy, we\nincorporate Group Relative Policy Optimization (GRPO) based post-training\nscheme with specially designed layout reward functions for next-set-based AR\nmodels. Experimental results demonstrate that SMARLI is able to seamlessly\nintegrate layout tokens with text and image tokens without compromising\ngeneration quality. It achieves superior layoutaware control while maintaining\nthe structural simplicity and generation efficiency of AR models.",
        "url": "http://arxiv.org/abs/2509.12046v1",
        "published_date": "2025-09-15T15:27:29+00:00",
        "updated_date": "2025-09-15T15:27:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zirui Zheng",
            "Takashi Isobe",
            "Tong Shen",
            "Xu Jia",
            "Jianbin Zhao",
            "Xiaomin Li",
            "Mengmeng Ge",
            "Baolu Li",
            "Qinghe Wang",
            "Dong Li",
            "Dong Zhou",
            "Yunzhi Zhuge",
            "Huchuan Lu",
            "Emad Barsoum"
        ],
        "tldr": "This paper introduces SMARLI, a novel autoregressive text-to-image generation framework with structured masking to effectively integrate spatial layout constraints, achieving superior layout-aware control and generation efficiency.",
        "tldr_zh": "该论文介绍了一种名为SMARLI的新型自回归文本到图像生成框架，该框架采用结构化掩码来有效地整合空间布局约束，从而实现了卓越的布局感知控制和生成效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness",
        "summary": "Diffusion models have achieved unprecedented success in image generation but\npose increasing risks in terms of privacy, fairness, and security. A growing\ndemand exists to \\emph{erase} sensitive or harmful concepts (e.g., NSFW\ncontent, private individuals, artistic styles) from these models while\npreserving their overall generative capabilities. We introduce \\textbf{SCORE}\n(Secure and Concept-Oriented Robust Erasure), a novel framework for robust\nconcept removal in diffusion models. SCORE formulates concept erasure as an\n\\emph{adversarial independence} problem, theoretically guaranteeing that the\nmodel's outputs become statistically independent of the erased concept. Unlike\nprior heuristic methods, SCORE minimizes the mutual information between a\ntarget concept and generated outputs, yielding provable erasure guarantees. We\nprovide formal proofs establishing convergence properties and derive upper\nbounds on residual concept leakage. Empirically, we evaluate SCORE on Stable\nDiffusion and FLUX across four challenging benchmarks: object erasure, NSFW\nremoval, celebrity face suppression, and artistic style unlearning. SCORE\nconsistently outperforms state-of-the-art methods including EraseAnything, ANT,\nMACE, ESD, and UCE, achieving up to \\textbf{12.5\\%} higher erasure efficacy\nwhile maintaining comparable or superior image quality. By integrating\nadversarial optimization, trajectory consistency, and saliency-driven\nfine-tuning, SCORE sets a new standard for secure and robust concept erasure in\ndiffusion models.",
        "url": "http://arxiv.org/abs/2509.12024v1",
        "published_date": "2025-09-15T15:05:50+00:00",
        "updated_date": "2025-09-15T15:05:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixuan Fu",
            "Yan Ren",
            "Finn Carter",
            "Chenyue Wen",
            "Le Ku",
            "Daheng Yu",
            "Emily Davis",
            "Bo Zhang"
        ],
        "tldr": "The paper introduces SCORE, a novel framework for robust concept erasure in diffusion models, theoretically guaranteeing statistical independence between generated outputs and erased concepts, and empirically outperforming state-of-the-art methods.",
        "tldr_zh": "该论文介绍了 SCORE，一种用于扩散模型中稳健概念擦除的新框架，从理论上保证生成输出与擦除概念之间的统计独立性，并且在经验上优于最先进的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning to Generate 4D LiDAR Sequences",
        "summary": "While generative world models have advanced video and occupancy-based data\nsynthesis, LiDAR generation remains underexplored despite its importance for\naccurate 3D perception. Extending generation to 4D LiDAR data introduces\nchallenges in controllability, temporal stability, and evaluation. We present\nLiDARCrafter, a unified framework that converts free-form language into\neditable LiDAR sequences. Instructions are parsed into ego-centric scene\ngraphs, which a tri-branch diffusion model transforms into object layouts,\ntrajectories, and shapes. A range-image diffusion model generates the initial\nscan, and an autoregressive module extends it into a temporally coherent\nsequence. The explicit layout design further supports object-level editing,\nsuch as insertion or relocation. To enable fair assessment, we provide\nEvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On\nnuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and\ntemporal consistency, offering a foundation for LiDAR-based simulation and data\naugmentation.",
        "url": "http://arxiv.org/abs/2509.11959v1",
        "published_date": "2025-09-15T14:14:48+00:00",
        "updated_date": "2025-09-15T14:14:48+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Ao Liang",
            "Youquan Liu",
            "Yu Yang",
            "Dongyue Lu",
            "Linfeng Li",
            "Lingdong Kong",
            "Huaici Zhao",
            "Wei Tsang Ooi"
        ],
        "tldr": "The paper introduces LiDARCrafter, a framework for generating editable 4D LiDAR sequences from natural language, achieving state-of-the-art results in fidelity, controllability, and temporal consistency on nuScenes dataset.",
        "tldr_zh": "该论文介绍了LiDARCrafter，一个通过自然语言生成可编辑4D激光雷达序列的框架，在nuScenes数据集上实现了最先进的保真度、可控性和时间一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation",
        "summary": "Poetry is an expressive form of art that invites multiple interpretations, as\nreaders often bring their own emotions, experiences, and cultural backgrounds\ninto their understanding of a poem. Recognizing this, we aim to generate images\nfor poems and improve these images in a zero-shot setting, enabling audiences\nto modify images as per their requirements. To achieve this, we introduce a\nnovel Weighted Prompt Manipulation (WPM) technique, which systematically\nmodifies attention weights and text embeddings within diffusion models. By\ndynamically adjusting the importance of specific words, WPM enhances or\nsuppresses their influence in the final generated image, leading to\nsemantically richer and more contextually accurate visualizations. Our approach\nexploits diffusion models and large language models (LLMs) such as GPT in\nconjunction with existing poetry datasets, ensuring a comprehensive and\nstructured methodology for improved image generation in the literary domain. To\nthe best of our knowledge, this is the first attempt at integrating weighted\nprompt manipulation for enhancing imagery in poetic language.",
        "url": "http://arxiv.org/abs/2509.11878v1",
        "published_date": "2025-09-15T12:58:38+00:00",
        "updated_date": "2025-09-15T12:58:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sofia Jamil",
            "Kotla Sai Charan",
            "Sriparna Saha",
            "Koustava Goswami",
            "K J Joseph"
        ],
        "tldr": "This paper introduces Weighted Prompt Manipulation (WPM), a method for modifying image generation based on poems in a zero-shot setting using diffusion models and LLMs, allowing users to customize images based on their interpretation of the poem.",
        "tldr_zh": "该论文介绍了加权提示操纵（WPM），一种基于诗歌在零样本设置中修改图像生成的方法，它使用扩散模型和大型语言模型（LLM），允许用户根据他们对诗歌的理解来定制图像。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]