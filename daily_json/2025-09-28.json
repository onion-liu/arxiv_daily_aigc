[
    {
        "title": "WorldSplat: Gaussian-Centric Feed-Forward 4D Scene Generation for Autonomous Driving",
        "summary": "Recent advances in driving-scene generation and reconstruction have\ndemonstrated significant potential for enhancing autonomous driving systems by\nproducing scalable and controllable training data. Existing generation methods\nprimarily focus on synthesizing diverse and high-fidelity driving videos;\nhowever, due to limited 3D consistency and sparse viewpoint coverage, they\nstruggle to support convenient and high-quality novel-view synthesis (NVS).\nConversely, recent 3D/4D reconstruction approaches have significantly improved\nNVS for real-world driving scenes, yet inherently lack generative capabilities.\nTo overcome this dilemma between scene generation and reconstruction, we\npropose \\textbf{WorldSplat}, a novel feed-forward framework for 4D\ndriving-scene generation. Our approach effectively generates consistent\nmulti-track videos through two key steps: ((i)) We introduce a 4D-aware latent\ndiffusion model integrating multi-modal information to produce pixel-aligned 4D\nGaussians in a feed-forward manner. ((ii)) Subsequently, we refine the novel\nview videos rendered from these Gaussians using a enhanced video diffusion\nmodel. Extensive experiments conducted on benchmark datasets demonstrate that\n\\textbf{WorldSplat} effectively generates high-fidelity, temporally and\nspatially consistent multi-track novel view driving videos.",
        "url": "http://arxiv.org/abs/2509.23402v1",
        "published_date": "2025-09-27T16:47:44+00:00",
        "updated_date": "2025-09-27T16:47:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyue Zhu",
            "Zhanqian Wu",
            "Zhenxin Zhu",
            "Lijun Zhou",
            "Haiyang Sun",
            "Bing Wan",
            "Kun Ma",
            "Guang Chen",
            "Hangjun Ye",
            "Jin Xie",
            "jian Yang"
        ],
        "tldr": "WorldSplat is a novel feed-forward framework for 4D driving-scene generation that combines a 4D-aware latent diffusion model with a video diffusion model to generate consistent multi-track novel view driving videos.",
        "tldr_zh": "WorldSplat 是一种新的前馈 4D 驾驶场景生成框架，它结合了 4D 感知潜在扩散模型和视频扩散模型，以生成一致的多轨新视角驾驶视频。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dynamic-TreeRPO: Breaking the Independent Trajectory Bottleneck with Structured Sampling",
        "summary": "The integration of Reinforcement Learning (RL) into flow matching models for\ntext-to-image (T2I) generation has driven substantial advances in generation\nquality. However, these gains often come at the cost of exhaustive exploration\nand inefficient sampling strategies due to slight variation in the sampling\ngroup. Building on this insight, we propose Dynamic-TreeRPO, which implements\nthe sliding-window sampling strategy as a tree-structured search with dynamic\nnoise intensities along depth. We perform GRPO-guided optimization and\nconstrained Stochastic Differential Equation (SDE) sampling within this tree\nstructure. By sharing prefix paths of the tree, our design effectively\namortizes the computational overhead of trajectory search. With well-designed\nnoise intensities for each tree layer, Dynamic-TreeRPO can enhance the\nvariation of exploration without any extra computational cost. Furthermore, we\nseamlessly integrate Supervised Fine-Tuning (SFT) and RL paradigm within\nDynamic-TreeRPO to construct our proposed LayerTuning-RL, reformulating the\nloss function of SFT as a dynamically weighted Progress Reward Model (PRM)\nrather than a separate pretraining method. By associating this weighted PRM\nwith dynamic-adaptive clipping bounds, the disruption of exploration process in\nDynamic-TreeRPO is avoided. Benefiting from the tree-structured sampling and\nthe LayerTuning-RL paradigm, our model dynamically explores a diverse search\nspace along effective directions. Compared to existing baselines, our approach\ndemonstrates significant superiority in terms of semantic consistency, visual\nfidelity, and human preference alignment on established benchmarks, including\nHPS-v2.1, PickScore, and ImageReward. In particular, our model outperforms SoTA\nby $4.9\\%$, $5.91\\%$, and $8.66\\%$ on those benchmarks, respectively, while\nimproving the training efficiency by nearly $50\\%$.",
        "url": "http://arxiv.org/abs/2509.23352v1",
        "published_date": "2025-09-27T14:59:31+00:00",
        "updated_date": "2025-09-27T14:59:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiaolong Fu",
            "Lichen Ma",
            "Zipeng Guo",
            "Gaojing Zhou",
            "Chongxiao Wang",
            "ShiPing Dong",
            "Shizhe Zhou",
            "Shizhe Zhou",
            "Ximan Liu",
            "Jingling Fu",
            "Tan Lit Sin",
            "Yu Shi",
            "Zhen Chen",
            "Junshi Huang",
            "Jason Li"
        ],
        "tldr": "Dynamic-TreeRPO improves text-to-image generation by using a tree-structured search with dynamic noise intensities, a novel LayerTuning-RL paradigm, and shows significant performance and efficiency gains on standard benchmarks.",
        "tldr_zh": "Dynamic-TreeRPO通过使用具有动态噪声强度的树状结构搜索和一种新颖的LayerTuning-RL范式来改进文本到图像的生成，并在标准基准测试中显示出显着的性能和效率提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Stochastic Interpolants via Conditional Dependent Coupling",
        "summary": "Existing image generation models face critical challenges regarding the\ntrade-off between computation and fidelity. Specifically, models relying on a\npretrained Variational Autoencoder (VAE) suffer from information loss, limited\ndetail, and the inability to support end-to-end training. In contrast, models\noperating directly in the pixel space incur prohibitive computational cost.\nAlthough cascade models can mitigate computational cost, stage-wise separation\nprevents effective end-to-end optimization, hampers knowledge sharing, and\noften results in inaccurate distribution learning within each stage. To address\nthese challenges, we introduce a unified multistage generative framework based\non our proposed Conditional Dependent Coupling strategy. It decomposes the\ngenerative process into interpolant trajectories at multiple stages, ensuring\naccurate distribution learning while enabling end-to-end optimization.\nImportantly, the entire process is modeled as a single unified Diffusion\nTransformer, eliminating the need for disjoint modules and also enabling\nknowledge sharing. Extensive experiments demonstrate that our method achieves\nboth high fidelity and efficiency across multiple resolutions.",
        "url": "http://arxiv.org/abs/2509.23122v1",
        "published_date": "2025-09-27T05:03:08+00:00",
        "updated_date": "2025-09-27T05:03:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenrui Ma",
            "Xi Xiao",
            "Tianyang Wang",
            "Xiao Wang",
            "Yanning Shen"
        ],
        "tldr": "The paper presents a novel multistage generative framework, Stochastic Interpolants via Conditional Dependent Coupling, using a unified Diffusion Transformer for efficient and high-fidelity image generation through end-to-end optimization and knowledge sharing.",
        "tldr_zh": "该论文提出了一种新的多阶段生成框架，即通过条件依赖耦合的随机插值法，它使用统一的扩散Transformer，通过端到端优化和知识共享来实现高效、高保真的图像生成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ARSS: Taming Decoder-only Autoregressive Visual Generation for View Synthesis From Single View",
        "summary": "Despite their exceptional generative quality, diffusion models have limited\napplicability to world modeling tasks, such as novel view generation from\nsparse inputs. This limitation arises because diffusion models generate outputs\nin a non-causal manner, often leading to distortions or inconsistencies across\nviews, and making it difficult to incrementally adapt accumulated knowledge to\nnew queries. In contrast, autoregressive (AR) models operate in a causal\nfashion, generating each token based on all previously generated tokens. In\nthis work, we introduce \\textbf{ARSS}, a novel framework that leverages a\nGPT-style decoder-only AR model to generate novel views from a single image,\nconditioned on a predefined camera trajectory. We employ a video tokenizer to\nmap continuous image sequences into discrete tokens and propose a camera\nencoder that converts camera trajectories into 3D positional guidance. Then to\nenhance generation quality while preserving the autoregressive structure, we\npropose a autoregressive transformer module that randomly permutes the spatial\norder of tokens while maintaining their temporal order. Extensive qualitative\nand quantitative experiments on public datasets demonstrate that our method\nperforms comparably to, or better than, state-of-the-art view synthesis\napproaches based on diffusion models. Our code will be released upon paper\nacceptance.",
        "url": "http://arxiv.org/abs/2509.23008v1",
        "published_date": "2025-09-27T00:03:09+00:00",
        "updated_date": "2025-09-27T00:03:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenbin Teng",
            "Gonglin Chen",
            "Haiwei Chen",
            "Yajie Zhao"
        ],
        "tldr": "The paper presents ARSS, a novel autoregressive framework using a GPT-style decoder to generate novel views from a single image conditioned on a camera trajectory, outperforming diffusion-based methods in view synthesis.",
        "tldr_zh": "该论文提出了ARSS，一种新颖的自回归框架，采用GPT风格的解码器，根据相机轨迹从单张图像生成新的视角，在视角合成方面优于基于扩散的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Soft-Di[M]O: Improving One-Step Discrete Image Generation with Soft Embeddings",
        "summary": "One-step generators distilled from Masked Diffusion Models (MDMs) compress\nmultiple sampling steps into a single forward pass, enabling efficient text and\nimage synthesis. However, they suffer two key limitations: they inherit\nmodeling bias from the teacher, and their discrete token outputs block gradient\nflow, preventing post-distillation refinements such as adversarial training,\nreward-based fine-tuning, and Test-Time Embedding Optimization (TTEO). In this\nwork, we introduce soft embeddings, a simple relaxation that replaces discrete\ntokens with the expected embeddings under the generator's output distribution.\nSoft embeddings preserve representation fidelity for one-step discrete\ngenerator while providing a fully differentiable continuous surrogate that is\ncompatible with teacher backbones and tokenizer decoders. Integrating soft\nembeddings into the Di[M]O distillation framework (denoted Soft-Di[M]O) makes\none-step generators end-to-end trainable and enables straightforward\napplication of GAN-based refinement, differentiable reward fine-tuning, and\nTTEO. Empirically, across multiple MDM teachers (e.g., MaskBit, MaskGen),\nSoft-Di[M]O achieves state-of-the-art one-step results: improved class-to-image\nperformance, a one-step FID of 1.56 on ImageNet-256 with GAN-based refinement,\nalong with higher GenEval and HPS scores on text-to-image with reward\nfine-tuning, and further gains from TTEO.",
        "url": "http://arxiv.org/abs/2509.22925v1",
        "published_date": "2025-09-26T20:51:20+00:00",
        "updated_date": "2025-09-26T20:51:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yuanzhi Zhu",
            "Xi Wang",
            "Stéphane Lathuilière",
            "Vicky Kalogeiton"
        ],
        "tldr": "This paper introduces soft embeddings within the Di[M]O framework (Soft-Di[M]O) to improve one-step discrete image generation by enabling end-to-end training and refinement through GANs and reward fine-tuning, achieving state-of-the-art results.",
        "tldr_zh": "本文提出了一种在Di[M]O框架中使用软嵌入的方法 (Soft-Di[M]O)，通过支持端到端训练和GANs以及奖励微调的优化，改进了一步离散图像生成，并取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ControlEvents: Controllable Synthesis of Event Camera Datawith Foundational Prior from Image Diffusion Models",
        "summary": "In recent years, event cameras have gained significant attention due to their\nbio-inspired properties, such as high temporal resolution and high dynamic\nrange. However, obtaining large-scale labeled ground-truth data for event-based\nvision tasks remains challenging and costly. In this paper, we present\nControlEvents, a diffusion-based generative model designed to synthesize\nhigh-quality event data guided by diverse control signals such as class text\nlabels, 2D skeletons, and 3D body poses. Our key insight is to leverage the\ndiffusion prior from foundation models, such as Stable Diffusion, enabling\nhigh-quality event data generation with minimal fine-tuning and limited labeled\ndata. Our method streamlines the data generation process and significantly\nreduces the cost of producing labeled event datasets. We demonstrate the\neffectiveness of our approach by synthesizing event data for visual\nrecognition, 2D skeleton estimation, and 3D body pose estimation. Our\nexperiments show that the synthesized labeled event data enhances model\nperformance in all tasks. Additionally, our approach can generate events based\non unseen text labels during training, illustrating the powerful text-based\ngeneration capabilities inherited from foundation models.",
        "url": "http://arxiv.org/abs/2509.22864v1",
        "published_date": "2025-09-26T19:22:07+00:00",
        "updated_date": "2025-09-26T19:22:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yixuan Hu",
            "Yuxuan Xue",
            "Simon Klenk",
            "Daniel Cremers",
            "Gerard Pons-Moll"
        ],
        "tldr": "The paper introduces ControlEvents, a diffusion-based generative model that synthesizes event camera data controlled by text, 2D skeletons, and 3D poses, leveraging priors from foundation models like Stable Diffusion to generate high-quality labeled event data with minimal fine-tuning.",
        "tldr_zh": "该论文介绍了ControlEvents，一种基于扩散的生成模型，通过文本、2D骨骼和3D姿势控制合成事件相机数据，利用Stable Diffusion等基础模型的先验知识，以最少的微调生成高质量的标记事件数据。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Follow-Your-Preference: Towards Preference-Aligned Image Inpainting",
        "summary": "This paper investigates image inpainting with preference alignment. Instead\nof introducing a novel method, we go back to basics and revisit fundamental\nproblems in achieving such alignment. We leverage the prominent direct\npreference optimization approach for alignment training and employ public\nreward models to construct preference training datasets. Experiments are\nconducted across nine reward models, two benchmarks, and two baseline models\nwith varying structures and generative algorithms. Our key findings are as\nfollows: (1) Most reward models deliver valid reward scores for constructing\npreference data, even if some of them are not reliable evaluators. (2)\nPreference data demonstrates robust trends in both candidate scaling and sample\nscaling across models and benchmarks. (3) Observable biases in reward models,\nparticularly in brightness, composition, and color scheme, render them\nsusceptible to cause reward hacking. (4) A simple ensemble of these models\nyields robust and generalizable results by mitigating such biases. Built upon\nthese observations, our alignment models significantly outperform prior models\nacross standard metrics, GPT-4 assessments, and human evaluations, without any\nchanges to model structures or the use of new datasets. We hope our work can\nset a simple yet solid baseline, pushing this promising frontier. Our code is\nopen-sourced at: https://github.com/shenytzzz/Follow-Your-Preference.",
        "url": "http://arxiv.org/abs/2509.23082v1",
        "published_date": "2025-09-27T03:32:30+00:00",
        "updated_date": "2025-09-27T03:32:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yutao Shen",
            "Junkun Yuan",
            "Toru Aonishi",
            "Hideki Nakayama",
            "Yue Ma"
        ],
        "tldr": "This paper explores preference-aligned image inpainting using direct preference optimization and public reward models. It identifies biases in reward models and proposes a simple ensemble to improve performance, achieving state-of-the-art results without modifying model structures.",
        "tldr_zh": "本文探讨了基于偏好对齐的图像修复，利用直接偏好优化和公开奖励模型。它发现了奖励模型中的偏差，并提出了一种简单的集成方法来提高性能，无需修改模型结构即可达到最先进的效果。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LLMs Behind the Scenes: Enabling Narrative Scene Illustration",
        "summary": "Generative AI has established the opportunity to readily transform content\nfrom one medium to another. This capability is especially powerful for\nstorytelling, where visual illustrations can illuminate a story originally\nexpressed in text. In this paper, we focus on the task of narrative scene\nillustration, which involves automatically generating an image depicting a\nscene in a story. Motivated by recent progress on text-to-image models, we\nconsider a pipeline that uses LLMs as an interface for prompting text-to-image\nmodels to generate scene illustrations given raw story text. We apply\nvariations of this pipeline to a prominent story corpus in order to synthesize\nillustrations for scenes in these stories. We conduct a human annotation task\nto obtain pairwise quality judgments for these illustrations. The outcome of\nthis process is the SceneIllustrations dataset, which we release as a new\nresource for future work on cross-modal narrative transformation. Through our\nanalysis of this dataset and experiments modeling illustration quality, we\ndemonstrate that LLMs can effectively verbalize scene knowledge implicitly\nevoked by story text. Moreover, this capability is impactful for generating and\nevaluating illustrations.",
        "url": "http://arxiv.org/abs/2509.22940v1",
        "published_date": "2025-09-26T21:15:18+00:00",
        "updated_date": "2025-09-26T21:15:18+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Melissa Roemmele",
            "John Joon Young Chung",
            "Taewook Kim",
            "Yuqian Sun",
            "Alex Calderwood",
            "Max Kreminski"
        ],
        "tldr": "This paper explores using LLMs to prompt text-to-image models for narrative scene illustration, introduces the SceneIllustrations dataset, and demonstrates LLMs' effectiveness in verbalizing scene knowledge for better illustration generation and evaluation.",
        "tldr_zh": "本文研究了使用大型语言模型（LLM）来提示文本到图像模型，以实现叙事场景的插图生成，介绍了SceneIllustrations数据集，并展示了LLM在将场景知识转化为文字方面的有效性，从而改进插图生成和评估。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]