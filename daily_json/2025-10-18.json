[
    {
        "title": "Unimedvl: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis",
        "summary": "Medical diagnostic applications require models that can process multimodal\nmedical inputs (images, patient histories, lab results) and generate diverse\noutputs including both textual reports and visual content (annotations,\nsegmentation masks, and images). Despite this need, existing medical AI systems\ndisrupt this unified process: medical image understanding models interpret\nimages but cannot generate visual outputs, while medical image generation\nmodels synthesize images but cannot provide textual explanations. This leads to\ngaps in data representation, feature integration, and task-level multimodal\ncapabilities. To this end, we propose a multi-level framework that draws\ninspiration from diagnostic workflows through the\nObservation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation\nlevel, we construct UniMed-5M, a dataset comprising over 5.6M samples that\nreformat diverse unimodal data into multimodal pairs for foundational\nobservation. At the knowledge level, we propose Progressive Curriculum Learning\nthat systematically introduces medical multimodal knowledge. At the analysis\nlevel, we introduce UniMedVL, the first medical unified multimodal model for\nthe simultaneous analysis of image understanding and generation tasks within a\nsingle architecture. UniMedVL achieves superior performance on five medical\nimage understanding benchmarks, while matching specialized models in generation\nquality across eight medical imaging modalities. Crucially, our unified\narchitecture enables bidirectional knowledge sharing: generation tasks enhance\nvisual understanding features, demonstrating that integrating traditionally\nseparate capabilities within a single medical framework unlocks improvements\nacross diverse medical vision-language tasks. Code is available at\nhttps://github.com/uni-medical/UniMedVL.",
        "url": "http://arxiv.org/abs/2510.15710v1",
        "published_date": "2025-10-17T14:54:58+00:00",
        "updated_date": "2025-10-17T14:54:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junzhi Ning",
            "Wei Li",
            "Cheng Tang",
            "Jiashi Lin",
            "Chenglong Ma",
            "Chaoyang Zhang",
            "Jiyao Liu",
            "Ying Chen",
            "Shujian Gao",
            "Lihao Liu",
            "Yuandong Pu",
            "Huihui Xu",
            "Chenhui Gou",
            "Ziyan Huang",
            "Yi Xin",
            "Qi Qin",
            "Zhongying Deng",
            "Diping Song",
            "Bin Fu",
            "Guang Yang",
            "Yuanfeng Ji",
            "Tianbin Li",
            "Yanzhou Su",
            "Jin Ye",
            "Shixiang Tang",
            "Ming Hu",
            "Junjun He"
        ],
        "tldr": "The paper introduces UniMedVL, a unified multimodal model for medical image understanding and generation, trained on a large dataset (UniMed-5M) using progressive curriculum learning. It demonstrates superior performance on both understanding and generation tasks while enabling bidirectional knowledge sharing.",
        "tldr_zh": "该论文介绍了UniMedVL，一个用于医学图像理解和生成的统一多模态模型，该模型使用渐进式课程学习在大型数据集(UniMed-5M)上训练。它在理解和生成任务上都表现出卓越的性能，同时实现了双向知识共享。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "TGT: Text-Grounded Trajectories for Locally Controlled Video Generation",
        "summary": "Text-to-video generation has advanced rapidly in visual fidelity, whereas\nstandard methods still have limited ability to control the subject composition\nof generated scenes. Prior work shows that adding localized text control\nsignals, such as bounding boxes or segmentation masks, can help. However, these\nmethods struggle in complex scenarios and degrade in multi-object settings,\noffering limited precision and lacking a clear correspondence between\nindividual trajectories and visual entities as the number of controllable\nobjects increases. We introduce Text-Grounded Trajectories (TGT), a framework\nthat conditions video generation on trajectories paired with localized text\ndescriptions. We propose Location-Aware Cross-Attention (LACA) to integrate\nthese signals and adopt a dual-CFG scheme to separately modulate local and\nglobal text guidance. In addition, we develop a data processing pipeline that\nproduces trajectories with localized descriptions of tracked entities, and we\nannotate two million high quality video clips to train TGT. Together, these\ncomponents enable TGT to use point trajectories as intuitive motion handles,\npairing each trajectory with text to control both appearance and motion.\nExtensive experiments show that TGT achieves higher visual quality, more\naccurate text alignment, and improved motion controllability compared with\nprior approaches. Website: https://textgroundedtraj.github.io.",
        "url": "http://arxiv.org/abs/2510.15104v1",
        "published_date": "2025-10-16T19:45:27+00:00",
        "updated_date": "2025-10-16T19:45:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guofeng Zhang",
            "Angtian Wang",
            "Jacob Zhiyuan Fang",
            "Liming Jiang",
            "Haotian Yang",
            "Bo Liu",
            "Yiding Yang",
            "Guang Chen",
            "Longyin Wen",
            "Alan Yuille",
            "Chongyang Ma"
        ],
        "tldr": "The paper introduces Text-Grounded Trajectories (TGT), a framework for locally controlled video generation using trajectories paired with localized text descriptions, and outperforms existing methods in visual quality, text alignment, and motion controllability.",
        "tldr_zh": "该论文介绍了一种名为文本导向轨迹（TGT）的框架，用于局部控制的视频生成，它使用与本地文本描述配对的轨迹，并在视觉质量、文本对齐和运动可控性方面优于现有方法。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "BLIP3o-NEXT: Next Frontier of Native Image Generation",
        "summary": "We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3\nseries that advances the next frontier of native image generation. BLIP3o-NEXT\nunifies text-to-image generation and image editing within a single\narchitecture, demonstrating strong image generation and image editing\ncapabilities. In developing the state-of-the-art native image generation model,\nwe identify four key insights: (1) Most architectural choices yield comparable\nperformance; an architecture can be deemed effective provided it scales\nefficiently and supports fast inference; (2) The successful application of\nreinforcement learning can further push the frontier of native image\ngeneration; (3) Image editing still remains a challenging task, yet instruction\nfollowing and the consistency between generated and reference images can be\nsignificantly enhanced through post-training and data engine; (4) Data quality\nand scale continue to be decisive factors that determine the upper bound of\nmodel performance. Building upon these insights, BLIP3o-NEXT leverages an\nAutoregressive + Diffusion architecture in which an autoregressive model first\ngenerates discrete image tokens conditioned on multimodal inputs, whose hidden\nstates are then used as conditioning signals for a diffusion model to generate\nhigh-fidelity images. This architecture integrates the reasoning strength and\ninstruction following of autoregressive models with the fine-detail rendering\nability of diffusion models, achieving a new level of coherence and realism.\nExtensive evaluations of various text-to-image and image-editing benchmarks\nshow that BLIP3o-NEXT achieves superior performance over existing models.",
        "url": "http://arxiv.org/abs/2510.15857v1",
        "published_date": "2025-10-17T17:50:58+00:00",
        "updated_date": "2025-10-17T17:50:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiuhai Chen",
            "Le Xue",
            "Zhiyang Xu",
            "Xichen Pan",
            "Shusheng Yang",
            "Can Qin",
            "An Yan",
            "Honglu Zhou",
            "Zeyuan Chen",
            "Lifu Huang",
            "Tianyi Zhou",
            "Junnan Li",
            "Silvio Savarese",
            "Caiming Xiong",
            "Ran Xu"
        ],
        "tldr": "BLIP3o-NEXT is a fully open-source foundation model unifying text-to-image generation and image editing, achieving state-of-the-art performance using an Autoregressive + Diffusion architecture and emphasizing data quality and scale.",
        "tldr_zh": "BLIP3o-NEXT是一个完全开源的基础模型，统一了文本到图像的生成和图像编辑，通过自回归+扩散架构实现了最先进的性能，并强调了数据质量和规模的重要性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Controlling the image generation process with parametric activation functions",
        "summary": "As image generative models continue to increase not only in their fidelity\nbut also in their ubiquity the development of tools that leverage direct\ninteraction with their internal mechanisms in an interpretable way has received\nlittle attention In this work we introduce a system that allows users to\ndevelop a better understanding of the model through interaction and\nexperimentation By giving users the ability to replace activation functions of\na generative network with parametric ones and a way to set the parameters of\nthese functions we introduce an alternative approach to control the networks\noutput We demonstrate the use of our method on StyleGAN2 and BigGAN networks\ntrained on FFHQ and ImageNet respectively.",
        "url": "http://arxiv.org/abs/2510.15778v1",
        "published_date": "2025-10-17T16:02:23+00:00",
        "updated_date": "2025-10-17T16:02:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ilia Pavlov"
        ],
        "tldr": "This paper introduces a method for controlling image generation by allowing users to replace and parameterize activation functions in generative models like StyleGAN2 and BigGAN, offering a more interactive approach to understanding and manipulating the generation process.",
        "tldr_zh": "本文介绍了一种通过允许用户替换和参数化生成模型（如 StyleGAN2 和 BigGAN）中的激活函数来控制图像生成的方法，从而为理解和操纵生成过程提供了一种更具互动性的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset",
        "summary": "Instruction-based video editing promises to democratize content creation, yet\nits progress is severely hampered by the scarcity of large-scale, high-quality\ntraining data. We introduce Ditto, a holistic framework designed to tackle this\nfundamental challenge. At its heart, Ditto features a novel data generation\npipeline that fuses the creative diversity of a leading image editor with an\nin-context video generator, overcoming the limited scope of existing models. To\nmake this process viable, our framework resolves the prohibitive cost-quality\ntrade-off by employing an efficient, distilled model architecture augmented by\na temporal enhancer, which simultaneously reduces computational overhead and\nimproves temporal coherence. Finally, to achieve full scalability, this entire\npipeline is driven by an intelligent agent that crafts diverse instructions and\nrigorously filters the output, ensuring quality control at scale. Using this\nframework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of\none million high-fidelity video editing examples. We trained our model, Editto,\non Ditto-1M with a curriculum learning strategy. The results demonstrate\nsuperior instruction-following ability and establish a new state-of-the-art in\ninstruction-based video editing.",
        "url": "http://arxiv.org/abs/2510.15742v1",
        "published_date": "2025-10-17T15:31:40+00:00",
        "updated_date": "2025-10-17T15:31:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qingyan Bai",
            "Qiuyu Wang",
            "Hao Ouyang",
            "Yue Yu",
            "Hanlin Wang",
            "Wen Wang",
            "Ka Leong Cheng",
            "Shuailei Ma",
            "Yanhong Zeng",
            "Zichen Liu",
            "Yinghao Xu",
            "Yujun Shen",
            "Qifeng Chen"
        ],
        "tldr": "The paper introduces Ditto, a framework for generating a large-scale, high-quality synthetic dataset (Ditto-1M) for instruction-based video editing, and presents Editto, a model trained on this dataset that achieves state-of-the-art results.",
        "tldr_zh": "该论文介绍了Ditto，一个用于生成大规模、高质量合成数据集 (Ditto-1M) 的框架，该数据集用于基于指令的视频编辑，并提出了 Editto，一个在该数据集上训练的模型，该模型取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Latent Diffusion Model without Variational Autoencoder",
        "summary": "Recent progress in diffusion-based visual generation has largely relied on\nlatent diffusion models with variational autoencoders (VAEs). While effective\nfor high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited\ntraining efficiency, slow inference, and poor transferability to broader vision\ntasks. These issues stem from a key limitation of VAE latent spaces: the lack\nof clear semantic separation and strong discriminative structure. Our analysis\nconfirms that these properties are crucial not only for perception and\nunderstanding tasks, but also for the stable and efficient training of latent\ndiffusion models. Motivated by this insight, we introduce SVG, a novel latent\ndiffusion model without variational autoencoders, which leverages\nself-supervised representations for visual generation. SVG constructs a feature\nspace with clear semantic discriminability by leveraging frozen DINO features,\nwhile a lightweight residual branch captures fine-grained details for\nhigh-fidelity reconstruction. Diffusion models are trained directly on this\nsemantically structured latent space to facilitate more efficient learning. As\na result, SVG enables accelerated diffusion training, supports few-step\nsampling, and improves generative quality. Experimental results further show\nthat SVG preserves the semantic and discriminative capabilities of the\nunderlying self-supervised representations, providing a principled pathway\ntoward task-general, high-quality visual representations.",
        "url": "http://arxiv.org/abs/2510.15301v1",
        "published_date": "2025-10-17T04:17:44+00:00",
        "updated_date": "2025-10-17T04:17:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Minglei Shi",
            "Haolin Wang",
            "Wenzhao Zheng",
            "Ziyang Yuan",
            "Xiaoshi Wu",
            "Xintao Wang",
            "Pengfei Wan",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "tldr": "The paper introduces SVG, a latent diffusion model for visual generation that replaces VAEs with self-supervised (DINO) representations to improve training efficiency, sampling speed, and generative quality.",
        "tldr_zh": "该论文介绍了SVG，一种用于视觉生成的潜在扩散模型，它用自监督（DINO）表示取代VAE，以提高训练效率、采样速度和生成质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion",
        "summary": "We present DriveGen3D, a novel framework for generating high-quality and\nhighly controllable dynamic 3D driving scenes that addresses critical\nlimitations in existing methodologies. Current approaches to driving scene\nsynthesis either suffer from prohibitive computational demands for extended\ntemporal generation, focus exclusively on prolonged video synthesis without 3D\nrepresentation, or restrict themselves to static single-scene reconstruction.\nOur work bridges this methodological gap by integrating accelerated long-term\nvideo generation with large-scale dynamic scene reconstruction through\nmultimodal conditional control. DriveGen3D introduces a unified pipeline\nconsisting of two specialized components: FastDrive-DiT, an efficient video\ndiffusion transformer for high-resolution, temporally coherent video synthesis\nunder text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a\nfeed-forward reconstruction module that rapidly builds 3D Gaussian\nrepresentations across time, ensuring spatial-temporal consistency. Together,\nthese components enable real-time generation of extended driving videos (up to\n$424\\times800$ at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM\nof 0.811 and PSNR of 22.84 on novel view synthesis, all while maintaining\nparameter efficiency.",
        "url": "http://arxiv.org/abs/2510.15264v1",
        "published_date": "2025-10-17T03:00:08+00:00",
        "updated_date": "2025-10-17T03:00:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weijie Wang",
            "Jiagang Zhu",
            "Zeyu Zhang",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Guosheng Zhao",
            "Chaojun Ni",
            "Haoxiao Wang",
            "Guan Huang",
            "Xinze Chen",
            "Yukun Zhou",
            "Wenkang Qin",
            "Duochao Shi",
            "Haoyun Li",
            "Guanghong Jia",
            "Jiwen Lu"
        ],
        "tldr": "DriveGen3D introduces a novel framework for generating high-quality, controllable dynamic 3D driving scenes by integrating efficient video diffusion with large-scale dynamic scene reconstruction, enabling real-time generation of extended driving videos and corresponding 3D scenes.",
        "tldr_zh": "DriveGen3D 提出了一个新颖的框架，通过将高效的视频扩散与大规模动态场景重建相结合，生成高质量、可控的动态 3D 驾驶场景，从而能够实时生成扩展的驾驶视频和相应的 3D 场景。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery",
        "summary": "Synthesizing large-scale, explorable, and geometrically accurate 3D urban\nscenes is a challenging yet valuable task in providing immersive and embodied\napplications. The challenges lie in the lack of large-scale and high-quality\nreal-world 3D scans for training generalizable generative models. In this\npaper, we take an alternative route to create large-scale 3D scenes by\nsynergizing the readily available satellite imagery that supplies realistic\ncoarse geometry and the open-domain diffusion model for creating high-quality\nclose-up appearances. We propose \\textbf{Skyfall-GS}, the first city-block\nscale 3D scene creation framework without costly 3D annotations, also featuring\nreal-time, immersive 3D exploration. We tailor a curriculum-driven iterative\nrefinement strategy to progressively enhance geometric completeness and\nphotorealistic textures. Extensive experiments demonstrate that Skyfall-GS\nprovides improved cross-view consistent geometry and more realistic textures\ncompared to state-of-the-art approaches. Project page:\nhttps://skyfall-gs.jayinnn.dev/",
        "url": "http://arxiv.org/abs/2510.15869v1",
        "published_date": "2025-10-17T17:59:51+00:00",
        "updated_date": "2025-10-17T17:59:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jie-Ying Lee",
            "Yi-Ruei Liu",
            "Shr-Ruei Tsai",
            "Wei-Cheng Chang",
            "Chung-Ho Wu",
            "Jiewen Chan",
            "Zhenjun Zhao",
            "Chieh Hubert Lin",
            "Yu-Lun Liu"
        ],
        "tldr": "Skyfall-GS synthesizes large-scale 3D urban scenes from satellite imagery by combining coarse geometry with open-domain diffusion models, achieving real-time exploration without requiring costly 3D annotations.",
        "tldr_zh": "Skyfall-GS通过结合粗略几何形状和开放域扩散模型，从卫星图像合成大规模3D城市场景，实现实时探索，且无需昂贵的3D标注。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior",
        "summary": "In this paper, we study the content-aware layout generation problem, which\naims to automatically generate layouts that are harmonious with a given\nbackground image. Existing methods usually deal with this task with a\nsingle-step reasoning framework. The lack of a feedback-based self-correction\nmechanism leads to their failure rates significantly increasing when faced with\ncomplex element layout planning. To address this challenge, we introduce SEGA,\na novel Stepwise Evolution Paradigm for Content-Aware Layout Generation.\nInspired by the systematic mode of human thinking, SEGA employs a hierarchical\nreasoning framework with a coarse-to-fine strategy: first, a coarse-level\nmodule roughly estimates the layout planning results; then, another refining\nmodule performs fine-level reasoning regarding the coarse planning results.\nFurthermore, we incorporate layout design principles as prior knowledge into\nthe model to enhance its layout planning ability. Besides, we present\nGenPoster-100K that is a new large-scale poster dataset with rich\nmeta-information annotation. The experiments demonstrate the effectiveness of\nour approach by achieving the state-of-the-art results on multiple benchmark\ndatasets. Our project page is at: https://brucew91.github.io/SEGA.github.io/",
        "url": "http://arxiv.org/abs/2510.15749v1",
        "published_date": "2025-10-17T15:36:26+00:00",
        "updated_date": "2025-10-17T15:36:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoran Wang",
            "Bo Zhao",
            "Jinghui Wang",
            "Hanzhang Wang",
            "Huan Yang",
            "Wei Ji",
            "Hao Liu",
            "Xinyan Xiao"
        ],
        "tldr": "The paper introduces SEGA, a novel stepwise evolution paradigm for content-aware layout generation, employing a hierarchical reasoning framework and design priors, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了SEGA，一种用于内容感知布局生成的新型逐步演化范式，采用分层推理框架和设计先验，实现了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Salient Concept-Aware Generative Data Augmentation",
        "summary": "Recent generative data augmentation methods conditioned on both image and\ntext prompts struggle to balance between fidelity and diversity, as it is\nchallenging to preserve essential image details while aligning with varied text\nprompts. This challenge arises because representations in the synthesis process\noften become entangled with non-essential input image attributes such as\nenvironmental contexts, creating conflicts with text prompts intended to modify\nthese elements. To address this, we propose a personalized image generation\nframework that uses a salient concept-aware image embedding model to reduce the\ninfluence of irrelevant visual details during the synthesis process, thereby\nmaintaining intuitive alignment between image and text inputs. By generating\nimages that better preserve class-discriminative features with additional\ncontrolled variations, our framework effectively enhances the diversity of\ntraining datasets and thereby improves the robustness of downstream models. Our\napproach demonstrates superior performance across eight fine-grained vision\ndatasets, outperforming state-of-the-art augmentation methods with averaged\nclassification accuracy improvements by 0.73% and 6.5% under conventional and\nlong-tail settings, respectively.",
        "url": "http://arxiv.org/abs/2510.15194v1",
        "published_date": "2025-10-16T23:31:55+00:00",
        "updated_date": "2025-10-16T23:31:55+00:00",
        "categories": [
            "cs.CV",
            "68T45 (Machine learning)",
            "I.2.10; I.2.6; I.4.8; I.5.1; I.5.4"
        ],
        "authors": [
            "Tianchen Zhao",
            "Xuanbai Chen",
            "Zhihua Li",
            "Jun Fang",
            "Dongsheng An",
            "Xiang Xu",
            "Zhuowen Tu",
            "Yifan Xing"
        ],
        "tldr": "This paper introduces a salient concept-aware generative data augmentation framework that improves the fidelity and diversity of generated images by reducing the influence of irrelevant visual details, leading to better performance on fine-grained classification tasks.",
        "tldr_zh": "该论文提出了一种显著概念感知的生成数据增强框架，通过减少不相关视觉细节的影响，提高了生成图像的保真度和多样性，从而在细粒度分类任务中表现出更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Post-Processing Methods for Improving Accuracy in MRI Inpainting",
        "summary": "Magnetic Resonance Imaging (MRI) is the primary imaging modality used in the\ndiagnosis, assessment, and treatment planning for brain pathologies. However,\nmost automated MRI analysis tools, such as segmentation and registration\npipelines, are optimized for healthy anatomies and often fail when confronted\nwith large lesions such as tumors. To overcome this, image inpainting\ntechniques aim to locally synthesize healthy brain tissues in tumor regions,\nenabling the reliable application of general-purpose tools. In this work, we\nsystematically evaluate state-of-the-art inpainting models and observe a\nsaturation in their standalone performance. In response, we introduce a\nmethodology combining model ensembling with efficient post-processing\nstrategies such as median filtering, histogram matching, and pixel averaging.\nFurther anatomical refinement is achieved via a lightweight U-Net enhancement\nstage. Comprehensive evaluation demonstrates that our proposed pipeline\nimproves the anatomical plausibility and visual fidelity of inpainted regions,\nyielding higher accuracy and more robust outcomes than individual baseline\nmodels. By combining established models with targeted post-processing, we\nachieve improved and more accessible inpainting outcomes, supporting broader\nclinical deployment and sustainable, resource-conscious research. Our 2025\nBraTS inpainting docker is available at\nhttps://hub.docker.com/layers/aparida12/brats2025/inpt.",
        "url": "http://arxiv.org/abs/2510.15282v1",
        "published_date": "2025-10-17T03:42:23+00:00",
        "updated_date": "2025-10-17T03:42:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Nishad Kulkarni",
            "Krithika Iyer",
            "Austin Tapp",
            "Abhijeet Parida",
            "Daniel Capellán-Martín",
            "Zhifan Jiang",
            "María J. Ledesma-Carbayo",
            "Syed Muhammad Anwar",
            "Marius George Linguraru"
        ],
        "tldr": "This paper introduces a post-processing pipeline for improving MRI inpainting accuracy, combining model ensembling, filtering, histogram matching, pixel averaging, and a U-Net enhancement stage, leading to improved anatomical plausibility and accuracy.",
        "tldr_zh": "本文介绍了一种用于提高MRI图像修复精度的后处理流程，结合了模型集成、滤波、直方图匹配、像素平均和U-Net增强阶段，从而提高了重建区域的解剖学合理性和准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Generalized Dynamics Generation towards Scannable Physical World Model",
        "summary": "Digital twin worlds with realistic interactive dynamics presents a new\nopportunity to develop generalist embodied agents in scannable environments\nwith complex physical behaviors. To this end, we present GDGen (Generalized\nRepresentation for Generalized Dynamics Generation), a framework that takes a\npotential energy perspective to seamlessly integrate rigid body, articulated\nbody, and soft body dynamics into a unified, geometry-agnostic system. GDGen\noperates from the governing principle that the potential energy for any stable\nphysical system should be low. This fresh perspective allows us to treat the\nworld as one holistic entity and infer underlying physical properties from\nsimple motion observations. We extend classic elastodynamics by introducing\ndirectional stiffness to capture a broad spectrum of physical behaviors,\ncovering soft elastic, articulated, and rigid body systems. We propose a\nspecialized network to model the extended material property and employ a neural\nfield to represent deformation in a geometry-agnostic manner. Extensive\nexperiments demonstrate that GDGen robustly unifies diverse simulation\nparadigms, offering a versatile foundation for creating interactive virtual\nenvironments and training robotic agents in complex, dynamically rich\nscenarios.",
        "url": "http://arxiv.org/abs/2510.15041v1",
        "published_date": "2025-10-16T18:00:58+00:00",
        "updated_date": "2025-10-16T18:00:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yichen Li",
            "Zhiyi Li",
            "Brandon Feng",
            "Dinghuai Zhang",
            "Antonio Torralba"
        ],
        "tldr": "The paper introduces GDGen, a framework using potential energy to unify rigid, articulated, and soft body dynamics for creating realistic and interactive digital twin environments, which are useful for training embodied agents.",
        "tldr_zh": "该论文介绍了GDGen，一个利用势能统一刚体、铰接体和柔体动力学的框架，用于创建逼真和可交互的数字孪生环境，可用于训练具身智能体。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]