[
    {
        "title": "Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model",
        "summary": "Standard Latent Diffusion Models rely on a complex, three-part architecture consisting of a separate encoder, decoder, and diffusion network, which are trained in multiple stages. This modular design is computationally inefficient, leads to suboptimal performance, and prevents the unification of diffusion with the single-network architectures common in vision foundation models. Our goal is to unify these three components into a single, end-to-end trainable network. We first demonstrate that a naive joint training approach fails catastrophically due to ``latent collapse'', where the diffusion training objective interferes with the network's ability to learn a good latent representation. We identify the root causes of this instability by drawing a novel analogy between diffusion and self-distillation based unsupervised learning method. Based on this insight, we propose Diffusion as Self-Distillation (DSD), a new framework with key modifications to the training objective that stabilize the latent space. This approach enables, for the first time, the stable end-to-end training of a single network that simultaneously learns to encode, decode, and perform diffusion. DSD achieves outstanding performance on the ImageNet $256\\times 256$ conditional generation task: FID=13.44/6.38/4.25 with only 42M/118M/205M parameters and 50 training epochs on ImageNet, without using classifier-free-guidance.",
        "url": "http://arxiv.org/abs/2511.14716v1",
        "published_date": "2025-11-18T17:58:16+00:00",
        "updated_date": "2025-11-18T17:58:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiyuan Wang",
            "Muhan Zhang"
        ],
        "tldr": "The paper introduces Diffusion as Self-Distillation (DSD), a novel approach enabling end-to-end training of latent diffusion models within a single network, achieving competitive performance on ImageNet generation with significantly fewer parameters and training epochs.",
        "tldr_zh": "该论文介绍了扩散即自蒸馏（DSD），一种新颖的方法，可以在单个网络中实现潜在扩散模型的端到端训练，以更少的参数和更少的训练周期在ImageNet生成上实现了具有竞争力的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation",
        "summary": "The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim",
        "url": "http://arxiv.org/abs/2511.14712v1",
        "published_date": "2025-11-18T17:56:04+00:00",
        "updated_date": "2025-11-18T17:56:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunfeng Wu",
            "Jiayi Song",
            "Zhenxiong Tan",
            "Zihao He",
            "Songhua Liu"
        ],
        "tldr": "The paper introduces FreeSwim, a training-free method for generating ultra-high-resolution videos by improving sliding-window attention in pre-trained Diffusion Transformers, achieving state-of-the-art performance on VBench with high efficiency.",
        "tldr_zh": "该论文介绍了FreeSwim，一种无需训练的方法，通过改进预训练扩散Transformer中的滑动窗口注意力机制来生成超高分辨率视频，并在VBench上实现了最先进的性能，且效率很高。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Text-Driven Reasoning Video Editing via Reinforcement Learning on Digital Twin Representations",
        "summary": "Text-driven video editing enables users to modify video content only using text queries. While existing methods can modify video content if explicit descriptions of editing targets with precise spatial locations and temporal boundaries are provided, these requirements become impractical when users attempt to conceptualize edits through implicit queries referencing semantic properties or object relationships. We introduce reasoning video editing, a task where video editing models must interpret implicit queries through multi-hop reasoning to infer editing targets before executing modifications, and a first model attempting to solve this complex task, RIVER (Reasoning-based Implicit Video Editor). RIVER decouples reasoning from generation through digital twin representations of video content that preserve spatial relationships, temporal trajectories, and semantic attributes. A large language model then processes this representation jointly with the implicit query, performing multi-hop reasoning to determine modifications, then outputs structured instructions that guide a diffusion-based editor to execute pixel-level changes. RIVER training uses reinforcement learning with rewards that evaluate reasoning accuracy and generation quality. Finally, we introduce RVEBenchmark, a benchmark of 100 videos with 519 implicit queries spanning three levels and categories of reasoning complexity specifically for reasoning video editing. RIVER demonstrates best performance on the proposed RVEBenchmark and also achieves state-of-the-art performance on two additional video editing benchmarks (VegGIE and FiVE), where it surpasses six baseline methods.",
        "url": "http://arxiv.org/abs/2511.14100v1",
        "published_date": "2025-11-18T03:37:19+00:00",
        "updated_date": "2025-11-18T03:37:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiqing Shen",
            "Chenjia Li",
            "Mathias Unberath"
        ],
        "tldr": "This paper introduces a new task, reasoning video editing, where models must infer editing targets from implicit queries and proposes a novel model, RIVER, that uses digital twin representations and reinforcement learning for training, achieving state-of-the-art performance on multiple benchmarks.",
        "tldr_zh": "本文介绍了一个新的任务，即推理视频编辑，其中模型必须从隐式查询中推断编辑目标。同时，提出了一种新的模型RIVER，该模型使用数字孪生表示和强化学习进行训练，并在多个基准测试中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark",
        "summary": "While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physics-governed dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoning -- materializing thought as frame-by-frame visual sequences, with each frame representing a physically-grounded reasoning step. Despite compelling demonstrations, a challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions -- from perceptual logic to abstract planning -- and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLM-assisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators.",
        "url": "http://arxiv.org/abs/2511.13853v1",
        "published_date": "2025-11-17T19:11:39+00:00",
        "updated_date": "2025-11-17T19:11:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinxin Liu",
            "Zhaopan Xu",
            "Kai Wang",
            "Yong Jae Lee",
            "Yuzhang Shang"
        ],
        "tldr": "The paper introduces Gen-ViRe, a new benchmark to evaluate the reasoning capabilities of video generation models, highlighting a gap between visual quality and actual reasoning depth in current state-of-the-art systems.",
        "tldr_zh": "本文介绍了 Gen-ViRe，一个新的基准测试，用于评估视频生成模型的推理能力，突出了当前最先进系统中视觉质量与实际推理深度之间的差距。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models",
        "summary": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.",
        "url": "http://arxiv.org/abs/2511.13704v1",
        "published_date": "2025-11-17T18:52:44+00:00",
        "updated_date": "2025-11-17T18:52:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Harold Haodong Chen",
            "Disen Lan",
            "Wen-Jie Shu",
            "Qingyang Liu",
            "Zihan Wang",
            "Sirui Chen",
            "Wenkai Cheng",
            "Kanghao Chen",
            "Hongfei Zhang",
            "Zixin Zhang",
            "Rongjin Guo",
            "Yu Cheng",
            "Ying-Cong Chen"
        ],
        "tldr": "The paper introduces TiViBench, a new benchmark for evaluating reasoning capabilities in video generation models, and VideoTPO, a test-time preference optimization strategy to improve reasoning performance, demonstrating results on commercial and open-source models.",
        "tldr_zh": "本文提出了 TiViBench，一个新的用于评估视频生成模型推理能力的基准，以及 VideoTPO，一种用于提升推理性能的测试时偏好优化策略，并在商业和开源模型上展示了结果。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 10,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Let Language Constrain Geometry: Vision-Language Models as Semantic and Spatial Critics for 3D Generation",
        "summary": "Text-to-3D generation has advanced rapidly, yet state-of-the-art models, encompassing both optimization-based and feed-forward architectures, still face two fundamental limitations. First, they struggle with coarse semantic alignment, often failing to capture fine-grained prompt details. Second, they lack robust 3D spatial understanding, leading to geometric inconsistencies and catastrophic failures in part assembly and spatial relationships. To address these challenges, we propose VLM3D, a general framework that repurposes large vision-language models (VLMs) as powerful, differentiable semantic and spatial critics. Our core contribution is a dual-query critic signal derived from the VLM's Yes or No log-odds, which assesses both semantic fidelity and geometric coherence. We demonstrate the generality of this guidance signal across two distinct paradigms: (1) As a reward objective for optimization-based pipelines, VLM3D significantly outperforms existing methods on standard benchmarks. (2) As a test-time guidance module for feed-forward pipelines, it actively steers the iterative sampling process of SOTA native 3D models to correct severe spatial errors. VLM3D establishes a principled and generalizable path to inject the VLM's rich, language-grounded understanding of both semantics and space into diverse 3D generative pipelines.",
        "url": "http://arxiv.org/abs/2511.14271v1",
        "published_date": "2025-11-18T09:05:26+00:00",
        "updated_date": "2025-11-18T09:05:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weimin Bai",
            "Yubo Li",
            "Weijian Luo",
            "Zeqiang Lai",
            "Yequan Wang",
            "Wenzheng Chen",
            "He Sun"
        ],
        "tldr": "The paper introduces VLM3D, a framework that utilizes vision-language models as critics to improve text-to-3D generation by addressing issues in semantic alignment and 3D spatial understanding. It's applicable to both optimization-based and feed-forward 3D generation pipelines.",
        "tldr_zh": "该论文介绍了VLM3D，一个利用视觉语言模型作为评价器的框架，通过解决语义对齐和3D空间理解中的问题来改进文本到3D的生成。它适用于基于优化和前馈的3D生成流程。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution",
        "summary": "We introduce Orion, a visual agent framework that can take in any modality and generate any modality. Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results. Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance on MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic vision-language models to production-grade visual intelligence. By combining neural perception with symbolic execution, Orion enables autonomous visual reasoning, marking a transition from passive visual understanding to active, tool-driven visual intelligence.",
        "url": "http://arxiv.org/abs/2511.14210v1",
        "published_date": "2025-11-18T07:41:02+00:00",
        "updated_date": "2025-11-18T07:41:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "N Dinesh Reddy",
            "Sudeep Pillai"
        ],
        "tldr": "Orion is a visual agent framework that leverages tool-calling to perform complex multi-step visual workflows, achieving state-of-the-art results on various multimodal benchmarks, moving beyond descriptive outputs to active, tool-driven visual intelligence.",
        "tldr_zh": "Orion是一个视觉代理框架，它利用工具调用来执行复杂的多步骤视觉工作流程，在各种多模态基准测试上取得了最先进的结果，从描述性输出转变为主动的、工具驱动的视觉智能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semantic Context Matters: Improving Conditioning for Autoregressive Models",
        "summary": "Recently, autoregressive (AR) models have shown strong potential in image generation, offering better scalability and easier integration with unified multi-modal systems compared to diffusion-based methods. However, extending AR models to general image editing remains challenging due to weak and inefficient conditioning, often leading to poor instruction adherence and visual artifacts. To address this, we propose SCAR, a Semantic-Context-driven method for Autoregressive models. SCAR introduces two key components: Compressed Semantic Prefilling, which encodes high-level semantics into a compact and efficient prefix, and Semantic Alignment Guidance, which aligns the last visual hidden states with target semantics during autoregressive decoding to enhance instruction fidelity. Unlike decoding-stage injection methods, SCAR builds upon the flexibility and generality of vector-quantized-based prefilling while overcoming its semantic limitations and high cost. It generalizes across both next-token and next-set AR paradigms with minimal architectural changes. SCAR achieves superior visual fidelity and semantic alignment on both instruction editing and controllable generation benchmarks, outperforming prior AR-based methods while maintaining controllability. All code will be released.",
        "url": "http://arxiv.org/abs/2511.14063v1",
        "published_date": "2025-11-18T02:42:24+00:00",
        "updated_date": "2025-11-18T02:42:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dongyang Jin",
            "Ryan Xu",
            "Jianhao Zeng",
            "Rui Lan",
            "Yancheng Bai",
            "Lei Sun",
            "Xiangxiang Chu"
        ],
        "tldr": "The paper introduces SCAR, a method to improve instruction adherence and visual quality in autoregressive image generation models by enhancing semantic conditioning during prefilling and decoding.",
        "tldr_zh": "该论文介绍了SCAR，一种通过增强预填充和解码过程中的语义条件来提高自回归图像生成模型中指令遵循度和视觉质量的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios",
        "summary": "Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.",
        "url": "http://arxiv.org/abs/2511.13970v1",
        "published_date": "2025-11-17T22:58:27+00:00",
        "updated_date": "2025-11-17T22:58:27+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Sanjay Acharjee",
            "Abir Khan Ratul",
            "Diego Patino",
            "Md Nazmus Sakib"
        ],
        "tldr": "This paper introduces a scene graph-guided generative AI framework using GPT-4o and diffusion models to synthesize photorealistic images of industrial hazard scenarios, evaluated with a novel VQA Graph Score.",
        "tldr_zh": "该论文介绍了一种基于场景图引导的生成式人工智能框架，利用GPT-4o和扩散模型合成逼真的工业危险场景图像，并使用一种新的VQA图评分进行评估。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Temporal Realism Evaluation of Generated Videos Using Compressed-Domain Motion Vectors",
        "summary": "Temporal realism remains a central weakness of current generative video models, as most evaluation metrics prioritize spatial appearance and offer limited sensitivity to motion. We introduce a scalable, model-agnostic framework that assesses temporal behavior using motion vectors (MVs) extracted directly from compressed video streams. Codec-generated MVs from standards such as H.264 and HEVC provide lightweight, resolution-consistent descriptors of motion dynamics. We quantify realism by computing Kullback-Leibler, Jensen-Shannon, and Wasserstein divergences between MV statistics of real and generated videos. Experiments on the GenVidBench dataset containing videos from eight state-of-the-art generators reveal systematic discrepancies from real motion: entropy-based divergences rank Pika and SVD as closest to real videos, MV-sum statistics favor VC2 and Text2Video-Zero, and CogVideo shows the largest deviations across both measures. Visualizations of MV fields and class-conditional motion heatmaps further reveal center bias, sparse and piecewise constant flows, and grid-like artifacts that frame-level metrics do not capture. Beyond evaluation, we investigate MV-RGB fusion through channel concatenation, cross-attention, joint embedding, and a motion-aware fusion module. Incorporating MVs improves downstream classification across ResNet, I3D, and TSN backbones, with ResNet-18 and ResNet-34 reaching up to 97.4% accuracy and I3D achieving 99.0% accuracy on real-versus-generated discrimination. These findings demonstrate that compressed-domain MVs provide an effective temporal signal for diagnosing motion defects in generative videos and for strengthening temporal reasoning in discriminative models. The implementation is available at: https://github.com/KurbanIntelligenceLab/Motion-Vector-Learning",
        "url": "http://arxiv.org/abs/2511.13897v1",
        "published_date": "2025-11-17T20:47:06+00:00",
        "updated_date": "2025-11-17T20:47:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mert Onur Cakiroglu",
            "Idil Bilge Altun",
            "Zhihe Lu",
            "Mehmet Dalkilic",
            "Hasan Kurban"
        ],
        "tldr": "This paper introduces a framework for evaluating the temporal realism of generated videos by analyzing motion vectors from compressed video streams, revealing motion artifacts and improving real/fake video discrimination.",
        "tldr_zh": "本文介绍了一个通过分析压缩视频流中的运动向量来评估生成视频的时间真实性的框架，揭示了运动伪影并提高了真实/伪造视频的辨别能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Back to Basics: Let Denoising Generative Models Denoise",
        "summary": "Today's denoising diffusion models do not \"denoise\" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than \"$\\textbf{Just image Transformers}$\", or $\\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.",
        "url": "http://arxiv.org/abs/2511.13720v1",
        "published_date": "2025-11-17T18:59:57+00:00",
        "updated_date": "2025-11-17T18:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianhong Li",
            "Kaiming He"
        ],
        "tldr": "This paper proposes a denoising diffusion model that directly predicts clean images using image Transformers (JiT) on pixels, achieving competitive results on ImageNet by leveraging the manifold assumption.",
        "tldr_zh": "该论文提出了一种去噪扩散模型，该模型直接使用图像Transformer (JiT)预测清晰的图像，并通过利用流形假设在ImageNet上取得有竞争力的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine",
        "summary": "Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.",
        "url": "http://arxiv.org/abs/2511.13713v1",
        "published_date": "2025-11-17T18:57:39+00:00",
        "updated_date": "2025-11-17T18:57:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xincheng Shuai",
            "Zhenyuan Qin",
            "Henghui Ding",
            "Dacheng Tao"
        ],
        "tldr": "The paper introduces FFSE, a 3D-aware autoregressive framework for performing intuitive and physically-consistent object manipulation in real-world images, supporting multi-round editing and realistic background effects.",
        "tldr_zh": "该论文介绍了一种名为FFSE的3D感知自回归框架，用于在真实世界图像中执行直观且物理一致的对象操作，支持多轮编辑和逼真的背景效果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation",
        "summary": "Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\\href{https://github.com/allenai/olmoearth_pretrain}{\\text{https://github.com/allenai/olmoearth_pretrain}}$.",
        "url": "http://arxiv.org/abs/2511.13655v1",
        "published_date": "2025-11-17T18:06:26+00:00",
        "updated_date": "2025-11-17T18:06:26+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Henry Herzog",
            "Favyen Bastani",
            "Yawen Zhang",
            "Gabriel Tseng",
            "Joseph Redmon",
            "Hadrien Sablon",
            "Ryan Park",
            "Jacob Morrison",
            "Alexandra Buraczynski",
            "Karen Farley",
            "Joshua Hansen",
            "Andrew Howe",
            "Patrick Alan Johnson",
            "Mark Otterlee",
            "Ted Schmitt",
            "Hunter Pitelka",
            "Stephen Daspit",
            "Rachel Ratner",
            "Christopher Wilhelm",
            "Sebastian Wood",
            "Mike Jacobi",
            "Hannah Kerner",
            "Evan Shelhamer",
            "Ali Farhadi",
            "Ranjay Krishna",
            "Patrick Beukema"
        ],
        "tldr": "OlmoEarth is a new multimodal foundation model for Earth observation data, achieving state-of-the-art performance on various benchmarks and deployed as a platform for non-profits and NGOs.",
        "tldr_zh": "OlmoEarth 是一种新的用于地球观测数据的多模态基础模型，在各种基准测试中实现了最先进的性能，并已部署为一个供非营利组织和非政府组织使用的平台。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GEN3D: Generating Domain-Free 3D Scenes from a Single Image",
        "summary": "Despite recent advancements in neural 3D reconstruction, the dependence on dense multi-view captures restricts their broader applicability. Additionally, 3D scene generation is vital for advancing embodied AI and world models, which depend on diverse, high-quality scenes for learning and evaluation. In this work, we propose Gen3d, a novel method for generation of high-quality, wide-scope, and generic 3D scenes from a single image. After the initial point cloud is created by lifting the RGBD image, Gen3d maintains and expands its world model. The 3D scene is finalized through optimizing a Gaussian splatting representation. Extensive experiments on diverse datasets demonstrate the strong generalization capability and superior performance of our method in generating a world model and Synthesizing high-fidelity and consistent novel views.",
        "url": "http://arxiv.org/abs/2511.14291v1",
        "published_date": "2025-11-18T09:40:43+00:00",
        "updated_date": "2025-11-18T09:40:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuxin Zhang",
            "Ziyu Lu",
            "Hongbo Duan",
            "Keyu Fan",
            "Pengting Luo",
            "Peiyu Zhuang",
            "Mengyu Yang",
            "Houde Liu"
        ],
        "tldr": "Gen3D proposes a method for generating high-quality, domain-free 3D scenes from a single image by expanding an initial point cloud and optimizing a Gaussian splatting representation, showcasing strong generalization and novel view synthesis.",
        "tldr_zh": "Gen3D提出了一种由单张图像生成高质量、无领域限制3D场景的方法，通过扩展初始点云并优化高斯溅射表示来实现，展示了强大的泛化能力和新视角合成能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GloTok: Global Perspective Tokenizer for Image Reconstruction and Generation",
        "summary": "Existing state-of-the-art image tokenization methods leverage diverse semantic features from pre-trained vision models for additional supervision, to expand the distribution of latent representations and thereby improve the quality of image reconstruction and generation. These methods employ a locally supervised approach for semantic supervision, which limits the uniformity of semantic distribution. However, VA-VAE proves that a more uniform feature distribution yields better generation performance. In this work, we introduce a Global Perspective Tokenizer (GloTok), which utilizes global relational information to model a more uniform semantic distribution of tokenized features. Specifically, a codebook-wise histogram relation learning method is proposed to transfer the semantics, which are modeled by pre-trained models on the entire dataset, to the semantic codebook. Then, we design a residual learning module that recovers the fine-grained details to minimize the reconstruction error caused by quantization. Through the above design, GloTok delivers more uniformly distributed semantic latent representations, which facilitates the training of autoregressive (AR) models for generating high-quality images without requiring direct access to pre-trained models during the training process. Experiments on the standard ImageNet-1k benchmark clearly show that our proposed method achieves state-of-the-art reconstruction performance and generation quality.",
        "url": "http://arxiv.org/abs/2511.14184v1",
        "published_date": "2025-11-18T06:40:26+00:00",
        "updated_date": "2025-11-18T06:40:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuan Zhao",
            "Zhongyu Zhang",
            "Yuge Huang",
            "Yuxi Mi",
            "Guodong Mu",
            "Shouhong Ding",
            "Jun Wang",
            "Rizen Guo",
            "Shuigeng Zhou"
        ],
        "tldr": "GloTok introduces a global relation-based image tokenizer to achieve more uniform semantic latent representations, improving image reconstruction and generation without direct access to pretrained models during training.",
        "tldr_zh": "GloTok 提出了一种基于全局关系图像的 tokenizer，以实现更统一的语义潜在表示，从而提高图像重建和生成质量，且在训练过程中无需直接访问预训练模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Error-Driven Scene Editing for 3D Grounding in Large Language Models",
        "summary": "Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured \"Decompose, Diagnostic Evaluation, Edit, and Re-train\" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.",
        "url": "http://arxiv.org/abs/2511.14086v1",
        "published_date": "2025-11-18T03:13:29+00:00",
        "updated_date": "2025-11-18T03:13:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Yue Zhang",
            "Zun Wang",
            "Han Lin",
            "Jialu Li",
            "Jianing Yang",
            "Yonatan Bitton",
            "Idan Szpektor",
            "Mohit Bansal"
        ],
        "tldr": "The paper introduces DEER-3D, an error-driven framework for editing 3D scenes to improve the grounding accuracy of 3D Large Language Models by generating targeted visual counterfactuals and iteratively fine-tuning the model using decomposed predicate-level errors.",
        "tldr_zh": "该论文介绍了DEER-3D，一个通过编辑3D场景来提高3D大型语言模型接地的准确性的框架。 该框架通过生成有针对性的视觉反事实数据并使用分解的谓词级别错误迭代地微调模型来实现。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FashionMAC: Deformation-Free Fashion Image Generation with Fine-Grained Model Appearance Customization",
        "summary": "Garment-centric fashion image generation aims to synthesize realistic and controllable human models dressing a given garment, which has attracted growing interest due to its practical applications in e-commerce. The key challenges of the task lie in two aspects: (1) faithfully preserving the garment details, and (2) gaining fine-grained controllability over the model's appearance. Existing methods typically require performing garment deformation in the generation process, which often leads to garment texture distortions. Also, they fail to control the fine-grained attributes of the generated models, due to the lack of specifically designed mechanisms. To address these issues, we propose FashionMAC, a novel diffusion-based deformation-free framework that achieves high-quality and controllable fashion showcase image generation. The core idea of our framework is to eliminate the need for performing garment deformation and directly outpaint the garment segmented from a dressed person, which enables faithful preservation of the intricate garment details. Moreover, we propose a novel region-adaptive decoupled attention (RADA) mechanism along with a chained mask injection strategy to achieve fine-grained appearance controllability over the synthesized human models. Specifically, RADA adaptively predicts the generated regions for each fine-grained text attribute and enforces the text attribute to focus on the predicted regions by a chained mask injection strategy, significantly enhancing the visual fidelity and the controllability. Extensive experiments validate the superior performance of our framework compared to existing state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2511.14031v1",
        "published_date": "2025-11-18T01:22:14+00:00",
        "updated_date": "2025-11-18T01:22:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rong Zhang",
            "Jinxiao Li",
            "Jingnan Wang",
            "Zhiwen Zuo",
            "Jianfeng Dong",
            "Wei Li",
            "Chi Wang",
            "Weiwei Xu",
            "Xun Wang"
        ],
        "tldr": "FashionMAC is a diffusion-based framework for generating realistic and controllable fashion images without garment deformation, employing a region-adaptive decoupled attention mechanism for fine-grained appearance control.",
        "tldr_zh": "FashionMAC是一个基于扩散模型的框架，用于生成逼真且可控的时尚图像，无需服装变形，并采用区域自适应解耦注意力机制，实现细粒度的外观控制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation",
        "summary": "Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.",
        "url": "http://arxiv.org/abs/2511.13689v2",
        "published_date": "2025-11-17T18:41:16+00:00",
        "updated_date": "2025-11-18T04:27:26+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Sofia Jamil",
            "Kotla Sai Charan",
            "Sriparna Saha",
            "Koustava Goswami",
            "Joseph K J"
        ],
        "tldr": "The paper introduces the TAI framework for translating and generating images from Indian poetry, using LLMs and Latent Diffusion Models, and contributes a new dataset of morphologically rich Indian poems.",
        "tldr_zh": "该论文介绍了TAI框架，用于翻译印度诗歌并据此生成图像，利用大型语言模型和潜在扩散模型，并贡献了一个新的包含形态丰富的印度诗歌的数据集。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]