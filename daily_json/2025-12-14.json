[
    {
        "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
        "summary": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
        "url": "http://arxiv.org/abs/2512.11799v1",
        "published_date": "2025-12-12T18:59:54+00:00",
        "updated_date": "2025-12-12T18:59:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ye Fang",
            "Tong Wu",
            "Valentin Deschaintre",
            "Duygu Ceylan",
            "Iliyan Georgiev",
            "Chun-Hao Paul Huang",
            "Yiwei Hu",
            "Xuelin Chen",
            "Tuanfeng Yang Wang"
        ],
        "tldr": "V-RGBX is a novel end-to-end framework for intrinsic-aware video editing, enabling photorealistic video synthesis and editing via manipulation of intrinsic scene properties like albedo and normals using keyframes.",
        "tldr_zh": "V-RGBX 是一种新的端到端框架，可实现内在感知视频编辑，通过使用关键帧操纵诸如反照率和法线之类的内在场景属性来实现逼真的视频合成和编辑。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis",
        "summary": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.",
        "url": "http://arxiv.org/abs/2512.11797v1",
        "published_date": "2025-12-12T18:59:45+00:00",
        "updated_date": "2025-12-12T18:59:45+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Junjie Ye",
            "Rong Xue",
            "Basile Van Hoorick",
            "Pavel Tokmakov",
            "Muhammad Zubair Irshad",
            "Yue Wang",
            "Vitor Guizilini"
        ],
        "tldr": "AnchorDream repurposes video diffusion models for robot data synthesis by conditioning on robot motion renderings to generate diverse and plausible robot interaction data, leading to significant improvements in downstream policy learning.",
        "tldr_zh": "AnchorDream 通过使用机器人运动渲染作为条件，重新利用视频扩散模型进行机器人数据合成，从而生成多样且合理的机器人交互数据，并显著提高了下游策略学习的效果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
        "summary": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .",
        "url": "http://arxiv.org/abs/2512.11792v1",
        "published_date": "2025-12-12T18:56:35+00:00",
        "updated_date": "2025-12-12T18:56:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Fei",
            "George Stoica",
            "Jingyuan Liu",
            "Qifeng Chen",
            "Ranjay Krishna",
            "Xiaojuan Wang",
            "Benlin Liu"
        ],
        "tldr": "This paper introduces SAM2VideoX, a novel video diffusion model leveraging structure-preserving motion priors distilled from an autoregressive video tracking model, achieving significant improvements in video generation fidelity and realism, particularly for articulated objects.",
        "tldr_zh": "本文介绍了一种名为SAM2VideoX的新型视频扩散模型，该模型利用从自回归视频跟踪模型中提取的结构保持运动先验，在视频生成保真度和真实性方面取得了显著改进，尤其是在关节物体上。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]