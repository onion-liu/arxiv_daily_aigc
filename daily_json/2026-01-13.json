[
    {
        "title": "Beyond External Guidance: Unleashing the Semantic Richness Inside Diffusion Transformers for Improved Training",
        "summary": "Recent works such as REPA have shown that guiding diffusion models with external semantic features (e.g., DINO) can significantly accelerate the training of diffusion transformers (DiTs). However, this requires the use of pretrained external networks, introducing additional dependencies and reducing flexibility. In this work, we argue that DiTs actually have the power to guide the training of themselves, and propose \\textbf{Self-Transcendence}, a simple yet effective method that achieves fast convergence using internal feature supervision only. It is found that the slow convergence in DiT training primarily stems from the difficulty of representation learning in shallow layers. To address this, we initially train the DiT model by aligning its shallow features with the latent representations from the pretrained VAE for a short phase (e.g., 40 epochs), then apply classifier-free guidance to the intermediate features, enhancing their discriminative capability and semantic expressiveness. These enriched internal features, learned entirely within the model, are used as supervision signals to guide a new DiT training. Compared to existing self-contained methods, our approach brings a significant performance boost. It can even surpass REPA in terms of generation quality and convergence speed, but without the need for any external pretrained models. Our method is not only more flexible for different backbones but also has the potential to be adopted for a wider range of diffusion-based generative tasks. The source code of our method can be found at https://github.com/csslc/Self-Transcendence.",
        "url": "http://arxiv.org/abs/2601.07773v1",
        "published_date": "2026-01-12T17:52:11+00:00",
        "updated_date": "2026-01-12T17:52:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lingchen Sun",
            "Rongyuan Wu",
            "Zhengqiang Zhang",
            "Ruibin Li",
            "Yujing Sun",
            "Shuaizheng Liu",
            "Lei Zhang"
        ],
        "tldr": "The paper introduces Self-Transcendence, a novel method that improves diffusion transformer training by using internal feature supervision, achieving comparable or superior results to external feature guidance methods without relying on external pretrained models.",
        "tldr_zh": "该论文介绍了一种名为Self-Transcendence的新方法，该方法通过使用内部特征监督来改进扩散Transformer的训练，在不依赖外部预训练模型的情况下，实现了与外部特征引导方法相当或更好的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Sketch to Fresco: Efficient Diffusion Transformer with Progressive Resolution",
        "summary": "Diffusion Transformers achieve impressive generative quality but remain computationally expensive due to iterative sampling. Recently, dynamic resolution sampling has emerged as a promising acceleration technique by reducing the resolution of early sampling steps. However, existing methods rely on heuristic re-noising at every resolution transition, injecting noise that breaks cross-stage consistency and forces the model to relearn global structure. In addition, these methods indiscriminately upsample the entire latent space at once without checking which regions have actually converged, causing accumulated errors, and visible artifacts. Therefore, we propose \\textbf{Fresco}, a dynamic resolution framework that unifies re-noise and global structure across stages with progressive upsampling, preserving both the efficiency of low-resolution drafting and the fidelity of high-resolution refinement, with all stages aligned toward the same final target. Fresco achieves near-lossless acceleration across diverse domains and models, including 10$\\times$ speedup on FLUX, and 5$\\times$ on HunyuanVideo, while remaining orthogonal to distillation, quantization and feature caching, reaching 22$\\times$ speedup when combined with distilled models. Our code is in supplementary material and will be released on Github.",
        "url": "http://arxiv.org/abs/2601.07462v1",
        "published_date": "2026-01-12T12:15:30+00:00",
        "updated_date": "2026-01-12T12:15:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shikang Zheng",
            "Guantao Chen",
            "Lixuan He",
            "Jiacheng Liu",
            "Yuqi Lin",
            "Chang Zou",
            "Linfeng Zhang"
        ],
        "tldr": "The paper introduces Fresco, a dynamic resolution framework for Diffusion Transformers that accelerates the generative process by unifying re-noising and global structure across stages with progressive upsampling, achieving significant speedups with minimal quality loss across diverse domains.",
        "tldr_zh": "该论文介绍了一种用于扩散Transformer的动态分辨率框架Fresco，通过统一重噪声和跨阶段的全局结构与渐进式上采样来加速生成过程，在不同领域实现了显著的加速，且质量损失最小。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Forecast the Principal, Stabilize the Residual: Subspace-Aware Feature Caching for Efficient Diffusion Transformers",
        "summary": "Diffusion Transformer (DiT) models have achieved unprecedented quality in image and video generation, yet their iterative sampling process remains computationally prohibitive. To accelerate inference, feature caching methods have emerged by reusing intermediate representations across timesteps. However, existing caching approaches treat all feature components uniformly. We reveal that DiT feature spaces contain distinct principal and residual subspaces with divergent temporal behavior: the principal subspace evolves smoothly and predictably, while the residual subspace exhibits volatile, low-energy oscillations that resist accurate prediction. Building on this insight, we propose SVD-Cache, a subspace-aware caching framework that decomposes diffusion features via Singular Value Decomposition (SVD), applies exponential moving average (EMA) prediction to the dominant low-rank components, and directly reuses the residual subspace. Extensive experiments demonstrate that SVD-Cache achieves near-lossless across diverse models and methods, including 5.55$\\times$ speedup on FLUX and HunyuanVideo, and compatibility with model acceleration techniques including distillation, quantization and sparse attention. Our code is in supplementary material and will be released on Github.",
        "url": "http://arxiv.org/abs/2601.07396v1",
        "published_date": "2026-01-12T10:30:12+00:00",
        "updated_date": "2026-01-12T10:30:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guantao Chen",
            "Shikang Zheng",
            "Yuqi Lin",
            "Linfeng Zhang"
        ],
        "tldr": "The paper introduces SVD-Cache, a subspace-aware feature caching method for Diffusion Transformers that leverages SVD and EMA to accelerate inference by exploiting the different temporal behaviors of principal and residual feature subspaces, achieving significant speedups with near-lossless quality.",
        "tldr_zh": "该论文介绍了 SVD-Cache，一种用于扩散 Transformer 的子空间感知特征缓存方法，它利用 SVD 和 EMA 通过利用主要和残余特征子空间的不同时间行为来加速推理，以近乎无损的质量实现显着的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Inference-Time Scaling for Visual AutoRegressive modeling by Searching Representative Samples",
        "summary": "While inference-time scaling has significantly enhanced generative quality in large language and diffusion models, its application to vector-quantized (VQ) visual autoregressive modeling (VAR) remains unexplored. We introduce VAR-Scaling, the first general framework for inference-time scaling in VAR, addressing the critical challenge of discrete latent spaces that prohibit continuous path search. We find that VAR scales exhibit two distinct pattern types: general patterns and specific patterns, where later-stage specific patterns conditionally optimize early-stage general patterns. To overcome the discrete latent space barrier in VQ models, we map sampling spaces to quasi-continuous feature spaces via kernel density estimation (KDE), where high-density samples approximate stable, high-quality solutions. This transformation enables effective navigation of sampling distributions. We propose a density-adaptive hybrid sampling strategy: Top-k sampling focuses on high-density regions to preserve quality near distribution modes, while Random-k sampling explores low-density areas to maintain diversity and prevent premature convergence. Consequently, VAR-Scaling optimizes sample fidelity at critical scales to enhance output quality. Experiments in class-conditional and text-to-image evaluations demonstrate significant improvements in inference process. The code is available at https://github.com/WD7ang/VAR-Scaling.",
        "url": "http://arxiv.org/abs/2601.07293v1",
        "published_date": "2026-01-12T08:00:55+00:00",
        "updated_date": "2026-01-12T08:00:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weidong Tang",
            "Xinyan Wan",
            "Siyu Li",
            "Xiumei Wang"
        ],
        "tldr": "The paper introduces VAR-Scaling, a novel inference-time scaling framework for vector-quantized visual autoregressive models. It addresses the challenge of discrete latent spaces by using kernel density estimation and a hybrid sampling strategy, leading to improved image generation quality.",
        "tldr_zh": "该论文介绍了VAR-Scaling，这是一种用于向量量化视觉自回归模型的新型推理时缩放框架。它通过使用核密度估计和混合采样策略来解决离散潜在空间的问题，从而提高图像生成质量。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models",
        "summary": "The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model's learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\\%).",
        "url": "http://arxiv.org/abs/2601.07287v1",
        "published_date": "2026-01-12T07:48:26+00:00",
        "updated_date": "2026-01-12T07:48:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanyang Yin",
            "Yufan Deng",
            "Shenghai Yuan",
            "Kaipeng Zhang",
            "Xiao Yang",
            "Feng Zhao"
        ],
        "tldr": "This paper introduces Focal Guidance (FG) to improve text adherence in Image-to-Video (I2V) generation models by addressing semantic weakness in intermediate layers, using CLIP-guided key region anchoring and attention map transfer. They also introduce a benchmark for evaluating instruction following.",
        "tldr_zh": "本文提出了一种焦点引导（FG）方法，通过解决中间层中的语义弱点，提高图像到视频（I2V）生成模型中的文本一致性，采用CLIP引导的关键区域锚定和注意力图转移。他们还引入了一个用于评估指令遵循的基准。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Language-Grounded Multi-Domain Image Translation via Semantic Difference Guidance",
        "summary": "Multi-domain image-to-image translation re quires grounding semantic differences ex pressed in natural language prompts into corresponding visual transformations, while preserving unrelated structural and seman tic content. Existing methods struggle to maintain structural integrity and provide fine grained, attribute-specific control, especially when multiple domains are involved. We propose LACE (Language-grounded Attribute Controllable Translation), built on two compo nents: (1) a GLIP-Adapter that fuses global semantics with local structural features to pre serve consistency, and (2) a Multi-Domain Control Guidance mechanism that explicitly grounds the semantic delta between source and target prompts into per-attribute translation vec tors, aligning linguistic semantics with domain level visual changes. Together, these modules enable compositional multi-domain control with independent strength modulation for each attribute. Experiments on CelebA(Dialog) and BDD100K demonstrate that LACE achieves high visual fidelity, structural preservation, and interpretable domain-specific control, surpass ing prior baselines. This positions LACE as a cross-modal content generation framework bridging language semantics and controllable visual translation.",
        "url": "http://arxiv.org/abs/2601.07221v1",
        "published_date": "2026-01-12T05:36:15+00:00",
        "updated_date": "2026-01-12T05:36:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jongwon Ryu",
            "Joonhyung Park",
            "Jaeho Han",
            "Yeong-Seok Kim",
            "Hye-rin Kim",
            "Sunjae Yoon",
            "Junyeong Kim"
        ],
        "tldr": "The paper introduces LACE, a language-grounded image-to-image translation framework that uses semantic difference guidance to achieve fine-grained, attribute-specific control while preserving structural integrity, outperforming previous methods on CelebA(Dialog) and BDD100K datasets.",
        "tldr_zh": "本文介绍了一种名为LACE的语言引导图像到图像翻译框架，该框架使用语义差异指导来实现细粒度的、特定于属性的控制，同时保持结构完整性，并在CelebA(Dialog)和BDD100K数据集上优于以往的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VENUS: Visual Editing with Noise Inversion Using Scene Graphs",
        "summary": "State-of-the-art text-based image editing models often struggle to balance background preservation with semantic consistency, frequently resulting either in the synthesis of entirely new images or in outputs that fail to realize the intended edits. In contrast, scene graph-based image editing addresses this limitation by providing a structured representation of semantic entities and their relations, thereby offering improved controllability. However, existing scene graph editing methods typically depend on model fine-tuning, which incurs high computational cost and limits scalability. To this end, we introduce VENUS (Visual Editing with Noise inversion Using Scene graphs), a training-free framework for scene graph-guided image editing. Specifically, VENUS employs a split prompt conditioning strategy that disentangles the target object of the edit from its background context, while simultaneously leveraging noise inversion to preserve fidelity in unedited regions. Moreover, our proposed approach integrates scene graphs extracted from multimodal large language models with diffusion backbones, without requiring any additional training. Empirically, VENUS substantially improves both background preservation and semantic alignment on PIE-Bench, increasing PSNR from 22.45 to 24.80, SSIM from 0.79 to 0.84, and reducing LPIPS from 0.100 to 0.070 relative to the state-of-the-art scene graph editing model (SGEdit). In addition, VENUS enhances semantic consistency as measured by CLIP similarity (24.97 vs. 24.19). On EditVal, VENUS achieves the highest fidelity with a 0.87 DINO score and, crucially, reduces per-image runtime from 6-10 minutes to only 20-30 seconds. Beyond scene graph-based editing, VENUS also surpasses strong text-based editing baselines such as LEDIT++ and P2P+DirInv, thereby demonstrating consistent improvements across both paradigms.",
        "url": "http://arxiv.org/abs/2601.07219v1",
        "published_date": "2026-01-12T05:24:58+00:00",
        "updated_date": "2026-01-12T05:24:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Thanh-Nhan Vo",
            "Trong-Thuan Nguyen",
            "Tam V. Nguyen",
            "Minh-Triet Tran"
        ],
        "tldr": "VENUS is a training-free framework for scene graph-guided image editing that utilizes noise inversion and split prompt conditioning for improved background preservation and semantic consistency, achieving state-of-the-art results.",
        "tldr_zh": "VENUS是一个无需训练的场景图引导图像编辑框架，利用噪声反演和分离提示调节来提高背景保持和语义一致性，实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SceneNAT: Masked Generative Modeling for Language-Guided Indoor Scene Synthesis",
        "summary": "We present SceneNAT, a single-stage masked non-autoregressive Transformer that synthesizes complete 3D indoor scenes from natural language instructions through only a few parallel decoding passes, offering improved performance and efficiency compared to prior state-of-the-art approaches. SceneNAT is trained via masked modeling over fully discretized representations of both semantic and spatial attributes. By applying a masking strategy at both the attribute level and the instance level, the model can better capture intra-object and inter-object structure. To boost relational reasoning, SceneNAT employs a dedicated triplet predictor for modeling the scene's layout and object relationships by mapping a set of learnable relation queries to a sparse set of symbolic triplets (subject, predicate, object). Extensive experiments on the 3D-FRONT dataset demonstrate that SceneNAT achieves superior performance compared to state-of-the-art autoregressive and diffusion baselines in both semantic compliance and spatial arrangement accuracy, while operating with substantially lower computational cost.",
        "url": "http://arxiv.org/abs/2601.07218v1",
        "published_date": "2026-01-12T05:24:27+00:00",
        "updated_date": "2026-01-12T05:24:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jeongjun Choi",
            "Yeonsoo Park",
            "H. Jin Kim"
        ],
        "tldr": "SceneNAT is a single-stage masked non-autoregressive Transformer for language-guided 3D indoor scene synthesis, demonstrating improved performance and efficiency compared to state-of-the-art methods.",
        "tldr_zh": "SceneNAT是一个用于语言引导的3D室内场景合成的单阶段掩码非自回归Transformer，与最先进的方法相比，性能和效率都有所提高。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]