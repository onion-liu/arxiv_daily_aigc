[
    {
        "title": "Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback",
        "summary": "Recent advances in diffusion models have significantly improved audio-driven\nhuman video generation, surpassing traditional methods in both quality and\ncontrollability. However, existing approaches still face challenges in lip-sync\naccuracy, temporal coherence for long video generation, and multi-character\nanimation. In this work, we propose a diffusion transformer (DiT)-based\nframework for generating lifelike talking videos of arbitrary length, and\nintroduce a training-free method for multi-character audio-driven animation.\nFirst, we employ a LoRA-based training strategy combined with a position shift\ninference approach, which enables efficient long video generation while\npreserving the capabilities of the foundation model. Moreover, we combine\npartial parameter updates with reward feedback to enhance both lip\nsynchronization and natural body motion. Finally, we propose a training-free\napproach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character\nanimation, which requires no specialized datasets or model modifications and\nsupports audio-driven animation for three or more characters. Experimental\nresults demonstrate that our method outperforms existing state-of-the-art\napproaches, achieving high-quality, temporally coherent, and multi-character\naudio-driven video generation in a simple, efficient, and cost-effective\nmanner.",
        "url": "http://arxiv.org/abs/2510.12089v1",
        "published_date": "2025-10-14T02:50:05+00:00",
        "updated_date": "2025-10-14T02:50:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xingpei Ma",
            "Shenneng Huang",
            "Jiaran Cai",
            "Yuansheng Guan",
            "Shen Zheng",
            "Hanfeng Zhao",
            "Qiang Zhang",
            "Shunsi Zhang"
        ],
        "tldr": "This paper introduces a training-free diffusion transformer-based framework (Playmate2) for high-quality, temporally coherent, multi-character audio-driven video generation, addressing issues of lip-sync accuracy and enabling animation of multiple characters without requiring specialized datasets.",
        "tldr_zh": "本文介绍了一个无需训练的基于扩散变压器的框架（Playmate2），用于生成高质量、时间连贯的多角色音频驱动视频，解决了口型同步准确性的问题，并支持在不需要专业数据集的情况下对多个角色进行动画处理。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
        "summary": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a\npromising path to achieving a more generalized driving intelligence. However,\nVLA models are limited by a ``supervision deficit'': the vast model capacity is\nsupervised by sparse, low-dimensional actions, leaving much of their\nrepresentational power underutilized. To remedy this, we propose\n\\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to\npredict future images. This task generates a dense, self-supervised signal that\ncompels the model to learn the underlying dynamics of the driving environment.\nWe showcase the paradigm's versatility by instantiating it for two dominant VLA\narchetypes: an autoregressive world model for VLAs that use discrete visual\ntokens, and a diffusion world model for those operating on continuous visual\nfeatures. Building on the rich representations learned from world modeling, we\nintroduce a lightweight action expert to address the inference latency for\nreal-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a\n680x larger in-house dataset demonstrate that DriveVLA-W0 significantly\noutperforms BEV and VLA baselines. Crucially, it amplifies the data scaling\nlaw, showing that performance gains accelerate as the training dataset size\nincreases.",
        "url": "http://arxiv.org/abs/2510.12796v1",
        "published_date": "2025-10-14T17:59:47+00:00",
        "updated_date": "2025-10-14T17:59:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yingyan Li",
            "Shuyao Shang",
            "Weisong Liu",
            "Bing Zhan",
            "Haochen Wang",
            "Yuqi Wang",
            "Yuntao Chen",
            "Xiaoman Wang",
            "Yasong An",
            "Chufeng Tang",
            "Lu Hou",
            "Lue Fan",
            "Zhaoxiang Zhang"
        ],
        "tldr": "The paper introduces DriveVLA-W0, a training paradigm for Vision-Language-Action models in autonomous driving that uses world modeling to predict future images, addressing the supervision deficit and improving data scaling.",
        "tldr_zh": "该论文介绍了DriveVLA-W0，一种用于自动驾驶中视觉-语言-动作模型的训练范式，它使用世界建模来预测未来图像，解决了监督不足的问题并提高了数据扩展能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation",
        "summary": "Although recent advances in visual generation have been remarkable, most\nexisting architectures still depend on distinct encoders for images and text.\nThis separation constrains diffusion models' ability to perform cross-modal\nreasoning and knowledge transfer. Prior attempts to bridge this gap often use\nthe last layer information from VLM, employ multiple visual encoders, or train\nlarge unified models jointly for text and image generation, which demands\nsubstantial computational resources and large-scale data, limiting its\naccessibility.We present UniFusion, a diffusion-based generative model\nconditioned on a frozen large vision-language model (VLM) that serves as a\nunified multimodal encoder. At the core of UniFusion is the Layerwise Attention\nPooling (LAP) mechanism that extracts both high level semantics and low level\ndetails from text and visual tokens of a frozen VLM to condition a diffusion\ngenerative model. We demonstrate that LAP outperforms other shallow fusion\narchitectures on text-image alignment for generation and faithful transfer of\nvisual information from VLM to the diffusion model which is key for editing. We\npropose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),\nwhich conditions a diffusion transformer (DiT) only on the text tokens\ngenerated by the VLM during in-model prompt rewriting. VERIFI combines the\nalignment of the conditioning distribution with the VLM's reasoning\ncapabilities for increased capabilities and flexibility at inference. In\naddition, finetuning on editing task not only improves text-image alignment for\ngeneration, indicative of cross-modality knowledge transfer, but also exhibits\ntremendous generalization capabilities. Our model when trained on single image\nediting, zero-shot generalizes to multiple image references further motivating\nthe unified encoder design of UniFusion.",
        "url": "http://arxiv.org/abs/2510.12789v1",
        "published_date": "2025-10-14T17:57:56+00:00",
        "updated_date": "2025-10-14T17:57:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Kevin Li",
            "Manuel Brack",
            "Sudeep Katakol",
            "Hareesh Ravi",
            "Ajinkya Kale"
        ],
        "tldr": "UniFusion presents a diffusion-based image generation model using a frozen VLM as a unified encoder with a novel Layerwise Attention Pooling mechanism and VLM-Enabled Rewriting Injection to achieve improved cross-modal reasoning and editing with better efficiency and generalization.",
        "tldr_zh": "UniFusion提出了一种基于扩散的图像生成模型，该模型采用冻结的VLM作为统一编码器，具有新颖的层级注意力池化机制和VLM驱动的重写注入，以实现改进的跨模态推理和编辑，并具有更好的效率和泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models",
        "summary": "Recently, remarkable progress has been made in Unified Multimodal Models\n(UMMs), which integrate vision-language generation and understanding\ncapabilities within a single framework. However, a significant gap exists where\na model's strong visual understanding often fails to transfer to its visual\ngeneration. A model might correctly understand an image based on user\ninstructions, yet be unable to generate a faithful image from text prompts.\nThis phenomenon directly raises a compelling question: Can a model achieve\nself-improvement by using its understanding module to reward its generation\nmodule? To bridge this gap and achieve self-improvement, we introduce SRUM, a\nself-rewarding post-training framework that can be directly applied to existing\nUMMs of various designs. SRUM creates a feedback loop where the model's own\nunderstanding module acts as an internal ``evaluator'', providing corrective\nsignals to improve its generation module, without requiring additional\nhuman-labeled data. To ensure this feedback is comprehensive, we designed a\nglobal-local dual reward system. To tackle the inherent structural complexity\nof images, this system offers multi-scale guidance: a \\textbf{global reward}\nensures the correctness of the overall visual semantics and layout, while a\n\\textbf{local reward} refines fine-grained, object-level fidelity. SRUM leads\nto powerful capabilities and shows strong generalization, boosting performance\non T2I-CompBench from 82.18 to \\textbf{88.37} and on T2I-ReasonBench from 43.82\nto \\textbf{46.75}. Overall, our work establishes a powerful new paradigm for\nenabling a UMMs' understanding module to guide and enhance its own generation\nvia self-rewarding.",
        "url": "http://arxiv.org/abs/2510.12784v1",
        "published_date": "2025-10-14T17:56:11+00:00",
        "updated_date": "2025-10-14T17:56:11+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "I.4.0"
        ],
        "authors": [
            "Weiyang Jin",
            "Yuwei Niu",
            "Jiaqi Liao",
            "Chengqi Duan",
            "Aoxue Li",
            "Shenghua Gao",
            "Xihui Liu"
        ],
        "tldr": "The paper introduces SRUM, a self-rewarding framework that uses a multimodal model's understanding module to improve its generation module, leading to performance gains in text-to-image generation benchmarks.",
        "tldr_zh": "该论文介绍了SRUM，一个自奖励框架，它利用多模态模型的理解模块来改进其生成模块，从而在文本到图像的生成基准测试中取得了性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training",
        "summary": "Pixel-space generative models are often more difficult to train and generally\nunderperform compared to their latent-space counterparts, leaving a persistent\nperformance and efficiency gap. In this paper, we introduce a novel two-stage\ntraining framework that closes this gap for pixel-space diffusion and\nconsistency models. In the first stage, we pre-train encoders to capture\nmeaningful semantics from clean images while aligning them with points along\nthe same deterministic sampling trajectory, which evolves points from the prior\nto the data distribution. In the second stage, we integrate the encoder with a\nrandomly initialized decoder and fine-tune the complete model end-to-end for\nboth diffusion and consistency models. Our training framework demonstrates\nstrong empirical performance on ImageNet dataset. Specifically, our diffusion\nmodel reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75\nnumber of function evaluations (NFE), surpassing prior pixel-space methods by a\nlarge margin in both generation quality and efficiency while rivaling leading\nVAE-based models at comparable training cost. Furthermore, on ImageNet-256, our\nconsistency model achieves an impressive FID of 8.82 in a single sampling step,\nsignificantly surpassing its latent-space counterpart. To the best of our\nknowledge, this marks the first successful training of a consistency model\ndirectly on high-resolution images without relying on pre-trained VAEs or\ndiffusion models.",
        "url": "http://arxiv.org/abs/2510.12586v1",
        "published_date": "2025-10-14T14:41:16+00:00",
        "updated_date": "2025-10-14T14:41:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiachen Lei",
            "Keli Liu",
            "Julius Berner",
            "Haiming Yu",
            "Hongkai Zheng",
            "Jiahong Wu",
            "Xiangxiang Chu"
        ],
        "tldr": "This paper introduces a two-stage training framework involving self-supervised pre-training to improve the performance and efficiency of pixel-space generative models (diffusion and consistency models), achieving state-of-the-art FID scores on ImageNet.",
        "tldr_zh": "本文提出了一种两阶段训练框架，通过自监督预训练来提高像素空间生成模型（扩散模型和一致性模型）的性能和效率，在 ImageNet 上实现了最先进的 FID 分数。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LayerSync: Self-aligning Intermediate Layers",
        "summary": "We propose LayerSync, a domain-agnostic approach for improving the generation\nquality and the training efficiency of diffusion models. Prior studies have\nhighlighted the connection between the quality of generation and the\nrepresentations learned by diffusion models, showing that external guidance on\nmodel intermediate representations accelerates training. We reconceptualize\nthis paradigm by regularizing diffusion models with their own intermediate\nrepresentations. Building on the observation that representation quality varies\nacross diffusion model layers, we show that the most semantically rich\nrepresentations can act as an intrinsic guidance for weaker ones, reducing the\nneed for external supervision. Our approach, LayerSync, is a self-sufficient,\nplug-and-play regularizer term with no overhead on diffusion model training and\ngeneralizes beyond the visual domain to other modalities. LayerSync requires no\npretrained models nor additional data. We extensively evaluate the method on\nimage generation and demonstrate its applicability to other domains such as\naudio, video, and motion generation. We show that it consistently improves the\ngeneration quality and the training efficiency. For example, we speed up the\ntraining of flow-based transformer by over 8.75x on ImageNet dataset and\nimproved the generation quality by 23.6%. The code is available at\nhttps://github.com/vita-epfl/LayerSync.",
        "url": "http://arxiv.org/abs/2510.12581v1",
        "published_date": "2025-10-14T14:39:14+00:00",
        "updated_date": "2025-10-14T14:39:14+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yasaman Haghighi",
            "Bastien van Delft",
            "Mariam Hassan",
            "Alexandre Alahi"
        ],
        "tldr": "LayerSync is a domain-agnostic, self-supervised regularization technique for diffusion models that improves generation quality and training efficiency by aligning intermediate layers, achieving significant speedups and quality improvements across various modalities.",
        "tldr_zh": "LayerSync是一种领域无关的自监督正则化技术，用于扩散模型，它通过对齐中间层来提高生成质量和训练效率，并在各种模态上实现了显著的加速和质量提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vectorized Video Representation with Easy Editing via Hierarchical Spatio-Temporally Consistent Proxy Embedding",
        "summary": "Current video representations heavily rely on unstable and over-grained\npriors for motion and appearance modelling, \\emph{i.e.}, pixel-level matching\nand tracking. A tracking error of just a few pixels would lead to the collapse\nof the visual object representation, not to mention occlusions and large motion\nfrequently occurring in videos. To overcome the above mentioned vulnerability,\nthis work proposes spatio-temporally consistent proxy nodes to represent\ndynamically changing objects/scenes in the video. On the one hand, the\nhierarchical proxy nodes have the ability to stably express the multi-scale\nstructure of visual objects, so they are not affected by accumulated tracking\nerror, long-term motion, occlusion, and viewpoint variation. On the other hand,\nthe dynamic representation update mechanism of the proxy nodes adequately\nleverages spatio-temporal priors of the video to mitigate the impact of\ninaccurate trackers, thereby effectively handling drastic changes in scenes and\nobjects. Additionally, the decoupled encoding manner of the shape and texture\nrepresentations across different visual objects in the video facilitates\ncontrollable and fine-grained appearance editing capability. Extensive\nexperiments demonstrate that the proposed representation achieves high video\nreconstruction accuracy with fewer parameters and supports complex video\nprocessing tasks, including video in-painting and keyframe-based temporally\nconsistent video editing.",
        "url": "http://arxiv.org/abs/2510.12256v1",
        "published_date": "2025-10-14T08:05:30+00:00",
        "updated_date": "2025-10-14T08:05:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ye Chen",
            "Liming Tan",
            "Yupeng Zhu",
            "Yuanbin Wang",
            "Bingbing Ni"
        ],
        "tldr": "This paper introduces a novel video representation using hierarchical spatio-temporally consistent proxy nodes, addressing issues with current pixel-level methods and enabling easier video editing.",
        "tldr_zh": "本文提出了一种新的视频表示方法，使用分层时空一致的代理节点，解决了当前像素级方法的问题，并实现了更简单的视频编辑。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "BIGFix: Bidirectional Image Generation with Token Fixing",
        "summary": "Recent advances in image and video generation have raised significant\ninterest from both academia and industry. A key challenge in this field is\nimproving inference efficiency, as model size and the number of inference steps\ndirectly impact the commercial viability of generative models while also posing\nfundamental scientific challenges. A promising direction involves combining\nauto-regressive sequential token modeling with multi-token prediction per step,\nreducing inference time by up to an order of magnitude. However, predicting\nmultiple tokens in parallel can introduce structural inconsistencies due to\ntoken incompatibilities, as capturing complex joint dependencies during\ntraining remains challenging. Traditionally, once tokens are sampled, there is\nno mechanism to backtrack and refine erroneous predictions. We propose a method\nfor self-correcting image generation by iteratively refining sampled tokens. We\nachieve this with a novel training scheme that injects random tokens in the\ncontext, improving robustness and enabling token fixing during sampling. Our\nmethod preserves the efficiency benefits of parallel token prediction while\nsignificantly enhancing generation quality. We evaluate our approach on image\ngeneration using the ImageNet-256 and CIFAR-10 datasets, as well as on video\ngeneration with UCF-101 and NuScenes, demonstrating substantial improvements\nacross both modalities.",
        "url": "http://arxiv.org/abs/2510.12231v1",
        "published_date": "2025-10-14T07:34:44+00:00",
        "updated_date": "2025-10-14T07:34:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Victor Besnier",
            "David Hurych",
            "Andrei Bursuc",
            "Eduardo Valle"
        ],
        "tldr": "The paper introduces BIGFix, a method for self-correcting image and video generation by iteratively refining sampled tokens, enhancing generation quality while preserving efficiency.",
        "tldr_zh": "该论文介绍了BIGFix，一种通过迭代优化采样令牌来自动纠正图像和视频生成的方法，在保持效率的同时提高了生成质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MetaCaptioner: Towards Generalist Visual Captioning with Open-source Suites",
        "summary": "Generalist visual captioning goes beyond a simple appearance description\ntask, but requires integrating a series of visual cues into a caption and\nhandling various visual domains. In this task, current open-source models\npresent a large performance gap with commercial ones, which limits various\napplications such as data synthesis. To bridge the gap, this paper proposes\nCapFlow, a novel multi-agent collaboration workflow. CapFlow demonstrates for\nthe first time that, by capitalizing on open-source models, it is possible to\nachieve caption quality on par with GPT-4.1 in various domains with an 89.5%\nreduction in costs. By leveraging CapFlow as the data synthesizer, we produce\nhigh-quality visual captions from image and video domains at scale, and obtain\na generalist visual captioner via fine-tuning, namely MetaCaptioner. Through\nextensive experiments, we show that MetaCaptioner not only achieves comparable\ncaptioning capabilities with commercial models but also reaches top-tier\nmultimodal performance in the open-source community. We hope CapFlow and\nMetaCaptioner can benefit future multimodal research by providing a strong and\ncost-effective visual captioning solution.",
        "url": "http://arxiv.org/abs/2510.12126v1",
        "published_date": "2025-10-14T04:03:25+00:00",
        "updated_date": "2025-10-14T04:03:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenxin Lei",
            "Zhangwei Gao",
            "Changyao Tian",
            "Erfei Cui",
            "Guanzhou Chen",
            "Danni Yang",
            "Yuchen Duan",
            "Zhaokai Wang",
            "Wenhao Li",
            "Weiyun Wang",
            "Xiangyu Zhao",
            "Jiayi Ji",
            "Yu Qiao",
            "Wenhai Wang",
            "Gen Luo"
        ],
        "tldr": "The paper introduces CapFlow, a workflow leveraging open-source models to achieve GPT-4 level captioning quality at a fraction of the cost. It then uses CapFlow for data synthesis to fine-tune a generalist visual captioner, MetaCaptioner, which achieves comparable performance to commercial models and top-tier open-source multimodal performance.",
        "tldr_zh": "本文介绍了一个名为CapFlow的工作流程，它利用开源模型以极低的成本实现了与GPT-4相当的图像描述质量。随后，它使用CapFlow进行数据合成，微调出一个通用视觉描述器MetaCaptioner，其性能与商业模型相当，并达到了顶级的开源多模态性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "VIDMP3: Video Editing by Representing Motion with Pose and Position Priors",
        "summary": "Motion-preserved video editing is crucial for creators, particularly in\nscenarios that demand flexibility in both the structure and semantics of\nswapped objects. Despite its potential, this area remains underexplored.\nExisting diffusion-based editing methods excel in structure-preserving tasks,\nusing dense guidance signals to ensure content integrity. While some recent\nmethods attempt to address structure-variable editing, they often suffer from\nissues such as temporal inconsistency, subject identity drift, and the need for\nhuman intervention. To address these challenges, we introduce VidMP3, a novel\napproach that leverages pose and position priors to learn a generalized motion\nrepresentation from source videos. Our method enables the generation of new\nvideos that maintain the original motion while allowing for structural and\nsemantic flexibility. Both qualitative and quantitative evaluations demonstrate\nthe superiority of our approach over existing methods. The code will be made\npublicly available at https://github.com/sandeep-sm/VidMP3.",
        "url": "http://arxiv.org/abs/2510.12069v1",
        "published_date": "2025-10-14T02:20:12+00:00",
        "updated_date": "2025-10-14T02:20:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sandeep Mishra",
            "Oindrila Saha",
            "Alan C. Bovik"
        ],
        "tldr": "VidMP3 introduces a novel pose and position prior-based approach for motion-preserved video editing, enabling structural and semantic flexibility while maintaining temporal consistency, outperforming existing diffusion-based methods.",
        "tldr_zh": "VidMP3 提出了一种新颖的基于姿势和位置先验的运动保持视频编辑方法，可在保持时间一致性的同时实现结构和语义的灵活性，优于现有的基于扩散的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MosaicDiff: Training-free Structural Pruning for Diffusion Model Acceleration Reflecting Pretraining Dynamics",
        "summary": "Diffusion models are renowned for their generative capabilities, yet their\npretraining processes exhibit distinct phases of learning speed that have been\nentirely overlooked in prior post-training acceleration efforts in the\ncommunity. In this study, we introduce a novel framework called MosaicDiff that\naligns diffusion pretraining dynamics with post-training sampling acceleration\nvia trajectory-aware structural pruning. Our approach leverages the observation\nthat the middle, fast-learning stage of diffusion pretraining requires more\nconservative pruning to preserve critical model features, while the early and\nlater, slow-learning stages benefit from a more aggressive pruning strategy.\nThis adaptive pruning mechanism is the first to explicitly mirror the inherent\nlearning speed variations of diffusion pretraining, thereby harmonizing the\nmodel's inner training dynamics with its accelerated sampling process.\nExtensive experiments on DiT and SDXL demonstrate that our method achieves\nsignificant speed-ups in sampling without compromising output quality,\noutperforming previous state-of-the-art methods by large margins, also\nproviding a new viewpoint for more efficient and robust training-free diffusion\nacceleration.",
        "url": "http://arxiv.org/abs/2510.11962v1",
        "published_date": "2025-10-13T21:51:04+00:00",
        "updated_date": "2025-10-13T21:51:04+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Bowei Guo",
            "Shengkun Tang",
            "Cong Zeng",
            "Zhiqiang Shen"
        ],
        "tldr": "MosaicDiff introduces a training-free structural pruning method for accelerating diffusion models by adaptively pruning based on the learning dynamics observed during pretraining, achieving significant speed-ups without compromising quality.",
        "tldr_zh": "MosaicDiff 提出了一种无需训练的结构剪枝方法，通过根据预训练期间观察到的学习动态进行自适应剪枝来加速扩散模型，在不影响质量的前提下实现了显著的速度提升。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning Human Motion with Temporally Conditional Mamba",
        "summary": "Learning human motion based on a time-dependent input signal presents a\nchallenging yet impactful task with various applications. The goal of this task\nis to generate or estimate human movement that consistently reflects the\ntemporal patterns of conditioning inputs. Existing methods typically rely on\ncross-attention mechanisms to fuse the condition with motion. However, this\napproach primarily captures global interactions and struggles to maintain\nstep-by-step temporal alignment. To address this limitation, we introduce\nTemporally Conditional Mamba, a new mamba-based model for human motion\ngeneration. Our approach integrates conditional information into the recurrent\ndynamics of the Mamba block, enabling better temporally aligned motion. To\nvalidate the effectiveness of our method, we evaluate it on a variety of human\nmotion tasks. Extensive experiments demonstrate that our model significantly\nimproves temporal alignment, motion realism, and condition consistency over\nstate-of-the-art approaches. Our project page is available at\nhttps://zquang2202.github.io/TCM.",
        "url": "http://arxiv.org/abs/2510.12573v1",
        "published_date": "2025-10-14T14:29:51+00:00",
        "updated_date": "2025-10-14T14:29:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Quang Nguyen",
            "Tri Le",
            "Baoru Huang",
            "Minh Nhat Vu",
            "Ngan Le",
            "Thieu Vo",
            "Anh Nguyen"
        ],
        "tldr": "This paper introduces Temporally Conditional Mamba (TCM), a Mamba-based model, for generating human motion from time-dependent input, showing improvements in temporal alignment, realism, and consistency compared to existing methods which rely on cross-attention.",
        "tldr_zh": "本文介绍了时间条件Mamba（TCM），一种基于Mamba的模型，用于从时间相关的输入生成人体运动，与依赖交叉注意的现有方法相比，在时间对齐、逼真度和一致性方面有所改进。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion",
        "summary": "Recent work has explored a range of model families for human motion\ngeneration, including Variational Autoencoders (VAEs), Generative Adversarial\nNetworks (GANs), and diffusion-based models. Despite their differences, many\nmethods rely on over-parameterized input features and auxiliary losses to\nimprove empirical results. These strategies should not be strictly necessary\nfor diffusion models to match the human motion distribution. We show that on\npar with state-of-the-art results in unconditional human motion generation are\nachievable with a score-based diffusion model using only careful feature-space\nnormalization and analytically derived weightings for the standard L2\nscore-matching loss, while generating both motion and shape directly, thereby\navoiding slow post hoc shape recovery from joints. We build the method step by\nstep, with a clear theoretical motivation for each component, and provide\ntargeted ablations demonstrating the effectiveness of each proposed addition in\nisolation.",
        "url": "http://arxiv.org/abs/2510.12537v1",
        "published_date": "2025-10-14T14:02:22+00:00",
        "updated_date": "2025-10-14T14:02:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "David Björkstrand",
            "Tiesheng Wang",
            "Lars Bretzner",
            "Josephine Sullivan"
        ],
        "tldr": "This paper presents a score-based diffusion model for unconditional human motion and shape generation, achieving state-of-the-art results with a simplified architecture and analytically derived loss weightings, avoiding post-hoc shape recovery.",
        "tldr_zh": "这篇论文提出了一个基于分数的扩散模型，用于生成无条件的人体运动和形状。该模型采用简化的架构和解析导出的损失权重，实现了最先进的结果，避免了事后形状恢复。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation",
        "summary": "In this study, we present IL3D, a large-scale dataset meticulously designed\nfor large language model (LLM)-driven 3D scene generation, addressing the\npressing demand for diverse, high-quality training data in indoor layout\ndesign. Comprising 27,816 indoor layouts across 18 prevalent room types and a\nlibrary of 29,215 high-fidelity 3D object assets, IL3D is enriched with\ninstance-level natural language annotations to support robust multimodal\nlearning for vision-language tasks. We establish rigorous benchmarks to\nevaluate LLM-driven scene generation. Experimental results show that supervised\nfine-tuning (SFT) of LLMs on IL3D significantly improves generalization and\nsurpasses the performance of SFT on other datasets. IL3D offers flexible\nmultimodal data export capabilities, including point clouds, 3D bounding boxes,\nmultiview images, depth maps, normal maps, and semantic masks, enabling\nseamless adaptation to various visual tasks. As a versatile and robust\nresource, IL3D significantly advances research in 3D scene generation and\nembodied intelligence, by providing high-fidelity scene data to support\nenvironment perception tasks of embodied agents.",
        "url": "http://arxiv.org/abs/2510.12095v1",
        "published_date": "2025-10-14T03:02:33+00:00",
        "updated_date": "2025-10-14T03:02:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenxu Zhou",
            "Kaixuan Nie",
            "Hang Du",
            "Dong Yin",
            "Wei Huang",
            "Siqiang Guo",
            "Xiaobo Zhang",
            "Pengbo Hu"
        ],
        "tldr": "The paper introduces IL3D, a large-scale indoor layout dataset with rich annotations designed to train LLMs for 3D scene generation, demonstrating improved performance over other datasets.",
        "tldr_zh": "该论文介绍了一个大规模室内布局数据集IL3D，该数据集具有丰富的注释，旨在训练LLM进行3D场景生成，并展示了优于其他数据集的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Your VAR Model is Secretly an Efficient and Explainable Generative Classifier",
        "summary": "Generative classifiers, which leverage conditional generative models for\nclassification, have recently demonstrated desirable properties such as\nrobustness to distribution shifts. However, recent progress in this area has\nbeen largely driven by diffusion-based models, whose substantial computational\ncost severely limits scalability. This exclusive focus on diffusion-based\nmethods has also constrained our understanding of generative classifiers. In\nthis work, we propose a novel generative classifier built on recent advances in\nvisual autoregressive (VAR) modeling, which offers a new perspective for\nstudying generative classifiers. To further enhance its performance, we\nintroduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a\nsuperior trade-off between accuracy and inference speed, thereby significantly\nimproving practical applicability. Moreover, we show that the VAR-based method\nexhibits fundamentally different properties from diffusion-based methods. In\nparticular, due to its tractable likelihood, the VAR-based classifier enables\nvisual explainability via token-wise mutual information and demonstrates\ninherent resistance to catastrophic forgetting in class-incremental learning\ntasks.",
        "url": "http://arxiv.org/abs/2510.12060v1",
        "published_date": "2025-10-14T01:59:01+00:00",
        "updated_date": "2025-10-14T01:59:01+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yi-Chung Chen",
            "David I. Inouye",
            "Jing Gao"
        ],
        "tldr": "This paper introduces a novel generative classifier, A-VARC$^+$, based on visual autoregressive (VAR) modeling, offering improved speed, accuracy, explainability and resistance to catastrophic forgetting compared to diffusion-based methods.",
        "tldr_zh": "该论文提出了一个基于视觉自回归(VAR)建模的新型生成式分类器A-VARC$^+$，与基于扩散的方法相比，该分类器在速度、精度、可解释性以及抵抗灾难性遗忘方面有所提高。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]