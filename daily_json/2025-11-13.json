[
    {
        "title": "PAN: A World Model for General, Interactable, and Long-Horizon World Simulation",
        "summary": "A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.",
        "url": "http://arxiv.org/abs/2511.09057v1",
        "published_date": "2025-11-12T07:20:35+00:00",
        "updated_date": "2025-11-13T01:29:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "PAN Team",
            "Jiannan Xiang",
            "Yi Gu",
            "Zihan Liu",
            "Zeyu Feng",
            "Qiyue Gao",
            "Yiyan Hu",
            "Benhao Huang",
            "Guangyi Liu",
            "Yichi Yang",
            "Kun Zhou",
            "Davit Abrahamyan",
            "Arif Ahmad",
            "Ganesh Bannur",
            "Junrong Chen",
            "Kimi Chen",
            "Mingkai Deng",
            "Ruobing Han",
            "Xinqi Huang",
            "Haoqiang Kang",
            "Zheqi Li",
            "Enze Ma",
            "Hector Ren",
            "Yashowardhan Shinde",
            "Rohan Shingre",
            "Ramsundar Tanikella",
            "Kaiming Tao",
            "Dequan Yang",
            "Xinle Yu",
            "Cong Zeng",
            "Binglin Zhou",
            "Hector Liu",
            "Zhiting Hu",
            "Eric P. Xing"
        ],
        "tldr": "The paper introduces PAN, a general world model for action-conditioned, long-horizon video simulation, combining an LLM-based autoregressive latent dynamics backbone with a video diffusion decoder.",
        "tldr_zh": "该论文介绍了PAN，一个通用的世界模型，用于动作条件下的长时程视频模拟，它结合了基于LLM的自回归潜在动力学主干网络和一个视频diffusion解码器。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
        "summary": "The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a \"window\" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.",
        "url": "http://arxiv.org/abs/2511.08585v1",
        "published_date": "2025-11-11T18:59:50+00:00",
        "updated_date": "2025-11-12T02:05:57+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jingtong Yue",
            "Ziqi Huang",
            "Zhaoxi Chen",
            "Xintao Wang",
            "Pengfei Wan",
            "Ziwei Liu"
        ],
        "tldr": "This paper surveys the evolution of video generation towards video foundation models that act as implicit world models, capable of simulating physical dynamics and agent interactions, ultimately providing a roadmap for future research in this direction.",
        "tldr_zh": "本文综述了视频生成向视频基础模型的演变，这些模型可以作为隐式的世界模型，能够模拟物理动态和智能体交互，并为未来此方向的研究提供路线图。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "FGM-HD: Boosting Generation Diversity of Fractal Generative Models through Hausdorff Dimension Induction",
        "summary": "Improving the diversity of generated results while maintaining high visual quality remains a significant challenge in image generation tasks. Fractal Generative Models (FGMs) are efficient in generating high-quality images, but their inherent self-similarity limits the diversity of output images. To address this issue, we propose a novel approach based on the Hausdorff Dimension (HD), a widely recognized concept in fractal geometry used to quantify structural complexity, which aids in enhancing the diversity of generated outputs. To incorporate HD into FGM, we propose a learnable HD estimation method that predicts HD directly from image embeddings, addressing computational cost concerns. However, simply introducing HD into a hybrid loss is insufficient to enhance diversity in FGMs due to: 1) degradation of image quality, and 2) limited improvement in generation diversity. To this end, during training, we adopt an HD-based loss with a monotonic momentum-driven scheduling strategy to progressively optimize the hyperparameters, obtaining optimal diversity without sacrificing visual quality. Moreover, during inference, we employ HD-guided rejection sampling to select geometrically richer outputs. Extensive experiments on the ImageNet dataset demonstrate that our FGM-HD framework yields a 39\\% improvement in output diversity compared to vanilla FGMs, while preserving comparable image quality. To our knowledge, this is the very first work introducing HD into FGM. Our method effectively enhances the diversity of generated outputs while offering a principled theoretical contribution to FGM development.",
        "url": "http://arxiv.org/abs/2511.08945v1",
        "published_date": "2025-11-12T03:45:15+00:00",
        "updated_date": "2025-11-13T01:20:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haowei Zhang",
            "Yuanpei Zhao",
            "Jizhe Zhou",
            "Mao Li"
        ],
        "tldr": "The paper introduces FGM-HD, a novel method that uses Hausdorff Dimension to improve the diversity of images generated by Fractal Generative Models while maintaining image quality, showing a 39% diversity improvement on ImageNet.",
        "tldr_zh": "该论文介绍了FGM-HD，一种新颖的方法，利用Hausdorff维度来提高分形生成模型生成的图像的多样性，同时保持图像质量，并在ImageNet上显示出39%的多样性提升。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "From Structure to Detail: Hierarchical Distillation for Efficient Diffusion Model",
        "summary": "The inference latency of diffusion models remains a critical barrier to their real-time application. While trajectory-based and distribution-based step distillation methods offer solutions, they present a fundamental trade-off. Trajectory-based methods preserve global structure but act as a \"lossy compressor\", sacrificing high-frequency details. Conversely, distribution-based methods can achieve higher fidelity but often suffer from mode collapse and unstable training. This paper recasts them from independent paradigms into synergistic components within our novel Hierarchical Distillation (HD) framework. We leverage trajectory distillation not as a final generator, but to establish a structural ``sketch\", providing a near-optimal initialization for the subsequent distribution-based refinement stage. This strategy yields an ideal initial distribution that enhances the ceiling of overall performance. To further improve quality, we introduce and refine the adversarial training process. We find standard discriminator structures are ineffective at refining an already high-quality generator. To overcome this, we introduce the Adaptive Weighted Discriminator (AWD), tailored for the HD pipeline. By dynamically allocating token weights, AWD focuses on local imperfections, enabling efficient detail refinement. Our approach demonstrates state-of-the-art performance across diverse tasks. On ImageNet $256\\times256$, our single-step model achieves an FID of 2.26, rivaling its 250-step teacher. It also achieves promising results on the high-resolution text-to-image MJHQ benchmark, proving its generalizability. Our method establishes a robust new paradigm for high-fidelity, single-step diffusion models.",
        "url": "http://arxiv.org/abs/2511.08930v1",
        "published_date": "2025-11-12T03:12:06+00:00",
        "updated_date": "2025-11-13T01:18:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hanbo Cheng",
            "Peng Wang",
            "Kaixiang Lei",
            "Qi Li",
            "Zhen Zou",
            "Pengfei Hu",
            "Jun Du"
        ],
        "tldr": "This paper introduces Hierarchical Distillation (HD), a novel framework combining trajectory and distribution-based distillation for efficient and high-fidelity single-step diffusion models, achieving state-of-the-art results with an adaptive weighted discriminator.",
        "tldr_zh": "本文提出了一种新的分层蒸馏（HD）框架，该框架结合了基于轨迹和分布的蒸馏方法，用于高效、高保真的单步扩散模型，并通过自适应加权判别器实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DT-NVS: Diffusion Transformers for Novel View Synthesis",
        "summary": "Generating novel views of a natural scene, e.g., every-day scenes both indoors and outdoors, from a single view is an under-explored problem, even though it is an organic extension to the object-centric novel view synthesis. Existing diffusion-based approaches focus rather on small camera movements in real scenes or only consider unnatural object-centric scenes, limiting their potential applications in real-world settings. In this paper we move away from these constrained regimes and propose a 3D diffusion model trained with image-only losses on a large-scale dataset of real-world, multi-category, unaligned, and casually acquired videos of everyday scenes. We propose DT-NVS, a 3D-aware diffusion model for generalized novel view synthesis that exploits a transformer-based architecture backbone. We make significant contributions to transformer and self-attention architectures to translate images to 3d representations, and novel camera conditioning strategies to allow training on real-world unaligned datasets. In addition, we introduce a novel training paradigm swapping the role of reference frame between the conditioning image and the sampled noisy input. We evaluate our approach on the 3D task of generalized novel view synthesis from a single input image and show improvements over state-of-the-art 3D aware diffusion models and deterministic approaches, while generating diverse outputs.",
        "url": "http://arxiv.org/abs/2511.08823v1",
        "published_date": "2025-11-11T22:40:00+00:00",
        "updated_date": "2025-11-13T01:10:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wonbong Jang",
            "Jonathan Tremblay",
            "Lourdes Agapito"
        ],
        "tldr": "This paper introduces DT-NVS, a 3D-aware diffusion model using transformers for novel view synthesis from a single image, trained on unaligned real-world video data. It presents improvements over existing methods and generates diverse outputs.",
        "tldr_zh": "本文介绍了DT-NVS，一种基于Transformer的3D感知扩散模型，用于从单张图像中进行新视角合成，该模型在未对齐的真实世界视频数据上进行训练。与现有方法相比，该模型有所改进，并能生成多样化的输出。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?",
        "summary": "This paper investigates the scaling properties of autoregressive next-pixel prediction, a simple, end-to-end yet under-explored framework for unified vision models. Starting with images at resolutions of 32x32, we train a family of Transformers using IsoFlops profiles across compute budgets up to 7e19 FLOPs and evaluate three distinct target metrics: next-pixel prediction objective, ImageNet classification accuracy, and generation quality measured by Fr'echet Distance. First, optimal scaling strategy is critically task-dependent. At a fixed 32x32 resolution alone, the optimal scaling properties for image classification and image generation diverge, where generation optimal setup requires the data size grow three to five times faster than for the classification optimal setup. Second, as image resolution increases, the optimal scaling strategy indicates that the model size must grow much faster than data size. Surprisingly, by projecting our findings, we discover that the primary bottleneck is compute rather than the amount of training data. As compute continues to grow four to five times annually, we forecast the feasibility of pixel-by-pixel modeling of images within the next five years.",
        "url": "http://arxiv.org/abs/2511.08704v1",
        "published_date": "2025-11-11T19:11:02+00:00",
        "updated_date": "2025-11-13T01:02:57+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Xinchen Yan",
            "Chen Liang",
            "Lijun Yu",
            "Adams Wei Yu",
            "Yifeng Lu",
            "Quoc V. Le"
        ],
        "tldr": "This paper studies the scaling properties of autoregressive next-pixel prediction for unified vision models, finding task-dependent optimal scaling strategies and predicting the feasibility of pixel-by-pixel modeling within five years.",
        "tldr_zh": "本文研究了用于统一视觉模型的自回归 next-pixel 预测的缩放特性，发现依赖于任务的最佳缩放策略，并预测了未来五年内逐像素建模的可行性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "3D4D: An Interactive, Editable, 4D World Model via 3D Video Generation",
        "summary": "We introduce 3D4D, an interactive 4D visualization framework that integrates WebGL with Supersplat rendering. It transforms static images and text into coherent 4D scenes through four core modules and employs a foveated rendering strategy for efficient, real-time multi-modal interaction. This framework enables adaptive, user-driven exploration of complex 4D environments. The project page and code are available at https://yunhonghe1021.github.io/NOVA/.",
        "url": "http://arxiv.org/abs/2511.08536v1",
        "published_date": "2025-11-11T18:16:53+00:00",
        "updated_date": "2025-11-12T02:03:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunhong He",
            "Zhengqing Yuan",
            "Zhengzhong Tu",
            "Yanfang Ye",
            "Lichao Sun"
        ],
        "tldr": "3D4D is a 4D visualization framework that uses WebGL and Supersplat rendering to transform static images and text into interactive and editable 4D scenes with real-time multi-modal interaction.",
        "tldr_zh": "3D4D是一个4D可视化框架，它使用WebGL和Supersplat渲染将静态图像和文本转换为具有实时多模态交互的交互式和可编辑的4D场景。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    }
]