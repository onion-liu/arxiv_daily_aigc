[
    {
        "title": "ReRoPE: Repurposing RoPE for Relative Camera Control",
        "summary": "Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/",
        "url": "http://arxiv.org/abs/2602.08068v1",
        "published_date": "2026-02-08T17:49:10+00:00",
        "updated_date": "2026-02-08T17:49:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chunyang Li",
            "Yuanbo Yang",
            "Jiahao Shao",
            "Hongyu Zhou",
            "Katja Schwarz",
            "Yiyi Liao"
        ],
        "tldr": "The paper introduces ReRoPE, a plug-and-play method for incorporating relative camera control into pre-trained video diffusion models by injecting camera pose information into underutilized frequency bands of Rotary Positional Embeddings, achieving training-efficient and high-fidelity video generation.",
        "tldr_zh": "该论文介绍了ReRoPE，一种即插即用的方法，通过将相机姿态信息注入到旋转位置嵌入中未充分利用的频段，从而将相对相机控制集成到预训练的视频扩散模型中，实现了训练高效且高保真的视频生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion",
        "summary": "Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/",
        "url": "http://arxiv.org/abs/2602.07775v1",
        "published_date": "2026-02-08T02:16:02+00:00",
        "updated_date": "2026-02-08T02:16:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haodong Li",
            "Shaoteng Liu",
            "Zhe Lin",
            "Manmohan Chandraker"
        ],
        "tldr": "The paper introduces \"Rolling Sink\", a training-free method to extend autoregressive video diffusion models to generate ultra-long videos by addressing the train-test gap between limited training horizons and open-ended testing horizons focusing on cache maintenance strategies during inference.",
        "tldr_zh": "该论文介绍了一种名为“滚动池 (Rolling Sink)”的训练自由方法，通过解决有限训练范围和开放测试范围之间的训练-测试差距，将自回归视频扩散模型扩展到生成超长视频，重点关注推理过程中的缓存维护策略。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation",
        "summary": "In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.",
        "url": "http://arxiv.org/abs/2602.07967v1",
        "published_date": "2026-02-08T13:29:46+00:00",
        "updated_date": "2026-02-08T13:29:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaofeng Tan",
            "Wanjiang Weng",
            "Haodong Lei",
            "Hongsong Wang"
        ],
        "tldr": "EasyTune addresses the inefficiency and high memory consumption of existing diffusion-based motion generation fine-tuning methods by optimizing at each denoising step and introducing a self-refinement preference learning mechanism, demonstrating significant performance improvements.",
        "tldr_zh": "EasyTune通过在每个去噪步骤进行优化并引入自细化偏好学习机制，解决了现有基于扩散的运动生成微调方法的效率低下和高内存消耗问题，并展示了显著的性能改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]