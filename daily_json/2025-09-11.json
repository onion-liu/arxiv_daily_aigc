[
    {
        "title": "RewardDance: Reward Scaling in Visual Generation",
        "summary": "Reward Models (RMs) are critical for improving generation models via\nReinforcement Learning (RL), yet the RM scaling paradigm in visual generation\nremains largely unexplored. It primarily due to fundamental limitations in\nexisting approaches: CLIP-based RMs suffer from architectural and input\nmodality constraints, while prevalent Bradley-Terry losses are fundamentally\nmisaligned with the next-token prediction mechanism of Vision-Language Models\n(VLMs), hindering effective scaling. More critically, the RLHF optimization\nprocess is plagued by Reward Hacking issue, where models exploit flaws in the\nreward signal without improving true quality. To address these challenges, we\nintroduce RewardDance, a scalable reward modeling framework that overcomes\nthese barriers through a novel generative reward paradigm. By reformulating the\nreward score as the model's probability of predicting a \"yes\" token, indicating\nthat the generated image outperforms a reference image according to specific\ncriteria, RewardDance intrinsically aligns reward objectives with VLM\narchitectures. This alignment unlocks scaling across two dimensions: (1) Model\nScaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context\nScaling: Integration of task-specific instructions, reference examples, and\nchain-of-thought (CoT) reasoning. Extensive experiments demonstrate that\nRewardDance significantly surpasses state-of-the-art methods in text-to-image,\ntext-to-video, and image-to-video generation. Crucially, we resolve the\npersistent challenge of \"reward hacking\": Our large-scale RMs exhibit and\nmaintain high reward variance during RL fine-tuning, proving their resistance\nto hacking and ability to produce diverse, high-quality outputs. It greatly\nrelieves the mode collapse problem that plagues smaller models.",
        "url": "http://arxiv.org/abs/2509.08826v1",
        "published_date": "2025-09-10T17:59:31+00:00",
        "updated_date": "2025-09-10T17:59:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jie Wu",
            "Yu Gao",
            "Zilyu Ye",
            "Ming Li",
            "Liang Li",
            "Hanzhong Guo",
            "Jie Liu",
            "Zeyue Xue",
            "Xiaoxia Hou",
            "Wei Liu",
            "Yan Zeng",
            "Weilin Huang"
        ],
        "tldr": "The paper introduces RewardDance, a new reward modeling framework for visual generation that addresses the limitations of existing approaches by aligning reward objectives with Vision-Language Model architectures, enabling scaling and robustness against reward hacking.",
        "tldr_zh": "该论文介绍了一种新的视觉生成奖励建模框架RewardDance，通过将奖励目标与视觉语言模型架构对齐，解决了现有方法的局限性，实现了扩展性并增强了对奖励黑客攻击的抵抗力。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video Artifacts",
        "summary": "Recent advances in probabilistic generative models have extended capabilities\nfrom static image synthesis to text-driven video generation. However, the\ninherent randomness of their generation process can lead to unpredictable\nartifacts, such as impossible physics and temporal inconsistency. Progress in\naddressing these challenges requires systematic benchmarks, yet existing\ndatasets primarily focus on generative images due to the unique spatio-temporal\ncomplexities of videos. To bridge this gap, we introduce GeneVA, a large-scale\nartifact dataset with rich human annotations that focuses on spatio-temporal\nartifacts in videos generated from natural text prompts. We hope GeneVA can\nenable and assist critical applications, such as benchmarking model performance\nand improving generative video quality.",
        "url": "http://arxiv.org/abs/2509.08818v1",
        "published_date": "2025-09-10T17:51:42+00:00",
        "updated_date": "2025-09-10T17:51:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jenna Kang",
            "Maria Silva",
            "Patsorn Sangkloy",
            "Kenneth Chen",
            "Niall Williams",
            "Qi Sun"
        ],
        "tldr": "The paper introduces GeneVA, a large-scale dataset of human annotations focusing on spatio-temporal artifacts in text-to-video generation, aiming to benchmark model performance and improve video quality.",
        "tldr_zh": "该论文介绍了GeneVA，一个大规模的人工标注数据集，专注于文本到视频生成中的时空伪影，旨在评估模型性能并提高视频质量。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain Translation",
        "summary": "Diffusion models excel at generating high-quality outputs but face challenges\nin data-scarce domains, where exhaustive retraining or costly paired data are\noften required. To address these limitations, we propose Latent Aligned\nDiffusion Bridges (LADB), a semi-supervised framework for sample-to-sample\ntranslation that effectively bridges domain gaps using partially paired data.\nBy aligning source and target distributions within a shared latent space, LADB\nseamlessly integrates pretrained source-domain diffusion models with a\ntarget-domain Latent Aligned Diffusion Model (LADM), trained on partially\npaired latent representations. This approach enables deterministic domain\nmapping without the need for full supervision. Compared to unpaired methods,\nwhich often lack controllability, and fully paired approaches that require\nlarge, domain-specific datasets, LADB strikes a balance between fidelity and\ndiversity by leveraging a mixture of paired and unpaired latent-target\ncouplings. Our experimental results demonstrate superior performance in\ndepth-to-image translation under partial supervision. Furthermore, we extend\nLADB to handle multi-source translation (from depth maps and segmentation\nmasks) and multi-target translation in a class-conditioned style transfer task,\nshowcasing its versatility in handling diverse and heterogeneous use cases.\nUltimately, we present LADB as a scalable and versatile solution for real-world\ndomain translation, particularly in scenarios where data annotation is costly\nor incomplete.",
        "url": "http://arxiv.org/abs/2509.08628v1",
        "published_date": "2025-09-10T14:23:07+00:00",
        "updated_date": "2025-09-10T14:23:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuqin Wang",
            "Tao Wu",
            "Yanfeng Zhang",
            "Lu Liu",
            "Dong Wang",
            "Mingwei Sun",
            "Yongliang Wang",
            "Niclas Zeller",
            "Daniel Cremers"
        ],
        "tldr": "The paper introduces Latent Aligned Diffusion Bridges (LADB), a semi-supervised diffusion framework for domain translation using partially paired data, demonstrating effective performance in depth-to-image and style transfer tasks, particularly where data annotation is limited.",
        "tldr_zh": "该论文介绍了Latent Aligned Diffusion Bridges (LADB)，一个使用部分配对数据的半监督扩散框架，用于领域翻译，在深度到图像和风格迁移任务中表现出良好的性能，尤其是在数据标注有限的情况下。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning",
        "summary": "Human-Centric Video Generation (HCVG) methods seek to synthesize human videos\nfrom multimodal inputs, including text, image, and audio. Existing methods\nstruggle to effectively coordinate these heterogeneous modalities due to two\nchallenges: the scarcity of training data with paired triplet conditions and\nthe difficulty of collaborating the sub-tasks of subject preservation and\naudio-visual sync with multimodal inputs. In this work, we present HuMo, a\nunified HCVG framework for collaborative multimodal control. For the first\nchallenge, we construct a high-quality dataset with diverse and paired text,\nreference images, and audio. For the second challenge, we propose a two-stage\nprogressive multimodal training paradigm with task-specific strategies. For the\nsubject preservation task, to maintain the prompt following and visual\ngeneration abilities of the foundation model, we adopt the minimal-invasive\nimage injection strategy. For the audio-visual sync task, besides the commonly\nadopted audio cross-attention layer, we propose a focus-by-predicting strategy\nthat implicitly guides the model to associate audio with facial regions. For\njoint learning of controllabilities across multimodal inputs, building on\npreviously acquired capabilities, we progressively incorporate the audio-visual\nsync task. During inference, for flexible and fine-grained multimodal control,\nwe design a time-adaptive Classifier-Free Guidance strategy that dynamically\nadjusts guidance weights across denoising steps. Extensive experimental results\ndemonstrate that HuMo surpasses specialized state-of-the-art methods in\nsub-tasks, establishing a unified framework for collaborative\nmultimodal-conditioned HCVG. Project Page:\nhttps://phantom-video.github.io/HuMo.",
        "url": "http://arxiv.org/abs/2509.08519v1",
        "published_date": "2025-09-10T11:54:29+00:00",
        "updated_date": "2025-09-10T11:54:29+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Liyang Chen",
            "Tianxiang Ma",
            "Jiawei Liu",
            "Bingchuan Li",
            "Zhuowei Chen",
            "Lijie Liu",
            "Xu He",
            "Gen Li",
            "Qian He",
            "Zhiyong Wu"
        ],
        "tldr": "The paper introduces HuMo, a unified framework for Human-Centric Video Generation that addresses the challenges of coordinating heterogeneous multimodal inputs (text, image, audio) through a new dataset, a two-stage training paradigm, and a time-adaptive inference strategy.",
        "tldr_zh": "本文介绍HuMo，一个统一的以人为中心的视频生成框架，通过一个新的数据集、一个两阶段训练范式和一个时间自适应的推理策略，解决了协调异构多模态输入（文本、图像、音频）的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video",
        "summary": "We propose a novel and general framework to disentangle video data into its\ndynamic motion and static content components. Our proposed method is a\nself-supervised pipeline with less assumptions and inductive biases than\nprevious works: it utilizes a transformer-based architecture to jointly\ngenerate flexible implicit features for frame-wise motion and clip-wise\ncontent, and incorporates a low-bitrate vector quantization as an information\nbottleneck to promote disentanglement and form a meaningful discrete motion\nspace. The bitrate-controlled latent motion and content are used as conditional\ninputs to a denoising diffusion model to facilitate self-supervised\nrepresentation learning. We validate our disentangled representation learning\nframework on real-world talking head videos with motion transfer and\nauto-regressive motion generation tasks. Furthermore, we also show that our\nmethod can generalize to other types of video data, such as pixel sprites of 2D\ncartoon characters. Our work presents a new perspective on self-supervised\nlearning of disentangled video representations, contributing to the broader\nfield of video analysis and generation.",
        "url": "http://arxiv.org/abs/2509.08376v1",
        "published_date": "2025-09-10T08:14:45+00:00",
        "updated_date": "2025-09-10T08:14:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiao Li",
            "Qi Chen",
            "Xiulian Peng",
            "Kai Yu",
            "Xie Chen",
            "Yan Lu"
        ],
        "tldr": "This paper introduces a self-supervised method using bitrate-controlled diffusion and transformer architectures to disentangle motion and content in video, demonstrating successful motion transfer and generation on talking heads and 2D cartoon sprites.",
        "tldr_zh": "本文提出了一种自监督方法，利用码率控制的扩散模型和Transformer架构来解开视频中的运动和内容，并在头部说话视频和2D卡通精灵上展示了成功的运动迁移和生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and Correct Image Interpretation Model Shortcuts",
        "summary": "Chest radiographs (CXRs) are among the most common tests in medicine.\nAutomated image interpretation may reduce radiologists\\' workload and expand\naccess to diagnostic expertise. Deep learning multi-task and foundation models\nhave shown strong performance for CXR interpretation but are vulnerable to\nshortcut learning, where models rely on spurious and off-target correlations\nrather than clinically relevant features to make decisions. We introduce\nRoentMod, a counterfactual image editing framework that generates anatomically\nrealistic CXRs with user-specified, synthetic pathology while preserving\nunrelated anatomical features of the original scan. RoentMod combines an\nopen-source medical image generator (RoentGen) with an image-to-image\nmodification model without requiring retraining. In reader studies with\nboard-certified radiologists and radiology residents, RoentMod-produced images\nappeared realistic in 93\\% of cases, correctly incorporated the specified\nfinding in 89-99\\% of cases, and preserved native anatomy comparable to real\nfollow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-task\nand foundation models frequently exploit off-target pathology as shortcuts,\nlimiting their specificity. Incorporating RoentMod-generated counterfactual\nimages during training mitigated this vulnerability, improving model\ndiscrimination across multiple pathologies by 3-19\\% AUC in internal validation\nand by 1-11\\% for 5 out of 6 tested pathologies in external testing. These\nfindings establish RoentMod as a broadly applicable tool for probing and\ncorrecting shortcut learning in medical AI. By enabling controlled\ncounterfactual interventions, RoentMod enhances the robustness and\ninterpretability of CXR interpretation models and provides a generalizable\nstrategy for improving foundation models in medical imaging.",
        "url": "http://arxiv.org/abs/2509.08640v1",
        "published_date": "2025-09-10T14:35:24+00:00",
        "updated_date": "2025-09-10T14:35:24+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "I.4, I.2, J.3"
        ],
        "authors": [
            "Lauren H. Cooke",
            "Matthias Jung",
            "Jan M. Brendel",
            "Nora M. Kerkovits",
            "Borek Foldyna",
            "Michael T. Lu",
            "Vineet K. Raghu"
        ],
        "tldr": "The paper introduces RoentMod, a framework for generating synthetic chest X-rays with controllable pathologies to identify and mitigate shortcut learning in CXR interpretation models, demonstrating improved model robustness and interpretability.",
        "tldr_zh": "该论文介绍了RoentMod，一个用于生成具有可控病理的人工合成胸部X光片的框架，旨在识别和减轻CXR解释模型中的捷径学习问题，并展示了模型鲁棒性和可解释性的提高。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    }
]