[
    {
        "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark",
        "summary": "The advancement of open-source text-to-image (T2I) models has been hindered\nby the absence of large-scale, reasoning-focused datasets and comprehensive\nevaluation benchmarks, resulting in a performance gap compared to leading\nclosed-source systems. To address this challenge, We introduce FLUX-Reason-6M\nand PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).\nFLUX-Reason-6M is a massive dataset consisting of 6 million high-quality\nFLUX-generated images and 20 million bilingual (English and Chinese)\ndescriptions specifically designed to teach complex reasoning. The image are\norganized according to six key characteristics: Imagination, Entity, Text\nrendering, Style, Affection, and Composition, and design explicit Generation\nChain-of-Thought (GCoT) to provide detailed breakdowns of image generation\nsteps. The whole data curation takes 15,000 A100 GPU days, providing the\ncommunity with a resource previously unattainable outside of large industrial\nlabs. PRISM-Bench offers a novel evaluation standard with seven distinct\ntracks, including a formidable Long Text challenge using GCoT. Through\ncarefully designed prompts, it utilizes advanced vision-language models for\nnuanced human-aligned assessment of prompt-image alignment and image\naesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench\nreveals critical performance gaps and highlights specific areas requiring\nimprovement. Our dataset, benchmark, and evaluation code are released to\ncatalyze the next wave of reasoning-oriented T2I generation. Project page:\nhttps://flux-reason-6m.github.io/ .",
        "url": "http://arxiv.org/abs/2509.09680v1",
        "published_date": "2025-09-11T17:59:59+00:00",
        "updated_date": "2025-09-11T17:59:59+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Rongyao Fang",
            "Aldrich Yu",
            "Chengqi Duan",
            "Linjiang Huang",
            "Shuai Bai",
            "Yuxuan Cai",
            "Kun Wang",
            "Si Liu",
            "Xihui Liu",
            "Hongsheng Li"
        ],
        "tldr": "The paper introduces FLUX-Reason-6M, a large-scale text-to-image dataset (6M images, 20M descriptions) designed for reasoning, and PRISM-Bench, a new benchmark for evaluating T2I models, aiming to improve open-source T2I model performance.",
        "tldr_zh": "该论文介绍了一个大规模文本到图像数据集FLUX-Reason-6M (600万张图像，2000万条描述)，专为推理而设计，以及PRISM-Bench，一个新的评估T2I模型的基准，旨在提升开源T2I模型的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Can Understanding and Generation Truly Benefit Together -- or Just Coexist?",
        "summary": "In this paper, we introduce an insightful paradigm through the Auto-Encoder\nlens-understanding as the encoder (I2T) that compresses images into text, and\ngeneration as the decoder (T2I) that reconstructs images from that text. Using\nreconstruction fidelity as the unified training objective, we enforce the\ncoherent bidirectional information flow between the understanding and\ngeneration processes, bringing mutual gains. To implement this, we propose UAE,\na novel framework for unified multimodal learning. We begin by pre-training the\ndecoder with large-scale long-context image captions to capture fine-grained\nsemantic and complex spatial relationships. We then propose Unified-GRPO via\nreinforcement learning (RL), which covers three stages: (1) A cold-start phase\nto gently initialize both encoder and decoder with a semantic reconstruction\nloss; (2) Generation for Understanding, where the encoder is trained to\ngenerate informative captions that maximize the decoder's reconstruction\nquality, enhancing its visual understanding; (3) Understanding for Generation,\nwhere the decoder is refined to reconstruct from these captions, forcing it to\nleverage every detail and improving its long-context instruction following and\ngeneration fidelity. For evaluation, we introduce Unified-Bench, the first\nbenchmark tailored to assess the degree of unification of the UMMs. A\nsurprising \"aha moment\" arises within the multimodal learning domain: as RL\nprogresses, the encoder autonomously produces more descriptive captions, while\nthe decoder simultaneously demonstrates a profound ability to understand these\nintricate descriptions, resulting in reconstructions of striking fidelity.",
        "url": "http://arxiv.org/abs/2509.09666v1",
        "published_date": "2025-09-11T17:57:59+00:00",
        "updated_date": "2025-09-11T17:57:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiyuan Yan",
            "Kaiqing Lin",
            "Zongjian Li",
            "Junyan Ye",
            "Hui Han",
            "Zhendong Wang",
            "Hao Liu",
            "Bin Lin",
            "Hao Li",
            "Xue Xu",
            "Xinyan Xiao",
            "Jingdong Wang",
            "Haifeng Wang",
            "Li Yuan"
        ],
        "tldr": "This paper introduces UAE, a framework for unified multimodal learning that uses an auto-encoder approach with reinforcement learning to improve both image understanding and generation, demonstrating improved reconstruction fidelity.",
        "tldr_zh": "本文介绍了一种统一的多模态学习框架UAE，该框架采用自编码器方法和强化学习来提高图像理解和生成能力，并展示了改进的重建保真度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis",
        "summary": "Recent advances in audio-driven avatar video generation have significantly\nenhanced audio-visual realism. However, existing methods treat instruction\nconditioning merely as low-level tracking driven by acoustic or visual cues,\nwithout modeling the communicative purpose conveyed by the instructions. This\nlimitation compromises their narrative coherence and character expressiveness.\nTo bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that\nunifies multimodal instruction understanding with photorealistic portrait\ngeneration. Our approach adopts a two-stage pipeline. In the first stage, we\ndesign a multimodal large language model (MLLM) director that produces a\nblueprint video conditioned on diverse instruction signals, thereby governing\nhigh-level semantics such as character motion and emotions. In the second\nstage, guided by blueprint keyframes, we generate multiple sub-clips in\nparallel using a first-last frame strategy. This global-to-local framework\npreserves fine-grained details while faithfully encoding the high-level intent\nbehind multimodal instructions. Our parallel architecture also enables fast and\nstable generation of long-duration videos, making it suitable for real-world\napplications such as digital human livestreaming and vlogging. To\ncomprehensively evaluate our method, we construct a benchmark of 375 curated\nsamples covering diverse instructions and challenging scenarios. Extensive\nexperiments demonstrate that Kling-Avatar is capable of generating vivid,\nfluent, long-duration videos at up to 1080p and 48 fps, achieving superior\nperformance in lip synchronization accuracy, emotion and dynamic\nexpressiveness, instruction controllability, identity preservation, and\ncross-domain generalization. These results establish Kling-Avatar as a new\nbenchmark for semantically grounded, high-fidelity audio-driven avatar\nsynthesis.",
        "url": "http://arxiv.org/abs/2509.09595v1",
        "published_date": "2025-09-11T16:34:57+00:00",
        "updated_date": "2025-09-11T16:34:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yikang Ding",
            "Jiwen Liu",
            "Wenyuan Zhang",
            "Zekun Wang",
            "Wentao Hu",
            "Liyuan Cui",
            "Mingming Lao",
            "Yingchao Shao",
            "Hui Liu",
            "Xiaohan Li",
            "Ming Chen",
            "Xiaoqiang Liu",
            "Yu-Shen Liu",
            "Pengfei Wan"
        ],
        "tldr": "The paper introduces Kling-Avatar, a cascaded framework for generating long-duration avatar videos conditioned on multimodal instructions, improving narrative coherence and expressiveness compared to existing methods.",
        "tldr_zh": "该论文介绍了一个名为Kling-Avatar的级联框架，该框架用于根据多模态指令生成长时间的Avatar视频，与现有方法相比，提高了叙事连贯性和表现力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation",
        "summary": "While large-scale human motion capture datasets have advanced human motion\ngeneration, modeling and generating dynamic 3D human-object interactions (HOIs)\nremain challenging due to dataset limitations. Existing datasets often lack\nextensive, high-quality motion and annotation and exhibit artifacts such as\ncontact penetration, floating, and incorrect hand motions. To address these\nissues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset\nand methodological advancements. First, we consolidate and standardize 21.81\nhours of HOI data from diverse sources, enriching it with detailed textual\nannotations. Second, we propose a unified optimization framework to enhance\ndata quality by reducing artifacts and correcting hand motions. Leveraging the\nprinciple of contact invariance, we maintain human-object relationships while\nintroducing motion variations, expanding the dataset to 30.70 hours. Third, we\ndefine six benchmarking tasks and develop a unified HOI generative modeling\nperspective, achieving state-of-the-art performance. Extensive experiments\nvalidate the utility of our dataset as a foundational resource for advancing 3D\nhuman-object interaction generation. To support continued research in this\narea, the dataset is publicly available at\nhttps://github.com/wzyabcas/InterAct, and will be actively maintained.",
        "url": "http://arxiv.org/abs/2509.09555v1",
        "published_date": "2025-09-11T15:43:54+00:00",
        "updated_date": "2025-09-11T15:43:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sirui Xu",
            "Dongting Li",
            "Yucheng Zhang",
            "Xiyan Xu",
            "Qi Long",
            "Ziyin Wang",
            "Yunzhi Lu",
            "Shuchang Dong",
            "Hezi Jiang",
            "Akshat Gupta",
            "Yu-Xiong Wang",
            "Liang-Yan Gui"
        ],
        "tldr": "The paper introduces InterAct, a large-scale 3D human-object interaction (HOI) dataset with improved data quality and a unified generative modeling framework to advance HOI generation research.",
        "tldr_zh": "该论文介绍了InterAct，一个大规模的3D人-物交互（HOI）数据集，通过改进数据质量和统一的生成建模框架，旨在推进HOI生成领域的研究。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders",
        "summary": "Video diffusion models have advanced rapidly in the recent years as a result\nof series of architectural innovations (e.g., diffusion transformers) and use\nof novel training objectives (e.g., flow matching). In contrast, less attention\nhas been paid to improving the feature representation power of such models. In\nthis work, we show that training video diffusion models can benefit from\naligning the intermediate features of the video generator with feature\nrepresentations of pre-trained vision encoders. We propose a new metric and\nconduct an in-depth analysis of various vision encoders to evaluate their\ndiscriminability and temporal consistency, thereby assessing their suitability\nfor video feature alignment. Based on the analysis, we present Align4Gen which\nprovides a novel multi-feature fusion and alignment method integrated into\nvideo diffusion model training. We evaluate Align4Gen both for unconditional\nand class-conditional video generation tasks and show that it results in\nimproved video generation as quantified by various metrics. Full video results\nare available on our project page: https://align4gen.github.io/align4gen/",
        "url": "http://arxiv.org/abs/2509.09547v1",
        "published_date": "2025-09-11T15:39:27+00:00",
        "updated_date": "2025-09-11T15:39:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dohun Lee",
            "Hyeonho Jeong",
            "Jiwook Kim",
            "Duygu Ceylan",
            "Jong Chul Ye"
        ],
        "tldr": "This paper introduces Align4Gen, a method that improves video diffusion models by aligning their intermediate features with those of pre-trained vision encoders, demonstrating improved video generation performance.",
        "tldr_zh": "本文介绍了Align4Gen，一种通过将视频扩散模型的中间特征与预训练视觉编码器的特征对齐来改进视频扩散模型的方法，并展示了视频生成性能的提升。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark and dataset from LMM",
        "summary": "Generative AI evolves the execution of complex workflows in industry, where\nthe large multimodal model empowers fashion design in the garment industry.\nCurrent generation AI models magically transform brainstorming into fancy\ndesigns easily, but the fine-grained customization still suffers from text\nuncertainty without professional background knowledge from end-users. Thus, we\npropose the Better Understanding Generation (BUG) workflow with LMM to\nautomatically create and fine-grain customize the cloth designs from chat with\nimage-into-prompt. Our framework unleashes users' creative potential beyond\nwords and also lowers the barriers of clothing design/editing without further\nhuman involvement. To prove the effectiveness of our model, we propose a new\nFashionEdit dataset that simulates the real-world clothing design workflow,\nevaluated from generation similarity, user satisfaction, and quality. The code\nand dataset: https://github.com/detectiveli/FashionEdit.",
        "url": "http://arxiv.org/abs/2509.09324v1",
        "published_date": "2025-09-11T10:14:36+00:00",
        "updated_date": "2025-09-11T10:14:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hui Li",
            "Yi You",
            "Qiqi Chen",
            "Bingfeng Zhang",
            "George Q. Huang"
        ],
        "tldr": "This paper introduces a new framework called Better Understanding Generation (BUG) that uses Large Multimodal Models (LMMs) and an image-into-prompt approach to facilitate fine-grained, customized fashion design, along with a new dataset, FashionEdit, to evaluate the approach.",
        "tldr_zh": "该论文介绍了一种名为Better Understanding Generation (BUG)的新框架，该框架使用大型多模态模型 (LMM) 和图像-提示方法来促进细粒度的定制服装设计， 同时推出了新的数据集FashionEdit来评估该方法。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Discovering Divergent Representations between Text-to-Image Models",
        "summary": "In this paper, we investigate when and how visual representations learned by\ntwo different generative models diverge. Given two text-to-image models, our\ngoal is to discover visual attributes that appear in images generated by one\nmodel but not the other, along with the types of prompts that trigger these\nattribute differences. For example, \"flames\" might appear in one model's\noutputs when given prompts expressing strong emotions, while the other model\ndoes not produce this attribute given the same prompts. We introduce CompCon\n(Comparing Concepts), an evolutionary search algorithm that discovers visual\nattributes more prevalent in one model's output than the other, and uncovers\nthe prompt concepts linked to these visual differences. To evaluate CompCon's\nability to find diverging representations, we create an automated data\ngeneration pipeline to produce ID2, a dataset of 60 input-dependent\ndifferences, and compare our approach to several LLM- and VLM-powered\nbaselines. Finally, we use CompCon to compare popular text-to-image models,\nfinding divergent representations such as how PixArt depicts prompts mentioning\nloneliness with wet streets and Stable Diffusion 3.5 depicts African American\npeople in media professions. Code at: https://github.com/adobe-research/CompCon",
        "url": "http://arxiv.org/abs/2509.08940v1",
        "published_date": "2025-09-10T19:07:55+00:00",
        "updated_date": "2025-09-10T19:07:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lisa Dunlap",
            "Joseph E. Gonzalez",
            "Trevor Darrell",
            "Fabian Caba Heilbron",
            "Josef Sivic",
            "Bryan Russell"
        ],
        "tldr": "The paper introduces CompCon, an evolutionary search algorithm, to discover and compare divergent visual representations between text-to-image models, and identifies prompt concepts linked to these differences.",
        "tldr_zh": "该论文介绍了一种名为CompCon的进化搜索算法，用于发现和比较文本到图像模型之间不同的视觉表示，并识别与这些差异相关的提示概念。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Locality in Image Diffusion Models Emerges from Data Statistics",
        "summary": "Among generative models, diffusion models are uniquely intriguing due to the\nexistence of a closed-form optimal minimizer of their training objective, often\nreferred to as the optimal denoiser. However, diffusion using this optimal\ndenoiser merely reproduces images in the training set and hence fails to\ncapture the behavior of deep diffusion models. Recent work has attempted to\ncharacterize this gap between the optimal denoiser and deep diffusion models,\nproposing analytical, training-free models that can generate images that\nresemble those generated by a trained UNet. The best-performing method\nhypothesizes that shift equivariance and locality inductive biases of\nconvolutional neural networks are the cause of the performance gap, hence\nincorporating these assumptions into its analytical model. In this work, we\npresent evidence that the locality in deep diffusion models emerges as a\nstatistical property of the image dataset, not due to the inductive bias of\nconvolutional neural networks. Specifically, we demonstrate that an optimal\nparametric linear denoiser exhibits similar locality properties to the deep\nneural denoisers. We further show, both theoretically and experimentally, that\nthis locality arises directly from the pixel correlations present in natural\nimage datasets. Finally, we use these insights to craft an analytical denoiser\nthat better matches scores predicted by a deep diffusion model than the prior\nexpert-crafted alternative.",
        "url": "http://arxiv.org/abs/2509.09672v1",
        "published_date": "2025-09-11T17:59:08+00:00",
        "updated_date": "2025-09-11T17:59:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Artem Lukoianov",
            "Chenyang Yuan",
            "Justin Solomon",
            "Vincent Sitzmann"
        ],
        "tldr": "This paper argues that locality in image diffusion models arises from the statistical properties of image datasets rather than the convolutional inductive biases, demonstrating this with an optimal linear denoiser and theoretical analysis.",
        "tldr_zh": "本文认为图像扩散模型中的局部性源于图像数据集的统计特性，而不是卷积的归纳偏置。通过最优线性去噪器和理论分析证明了这一点。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]