[
    {
        "title": "Generating an Image From 1,000 Words: Enhancing Text-to-Image With Structured Captions",
        "summary": "Text-to-image models have rapidly evolved from casual creative tools to\nprofessional-grade systems, achieving unprecedented levels of image quality and\nrealism. Yet, most models are trained to map short prompts into detailed\nimages, creating a gap between sparse textual input and rich visual outputs.\nThis mismatch reduces controllability, as models often fill in missing details\narbitrarily, biasing toward average user preferences and limiting precision for\nprofessional use. We address this limitation by training the first open-source\ntext-to-image model on long structured captions, where every training sample is\nannotated with the same set of fine-grained attributes. This design maximizes\nexpressive coverage and enables disentangled control over visual factors. To\nprocess long captions efficiently, we propose DimFusion, a fusion mechanism\nthat integrates intermediate tokens from a lightweight LLM without increasing\ntoken length. We also introduce the Text-as-a-Bottleneck Reconstruction (TaBR)\nevaluation protocol. By assessing how well real images can be reconstructed\nthrough a captioning-generation loop, TaBR directly measures controllability\nand expressiveness, even for very long captions where existing evaluation\nmethods fail. Finally, we demonstrate our contributions by training the\nlarge-scale model FIBO, achieving state-of-the-art prompt alignment among\nopen-source models. Model weights are publicly available at\nhttps://huggingface.co/briaai/FIBO",
        "url": "http://arxiv.org/abs/2511.06876v1",
        "published_date": "2025-11-10T09:25:25+00:00",
        "updated_date": "2025-11-10T09:25:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Eyal Gutflaish",
            "Eliran Kachlon",
            "Hezi Zisman",
            "Tal Hacham",
            "Nimrod Sarid",
            "Alexander Visheratin",
            "Saar Huberman",
            "Gal Davidi",
            "Guy Bukchin",
            "Kfir Goldberg",
            "Ron Mokady"
        ],
        "tldr": "This paper introduces FIBO, an open-source text-to-image model trained on long, structured captions to improve controllability and prompt alignment, along with DimFusion for efficient long caption processing and TaBR for controllability evaluation.",
        "tldr_zh": "本文介绍FIBO，一个基于长结构化字幕训练的开源文本到图像模型，旨在提高可控性和提示对齐。同时提出DimFusion用于高效处理长字幕，以及TaBR用于可控性评估。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "4DSTR: Advancing Generative 4D Gaussians with Spatial-Temporal Rectification for High-Quality and Consistent 4D Generation",
        "summary": "Remarkable advances in recent 2D image and 3D shape generation have induced a\nsignificant focus on dynamic 4D content generation. However, previous 4D\ngeneration methods commonly struggle to maintain spatial-temporal consistency\nand adapt poorly to rapid temporal variations, due to the lack of effective\nspatial-temporal modeling. To address these problems, we propose a novel 4D\ngeneration network called 4DSTR, which modulates generative 4D Gaussian\nSplatting with spatial-temporal rectification. Specifically, temporal\ncorrelation across generated 4D sequences is designed to rectify deformable\nscales and rotations and guarantee temporal consistency. Furthermore, an\nadaptive spatial densification and pruning strategy is proposed to address\nsignificant temporal variations by dynamically adding or deleting Gaussian\npoints with the awareness of their pre-frame movements. Extensive experiments\ndemonstrate that our 4DSTR achieves state-of-the-art performance in video-to-4D\ngeneration, excelling in reconstruction quality, spatial-temporal consistency,\nand adaptation to rapid temporal movements.",
        "url": "http://arxiv.org/abs/2511.07241v1",
        "published_date": "2025-11-10T15:57:03+00:00",
        "updated_date": "2025-11-10T15:57:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mengmeng Liu",
            "Jiuming Liu",
            "Yunpeng Zhang",
            "Jiangtao Li",
            "Michael Ying Yang",
            "Francesco Nex",
            "Hao Cheng"
        ],
        "tldr": "The paper introduces 4DSTR, a novel network for 4D content generation that uses spatial-temporal rectification to improve consistency and handle rapid temporal changes in video-to-4D generation tasks.",
        "tldr_zh": "该论文介绍了一种名为 4DSTR 的新型 4D 内容生成网络，它使用时空校正来提高一致性并处理视频到 4D 生成任务中的快速时间变化。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Omni-View: Unlocking How Generation Facilitates Understanding in Unified 3D Model based on Multiview images",
        "summary": "This paper presents Omni-View, which extends the unified multimodal\nunderstanding and generation to 3D scenes based on multiview images, exploring\nthe principle that \"generation facilitates understanding\". Consisting of\nunderstanding model, texture module, and geometry module, Omni-View jointly\nmodels scene understanding, novel view synthesis, and geometry estimation,\nenabling synergistic interaction between 3D scene understanding and generation\ntasks. By design, it leverages the spatiotemporal modeling capabilities of its\ntexture module responsible for appearance synthesis, alongside the explicit\ngeometric constraints provided by its dedicated geometry module, thereby\nenriching the model's holistic understanding of 3D scenes. Trained with a\ntwo-stage strategy, Omni-View achieves a state-of-the-art score of 55.4 on the\nVSI-Bench benchmark, outperforming existing specialized 3D understanding\nmodels, while simultaneously delivering strong performance in both novel view\nsynthesis and 3D scene generation.",
        "url": "http://arxiv.org/abs/2511.07222v1",
        "published_date": "2025-11-10T15:44:48+00:00",
        "updated_date": "2025-11-10T15:44:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "JiaKui Hu",
            "Shanshan Zhao",
            "Qing-Guo Chen",
            "Xuerui Qiu",
            "Jialun Liu",
            "Zhao Xu",
            "Weihua Luo",
            "Kaifu Zhang",
            "Yanye Lu"
        ],
        "tldr": "The paper introduces Omni-View, a unified 3D model that leverages multiview images for scene understanding, novel view synthesis, and geometry estimation, achieving state-of-the-art results on VSI-Bench by jointly modeling understanding and generation.",
        "tldr_zh": "该论文介绍了Omni-View，一个统一的3D模型，它利用多视角图像进行场景理解、新视角合成和几何估计，并通过联合建模理解和生成，在VSI-Bench上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Two-Stage System for Layout-Controlled Image Generation using Large Language Models and Diffusion Models",
        "summary": "Text-to-image diffusion models exhibit remarkable generative capabilities,\nbut lack precise control over object counts and spatial arrangements. This work\nintroduces a two-stage system to address these compositional limitations. The\nfirst stage employs a Large Language Model (LLM) to generate a structured\nlayout from a list of objects. The second stage uses a layout-conditioned\ndiffusion model to synthesize a photorealistic image adhering to this layout.\nWe find that task decomposition is critical for LLM-based spatial planning; by\nsimplifying the initial generation to core objects and completing the layout\nwith rule-based insertion, we improve object recall from 57.2% to 99.9% for\ncomplex scenes. For image synthesis, we compare two leading conditioning\nmethods: ControlNet and GLIGEN. After domain-specific finetuning on\ntable-setting datasets, we identify a key trade-off: ControlNet preserves\ntext-based stylistic control but suffers from object hallucination, while\nGLIGEN provides superior layout fidelity at the cost of reduced prompt-based\ncontrollability. Our end-to-end system successfully generates images with\nspecified object counts and plausible spatial arrangements, demonstrating the\nviability of a decoupled approach for compositionally controlled synthesis.",
        "url": "http://arxiv.org/abs/2511.06888v1",
        "published_date": "2025-11-10T09:40:48+00:00",
        "updated_date": "2025-11-10T09:40:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jan-Hendrik Koch",
            "Jonas Krumme",
            "Konrad Gadzicki"
        ],
        "tldr": "The paper presents a two-stage system using LLMs and diffusion models for layout-controlled image generation, achieving improved object recall and exploring the trade-offs between ControlNet and GLIGEN for layout conditioning.",
        "tldr_zh": "该论文提出了一个两阶段系统，利用大型语言模型和扩散模型进行布局控制的图像生成，提高了物体召回率，并探讨了 ControlNet 和 GLIGEN 在布局条件方面的权衡。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VAEVQ: Enhancing Discrete Visual Tokenization through Variational Modeling",
        "summary": "Vector quantization (VQ) transforms continuous image features into discrete\nrepresentations, providing compressed, tokenized inputs for generative models.\nHowever, VQ-based frameworks suffer from several issues, such as non-smooth\nlatent spaces, weak alignment between representations before and after\nquantization, and poor coherence between the continuous and discrete domains.\nThese issues lead to unstable codeword learning and underutilized codebooks,\nultimately degrading the performance of both reconstruction and downstream\ngeneration tasks. To this end, we propose VAEVQ, which comprises three key\ncomponents: (1) Variational Latent Quantization (VLQ), replacing the AE with a\nVAE for quantization to leverage its structured and smooth latent space,\nthereby facilitating more effective codeword activation; (2) Representation\nCoherence Strategy (RCS), adaptively modulating the alignment strength between\npre- and post-quantization features to enhance consistency and prevent\noverfitting to noise; and (3) Distribution Consistency Regularization (DCR),\naligning the entire codebook distribution with the continuous latent\ndistribution to improve utilization. Extensive experiments on two benchmark\ndatasets demonstrate that VAEVQ outperforms state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2511.06863v1",
        "published_date": "2025-11-10T09:07:23+00:00",
        "updated_date": "2025-11-10T09:07:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sicheng Yang",
            "Xing Hu",
            "Qiang Wu",
            "Dawei Yang"
        ],
        "tldr": "This paper introduces VAEVQ, a novel vector quantization method that enhances discrete visual tokenization using variational autoencoders and several regularization techniques to improve codebook utilization and performance in generative models.",
        "tldr_zh": "本文介绍了一种新的向量量化方法VAEVP，它利用变分自编码器和多种正则化技术来增强离散视觉标记化，从而提高生成模型中的码本利用率和性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "RaLD: Generating High-Resolution 3D Radar Point Clouds with Latent Diffusion",
        "summary": "Millimeter-wave radar offers a promising sensing modality for autonomous\nsystems thanks to its robustness in adverse conditions and low cost. However,\nits utility is significantly limited by the sparsity and low resolution of\nradar point clouds, which poses challenges for tasks requiring dense and\naccurate 3D perception. Despite that recent efforts have shown great potential\nby exploring generative approaches to address this issue, they often rely on\ndense voxel representations that are inefficient and struggle to preserve\nstructural detail. To fill this gap, we make the key observation that latent\ndiffusion models (LDMs), though successful in other modalities, have not been\neffectively leveraged for radar-based 3D generation due to a lack of compatible\nrepresentations and conditioning strategies. We introduce RaLD, a framework\nthat bridges this gap by integrating scene-level frustum-based LiDAR\nautoencoding, order-invariant latent representations, and direct radar spectrum\nconditioning. These insights lead to a more compact and expressive generation\nprocess. Experiments show that RaLD produces dense and accurate 3D point clouds\nfrom raw radar spectrums, offering a promising solution for robust perception\nin challenging environments.",
        "url": "http://arxiv.org/abs/2511.07067v1",
        "published_date": "2025-11-10T13:03:58+00:00",
        "updated_date": "2025-11-10T13:03:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruijie Zhang",
            "Bixin Zeng",
            "Shengpeng Wang",
            "Fuhui Zhou",
            "Wei Wang"
        ],
        "tldr": "The paper introduces RaLD, a latent diffusion model framework for generating high-resolution 3D radar point clouds from raw radar data, addressing the limitations of sparse radar data in autonomous systems.",
        "tldr_zh": "该论文介绍了RaLD，一个用于从原始雷达数据生成高分辨率3D雷达点云的潜在扩散模型框架，旨在解决自动驾驶系统中稀疏雷达数据的局限性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SinSEMI: A One-Shot Image Generation Model and Data-Efficient Evaluation Framework for Semiconductor Inspection Equipment",
        "summary": "In the early stages of semiconductor equipment development, obtaining large\nquantities of raw optical images poses a significant challenge. This data\nscarcity hinder the advancement of AI-powered solutions in semiconductor\nmanufacturing. To address this challenge, we introduce SinSEMI, a novel\none-shot learning approach that generates diverse and highly realistic images\nfrom single optical image. SinSEMI employs a multi-scale flow-based model\nenhanced with LPIPS (Learned Perceptual Image Patch Similarity) energy guidance\nduring sampling, ensuring both perceptual realism and output variety. We also\nintroduce a comprehensive evaluation framework tailored for this application,\nwhich enables a thorough assessment using just two reference images. Through\nthe evaluation against multiple one-shot generation techniques, we demonstrate\nSinSEMI's superior performance in visual quality, quantitative measures, and\ndownstream tasks. Our experimental results demonstrate that SinSEMI-generated\nimages achieve both high fidelity and meaningful diversity, making them\nsuitable as training data for semiconductor AI applications.",
        "url": "http://arxiv.org/abs/2511.06740v1",
        "published_date": "2025-11-10T06:01:10+00:00",
        "updated_date": "2025-11-10T06:01:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "ChunLiang Wu",
            "Xiaochun Li"
        ],
        "tldr": "The paper introduces SinSEMI, a one-shot image generation model for semiconductor inspection using a multi-scale flow-based approach with LPIPS guidance, along with a data-efficient evaluation framework.",
        "tldr_zh": "该论文介绍了SinSEMI，一种用于半导体检测的单样本图像生成模型，它采用多尺度流模型和LPIPS指导，同时还提出了一个数据高效的评估框架。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]