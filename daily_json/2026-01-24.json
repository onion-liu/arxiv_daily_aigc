[
    {
        "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation",
        "summary": "Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.",
        "url": "http://arxiv.org/abs/2601.16210v1",
        "published_date": "2026-01-22T18:58:55+00:00",
        "updated_date": "2026-01-22T18:58:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Onkar Susladkar",
            "Tushar Prakash",
            "Adheesh Juvekar",
            "Kiet A. Nguyen",
            "Dong-Hwan Jang",
            "Inderjit S Dhillon",
            "Ismini Lourentzou"
        ],
        "tldr": "PyraTok is a language-aligned pyramidal tokenizer for video understanding and generation, achieving SOTA results in various video tasks including text-to-video generation and zero-shot video understanding.",
        "tldr_zh": "PyraTok 是一种语言对齐的金字塔式分词器，用于视频理解和生成，在包括文本到视频生成和零样本视频理解在内的各种视频任务中实现了 SOTA 结果。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback",
        "summary": "Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \\href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.",
        "url": "http://arxiv.org/abs/2601.16214v1",
        "published_date": "2026-01-22T18:59:56+00:00",
        "updated_date": "2026-01-22T18:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenhang Ge",
            "Guibao Shen",
            "Jiawei Feng",
            "Luozhou Wang",
            "Hao Lu",
            "Xingye Tian",
            "Xin Tao",
            "Ying-Cong Chen"
        ],
        "tldr": "The paper introduces CamPilot, a novel approach to improve camera controllability in video diffusion models by using an efficient camera-aware 3D decoder and reward feedback learning based on 3D Gaussian representations, addressing limitations in existing reward models and computational overhead.",
        "tldr_zh": "该论文介绍了CamPilot，一种通过使用高效的相机感知3D解码器和基于3D高斯表示的奖励反馈学习来提高视频扩散模型中相机可控性的新方法，解决了现有奖励模型和计算开销的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
        "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.",
        "url": "http://arxiv.org/abs/2601.16208v1",
        "published_date": "2026-01-22T18:58:16+00:00",
        "updated_date": "2026-01-22T18:58:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shengbang Tong",
            "Boyang Zheng",
            "Ziteng Wang",
            "Bingda Tang",
            "Nanye Ma",
            "Ellis Brown",
            "Jihan Yang",
            "Rob Fergus",
            "Yann LeCun",
            "Saining Xie"
        ],
        "tldr": "The paper demonstrates that Representation Autoencoders (RAEs) outperform Variational Autoencoders (VAEs) as a foundation for large-scale text-to-image generation, showing faster convergence, better quality, and potential for multimodal reasoning in a shared latent space.",
        "tldr_zh": "该论文表明，表示自编码器 (RAE) 在大规模文本到图像生成方面优于变分自编码器 (VAE)，表现出更快的收敛速度、更高的质量，并且具有在共享潜在空间中进行多模态推理的潜力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360°",
        "summary": "Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.",
        "url": "http://arxiv.org/abs/2601.16192v1",
        "published_date": "2026-01-22T18:45:59+00:00",
        "updated_date": "2026-01-22T18:45:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyi Wu",
            "Daniel Watson",
            "Andrea Tagliasacchi",
            "David J. Fleet",
            "Marcus A. Brubaker",
            "Saurabh Saxena"
        ],
        "tldr": "This paper introduces 360Anything, a geometry-free method using diffusion transformers to lift perspective images and videos to 360° panoramas, achieving state-of-the-art performance without camera metadata and addressing seam artifacts.",
        "tldr_zh": "该论文介绍了360Anything，一种使用扩散Transformer将透视图像和视频转换为360°全景图的无几何方法，在没有相机元数据的情况下实现了最先进的性能，并解决了接缝伪影问题。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]