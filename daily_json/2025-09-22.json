[
    {
        "title": "Stencil: Subject-Driven Generation with Context Guidance",
        "summary": "Recent text-to-image diffusion models can generate striking visuals from text\nprompts, but they often fail to maintain subject consistency across generations\nand contexts. One major limitation of current fine-tuning approaches is the\ninherent trade-off between quality and efficiency. Fine-tuning large models\nimproves fidelity but is computationally expensive, while fine-tuning\nlightweight models improves efficiency but compromises image fidelity.\nMoreover, fine-tuning pre-trained models on a small set of images of the\nsubject can damage the existing priors, resulting in suboptimal results. To\nthis end, we present Stencil, a novel framework that jointly employs two\ndiffusion models during inference. Stencil efficiently fine-tunes a lightweight\nmodel on images of the subject, while a large frozen pre-trained model provides\ncontextual guidance during inference, injecting rich priors to enhance\ngeneration with minimal overhead. Stencil excels at generating high-fidelity,\nnovel renditions of the subject in less than a minute, delivering\nstate-of-the-art performance and setting a new benchmark in subject-driven\ngeneration.",
        "url": "http://arxiv.org/abs/2509.17120v1",
        "published_date": "2025-09-21T15:19:08+00:00",
        "updated_date": "2025-09-21T15:19:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gordon Chen",
            "Ziqi Huang",
            "Cheston Tan",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces Stencil, a framework using a fine-tuned lightweight model and a frozen large model for subject-driven image generation, achieving state-of-the-art performance and subject consistency.",
        "tldr_zh": "该论文介绍了 Stencil，一个使用微调的轻量级模型和冻结的大型模型进行主体驱动图像生成的框架，实现了最先进的性能和主体一致性。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "VCE: Safe Autoregressive Image Generation via Visual Contrast Exploitation",
        "summary": "Recently, autoregressive image generation models have wowed audiences with\ntheir remarkable capability in creating surprisingly realistic images. Models\nsuch as GPT-4o and LlamaGen can not only produce images that faithfully mimic\nrenowned artistic styles like Ghibli, Van Gogh, or Picasso, but also\npotentially generate Not-Safe-For-Work (NSFW) content, raising significant\nconcerns regarding copyright infringement and ethical use. Despite these\nconcerns, methods to safeguard autoregressive text-to-image models remain\nunderexplored. Previous concept erasure methods, primarily designed for\ndiffusion models that operate in denoising latent space, are not directly\napplicable to autoregressive models that generate images token by token. To\naddress this critical gap, we propose Visual Contrast Exploitation (VCE), a\nnovel framework comprising: (1) an innovative contrastive image pair\nconstruction paradigm that precisely decouples unsafe concepts from their\nassociated content semantics, and (2) a sophisticated DPO-based training\napproach that enhances the model's ability to identify and leverage visual\ncontrastive features from image pairs, enabling precise concept erasure. Our\ncomprehensive experiments across three challenging tasks-artist style erasure,\nexplicit content erasure, and object removal-demonstrate that our method\neffectively secures the model, achieving state-of-the-art results while erasing\nunsafe concepts and maintaining the integrity of unrelated safe concepts. The\ncode and models are available at https://github.com/Maplebb/VCE.",
        "url": "http://arxiv.org/abs/2509.16986v1",
        "published_date": "2025-09-21T09:00:27+00:00",
        "updated_date": "2025-09-21T09:00:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feng Han",
            "Chao Gong",
            "Zhipeng Wei",
            "Jingjing Chen",
            "Yu-Gang Jiang"
        ],
        "tldr": "This paper introduces Visual Contrast Exploitation (VCE), a novel framework to address safety concerns in autoregressive image generation by decoupling and erasing unsafe concepts while preserving image integrity, achieving state-of-the-art results.",
        "tldr_zh": "本文介绍了视觉对比利用（VCE），一个新的框架，通过解耦和擦除不安全概念，同时保持图像的完整性，来解决自回归图像生成中的安全问题，并达到了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VidCLearn: A Continual Learning Approach for Text-to-Video Generation",
        "summary": "Text-to-video generation is an emerging field in generative AI, enabling the\ncreation of realistic, semantically accurate videos from text prompts. While\ncurrent models achieve impressive visual quality and alignment with input text,\nthey typically rely on static knowledge, making it difficult to incorporate new\ndata without retraining from scratch. To address this limitation, we propose\nVidCLearn, a continual learning framework for diffusion-based text-to-video\ngeneration. VidCLearn features a student-teacher architecture where the student\nmodel is incrementally updated with new text-video pairs, and the teacher model\nhelps preserve previously learned knowledge through generative replay.\nAdditionally, we introduce a novel temporal consistency loss to enhance motion\nsmoothness and a video retrieval module to provide structural guidance at\ninference. Our architecture is also designed to be more computationally\nefficient than existing models while retaining satisfactory generation\nperformance. Experimental results show VidCLearn's superiority over baseline\nmethods in terms of visual quality, semantic alignment, and temporal coherence.",
        "url": "http://arxiv.org/abs/2509.16956v1",
        "published_date": "2025-09-21T07:34:19+00:00",
        "updated_date": "2025-09-21T07:34:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Luca Zanchetta",
            "Lorenzo Papa",
            "Luca Maiano",
            "Irene Amerini"
        ],
        "tldr": "The paper introduces VidCLearn, a continual learning framework for text-to-video generation that addresses the challenge of incorporating new data without retraining. It uses a student-teacher architecture, a temporal consistency loss, and a video retrieval module to improve visual quality, semantic alignment, and temporal coherence.",
        "tldr_zh": "该论文介绍了VidCLearn，一个用于文本到视频生成的持续学习框架，解决了在不重新训练的情况下整合新数据的挑战。它使用学生-教师架构、时间一致性损失和视频检索模块来提高视觉质量、语义对齐和时间连贯性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module",
        "summary": "Video and audio inpainting for mixed audio-visual content has become a\ncrucial task in multimedia editing recently. However, precisely removing an\nobject and its corresponding audio from a video without affecting the rest of\nthe scene remains a significant challenge. To address this, we propose\nVAInpaint, a novel pipeline that first utilizes a segmentation model to\ngenerate masks and guide a video inpainting model in removing objects. At the\nsame time, an LLM then analyzes the scene globally, while a region-specific\nmodel provides localized descriptions. Both the overall and regional\ndescriptions will be inputted into an LLM, which will refine the content and\nturn it into text queries for our text-driven audio separation model. Our audio\nseparation model is fine-tuned on a customized dataset comprising segmented\nMUSIC instrument images and VGGSound backgrounds to enhance its generalization\nperformance. Experiments show that our method achieves performance comparable\nto current benchmarks in both audio and video inpainting.",
        "url": "http://arxiv.org/abs/2509.17022v1",
        "published_date": "2025-09-21T10:31:56+00:00",
        "updated_date": "2025-09-21T10:31:56+00:00",
        "categories": [
            "cs.MM",
            "cs.CV",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Kam Man Wu",
            "Zeyue Tian",
            "Liya Ji",
            "Qifeng Chen"
        ],
        "tldr": "VAInpaint introduces a novel video-audio inpainting pipeline using LLMs and a tailored audio separation model to remove objects and their sounds from videos, achieving comparable performance to existing benchmarks. It leverages segmentation, LLM-based scene analysis, and a fine-tuned audio separation model.",
        "tldr_zh": "VAInpaint 提出了一个新颖的视频音频修复管线，利用大型语言模型（LLMs）和一个定制的音频分离模型，从视频中移除物体及其声音，并达到与现有基准相当的性能。 它结合了分割、基于 LLM 的场景分析和一个微调的音频分离模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]