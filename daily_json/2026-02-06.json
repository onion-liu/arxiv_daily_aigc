[
    {
        "title": "Pathwise Test-Time Correction for Autoregressive Long Video Generation",
        "summary": "Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.",
        "url": "http://arxiv.org/abs/2602.05871v1",
        "published_date": "2026-02-05T16:50:39+00:00",
        "updated_date": "2026-02-05T16:50:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xunzhi Xiang",
            "Zixuan Duan",
            "Guiyu Zhang",
            "Haiyu Zhang",
            "Zhe Gao",
            "Junta Wu",
            "Shaofeng Zhang",
            "Tengfei Wang",
            "Qi Fan",
            "Chunchao Guo"
        ],
        "tldr": "This paper introduces Test-Time Correction (TTC), a training-free method to mitigate error accumulation in autoregressive long video generation by calibrating intermediate states using the initial frame as a reference anchor, achieving comparable quality to training-based methods with negligible overhead.",
        "tldr_zh": "本文介绍了一种名为Test-Time Correction (TTC) 的免训练方法，通过使用初始帧作为参考锚点校准中间状态，以减轻自回归长视频生成中的误差累积，在几乎没有额外开销的情况下，可达到与基于训练的方法相媲美的质量。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation",
        "summary": "We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency.",
        "url": "http://arxiv.org/abs/2602.04876v1",
        "published_date": "2026-02-04T18:58:55+00:00",
        "updated_date": "2026-02-04T18:58:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Zhan",
            "Zizhang Li",
            "Hong-Xing Yu",
            "Jiajun Wu"
        ],
        "tldr": "PerpetualWonder introduces a hybrid generative simulator for long-horizon, action-conditioned 4D scene generation from a single image by unifying physical state and visual representation, enabling dynamics and appearance corrections.",
        "tldr_zh": "PerpetualWonder 引入了一种混合生成模拟器，用于从单个图像生成长时程、动作条件化的 4D 场景，通过统一物理状态和视觉表示，从而能够校正动力学和外观。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers",
        "summary": "Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.",
        "url": "http://arxiv.org/abs/2602.05605v1",
        "published_date": "2026-02-05T12:42:22+00:00",
        "updated_date": "2026-02-05T12:42:22+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jiaji Zhang",
            "Hailiang Zhao",
            "Guoxuan Zhu",
            "Ruichao Sun",
            "Jiaju Wu",
            "Xinkui Zhao",
            "Hanlin Tang",
            "Weiyi Lu",
            "Kan Liu",
            "Tao Lan",
            "Lin Qu",
            "Shuiguang Deng"
        ],
        "tldr": "Shiva-DiT introduces a differentiable top-k selection method for efficient Diffusion Transformers (DiTs) using residual-aware gradient estimation, achieving significant speedups and fidelity improvements in image generation.",
        "tldr_zh": "Shiva-DiT 提出了一种可微的 top-k 选择方法，用于高效的 Diffusion Transformers (DiT)，它利用了残差感知的梯度估计，在图像生成方面实现了显著的加速和保真度提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SSG: Scaled Spatial Guidance for Multi-Scale Visual Autoregressive Generation",
        "summary": "Visual autoregressive (VAR) models generate images through next-scale prediction, naturally achieving coarse-to-fine, fast, high-fidelity synthesis mirroring human perception. In practice, this hierarchy can drift at inference time, as limited capacity and accumulated error cause the model to deviate from its coarse-to-fine nature. We revisit this limitation from an information-theoretic perspective and deduce that ensuring each scale contributes high-frequency content not explained by earlier scales mitigates the train-inference discrepancy. With this insight, we propose Scaled Spatial Guidance (SSG), training-free, inference-time guidance that steers generation toward the intended hierarchy while maintaining global coherence. SSG emphasizes target high-frequency signals, defined as the semantic residual, isolated from a coarser prior. To obtain this prior, we leverage a principled frequency-domain procedure, Discrete Spatial Enhancement (DSE), which is devised to sharpen and better isolate the semantic residual through frequency-aware construction. SSG applies broadly across VAR models leveraging discrete visual tokens, regardless of tokenization design or conditioning modality. Experiments demonstrate SSG yields consistent gains in fidelity and diversity while preserving low latency, revealing untapped efficiency in coarse-to-fine image generation. Code is available at https://github.com/Youngwoo-git/SSG.",
        "url": "http://arxiv.org/abs/2602.05534v1",
        "published_date": "2026-02-05T10:48:58+00:00",
        "updated_date": "2026-02-05T10:48:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Youngwoo Shin",
            "Jiwan Hur",
            "Junmo Kim"
        ],
        "tldr": "The paper introduces Scaled Spatial Guidance (SSG), a training-free, inference-time guidance method for visual autoregressive models that improves fidelity and diversity by emphasizing high-frequency content and mitigating train-inference discrepancy. It achieves this through Discrete Spatial Enhancement (DSE) in the frequency domain to sharpen semantic residuals.",
        "tldr_zh": "该论文介绍了一种名为Scaled Spatial Guidance (SSG)的训练无关的推理时引导方法，用于视觉自回归模型。该方法通过强调高频内容并减轻训练-推理差异，从而提高图像的保真度和多样性。 它通过频域中的离散空间增强（DSE）来锐化语义残差来实现这一点。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching",
        "summary": "While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup performance, but it inevitably faces semantic and detail drop with further compression. Another widely adopted method, training-aware step-distillation, though successful in image generation, also faces drastic degradation in video generation with a few steps. Furthermore, the quality loss becomes more severe when simply applying training-free feature caching to the step-distilled models, due to the sparser sampling steps. This paper novelly introduces a distillation-compatible learnable feature caching mechanism for the first time. We employ a lightweight learnable neural predictor instead of traditional training-free heuristics for diffusion models, enabling a more accurate capture of the high-dimensional feature evolution process. Furthermore, we explore the challenges of highly compressed distillation on large-scale video models and propose a conservative Restricted MeanFlow approach to achieve more stable and lossless distillation. By undertaking these initiatives, we further push the acceleration boundaries to $11.8\\times$ while preserving generation quality. Extensive experiments demonstrate the effectiveness of our method. The code is in the supplementary materials and will be publicly available.",
        "url": "http://arxiv.org/abs/2602.05449v1",
        "published_date": "2026-02-05T08:45:08+00:00",
        "updated_date": "2026-02-05T08:45:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chang Zou",
            "Changlin Li",
            "Yang Li",
            "Patrol Li",
            "Jianbing Wu",
            "Xiao He",
            "Songtao Liu",
            "Zhao Zhong",
            "Kailin Huang",
            "Linfeng Zhang"
        ],
        "tldr": "This paper introduces a distillation-compatible learnable feature caching mechanism to accelerate video diffusion transformers, achieving significant speedups while preserving generation quality by addressing the quality loss with compressed distillation and feature caching.",
        "tldr_zh": "该论文提出了一种与蒸馏兼容的可学习特征缓存机制，旨在加速视频扩散Transformer，通过解决压缩蒸馏和特征缓存带来的质量损失，在保持生成质量的同时实现显著的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Stable Velocity: A Variance Perspective on Flow Matching",
        "summary": "While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.",
        "url": "http://arxiv.org/abs/2602.05435v1",
        "published_date": "2026-02-05T08:25:05+00:00",
        "updated_date": "2026-02-05T08:25:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Donglin Yang",
            "Yongxing Zhang",
            "Xin Yu",
            "Liang Hou",
            "Xin Tao",
            "Pengfei Wan",
            "Xiaojuan Qi",
            "Renjie Liao"
        ],
        "tldr": "The paper identifies and mitigates high-variance training targets in flow matching, leading to improved training efficiency and faster sampling via a novel method called Stable Velocity.",
        "tldr_zh": "该论文识别并缓解了流匹配中高方差的训练目标，从而通过一种名为稳定速度的新方法提高了训练效率和更快的采样。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAIL: Self-Amplified Iterative Learning for Diffusion Model Alignment with Minimal Human Feedback",
        "summary": "Aligning diffusion models with human preferences remains challenging, particularly when reward models are unavailable or impractical to obtain, and collecting large-scale preference datasets is prohibitively expensive. \\textit{This raises a fundamental question: can we achieve effective alignment using only minimal human feedback, without auxiliary reward models, by unlocking the latent capabilities within diffusion models themselves?} In this paper, we propose \\textbf{SAIL} (\\textbf{S}elf-\\textbf{A}mplified \\textbf{I}terative \\textbf{L}earning), a novel framework that enables diffusion models to act as their own teachers through iterative self-improvement. Starting from a minimal seed set of human-annotated preference pairs, SAIL operates in a closed-loop manner where the model progressively generates diverse samples, self-annotates preferences based on its evolving understanding, and refines itself using this self-augmented dataset. To ensure robust learning and prevent catastrophic forgetting, we introduce a ranked preference mixup strategy that carefully balances exploration with adherence to initial human priors. Extensive experiments demonstrate that SAIL consistently outperforms state-of-the-art methods across multiple benchmarks while using merely 6\\% of the preference data required by existing approaches, revealing that diffusion models possess remarkable self-improvement capabilities that, when properly harnessed, can effectively replace both large-scale human annotation and external reward models.",
        "url": "http://arxiv.org/abs/2602.05380v1",
        "published_date": "2026-02-05T06:58:38+00:00",
        "updated_date": "2026-02-05T06:58:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoxuan He",
            "Siming Fu",
            "Wanli Li",
            "Zhiyuan Li",
            "Dacheng Yin",
            "Kang Rong",
            "Fengyun Rao",
            "Bo Zhang"
        ],
        "tldr": "The paper introduces SAIL, a self-amplified iterative learning framework for aligning diffusion models with human preferences using minimal human feedback, showing it outperforms existing methods with significantly less data.",
        "tldr_zh": "该论文介绍了SAIL，一种自增强迭代学习框架，通过使用极少的人工反馈将扩散模型与人类偏好对齐。实验表明，它在远低于现有方法的数据量下，性能超越了现有技术。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation",
        "summary": "With the increasing versatility of text-to-image diffusion models, the ability to selectively erase undesirable concepts (e.g., harmful content) has become indispensable. However, existing concept erasure approaches primarily focus on removing unsafe concepts without providing guidance toward corresponding safe alternatives, which often leads to failure in preserving the structural and semantic consistency between the original and erased generations. In this paper, we propose a novel framework, PAIRed Erasing (PAIR), which reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs. We first generate safe counterparts from unsafe inputs while preserving structural and semantic fidelity, forming paired unsafe-safe multimodal data. Leveraging these pairs, we introduce two key components: (1) Paired Semantic Realignment, a guided objective that uses unsafe-safe pairs to explicitly map target concepts to semantically aligned safe anchors; and (2) Fisher-weighted Initialization for DoRA, which initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs, encouraging the generation of safe alternatives while selectively suppressing unsafe concepts. Together, these components enable fine-grained erasure that removes only the targeted concepts while maintaining overall semantic consistency. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.",
        "url": "http://arxiv.org/abs/2602.05339v1",
        "published_date": "2026-02-05T06:05:24+00:00",
        "updated_date": "2026-02-05T06:05:24+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yongwoo Kim",
            "Sungmin Cha",
            "Hyunsoo Kim",
            "Jaewon Lee",
            "Donghyun Kim"
        ],
        "tldr": "This paper introduces a novel approach, PAIRed Erasing (PAIR), for concept erasure in text-to-image diffusion models, focusing on maintaining semantic consistency by using unsafe-safe concept pairs for guided semantic realignment and Fisher-weighted adaptation.",
        "tldr_zh": "本文提出了一种新颖的PAIRed Erasing (PAIR) 方法，用于文本到图像扩散模型中的概念擦除，通过使用不安全-安全概念对进行引导式语义重对齐和Fisher加权自适应，从而专注于保持语义一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion",
        "summary": "Generating long-form content, such as minute-long videos and extended texts, is increasingly important for modern generative models. Block diffusion improves inference efficiency via KV caching and block-wise causal inference and has been widely adopted in diffusion language models and video generation. However, in long-context settings, block diffusion still incurs substantial overhead from repeatedly computing attention over a growing KV cache. We identify an underexplored property of block diffusion: cross-step redundancy of attention within a block. Our analysis shows that attention outputs from tokens outside the current block remain largely stable across diffusion steps, while block-internal attention varies significantly. Based on this observation, we propose FlashBlock, a cached block-external attention mechanism that reuses stable attention output, reducing attention computation and KV cache access without modifying the diffusion process. Moreover, FlashBlock is orthogonal to sparse attention and can be combined as a complementary residual reuse strategy, substantially improving model accuracy under aggressive sparsification. Experiments on diffusion language models and video generation demonstrate up to 1.44$\\times$ higher token throughput and up to 1.6$\\times$ reduction in attention time, with negligible impact on generation quality. Project page: https://caesarhhh.github.io/FlashBlock/.",
        "url": "http://arxiv.org/abs/2602.05305v1",
        "published_date": "2026-02-05T04:57:21+00:00",
        "updated_date": "2026-02-05T04:57:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Zhuokun Chen",
            "Jianfei Cai",
            "Bohan Zhuang"
        ],
        "tldr": "The paper introduces FlashBlock, a caching mechanism for block diffusion that exploits attention redundancy in long-context generation, improving throughput and reducing attention time in diffusion language models and video generation.",
        "tldr_zh": "该论文介绍了FlashBlock，一种用于块扩散的缓存机制，通过利用长文本生成中的注意力冗余，提高了扩散语言模型和视频生成的吞吐量并减少了注意力计算时间。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling",
        "summary": "Aligning video generative models with human preferences remains challenging: current approaches rely on Vision-Language Models (VLMs) for reward modeling, but these models struggle to capture subtle temporal dynamics. We propose a fundamentally different approach: repurposing video generative models, which are inherently designed to model temporal structure, as reward models. We present the Generative-Transformer-based Self-Supervised Video Judge (\\modelname), a novel evaluation model that transforms state-of-the-art video generation models into powerful temporally-aware reward models. Our key insight is that generative models can be reformulated as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, enabling them to discriminate video quality with remarkable precision when trained via contrastive objectives. To prevent the model from exploiting superficial differences between real and generated videos, we design challenging synthetic negative videos through controlled latent-space perturbations: temporal slicing, feature swapping, and frame shuffling, which simulate realistic but subtle visual degradations. This forces the model to learn meaningful spatiotemporal features rather than trivial artifacts. \\modelname achieves state-of-the-art performance on GenAI-Bench and MonteBench using only 30K human-annotations: $6\\times$ to $65\\times$ fewer than existing VLM-based approaches.",
        "url": "http://arxiv.org/abs/2602.05202v1",
        "published_date": "2026-02-05T01:54:01+00:00",
        "updated_date": "2026-02-05T01:54:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shivanshu Shekhar",
            "Uttaran Bhattacharya",
            "Raghavendra Addanki",
            "Mehrab Tanjim",
            "Somdeb Sarkhel",
            "Tong Zhang"
        ],
        "tldr": "The paper introduces GT-SVJ, a novel video reward model that repurposes video generative models as energy-based models (EBMs) to assess video quality using self-supervised contrastive learning, achieving state-of-the-art performance with significantly fewer human annotations.",
        "tldr_zh": "该论文介绍了GT-SVJ，一种新颖的视频奖励模型，它将视频生成模型重新用作基于能量的模型（EBM），以使用自监督对比学习评估视频质量，并以显着更少的人工标注实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Laminating Representation Autoencoders for Efficient Diffusion",
        "summary": "Recent work has shown that diffusion models can generate high-quality images by operating directly on SSL patch features rather than pixel-space latents. However, the dense patch grids from encoders like DINOv2 contain significant redundancy, making diffusion needlessly expensive. We introduce FlatDINO, a variational autoencoder that compresses this representation into a one-dimensional sequence of just 32 continuous tokens -an 8x reduction in sequence length and 48x compression in total dimensionality. On ImageNet 256x256, a DiT-XL trained on FlatDINO latents achieves a gFID of 1.80 with classifier-free guidance while requiring 8x fewer FLOPs per forward pass and up to 4.5x fewer FLOPs per training step compared to diffusion on uncompressed DINOv2 features. These are preliminary results and this work is in progress.",
        "url": "http://arxiv.org/abs/2602.04873v1",
        "published_date": "2026-02-04T18:57:33+00:00",
        "updated_date": "2026-02-04T18:57:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ramón Calvo-González",
            "François Fleuret"
        ],
        "tldr": "The paper introduces FlatDINO, a variational autoencoder that efficiently compresses DINOv2 patch features into a compact latent space for diffusion models, achieving significant computational savings and improved generation performance on ImageNet.",
        "tldr_zh": "该论文介绍了FlatDINO，一种变分自编码器，可将DINOv2补丁特征有效地压缩为紧凑的潜在空间，用于扩散模型，从而在ImageNet上实现了显著的计算节省和改进的生成性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Explainable Pathomics Feature Visualization via Correlation-aware Conditional Feature Editing",
        "summary": "Pathomics is a recent approach that offers rich quantitative features beyond what black-box deep learning can provide, supporting more reproducible and explainable biomarkers in digital pathology. However, many derived features (e.g., \"second-order moment\") remain difficult to interpret, especially across different clinical contexts, which limits their practical adoption. Conditional diffusion models show promise for explainability through feature editing, but they typically assume feature independence**--**an assumption violated by intrinsically correlated pathomics features. Consequently, editing one feature while fixing others can push the model off the biological manifold and produce unrealistic artifacts. To address this, we propose a Manifold-Aware Diffusion (MAD) framework for controllable and biologically plausible cell nuclei editing. Unlike existing approaches, our method regularizes feature trajectories within a disentangled latent space learned by a variational auto-encoder (VAE). This ensures that manipulating a target feature automatically adjusts correlated attributes to remain within the learned distribution of real cells. These optimized features then guide a conditional diffusion model to synthesize high-fidelity images. Experiments demonstrate that our approach is able to navigate the manifold of pathomics features when editing those features. The proposed method outperforms baseline methods in conditional feature editing while preserving structural coherence.",
        "url": "http://arxiv.org/abs/2602.05397v1",
        "published_date": "2026-02-05T07:28:54+00:00",
        "updated_date": "2026-02-05T07:28:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuechen Yang",
            "Junlin Guo",
            "Ruining Deng",
            "Junchao Zhu",
            "Zhengyi Lu",
            "Chongyu Qu",
            "Yanfan Zhu",
            "Xingyi Guo",
            "Yu Wang",
            "Shilin Zhao",
            "Haichun Yang",
            "Yuankai Huo"
        ],
        "tldr": "This paper introduces a manifold-aware diffusion framework (MAD) for explainable pathomics feature visualization, addressing the issue of feature correlation in conditional diffusion models by regularizing feature trajectories within a disentangled latent space learned by a VAE.",
        "tldr_zh": "本文介绍了一种manifold-aware diffusion框架 (MAD)，用于可解释的pathomics特征可视化。它通过在VAE学习的解耦潜在空间中调整特征轨迹，解决了条件扩散模型中特征相关性的问题。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multimodal Latent Reasoning via Hierarchical Visual Cues Injection",
        "summary": "The advancement of multimodal large language models (MLLMs) has enabled impressive perception capabilities. However, their reasoning process often remains a \"fast thinking\" paradigm, reliant on end-to-end generation or explicit, language-centric chains of thought (CoT), which can be inefficient, verbose, and prone to hallucination. This work posits that robust reasoning should evolve within a latent space, integrating multimodal signals seamlessly. We propose multimodal latent reasoning via HIerarchical Visual cuEs injection (\\emph{HIVE}), a novel framework that instills deliberate, \"slow thinking\" without depending on superficial textual rationales. Our method recursively extends transformer blocks, creating an internal loop for iterative reasoning refinement. Crucially, it injectively grounds this process with hierarchical visual cues from global scene context to fine-grained regional details directly into the model's latent representations. This enables the model to perform grounded, multi-step inference entirely in the aligned latent space. Extensive evaluations demonstrate that test-time scaling is effective when incorporating vision knowledge, and that integrating hierarchical information significantly enhances the model's understanding of complex scenes.",
        "url": "http://arxiv.org/abs/2602.05359v1",
        "published_date": "2026-02-05T06:31:12+00:00",
        "updated_date": "2026-02-05T06:31:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiming Zhang",
            "Qiangyu Yan",
            "Borui Jiang",
            "Kai Han"
        ],
        "tldr": "The paper introduces HIVE, a method for multimodal latent reasoning that injects hierarchical visual cues into MLLMs, enabling grounded, multi-step inference in the latent space without relying on textual rationales.",
        "tldr_zh": "该论文介绍了一种名为HIVE的多模态潜在推理方法，该方法将分层视觉线索注入到MLLM中，从而在潜在空间中实现有根据的多步骤推理，而无需依赖文本原理。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]