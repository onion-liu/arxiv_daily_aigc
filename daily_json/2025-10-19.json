[
    {
        "title": "TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement",
        "summary": "Autoregressive Model (AR) has shown remarkable success in conditional image\ngeneration. However, these approaches for multiple reference generation\nstruggle with decoupling different reference identities. In this work, we\npropose the TokenAR framework, specifically focused on a simple but effective\ntoken-level enhancement mechanism to address reference identity confusion\nproblem. Such token-level enhancement consists of three parts, 1). Token Index\nEmbedding clusters the tokens index for better representing the same reference\nimages; 2). Instruct Token Injection plays as a role of extra visual feature\ncontainer to inject detailed and complementary priors for reference tokens; 3).\nThe identity-token disentanglement strategy (ITD) explicitly guides the token\nrepresentations toward independently representing the features of each\nidentity.This token-enhancement framework significantly augments the\ncapabilities of existing AR based methods in conditional image generation,\nenabling good identity consistency while preserving high quality background\nreconstruction. Driven by the goal of high-quality and high-diversity in\nmulti-subject generation, we introduce the InstructAR Dataset, the first\nopen-source, large-scale, multi-reference input, open domain image generation\ndataset that includes 28K training pairs, each example has two reference\nsubjects, a relative prompt and a background with mask annotation, curated for\nmultiple reference image generation training and evaluating. Comprehensive\nexperiments validate that our approach surpasses current state-of-the-art\nmodels in multiple reference image generation task. The implementation code and\ndatasets will be made publicly. Codes are available, see\nhttps://github.com/lyrig/TokenAR",
        "url": "http://arxiv.org/abs/2510.16332v1",
        "published_date": "2025-10-18T03:36:26+00:00",
        "updated_date": "2025-10-18T03:36:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haiyue Sun",
            "Qingdong He",
            "Jinlong Peng",
            "Peng Tang",
            "Jiangning Zhang",
            "Junwei Zhu",
            "Xiaobin Hu",
            "Shuicheng Yan"
        ],
        "tldr": "The paper introduces TokenAR, a token-level enhancement framework for autoregressive models to improve multi-subject image generation by addressing identity confusion, and provides a new large-scale dataset called InstructAR for this task.",
        "tldr_zh": "该论文介绍了TokenAR，一种用于自回归模型的token级别增强框架，通过解决身份混淆问题来改进多主题图像生成，并为此任务提供了一个名为InstructAR的新的大规模数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution",
        "summary": "Recent advances in diffusion models have driven remarkable progress in image\ngeneration. However, the generation process remains computationally intensive,\nand users often need to iteratively refine prompts to achieve the desired\nresults, further increasing latency and placing a heavy burden on cloud\nresources. To address this challenge, we propose DiffusionX, a cloud-edge\ncollaborative framework for efficient multi-round, prompt-based generation. In\nthis system, a lightweight on-device diffusion model interacts with users by\nrapidly producing preview images, while a high-capacity cloud model performs\nfinal refinements after the prompt is finalized. We further introduce a noise\nlevel predictor that dynamically balances the computation load, optimizing the\ntrade-off between latency and cloud workload. Experiments show that DiffusionX\nreduces average generation time by 15.8% compared with Stable Diffusion v1.5,\nwhile maintaining comparable image quality. Moreover, it is only 0.9% slower\nthan Tiny-SD with significantly improved image quality, thereby demonstrating\nefficiency and scalability with minimal overhead.",
        "url": "http://arxiv.org/abs/2510.16326v1",
        "published_date": "2025-10-18T03:20:39+00:00",
        "updated_date": "2025-10-18T03:20:39+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yi Wei",
            "Shunpu Tang",
            "Liang Zhao",
            "Qiangian Yang"
        ],
        "tldr": "DiffusionX is a cloud-edge collaborative framework that accelerates prompt-based image generation by using a lightweight on-device model for previews and a high-capacity cloud model for final refinements, demonstrating a significant speedup compared to Stable Diffusion v1.5.",
        "tldr_zh": "DiffusionX是一个云边协同框架，通过使用轻量级的设备端模型进行预览和高容量的云端模型进行最终优化，加速了基于提示词的图像生成，与Stable Diffusion v1.5相比，速度显著提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention",
        "summary": "Ultra-high-resolution text-to-image generation demands both fine-grained\ntexture synthesis and globally coherent structure, yet current diffusion models\nremain constrained to sub-$1K \\times 1K$ resolutions due to the prohibitive\nquadratic complexity of attention and the scarcity of native $4K$ training\ndata. We present \\textbf{Scale-DiT}, a new diffusion framework that introduces\nhierarchical local attention with low-resolution global guidance, enabling\nefficient, scalable, and semantically coherent image synthesis at ultra-high\nresolutions. Specifically, high-resolution latents are divided into fixed-size\nlocal windows to reduce attention complexity from quadratic to near-linear,\nwhile a low-resolution latent equipped with scaled positional anchors injects\nglobal semantics. A lightweight LoRA adaptation bridges global and local\npathways during denoising, ensuring consistency across structure and detail. To\nmaximize inference efficiency, we repermute token sequence in Hilbert curve\norder and implement a fused-kernel for skipping masked operations, resulting in\na GPU-friendly design. Extensive experiments demonstrate that Scale-DiT\nachieves more than $2\\times$ faster inference and lower memory usage compared\nto dense attention baselines, while reliably scaling to $4K \\times 4K$\nresolution without requiring additional high-resolution training data. On both\nquantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,\nScale-DiT delivers superior global coherence and sharper local detail, matching\nor outperforming state-of-the-art methods that rely on native 4K training.\nTaken together, these results highlight hierarchical local attention with\nguided low-resolution anchors as a promising and effective approach for\nadvancing ultra-high-resolution image generation.",
        "url": "http://arxiv.org/abs/2510.16325v1",
        "published_date": "2025-10-18T03:15:26+00:00",
        "updated_date": "2025-10-18T03:15:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuyao Zhang",
            "Yu-Wing Tai"
        ],
        "tldr": "Scale-DiT introduces hierarchical local attention with low-resolution global guidance for efficient and high-quality ultra-high-resolution image generation, achieving state-of-the-art results without requiring native 4K training data.",
        "tldr_zh": "Scale-DiT 引入了分层局部注意力与低分辨率全局引导，用于高效且高质量的超高分辨率图像生成，无需原生 4K 训练数据即可达到最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting",
        "summary": "Bridging the gap between complex human instructions and precise 3D object\ngrounding remains a significant challenge in vision and robotics. Existing 3D\nsegmentation methods often struggle to interpret ambiguous, reasoning-based\ninstructions, while 2D vision-language models that excel at such reasoning lack\nintrinsic 3D spatial understanding. In this paper, we introduce REALM, an\ninnovative MLLM-agent framework that enables open-world reasoning-based\nsegmentation without requiring extensive 3D-specific post-training. We perform\nsegmentation directly on 3D Gaussian Splatting representations, capitalizing on\ntheir ability to render photorealistic novel views that are highly suitable for\nMLLM comprehension. As directly feeding one or more rendered views to the MLLM\ncan lead to high sensitivity to viewpoint selection, we propose a novel\nGlobal-to-Local Spatial Grounding strategy. Specifically, multiple global views\nare first fed into the MLLM agent in parallel for coarse-level localization,\naggregating responses to robustly identify the target object. Then, several\nclose-up novel views of the object are synthesized to perform fine-grained\nlocal segmentation, yielding accurate and consistent 3D masks. Extensive\nexperiments show that REALM achieves remarkable performance in interpreting\nboth explicit and implicit instructions across LERF, 3D-OVS, and our newly\nintroduced REALM3D benchmarks. Furthermore, our agent framework seamlessly\nsupports a range of 3D interaction tasks, including object removal,\nreplacement, and style transfer, demonstrating its practical utility and\nversatility. Project page: https://ChangyueShi.github.io/REALM.",
        "url": "http://arxiv.org/abs/2510.16410v1",
        "published_date": "2025-10-18T08:53:08+00:00",
        "updated_date": "2025-10-18T08:53:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changyue Shi",
            "Minghao Chen",
            "Yiping Mao",
            "Chuxiao Yang",
            "Xinyuan Hu",
            "Jiajun Ding",
            "Zhou Yu"
        ],
        "tldr": "REALM is an MLLM-agent framework for 3D object segmentation and editing on Gaussian Splatting, using a global-to-local spatial grounding strategy to interpret complex instructions without 3D-specific post-training and supports various 3D interaction tasks.",
        "tldr_zh": "REALM是一个MLLM-agent框架，用于在Gaussian Splatting上进行3D对象分割和编辑，采用全局到局部的空间定位策略来解释复杂的指令，无需3D特定的后期训练，并支持各种3D交互任务。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]