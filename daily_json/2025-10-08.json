[
    {
        "title": "Teleportraits: Training-Free People Insertion into Any Scene",
        "summary": "The task of realistically inserting a human from a reference image into a\nbackground scene is highly challenging, requiring the model to (1) determine\nthe correct location and poses of the person and (2) perform high-quality\npersonalization conditioned on the background. Previous approaches often treat\nthem as separate problems, overlooking their interconnections, and typically\nrely on training to achieve high performance. In this work, we introduce a\nunified training-free pipeline that leverages pre-trained text-to-image\ndiffusion models. We show that diffusion models inherently possess the\nknowledge to place people in complex scenes without requiring task-specific\ntraining. By combining inversion techniques with classifier-free guidance, our\nmethod achieves affordance-aware global editing, seamlessly inserting people\ninto scenes. Furthermore, our proposed mask-guided self-attention mechanism\nensures high-quality personalization, preserving the subject's identity,\nclothing, and body features from just a single reference image. To the best of\nour knowledge, we are the first to perform realistic human insertions into\nscenes in a training-free manner and achieve state-of-the-art results in\ndiverse composite scene images with excellent identity preservation in\nbackgrounds and subjects.",
        "url": "http://arxiv.org/abs/2510.05660v1",
        "published_date": "2025-10-07T08:12:57+00:00",
        "updated_date": "2025-10-07T08:12:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jialu Gao",
            "K J Joseph",
            "Fernando De La Torre"
        ],
        "tldr": "The paper introduces a training-free pipeline for realistic human insertion into scenes using pre-trained text-to-image diffusion models, achieving state-of-the-art results with excellent identity preservation.",
        "tldr_zh": "该论文介绍了一种无需训练的流程，利用预训练的文本到图像扩散模型将人物真实地插入到场景中，并在背景和人物中实现卓越身份保持的同时，达到了最先进的效果。",
        "relevance_score": 9,
        "novelty_claim_score": 10,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "ShapeGen4D: Towards High Quality 4D Shape Generation from Videos",
        "summary": "Video-conditioned 4D shape generation aims to recover time-varying 3D\ngeometry and view-consistent appearance directly from an input video. In this\nwork, we introduce a native video-to-4D shape generation framework that\nsynthesizes a single dynamic 3D representation end-to-end from the video. Our\nframework introduces three key components based on large-scale pre-trained 3D\nmodels: (i) a temporal attention that conditions generation on all frames while\nproducing a time-indexed dynamic representation; (ii) a time-aware point\nsampling and 4D latent anchoring that promote temporally consistent geometry\nand texture; and (iii) noise sharing across frames to enhance temporal\nstability. Our method accurately captures non-rigid motion, volume changes, and\neven topological transitions without per-frame optimization. Across diverse\nin-the-wild videos, our method improves robustness and perceptual fidelity and\nreduces failure modes compared with the baselines.",
        "url": "http://arxiv.org/abs/2510.06208v1",
        "published_date": "2025-10-07T17:58:11+00:00",
        "updated_date": "2025-10-07T17:58:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiraphon Yenphraphai",
            "Ashkan Mirzaei",
            "Jianqi Chen",
            "Jiaxu Zou",
            "Sergey Tulyakov",
            "Raymond A. Yeh",
            "Peter Wonka",
            "Chaoyang Wang"
        ],
        "tldr": "ShapeGen4D introduces a video-to-4D shape generation framework that leverages pre-trained 3D models and temporal attention mechanisms to synthesize dynamic 3D representations from videos, improving robustness and fidelity compared to baselines.",
        "tldr_zh": "ShapeGen4D 提出了一种视频到 4D 形状生成的框架，该框架利用预训练的 3D 模型和时间注意力机制，从视频中合成动态 3D 表示，与基线方法相比，提高了鲁棒性和保真度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation",
        "summary": "Recent advances in generative medical models are constrained by\nmodality-specific scenarios that hinder the integration of complementary\nevidence from imaging, pathology, and clinical notes. This fragmentation limits\ntheir evolution into foundation models that can learn and reason across the\nfull spectrum of biomedical data. We propose MeDiM, the first medical discrete\ndiffusion model that learns shared distributions across modalities without\nmodality-specific components. MeDiM unifies multiple generative tasks:\ntranslating between images and text, and jointly producing image-report pairs\nacross domains in response to prompts. Built on a discrete diffusion framework,\nMeDiM bridges vision and language representations through a shared\nprobabilistic space. To enable unified and flexible medical generation, we\nemploy a multimodal large language model (MLLM) as the diffusion backbone,\nleveraging its prior knowledge and cross-modal reasoning. Two key designs are\nintroduced: (1) removing the causal attention mask for bidirectional context,\nand (2) injecting continuous timestep embeddings for diffusion awareness.\nExperiments demonstrate high-fidelity medical generation (FID 16.60 on\nMIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR\n0.2650 and 0.2580). Jointly generated image-report pairs further enhance\ndownstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2,\nplus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports\ncoherent and clinically grounded multimodal outputs.",
        "url": "http://arxiv.org/abs/2510.06131v1",
        "published_date": "2025-10-07T17:06:57+00:00",
        "updated_date": "2025-10-07T17:06:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiawei Mao",
            "Yuhan Wang",
            "Lifeng Chen",
            "Can Zhao",
            "Yucheng Tang",
            "Dong Yang",
            "Liangqiong Qu",
            "Daguang Xu",
            "Yuyin Zhou"
        ],
        "tldr": "The paper introduces MeDiM, a medical discrete diffusion model using MLLMs for unified multimodal generation across images and text, showing improvements in medical generation and report accuracy.",
        "tldr_zh": "该论文介绍了 MeDiM，一种使用 MLLM 的医学离散扩散模型，用于统一图像和文本之间的多模态生成，并在医学生成和报告准确性方面显示出改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Controllable Audio-Visual Viewpoint Generation from 360° Spatial Information",
        "summary": "The generation of sounding videos has seen significant advancements with the\nadvent of diffusion models. However, existing methods often lack the\nfine-grained control needed to generate viewpoint-specific content from larger,\nimmersive 360-degree environments. This limitation restricts the creation of\naudio-visual experiences that are aware of off-camera events. To the best of\nour knowledge, this is the first work to introduce a framework for controllable\naudio-visual generation, addressing this unexplored gap. Specifically, we\npropose a diffusion model by introducing a set of powerful conditioning signals\nderived from the full 360-degree space: a panoramic saliency map to identify\nregions of interest, a bounding-box-aware signed distance map to define the\ntarget viewpoint, and a descriptive caption of the entire scene. By integrating\nthese controls, our model generates spatially-aware viewpoint videos and audios\nthat are coherently influenced by the broader, unseen environmental context,\nintroducing a strong controllability that is essential for realistic and\nimmersive audio-visual generation. We show audiovisual examples proving the\neffectiveness of our framework.",
        "url": "http://arxiv.org/abs/2510.06060v1",
        "published_date": "2025-10-07T15:53:31+00:00",
        "updated_date": "2025-10-07T15:53:31+00:00",
        "categories": [
            "cs.MM",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Christian Marinoni",
            "Riccardo Fosco Gramaccioni",
            "Eleonora Grassucci",
            "Danilo Comminiello"
        ],
        "tldr": "This paper introduces a diffusion model for controllable audio-visual viewpoint generation from 360° spatial information, using panoramic saliency maps, signed distance maps, and scene captions as conditioning signals.",
        "tldr_zh": "本文介绍了一种扩散模型，用于从360°空间信息中生成可控的视听视点，使用全景显著性图、有符号距离图和场景描述作为条件信号。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders",
        "summary": "In this work, we present FoleyGRAM, a novel approach to video-to-audio\ngeneration that emphasizes semantic conditioning through the use of aligned\nmultimodal encoders. Building on prior advancements in video-to-audio\ngeneration, FoleyGRAM leverages the Gramian Representation Alignment Measure\n(GRAM) to align embeddings across video, text, and audio modalities, enabling\nprecise semantic control over the audio generation process. The core of\nFoleyGRAM is a diffusion-based audio synthesis model conditioned on\nGRAM-aligned embeddings and waveform envelopes, ensuring both semantic richness\nand temporal alignment with the corresponding input video. We evaluate\nFoleyGRAM on the Greatest Hits dataset, a standard benchmark for video-to-audio\nmodels. Our experiments demonstrate that aligning multimodal encoders using\nGRAM enhances the system's ability to semantically align generated audio with\nvideo content, advancing the state of the art in video-to-audio synthesis.",
        "url": "http://arxiv.org/abs/2510.05829v1",
        "published_date": "2025-10-07T11:52:00+00:00",
        "updated_date": "2025-10-07T11:52:00+00:00",
        "categories": [
            "cs.SD",
            "cs.CV",
            "cs.LG",
            "cs.MM",
            "eess.AS"
        ],
        "authors": [
            "Riccardo Fosco Gramaccioni",
            "Christian Marinoni",
            "Eleonora Grassucci",
            "Giordano Cicchetti",
            "Aurelio Uncini",
            "Danilo Comminiello"
        ],
        "tldr": "FoleyGRAM is a video-to-audio generation model that uses Gramian Representation Alignment Measure (GRAM) to align embeddings across video, text, and audio modalities for semantically rich and temporally aligned audio synthesis, achieving state-of-the-art performance.",
        "tldr_zh": "FoleyGRAM 是一种视频到音频生成模型，它使用 Gramian 表征对齐测量 (GRAM) 来对齐视频、文本和音频模态之间的嵌入，从而实现语义丰富且时间对齐的音频合成，并取得了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Data Factory with Minimal Human Effort Using VLMs",
        "summary": "Generating enough and diverse data through augmentation offers an efficient\nsolution to the time-consuming and labour-intensive process of collecting and\nannotating pixel-wise images. Traditional data augmentation techniques often\nface challenges in manipulating high-level semantic attributes, such as\nmaterials and textures. In contrast, diffusion models offer a robust\nalternative, by effectively utilizing text-to-image or image-to-image\ntransformation. However, existing diffusion-based methods are either\ncomputationally expensive or compromise on performance. To address this issue,\nwe introduce a novel training-free pipeline that integrates pretrained\nControlNet and Vision-Language Models (VLMs) to generate synthetic images\npaired with pixel-level labels. This approach eliminates the need for manual\nannotations and significantly improves downstream tasks. To improve the\nfidelity and diversity, we add a Multi-way Prompt Generator, Mask Generator and\nHigh-quality Image Selection module. Our results on PASCAL-5i and COCO-20i\npresent promising performance and outperform concurrent work for one-shot\nsemantic segmentation.",
        "url": "http://arxiv.org/abs/2510.05722v1",
        "published_date": "2025-10-07T09:43:24+00:00",
        "updated_date": "2025-10-07T09:43:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaojiao Ye",
            "Jiaxing Zhong",
            "Qian Xie",
            "Yuzhou Zhou",
            "Niki Trigoni",
            "Andrew Markham"
        ],
        "tldr": "The paper introduces a training-free pipeline leveraging ControlNet and VLMs to generate synthetic images with pixel-level labels, addressing the limitations of traditional data augmentation and computationally expensive diffusion-based methods for semantic segmentation.",
        "tldr_zh": "该论文介绍了一种免训练的流水线，利用 ControlNet 和 VLMs 生成带有像素级标签的合成图像，解决了传统数据增强和计算昂贵的基于扩散方法在语义分割方面的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models",
        "summary": "Recent diffusion model research focuses on generating identity-consistent\nimages from a reference photo, but they struggle to accurately control age\nwhile preserving identity, and fine-tuning such models often requires costly\npaired images across ages. In this paper, we propose AgeBooth, a novel\nage-specific finetuning approach that can effectively enhance the age control\ncapability of adapterbased identity personalization models without the need for\nexpensive age-varied datasets. To reduce dependence on a large amount of\nage-labeled data, we exploit the linear nature of aging by introducing\nage-conditioned prompt blending and an age-specific LoRA fusion strategy that\nleverages SVDMix, a matrix fusion technique. These techniques enable\nhigh-quality generation of intermediate-age portraits. Our AgeBooth produces\nrealistic and identity-consistent face images across different ages from a\nsingle reference image. Experiments show that AgeBooth achieves superior age\ncontrol and visual quality compared to previous state-of-the-art editing-based\nmethods.",
        "url": "http://arxiv.org/abs/2510.05715v1",
        "published_date": "2025-10-07T09:25:09+00:00",
        "updated_date": "2025-10-07T09:25:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shihao Zhu",
            "Bohan Cao",
            "Ziheng Ouyang",
            "Zhen Li",
            "Peng-Tao Jiang",
            "Qibin Hou"
        ],
        "tldr": "AgeBooth introduces a novel age-specific finetuning approach for diffusion models that enhances age control in facial image generation while preserving identity, leveraging prompt blending and LoRA fusion to reduce reliance on age-labeled data.",
        "tldr_zh": "AgeBooth 提出了一种新颖的、特定年龄的扩散模型微调方法，增强了面部图像生成中年龄控制的能力，同时保留了身份信息，利用 prompt blending 和 LoRA 融合来减少对年龄标签数据的依赖。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Conditional Generation on Scale-based Visual Autoregressive Models",
        "summary": "Recent advances in autoregressive (AR) models have demonstrated their\npotential to rival diffusion models in image synthesis. However, for complex\nspatially-conditioned generation, current AR approaches rely on fine-tuning the\npre-trained model, leading to significant training costs. In this paper, we\npropose the Efficient Control Model (ECM), a plug-and-play framework featuring\na lightweight control module that introduces control signals via a distributed\narchitecture. This architecture consists of context-aware attention layers that\nrefine conditional features using real-time generated tokens, and a shared\ngated feed-forward network (FFN) designed to maximize the utilization of its\nlimited capacity and ensure coherent control feature learning. Furthermore,\nrecognizing the critical role of early-stage generation in determining semantic\nstructure, we introduce an early-centric sampling strategy that prioritizes\nlearning early control sequences. This approach reduces computational cost by\nlowering the number of training tokens per iteration, while a complementary\ntemperature scheduling during inference compensates for the resulting\ninsufficient training of late-stage tokens. Extensive experiments on\nscale-based AR models validate that our method achieves high-fidelity and\ndiverse control over image generation, surpassing existing baselines while\nsignificantly improving both training and inference efficiency.",
        "url": "http://arxiv.org/abs/2510.05610v1",
        "published_date": "2025-10-07T06:27:03+00:00",
        "updated_date": "2025-10-07T06:27:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaqi Liu",
            "Tao Huang",
            "Chang Xu"
        ],
        "tldr": "This paper introduces an efficient, plug-and-play control module (ECM) for spatially-conditioned image generation using scale-based autoregressive models, improving training and inference efficiency while maintaining high-fidelity and diverse control.",
        "tldr_zh": "本文介绍了一种高效的、即插即用的控制模块 (ECM)，用于基于尺度自回归模型的空间条件图像生成，在保持高保真度和多样化控制的同时，提高了训练和推理效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Chain-of-Thought Efficiency for Autoregressive Image Generation",
        "summary": "Autoregressive multimodal large language models have recently gained\npopularity for image generation, driven by advances in foundation models. To\nenhance alignment and detail, newer approaches employ chain-of-thought (CoT)\nreasoning, expanding user inputs into elaborated prompts prior to image\nsynthesis. However, this strategy can introduce unnecessary redundancy -- a\nphenomenon we call visual overthinking -- which increases computational costs\nand can introduce details that contradict the original prompt. In this work, we\nexplore how to generate more concise CoT sequences for more efficient image\ngeneration. We introduce ShortCoTI, a lightweight optimization framework that\nencourages more concise CoT while preserving output image quality. ShortCoTI\nrewards more concise prompts with an adaptive function that scales according to\nan estimated difficulty for each task. Incorporating this reward into a\nreinforcement learning paradigm reduces prompt reasoning length by 54% while\nmaintaining or slightly improving quality metrics across multiple benchmarks\n(T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates\nverbose explanations and repetitive refinements, producing reasoning prompts\nthat are both concise and semantically rich. As a result, ShortCoTI improves\ncomputational efficiency without compromising the fidelity or visual appeal of\ngenerated images.",
        "url": "http://arxiv.org/abs/2510.05593v1",
        "published_date": "2025-10-07T05:40:43+00:00",
        "updated_date": "2025-10-07T05:40:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Zeqi Gu",
            "Markos Georgopoulos",
            "Xiaoliang Dai",
            "Marjan Ghazvininejad",
            "Chu Wang",
            "Felix Juefei-Xu",
            "Kunpeng Li",
            "Yujun Shi",
            "Zecheng He",
            "Zijian He",
            "Jiawei Zhou",
            "Abe Davis",
            "Jialiang Wang"
        ],
        "tldr": "The paper introduces ShortCoTI, a reinforcement learning framework that optimizes chain-of-thought prompting for autoregressive image generation, reducing prompt length by 54% while maintaining or improving image quality.",
        "tldr_zh": "该论文介绍了一种名为ShortCoTI的强化学习框架，用于优化自回归图像生成的思维链提示，可将提示长度缩短54%，同时保持或提高图像质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation",
        "summary": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache .",
        "url": "http://arxiv.org/abs/2510.05367v1",
        "published_date": "2025-10-06T20:54:44+00:00",
        "updated_date": "2025-10-06T20:54:44+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yang Xiao",
            "Gen Li",
            "Kaiyuan Deng",
            "Yushu Wu",
            "Zheng Zhan",
            "Yanzhi Wang",
            "Xiaolong Ma",
            "Bo Hui"
        ],
        "tldr": "The paper introduces LightCache, a training-free acceleration method for video generation that optimizes memory usage by employing stage-specific strategies like asynchronous cache swapping, feature chunking, and slicing latents, resulting in faster inference speed with acceptable quality degradation.",
        "tldr_zh": "该论文介绍了LightCache，一种用于视频生成的免训练加速方法，通过采用异步缓存交换、特征分块和切片潜在变量等特定阶段的策略来优化内存使用，从而实现更快的推理速度和可接受的质量下降幅度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mitigating Diffusion Model Hallucinations with Dynamic Guidance",
        "summary": "Diffusion models, despite their impressive demos, often produce hallucinatory\nsamples with structural inconsistencies that lie outside of the support of the\ntrue data distribution. Such hallucinations can be attributed to excessive\nsmoothing between modes of the data distribution. However, semantic\ninterpolations are often desirable and can lead to generation diversity, thus\nwe believe a more nuanced solution is required. In this work, we introduce\nDynamic Guidance, which tackles this issue. Dynamic Guidance mitigates\nhallucinations by selectively sharpening the score function only along the\npre-determined directions known to cause artifacts, while preserving valid\nsemantic variations. To our knowledge, this is the first approach that\naddresses hallucinations at generation time rather than through post-hoc\nfiltering. Dynamic Guidance substantially reduces hallucinations on both\ncontrolled and natural image datasets, significantly outperforming baselines.",
        "url": "http://arxiv.org/abs/2510.05356v1",
        "published_date": "2025-10-06T20:31:13+00:00",
        "updated_date": "2025-10-06T20:31:13+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Kostas Triaridis",
            "Alexandros Graikos",
            "Aggelina Chatziagapi",
            "Grigorios G. Chrysos",
            "Dimitris Samaras"
        ],
        "tldr": "The paper introduces Dynamic Guidance to mitigate hallucinations in diffusion models by selectively sharpening the score function along artifact-causing directions during generation, outperforming baselines on image datasets.",
        "tldr_zh": "该论文提出了动态引导（Dynamic Guidance）方法，通过在生成过程中选择性地锐化沿导致伪影方向的分数函数，从而减轻扩散模型中的幻觉现象，并在图像数据集上优于基线方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion-Based Image Editing for Breaking Robust Watermarks",
        "summary": "Robust invisible watermarking aims to embed hidden information into images\nsuch that the watermark can survive various image manipulations. However, the\nrise of powerful diffusion-based image generation and editing techniques poses\na new threat to these watermarking schemes. In this paper, we present a\ntheoretical study and method demonstrating that diffusion models can\neffectively break robust image watermarks that were designed to resist\nconventional perturbations. We show that a diffusion-driven ``image\nregeneration'' process can erase embedded watermarks while preserving\nperceptual image content. We further introduce a novel guided diffusion attack\nthat explicitly targets the watermark signal during generation, significantly\ndegrading watermark detectability. Theoretically, we prove that as an image\nundergoes sufficient diffusion-based transformation, the mutual information\nbetween the watermarked image and the embedded watermark payload vanishes,\nresulting in decoding failure. Experimentally, we evaluate our approach on\nmultiple state-of-the-art watermarking schemes (including the deep\nlearning-based methods StegaStamp, TrustMark, and VINE) and demonstrate\nnear-zero watermark recovery rates after attack, while maintaining high visual\nfidelity of the regenerated images. Our findings highlight a fundamental\nvulnerability in current robust watermarking techniques against generative\nmodel-based attacks, underscoring the need for new watermarking strategies in\nthe era of generative AI.",
        "url": "http://arxiv.org/abs/2510.05978v1",
        "published_date": "2025-10-07T14:34:42+00:00",
        "updated_date": "2025-10-07T14:34:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunyi Ni",
            "Finn Carter",
            "Ze Niu",
            "Emily Davis",
            "Bo Zhang"
        ],
        "tldr": "The paper demonstrates that diffusion models can effectively break robust image watermarks, highlighting a vulnerability in current techniques against generative model-based attacks. They propose a novel guided diffusion attack that significantly degrades watermark detectability.",
        "tldr_zh": "该论文证明了扩散模型可以有效地破坏鲁棒的图像水印，突出了当前技术在生成模型攻击中的漏洞。他们提出了一种新的引导扩散攻击，可以显著降低水印的可检测性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach",
        "summary": "Automated video editing remains an underexplored task in the computer vision\nand multimedia domains, especially when contrasted with the growing interest in\nvideo generation and scene understanding. In this work, we address the specific\nchallenge of editing multicamera recordings of classical music concerts by\ndecomposing the problem into two key sub-tasks: when to cut and how to cut.\nBuilding on recent literature, we propose a novel multimodal architecture for\nthe temporal segmentation task (when to cut), which integrates log-mel\nspectrograms from the audio signals, plus an optional image embedding, and\nscalar temporal features through a lightweight convolutional-transformer\npipeline. For the spatial selection task (how to cut), we improve the\nliterature by updating from old backbones, e.g. ResNet, with a CLIP-based\nencoder and constraining distractor selection to segments from the same\nconcert. Our dataset was constructed following a pseudo-labeling approach, in\nwhich raw video data was automatically clustered into coherent shot segments.\nWe show that our models outperformed previous baselines in detecting cut points\nand provide competitive visual shot selection, advancing the state of the art\nin multimodal automated video editing.",
        "url": "http://arxiv.org/abs/2510.05661v1",
        "published_date": "2025-10-07T08:18:27+00:00",
        "updated_date": "2025-10-07T08:18:27+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Daniel Gonzálbez-Biosca",
            "Josep Cabacas-Maso",
            "Carles Ventura",
            "Ismael Benito-Altamirano"
        ],
        "tldr": "This paper presents a multimodal approach for automated video editing of classical music concerts, focusing on temporal segmentation and spatial shot selection using audio and visual features with a transformer-based architecture and CLIP-based encoder. It addresses the underexplored area of automated video editing.",
        "tldr_zh": "该论文提出了一种用于古典音乐会视频自动编辑的多模态方法，侧重于使用音频和视觉特征，通过基于Transformer的架构和基于CLIP的编码器进行时间分割和空间镜头选择。它解决了自动视频编辑这个尚未充分探索的领域。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction",
        "summary": "Autoregressive point cloud generation has long lagged behind diffusion-based\napproaches in quality. The performance gap stems from the fact that\nautoregressive models impose an artificial ordering on inherently unordered\npoint sets, forcing shape generation to proceed as a sequence of local\npredictions. This sequential bias emphasizes short-range continuity but\nundermines the model's capacity to capture long-range dependencies, hindering\nits ability to enforce global structural properties such as symmetry,\nconsistent topology, and large-scale geometric regularities. Inspired by the\nlevel-of-detail (LOD) principle in shape modeling, we propose PointNSP, a\ncoarse-to-fine generative framework that preserves global shape structure at\nlow resolutions and progressively refines fine-grained geometry at higher\nscales through a next-scale prediction paradigm. This multi-scale factorization\naligns the autoregressive objective with the permutation-invariant nature of\npoint sets, enabling rich intra-scale interactions while avoiding brittle fixed\norderings. Experiments on ShapeNet show that PointNSP establishes\nstate-of-the-art (SOTA) generation quality for the first time within the\nautoregressive paradigm. In addition, it surpasses strong diffusion-based\nbaselines in parameter, training, and inference efficiency. Finally, in dense\ngeneration with 8,192 points, PointNSP's advantages become even more\npronounced, underscoring its scalability potential.",
        "url": "http://arxiv.org/abs/2510.05613v1",
        "published_date": "2025-10-07T06:31:02+00:00",
        "updated_date": "2025-10-07T06:31:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ziqiao Meng",
            "Qichao Wang",
            "Zhiyang Dou",
            "Zixing Song",
            "Zhipeng Zhou",
            "Irwin King",
            "Peilin Zhao"
        ],
        "tldr": "The paper introduces PointNSP, a novel autoregressive point cloud generation framework that uses a coarse-to-fine approach with next-scale prediction, achieving SOTA results and improved efficiency compared to diffusion-based models.",
        "tldr_zh": "该论文介绍了PointNSP，一种新颖的自回归点云生成框架，它采用由粗到精的方法和下一尺度预测，实现了最先进的结果，并且比基于扩散的模型具有更高的效率。",
        "relevance_score": 2,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 5
    }
]