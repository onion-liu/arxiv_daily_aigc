[
    {
        "title": "LongCat-Video Technical Report",
        "summary": "Video generation is a critical pathway toward world models, with efficient\nlong video inference as a key capability. Toward this end, we introduce\nLongCat-Video, a foundational video generation model with 13.6B parameters,\ndelivering strong performance across multiple video generation tasks. It\nparticularly excels in efficient and high-quality long video generation,\nrepresenting our first step toward world models. Key features include: Unified\narchitecture for multiple tasks: Built on the Diffusion Transformer (DiT)\nframework, LongCat-Video supports Text-to-Video, Image-to-Video, and\nVideo-Continuation tasks with a single model; Long video generation:\nPretraining on Video-Continuation tasks enables LongCat-Video to maintain high\nquality and temporal coherence in the generation of minutes-long videos;\nEfficient inference: LongCat-Video generates 720p, 30fps videos within minutes\nby employing a coarse-to-fine generation strategy along both the temporal and\nspatial axes. Block Sparse Attention further enhances efficiency, particularly\nat high resolutions; Strong performance with multi-reward RLHF: Multi-reward\nRLHF training enables LongCat-Video to achieve performance on par with the\nlatest closed-source and leading open-source models. Code and model weights are\npublicly available to accelerate progress in the field.",
        "url": "http://arxiv.org/abs/2510.22200v1",
        "published_date": "2025-10-25T07:41:02+00:00",
        "updated_date": "2025-10-25T07:41:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Meituan LongCat Team",
            "Xunliang Cai",
            "Qilong Huang",
            "Zhuoliang Kang",
            "Hongyu Li",
            "Shijun Liang",
            "Liya Ma",
            "Siyu Ren",
            "Xiaoming Wei",
            "Rixu Xie",
            "Tong Zhang"
        ],
        "tldr": "LongCat-Video is a 13.6B parameter video generation model excelling in efficient and high-quality long video generation via a Diffusion Transformer architecture and multi-reward RLHF training, and the code/model are released.",
        "tldr_zh": "LongCat-Video是一个136亿参数的视频生成模型，擅长通过扩散Transformer架构和多奖励RLHF训练生成高效高质量的长视频，并且代码和模型已公开发布。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping",
        "summary": "Recently, GRPO-based reinforcement learning has shown remarkable progress in\noptimizing flow-matching models, effectively improving their alignment with\ntask-specific rewards. Within these frameworks, the policy update relies on\nimportance-ratio clipping to constrain overconfident positive and negative\ngradients. However, in practice, we observe a systematic shift in the\nimportance-ratio distribution-its mean falls below 1 and its variance differs\nsubstantially across timesteps. This left-shifted and inconsistent distribution\nprevents positive-advantage samples from entering the clipped region, causing\nthe mechanism to fail in constraining overconfident positive updates. As a\nresult, the policy model inevitably enters an implicit over-optimization\nstage-while the proxy reward continues to increase, essential metrics such as\nimage quality and text-prompt alignment deteriorate sharply, ultimately making\nthe learned policy impractical for real-world use. To address this issue, we\nintroduce GRPO-Guard, a simple yet effective enhancement to existing GRPO\nframeworks. Our method incorporates ratio normalization, which restores a\nbalanced and step-consistent importance ratio, ensuring that PPO clipping\nproperly constrains harmful updates across denoising timesteps. In addition, a\ngradient reweighting strategy equalizes policy gradients over noise conditions,\npreventing excessive updates from particular timestep regions. Together, these\ndesigns act as a regulated clipping mechanism, stabilizing optimization and\nsubstantially mitigating implicit over-optimization without relying on heavy KL\nregularization. Extensive experiments on multiple diffusion backbones (e.g.,\nSD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard\nsignificantly reduces over-optimization while maintaining or even improving\ngeneration quality.",
        "url": "http://arxiv.org/abs/2510.22319v1",
        "published_date": "2025-10-25T14:51:17+00:00",
        "updated_date": "2025-10-25T14:51:17+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jing Wang",
            "Jiajun Liang",
            "Jie Liu",
            "Henglin Liu",
            "Gongye Liu",
            "Jun Zheng",
            "Wanyuan Pang",
            "Ao Ma",
            "Zhenyu Xie",
            "Xintao Wang",
            "Meng Wang",
            "Pengfei Wan",
            "Xiaodan Liang"
        ],
        "tldr": "The paper introduces GRPO-Guard, a method to mitigate implicit over-optimization in flow-matching models by regulating the importance-ratio clipping mechanism in GRPO-based reinforcement learning, improving generation quality in diffusion models.",
        "tldr_zh": "该论文介绍了GRPO-Guard，一种通过调节GRPO强化学习中重要性比率裁剪机制来缓解流匹配模型中隐式过度优化的方法，从而提高扩散模型的生成质量。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Scaling Non-Parametric Sampling with Representation",
        "summary": "Scaling and architectural advances have produced strikingly photorealistic\nimage generative models, yet their mechanisms still remain opaque. Rather than\nadvancing scaling, our goal is to strip away complicated engineering tricks and\npropose a simple, non-parametric generative model. Our design is grounded in\nthree principles of natural images-(i) spatial non-stationarity, (ii) low-level\nregularities, and (iii) high-level semantics-and defines each pixel's\ndistribution from its local context window. Despite its minimal architecture\nand no training, the model produces high-fidelity samples on MNIST and visually\ncompelling CIFAR-10 images. This combination of simplicity and strong empirical\nperformance points toward a minimal theory of natural-image structure. The\nmodel's white-box nature also allows us to have a mechanistic understanding of\nhow the model generalizes and generates diverse images. We study it by tracing\neach generated pixel back to its source images. These analyses reveal a simple,\ncompositional procedure for \"part-whole generalization\", suggesting a\nhypothesis for how large neural network generative models learn to generalize.",
        "url": "http://arxiv.org/abs/2510.22196v1",
        "published_date": "2025-10-25T07:29:26+00:00",
        "updated_date": "2025-10-25T07:29:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Vincent Lu",
            "Aaron Truong",
            "Zeyu Yun",
            "Yubei Chen"
        ],
        "tldr": "The paper presents a simple, untrained, non-parametric generative model based on local image context, achieving high-fidelity samples on MNIST and compelling CIFAR-10 images. It offers mechanistic insights into generalization.",
        "tldr_zh": "该论文提出了一种基于局部图像上下文的简单、未经训练的非参数生成模型，在 MNIST 上实现了高保真样本，并在 CIFAR-10 上实现了引人注目的图像。它提供了对泛化的机制性见解。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Expert Validation of Synthetic Cervical Spine Radiographs Generated with a Denoising Diffusion Probabilistic Model",
        "summary": "Machine learning in neurosurgery is limited by challenges in assembling\nlarge, high-quality imaging datasets. Synthetic data offers a scalable,\nprivacy-preserving solution. We evaluated the feasibility of generating\nrealistic lateral cervical spine radiographs using a denoising diffusion\nprobabilistic model (DDPM) trained on 4,963 images from the Cervical Spine\nX-ray Atlas. Model performance was monitored via training/validation loss and\nFrechet inception distance, and synthetic image quality was assessed in a\nblinded \"clinical Turing test\" with six neuroradiologists and two\nspine-fellowship trained neurosurgeons. Experts reviewed 50 quartets containing\none real and three synthetic images, identifying the real image and rating\nrealism on a 4-point Likert scale. Experts correctly identified the real image\nin 29% of trials (Fleiss' kappa=0.061). Mean realism scores were comparable\nbetween real (3.323) and synthetic images (3.228, 3.258, and 3.320; p=0.383,\n0.471, 1.000). Nearest-neighbor analysis found no evidence of memorization. We\nalso provide a dataset of 20,063 synthetic radiographs. These results\ndemonstrate that DDPM-generated cervical spine X-rays are statistically\nindistinguishable in realism and quality from real clinical images, offering a\nnovel approach to creating large-scale neuroimaging datasets for ML\napplications in landmarking, segmentation, and classification.",
        "url": "http://arxiv.org/abs/2510.22166v1",
        "published_date": "2025-10-25T05:25:37+00:00",
        "updated_date": "2025-10-25T05:25:37+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Austin A. Barr",
            "Brij S. Karmur",
            "Anthony J. Winder",
            "Eddie Guo",
            "John T. Lysack",
            "James N. Scott",
            "William F. Morrish",
            "Muneer Eesa",
            "Morgan Willson",
            "David W. Cadotte",
            "Michael M. H. Yang",
            "Ian Y. M. Chan",
            "Sanju Lama",
            "Garnette R. Sutherland"
        ],
        "tldr": "The paper demonstrates the feasibility of generating realistic cervical spine radiographs using a denoising diffusion probabilistic model (DDPM) that are statistically indistinguishable from real clinical images based on expert evaluation, providing a potential solution for scalable neuroimaging datasets for machine learning.",
        "tldr_zh": "该论文展示了使用去噪扩散概率模型（DDPM）生成逼真的颈椎X光片的可行性，基于专家评估，这些X光片在统计上与真实的临床图像无法区分，为机器学习的可扩展神经影像数据集提供了一个潜在的解决方案。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation",
        "summary": "Capturing diversity is crucial in conditional and prompt-based image\ngeneration, particularly when conditions contain uncertainty that can lead to\nmultiple plausible outputs. To generate diverse images reflecting this\ndiversity, traditional methods often modify random seeds, making it difficult\nto discern meaningful differences between samples, or diversify the input\nprompt, which is limited in verbally interpretable diversity. We propose\nRainbow, a novel conditional image generation framework, applicable to any\npretrained conditional generative model, that addresses inherent\ncondition/prompt uncertainty and generates diverse plausible images. Rainbow is\nbased on a simple yet effective idea: decomposing the input condition into\ndiverse latent representations, each capturing an aspect of the uncertainty and\ngenerating a distinct image. First, we integrate a latent graph, parameterized\nby Generative Flow Networks (GFlowNets), into the prompt representation\ncomputation. Second, leveraging GFlowNets' advanced graph sampling capabilities\nto capture uncertainty and output diverse trajectories over the graph, we\nproduce multiple trajectories that collectively represent the input condition,\nleading to diverse condition representations and corresponding output images.\nEvaluations on natural image and medical image datasets demonstrate Rainbow's\nimprovement in both diversity and fidelity across image synthesis, image\ngeneration, and counterfactual generation tasks.",
        "url": "http://arxiv.org/abs/2510.22107v1",
        "published_date": "2025-10-25T01:25:50+00:00",
        "updated_date": "2025-10-25T01:25:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Bailey Trang",
            "Parham Saremi",
            "Alan Q. Wang",
            "Fangrui Huang",
            "Zahra TehraniNasab",
            "Amar Kumar",
            "Tal Arbel",
            "Li Fei-Fei",
            "Ehsan Adeli"
        ],
        "tldr": "The paper introduces Rainbow, a GFlowNet-based framework that decomposes input conditions into diverse latent representations using a latent graph, enhancing the diversity and fidelity of conditional image generation across various tasks.",
        "tldr_zh": "该论文介绍了一种名为Rainbow的框架，它基于GFlowNet，利用潜在图将输入条件分解为不同的潜在表示，从而提高了条件图像生成在各种任务中的多样性和保真度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FlowOpt: Fast Optimization Through Whole Flow Processes for Training-Free Editing",
        "summary": "The remarkable success of diffusion and flow-matching models has ignited a\nsurge of works on adapting them at test time for controlled generation tasks.\nExamples range from image editing to restoration, compression and\npersonalization. However, due to the iterative nature of the sampling process\nin those models, it is computationally impractical to use gradient-based\noptimization to directly control the image generated at the end of the process.\nAs a result, existing methods typically resort to manipulating each timestep\nseparately. Here we introduce FlowOpt - a zero-order (gradient-free)\noptimization framework that treats the entire flow process as a black box,\nenabling optimization through the whole sampling path without backpropagation\nthrough the model. Our method is both highly efficient and allows users to\nmonitor the intermediate optimization results and perform early stopping if\ndesired. We prove a sufficient condition on FlowOpt's step-size, under which\nconvergence to the global optimum is guaranteed. We further show how to\nempirically estimate this upper bound so as to choose an appropriate step-size.\nWe demonstrate how FlowOpt can be used for image editing, showcasing two\noptions: (i) inversion (determining the initial noise that generates a given\nimage), and (ii) directly steering the edited image to be similar to the source\nimage while conforming to a target text prompt. In both cases, FlowOpt achieves\nstate-of-the-art results while using roughly the same number of neural function\nevaluations (NFEs) as existing methods. Code and examples are available on the\nproject's webpage.",
        "url": "http://arxiv.org/abs/2510.22010v1",
        "published_date": "2025-10-24T20:24:26+00:00",
        "updated_date": "2025-10-24T20:24:26+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Or Ronai",
            "Vladimir Kulikov",
            "Tomer Michaeli"
        ],
        "tldr": "The paper introduces FlowOpt, a gradient-free optimization framework for editing images generated by flow-based generative models, achieving state-of-the-art results with comparable computational cost.",
        "tldr_zh": "该论文介绍了FlowOpt，一个用于编辑基于流的生成模型所生成图像的无梯度优化框架，以相当的计算成本实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Sprint: Sparse-Dense Residual Fusion for Efficient Diffusion Transformers",
        "summary": "Diffusion Transformers (DiTs) deliver state-of-the-art generative performance\nbut their quadratic training cost with sequence length makes large-scale\npretraining prohibitively expensive. Token dropping can reduce training cost,\nyet na\\\"ive strategies degrade representations, and existing methods are either\nparameter-heavy or fail at high drop ratios. We present SPRINT, Sparse--Dense\nResidual Fusion for Efficient Diffusion Transformers, a simple method that\nenables aggressive token dropping (up to 75%) while preserving quality. SPRINT\nleverages the complementary roles of shallow and deep layers: early layers\nprocess all tokens to capture local detail, deeper layers operate on a sparse\nsubset to cut computation, and their outputs are fused through residual\nconnections. Training follows a two-stage schedule: long masked pre-training\nfor efficiency followed by short full-token fine-tuning to close the\ntrain--inference gap. On ImageNet-1K 256x256, SPRINT achieves 9.8x training\nsavings with comparable FID/FDD, and at inference, its Path-Drop Guidance (PDG)\nnearly halves FLOPs while improving quality. These results establish SPRINT as\na simple, effective, and general solution for efficient DiT training.",
        "url": "http://arxiv.org/abs/2510.21986v1",
        "published_date": "2025-10-24T19:29:55+00:00",
        "updated_date": "2025-10-24T19:29:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dogyun Park",
            "Moayed Haji-Ali",
            "Yanyu Li",
            "Willi Menapace",
            "Sergey Tulyakov",
            "Hyunwoo J. Kim",
            "Aliaksandr Siarohin",
            "Anil Kag"
        ],
        "tldr": "The paper introduces SPRINT, a method for efficient training of Diffusion Transformers through sparse-dense residual fusion and token dropping, achieving significant training savings and improved inference efficiency while maintaining image generation quality.",
        "tldr_zh": "该论文介绍了SPRINT，一种通过稀疏-密集残差融合和令牌丢弃来高效训练扩散Transformer的方法，在保持图像生成质量的同时，实现了显著的训练节省和改进的推理效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MAGIC-Flow: Multiscale Adaptive Conditional Flows for Generation and Interpretable Classification",
        "summary": "Generative modeling has emerged as a powerful paradigm for representation\nlearning, but its direct applicability to challenging fields like medical\nimaging remains limited: mere generation, without task alignment, fails to\nprovide a robust foundation for clinical use. We propose MAGIC-Flow, a\nconditional multiscale normalizing flow architecture that performs generation\nand classification within a single modular framework. The model is built as a\nhierarchy of invertible and differentiable bijections, where the Jacobian\ndeterminant factorizes across sub-transformations. We show how this ensures\nexact likelihood computation and stable optimization, while invertibility\nenables explicit visualization of sample likelihoods, providing an\ninterpretable lens into the model's reasoning. By conditioning on class labels,\nMAGIC-Flow supports controllable sample synthesis and principled\nclass-probability estimation, effectively aiding both generative and\ndiscriminative objectives. We evaluate MAGIC-Flow against top baselines using\nmetrics for similarity, fidelity, and diversity. Across multiple datasets, it\naddresses generation and classification under scanner noise, and\nmodality-specific synthesis and identification. Results show MAGIC-Flow creates\nrealistic, diverse samples and improves classification. MAGIC-Flow is an\neffective strategy for generation and classification in data-limited domains,\nwith direct benefits for privacy-preserving augmentation, robust\ngeneralization, and trustworthy medical AI.",
        "url": "http://arxiv.org/abs/2510.22070v1",
        "published_date": "2025-10-24T23:11:25+00:00",
        "updated_date": "2025-10-24T23:11:25+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "eess.IV",
            "stat.ML"
        ],
        "authors": [
            "Luca Caldera",
            "Giacomo Bottacini",
            "Lara Cavinato"
        ],
        "tldr": "The paper introduces MAGIC-Flow, a conditional multiscale normalizing flow architecture for simultaneous generation and classification, particularly targeting medical imaging. It emphasizes interpretable likelihoods and controllable sample synthesis in data-limited domains.",
        "tldr_zh": "该论文介绍了MAGIC-Flow，一种用于同时生成和分类的条件多尺度归一化流架构，主要针对医学成像。它强调了可解释的似然性和在数据有限领域内可控的样本合成。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]