[
    {
        "title": "ReMix: Towards a Unified View of Consistent Character Generation and Editing",
        "summary": "Recent advances in large-scale text-to-image diffusion models (e.g., FLUX.1)\nhave greatly improved visual fidelity in consistent character generation and\nediting. However, existing methods rarely unify these tasks within a single\nframework. Generation-based approaches struggle with fine-grained identity\nconsistency across instances, while editing-based methods often lose spatial\ncontrollability and instruction alignment. To bridge this gap, we propose\nReMix, a unified framework for character-consistent generation and editing. It\nconstitutes two core components: the ReMix Module and IP-ControlNet. The ReMix\nModule leverages the multimodal reasoning ability of MLLMs to edit semantic\nfeatures of input images and adapt instruction embeddings to the native DiT\nbackbone without fine-tuning. While this ensures coherent semantic layouts,\npixel-level consistency and pose controllability remain challenging. To address\nthis, IP-ControlNet extends ControlNet to decouple semantic and layout cues\nfrom reference images and introduces an {\\epsilon}-equivariant latent space\nthat jointly denoises the reference and target images within a shared noise\nspace. Inspired by convergent evolution and quantum decoherence,i.e., where\nenvironmental noise drives state convergence, this design promotes feature\nalignment in the hidden space, enabling consistent object generation while\npreserving identity. ReMix supports a wide range of tasks, including\npersonalized generation, image editing, style transfer, and multi-condition\nsynthesis. Extensive experiments validate its effectiveness and efficiency as a\nunified framework for character-consistent image generation and editing.",
        "url": "http://arxiv.org/abs/2510.10156v1",
        "published_date": "2025-10-11T10:31:56+00:00",
        "updated_date": "2025-10-11T10:31:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Benjia Zhou",
            "Bin Fu",
            "Pei Cheng",
            "Yanru Wang",
            "Jiayuan Fan",
            "Tao Chen"
        ],
        "tldr": "The paper introduces ReMix, a unified framework for character-consistent image generation and editing that combines a multimodal reasoning module (ReMix Module) with an extended ControlNet (IP-ControlNet) for spatial control and consistency.",
        "tldr_zh": "该论文介绍了一种统一的框架ReMix，用于实现角色一致的图像生成和编辑。它结合了多模态推理模块（ReMix Module）和扩展的ControlNet（IP-ControlNet），以实现空间控制和一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Programs to Poses: Factored Real-World Scene Generation via Learned Program Libraries",
        "summary": "Real-world scenes, such as those in ScanNet, are difficult to capture, with\nhighly limited data available. Generating realistic scenes with varied object\nposes remains an open and challenging task. In this work, we propose\nFactoredScenes, a framework that synthesizes realistic 3D scenes by leveraging\nthe underlying structure of rooms while learning the variation of object poses\nfrom lived-in scenes. We introduce a factored representation that decomposes\nscenes into hierarchically organized concepts of room programs and object\nposes. To encode structure, FactoredScenes learns a library of functions\ncapturing reusable layout patterns from which scenes are drawn, then uses large\nlanguage models to generate high-level programs, regularized by the learned\nlibrary. To represent scene variations, FactoredScenes learns a\nprogram-conditioned model to hierarchically predict object poses, and retrieves\nand places 3D objects in a scene. We show that FactoredScenes generates\nrealistic, real-world rooms that are difficult to distinguish from real ScanNet\nscenes.",
        "url": "http://arxiv.org/abs/2510.10292v1",
        "published_date": "2025-10-11T17:14:24+00:00",
        "updated_date": "2025-10-11T17:14:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Joy Hsu",
            "Emily Jin",
            "Jiajun Wu",
            "Niloy J. Mitra"
        ],
        "tldr": "The paper introduces FactoredScenes, a framework using learned program libraries and LLMs to generate realistic 3D room scenes by disentangling room layouts and object poses, achieving results comparable to real ScanNet data.",
        "tldr_zh": "该论文介绍了FactoredScenes，一个利用学习的程序库和大型语言模型来生成逼真3D房间场景的框架，通过解耦房间布局和物体姿势，实现了与真实ScanNet数据相媲美的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]