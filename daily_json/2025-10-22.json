[
    {
        "title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention",
        "summary": "Recent advances in video generation have made it possible to produce visually\ncompelling videos, with wide-ranging applications in content creation,\nentertainment, and virtual reality. However, most existing diffusion\ntransformer based video generation models are limited to low-resolution outputs\n(<=720P) due to the quadratic computational complexity of the attention\nmechanism with respect to the output width and height. This computational\nbottleneck makes native high-resolution video generation (1080P/2K/4K)\nimpractical for both training and inference. To address this challenge, we\npresent UltraGen, a novel video generation framework that enables i) efficient\nand ii) end-to-end native high-resolution video synthesis. Specifically,\nUltraGen features a hierarchical dual-branch attention architecture based on\nglobal-local attention decomposition, which decouples full attention into a\nlocal attention branch for high-fidelity regional content and a global\nattention branch for overall semantic consistency. We further propose a\nspatially compressed global modeling strategy to efficiently learn global\ndependencies, and a hierarchical cross-window local attention mechanism to\nreduce computational costs while enhancing information flow across different\nlocal windows. Extensive experiments demonstrate that UltraGen can effectively\nscale pre-trained low-resolution video models to 1080P and even 4K resolution\nfor the first time, outperforming existing state-of-the-art methods and\nsuper-resolution based two-stage pipelines in both qualitative and quantitative\nevaluations.",
        "url": "http://arxiv.org/abs/2510.18775v1",
        "published_date": "2025-10-21T16:23:21+00:00",
        "updated_date": "2025-10-21T16:23:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Teng Hu",
            "Jiangning Zhang",
            "Zihan Su",
            "Ran Yi"
        ],
        "tldr": "The paper introduces UltraGen, a novel video generation framework that utilizes a hierarchical attention mechanism to efficiently generate high-resolution (up to 4K) videos by decoupling attention into global and local branches and employing spatial compression.",
        "tldr_zh": "该论文介绍了UltraGen，一种新型视频生成框架，它利用分层注意力机制有效地生成高分辨率（高达4K）视频，通过将注意力分解为全局和局部分支并采用空间压缩。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation",
        "summary": "Recent progress in text-to-image (T2I) generation underscores the importance\nof reliable benchmarks in evaluating how accurately generated images reflect\nthe semantics of their textual prompt. However, (1) existing benchmarks lack\nthe diversity of prompt scenarios and multilingual support, both essential for\nreal-world applicability; (2) they offer only coarse evaluations across primary\ndimensions, covering a narrow range of sub-dimensions, and fall short in\nfine-grained sub-dimension assessment. To address these limitations, we\nintroduce UniGenBench++, a unified semantic assessment benchmark for T2I\ngeneration. Specifically, it comprises 600 prompts organized hierarchically to\nensure both coverage and efficiency: (1) spans across diverse real-world\nscenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively\nprobes T2I models' semantic consistency over 10 primary and 27 sub evaluation\ncriteria, with each prompt assessing multiple testpoints. To rigorously assess\nmodel robustness to variations in language and prompt length, we provide both\nEnglish and Chinese versions of each prompt in short and long forms. Leveraging\nthe general world knowledge and fine-grained image understanding capabilities\nof a closed-source Multi-modal Large Language Model (MLLM), i.e.,\nGemini-2.5-Pro, an effective pipeline is developed for reliable benchmark\nconstruction and streamlined model assessment. Moreover, to further facilitate\ncommunity use, we train a robust evaluation model that enables offline\nassessment of T2I model outputs. Through comprehensive benchmarking of both\nopen- and closed-sourced T2I models, we systematically reveal their strengths\nand weaknesses across various aspects.",
        "url": "http://arxiv.org/abs/2510.18701v1",
        "published_date": "2025-10-21T14:56:46+00:00",
        "updated_date": "2025-10-21T14:56:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yibin Wang",
            "Zhimin Li",
            "Yuhang Zang",
            "Jiazi Bu",
            "Yujie Zhou",
            "Yi Xin",
            "Junjun He",
            "Chunyu Wang",
            "Qinglin Lu",
            "Cheng Jin",
            "Jiaqi Wang"
        ],
        "tldr": "The paper introduces UniGenBench++, a new benchmark for evaluating text-to-image generation models with improved diversity, multilingual support, fine-grained evaluation criteria, and a pipeline for streamlined model assessment using a multi-modal large language model.",
        "tldr_zh": "本文介绍了UniGenBench++，这是一个新的文本到图像生成模型评估基准，具有更高的多样性、多语言支持、细粒度评估标准，以及使用多模态大型语言模型进行简化模型评估的管道。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models",
        "summary": "The performance of Latent Diffusion Models (LDMs) is critically dependent on\nthe quality of their visual tokenizer. While recent works have explored\nincorporating Vision Foundation Models (VFMs) via distillation, we identify a\nfundamental flaw in this approach: it inevitably weakens the robustness of\nalignment with the original VFM, causing the aligned latents to deviate\nsemantically under distribution shifts. In this paper, we bypass distillation\nby proposing a more direct approach: Vision Foundation Model Variational\nAutoencoder (VFM-VAE). To resolve the inherent tension between the VFM's\nsemantic focus and the need for pixel-level fidelity, we redesign the VFM-VAE\ndecoder with Multi-Scale Latent Fusion and Progressive Resolution\nReconstruction blocks, enabling high-quality reconstruction from spatially\ncoarse VFM features. Furthermore, we provide a comprehensive analysis of\nrepresentation dynamics during diffusion training, introducing the proposed\nSE-CKNNA metric as a more precise tool for this diagnosis. This analysis allows\nus to develop a joint tokenizer-diffusion alignment strategy that dramatically\naccelerates convergence. Our innovations in tokenizer design and training\nstrategy lead to superior performance and efficiency: our system reaches a gFID\n(w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers).\nWith continued training to 640 epochs, it further attains a gFID (w/o CFG) of\n1.62, establishing direct VFM integration as a superior paradigm for LDMs.",
        "url": "http://arxiv.org/abs/2510.18457v1",
        "published_date": "2025-10-21T09:30:45+00:00",
        "updated_date": "2025-10-21T09:30:45+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Tianci Bi",
            "Xiaoyi Zhang",
            "Yan Lu",
            "Nanning Zheng"
        ],
        "tldr": "This paper proposes a novel Vision Foundation Model Variational Autoencoder (VFM-VAE) for Latent Diffusion Models (LDMs), bypassing distillation to improve performance and efficiency. They achieve a significant speedup and improved gFID scores compared to prior tokenizers.",
        "tldr_zh": "本文提出了一种新的视觉基础模型变分自编码器（VFM-VAE）用于潜在扩散模型（LDM），绕过了蒸馏过程，从而提高了性能和效率。与之前的tokenizer相比，他们实现了显著的加速和改进的gFID分数。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image Generation",
        "summary": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware.",
        "url": "http://arxiv.org/abs/2510.18716v1",
        "published_date": "2025-10-21T15:17:37+00:00",
        "updated_date": "2025-10-21T15:17:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siyong Jian",
            "Huan Wang"
        ],
        "tldr": "This paper introduces a KV cache compression framework for autoregressive image generation by decoupling attention heads into spatial-locality and semantic-sink types, achieving significant memory reduction and speedup with minimal quality loss.",
        "tldr_zh": "这篇论文提出了一种KV缓存压缩框架，用于自回归图像生成，通过将注意力头解耦为空间局部性和语义汇聚类型，实现了显著的内存减少和速度提升，且视觉质量损失极小。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation",
        "summary": "Long video generation with Diffusion Transformers (DiTs) is bottlenecked by\nthe quadratic scaling of full attention with sequence length. Since attention\nis highly redundant, outputs are dominated by a small subset of query-key\npairs. Existing sparse methods rely on blockwise coarse estimation, whose\naccuracy-efficiency trade-offs are constrained by block size. This paper\nintroduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention\nthat uses a lightweight, learnable token router to precisely match tokens\nwithout blockwise estimation. Through semantic-aware routing, MoGA enables\neffective long-range interactions. As a kernel-free method, MoGA integrates\nseamlessly with modern attention stacks, including FlashAttention and sequence\nparallelism. Building on MoGA, we develop an efficient long video generation\nmodel that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps,\nwith a context length of approximately 580k. Comprehensive experiments on\nvarious video generation tasks validate the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2510.18692v1",
        "published_date": "2025-10-21T14:50:42+00:00",
        "updated_date": "2025-10-21T14:50:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weinan Jia",
            "Yuning Lu",
            "Mengqi Huang",
            "Hualiang Wang",
            "Binyuan Huang",
            "Nan Chen",
            "Mu Liu",
            "Jidong Jiang",
            "Zhendong Mao"
        ],
        "tldr": "The paper introduces Mixture-of-Groups Attention (MoGA), a novel sparse attention mechanism that addresses the computational bottleneck in long video generation with Diffusion Transformers by using a learnable token router for efficient and precise token matching, achieving minute-level video generation at 24 fps.",
        "tldr_zh": "本文介绍了一种名为混合组注意力（MoGA）的新型稀疏注意力机制，它通过使用可学习的令牌路由器进行高效精准的令牌匹配，解决了扩散Transformer长视频生成中的计算瓶颈，实现了24 fps的分钟级视频生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model",
        "summary": "We present Kaleido, a subject-to-video~(S2V) generation framework, which aims\nto synthesize subject-consistent videos conditioned on multiple reference\nimages of target subjects. Despite recent progress in S2V generation models,\nexisting approaches remain inadequate at maintaining multi-subject consistency\nand at handling background disentanglement, often resulting in lower reference\nfidelity and semantic drift under multi-image conditioning. These shortcomings\ncan be attributed to several factors. Primarily, the training dataset suffers\nfrom a lack of diversity and high-quality samples, as well as cross-paired\ndata, i.e., paired samples whose components originate from different instances.\nIn addition, the current mechanism for integrating multiple reference images is\nsuboptimal, potentially resulting in the confusion of multiple subjects. To\novercome these limitations, we propose a dedicated data construction pipeline,\nincorporating low-quality sample filtering and diverse data synthesis, to\nproduce consistency-preserving training data. Moreover, we introduce Reference\nRotary Positional Encoding (R-RoPE) to process reference images, enabling\nstable and precise multi-image integration. Extensive experiments across\nnumerous benchmarks demonstrate that Kaleido significantly outperforms previous\nmethods in consistency, fidelity, and generalization, marking an advance in S2V\ngeneration.",
        "url": "http://arxiv.org/abs/2510.18573v1",
        "published_date": "2025-10-21T12:28:14+00:00",
        "updated_date": "2025-10-21T12:28:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhenxing Zhang",
            "Jiayan Teng",
            "Zhuoyi Yang",
            "Tiankun Cao",
            "Cheng Wang",
            "Xiaotao Gu",
            "Jie Tang",
            "Dan Guo",
            "Meng Wang"
        ],
        "tldr": "Kaleido is a new subject-to-video generation framework that addresses multi-subject consistency and background disentanglement issues by improving training data and introducing Reference Rotary Positional Encoding.",
        "tldr_zh": "Kaleido是一个新的主题到视频生成框架，通过改进训练数据并引入参考旋转位置编码，解决了多主题一致性和背景解耦问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LAND: Lung and Nodule Diffusion for 3D Chest CT Synthesis with Anatomical Guidance",
        "summary": "This work introduces a new latent diffusion model to generate high-quality 3D\nchest CT scans conditioned on 3D anatomical masks. The method synthesizes\nvolumetric images of size 256x256x256 at 1 mm isotropic resolution using a\nsingle mid-range GPU, significantly lowering the computational cost compared to\nexisting approaches. The conditioning masks delineate lung and nodule regions,\nenabling precise control over the output anatomical features. Experimental\nresults demonstrate that conditioning solely on nodule masks leads to\nanatomically incorrect outputs, highlighting the importance of incorporating\nglobal lung structure for accurate conditional synthesis. The proposed approach\nsupports the generation of diverse CT volumes with and without lung nodules of\nvarying attributes, providing a valuable tool for training AI models or\nhealthcare professionals.",
        "url": "http://arxiv.org/abs/2510.18446v1",
        "published_date": "2025-10-21T09:20:22+00:00",
        "updated_date": "2025-10-21T09:20:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Anna Oliveras",
            "Roger Marí",
            "Rafael Redondo",
            "Oriol Guardià",
            "Ana Tost",
            "Bhalaji Nagarajan",
            "Carolina Migliorelli",
            "Vicent Ribas",
            "Petia Radeva"
        ],
        "tldr": "This paper introduces a latent diffusion model (LAND) for generating high-quality 3D chest CT scans conditioned on anatomical masks of lungs and nodules, allowing for controlled synthesis and reduced computational cost.",
        "tldr_zh": "本文介绍了一种名为LAND的潜在扩散模型，用于生成高质量的3D胸部CT扫描，该模型以肺和结节的解剖掩模为条件，从而实现可控合成并降低计算成本。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization",
        "summary": "We introduce ImageGem, a dataset for studying generative models that\nunderstand fine-grained individual preferences. We posit that a key challenge\nhindering the development of such a generative model is the lack of in-the-wild\nand fine-grained user preference annotations. Our dataset features real-world\ninteraction data from 57K users, who collectively have built 242K customized\nLoRAs, written 3M text prompts, and created 5M generated images. With user\npreference annotations from our dataset, we were able to train better\npreference alignment models. In addition, leveraging individual user\npreference, we investigated the performance of retrieval models and a\nvision-language model on personalized image retrieval and generative model\nrecommendation. Finally, we propose an end-to-end framework for editing\ncustomized diffusion models in a latent weight space to align with individual\nuser preferences. Our results demonstrate that the ImageGem dataset enables,\nfor the first time, a new paradigm for generative model personalization.",
        "url": "http://arxiv.org/abs/2510.18433v1",
        "published_date": "2025-10-21T09:08:01+00:00",
        "updated_date": "2025-10-21T09:08:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.IR"
        ],
        "authors": [
            "Yuanhe Guo",
            "Linxi Xie",
            "Zhuoran Chen",
            "Kangrui Yu",
            "Ryan Po",
            "Guandao Yang",
            "Gordon Wetztein",
            "Hongyi Wen"
        ],
        "tldr": "ImageGem is a new dataset with real-world user interaction data for training and evaluating personalized generative models, allowing for fine-grained preference alignment and enabling a new paradigm for generative model personalization.",
        "tldr_zh": "ImageGem是一个新的数据集，包含真实世界用户交互数据，用于训练和评估个性化生成模型，实现细粒度的偏好对齐，并为生成模型个性化开辟了新范式。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback",
        "summary": "Direct preference optimization (DPO) methods have shown strong potential in\naligning text-to-image diffusion models with human preferences by training on\npaired comparisons. These methods improve training stability by avoiding the\nREINFORCE algorithm but still struggle with challenges such as accurately\nestimating image probabilities due to the non-linear nature of the sigmoid\nfunction and the limited diversity of offline datasets. In this paper, we\nintroduce Diffusion Denoising Ranking Optimization (Diffusion-DRO), a new\npreference learning framework grounded in inverse reinforcement learning.\nDiffusion-DRO removes the dependency on a reward model by casting preference\nlearning as a ranking problem, thereby simplifying the training objective into\na denoising formulation and overcoming the non-linear estimation issues found\nin prior methods. Moreover, Diffusion-DRO uniquely integrates offline expert\ndemonstrations with online policy-generated negative samples, enabling it to\neffectively capture human preferences while addressing the limitations of\noffline data. Comprehensive experiments show that Diffusion-DRO delivers\nimproved generation quality across a range of challenging and unseen prompts,\noutperforming state-of-the-art baselines in both both quantitative metrics and\nuser studies. Our source code and pre-trained models are available at\nhttps://github.com/basiclab/DiffusionDRO.",
        "url": "http://arxiv.org/abs/2510.18353v1",
        "published_date": "2025-10-21T07:22:34+00:00",
        "updated_date": "2025-10-21T07:22:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yi-Lun Wu",
            "Bo-Kai Ruan",
            "Chiang Tseng",
            "Hong-Han Shuai"
        ],
        "tldr": "This paper introduces Diffusion-DRO, a ranking-based preference optimization method for aligning diffusion models with human preferences, addressing limitations of existing DPO methods by simplifying the objective into a denoising formulation and integrating offline and online data.",
        "tldr_zh": "本文介绍了 Diffusion-DRO，一种基于排序的偏好优化方法，用于使扩散模型与人类偏好对齐。它通过将目标简化为去噪公式并集成离线和在线数据，从而解决了现有 DPO 方法的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GPTFace: Generative Pre-training of Facial-Linguistic Transformer by Span Masking and Weakly Correlated Text-image Data",
        "summary": "Compared to the prosperity of pre-training models in natural image\nunderstanding, the research on large-scale pre-training models for facial\nknowledge learning is still limited. Current approaches mainly rely on manually\nassembled and annotated face datasets for training, but labeling such datasets\nis labor-intensive and the trained models have limited scalability beyond the\ntraining data. To address these limitations, we present a generative\npre-training model for facial knowledge learning that leverages large-scale\nweb-built data for training. We use texts and images containing human faces\ncrawled from the internet and conduct pre-training on self-supervised tasks,\nincluding masked image/language modeling (MILM) and image-text matching (ITM).\nDuring the generation stage, we further utilize the image-text matching loss to\npull the generation distribution towards the control signal for controllable\nimage/text generation. Experimental results demonstrate that our model achieves\ncomparable performance to state-of-the-art pre-training models for various\nfacial downstream tasks, such as attribution classification and expression\nrecognition. Furthermore, our approach is also applicable to a wide range of\nface editing tasks, including face attribute editing, expression manipulation,\nmask removal, and photo inpainting.",
        "url": "http://arxiv.org/abs/2510.18345v1",
        "published_date": "2025-10-21T06:55:44+00:00",
        "updated_date": "2025-10-21T06:55:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yudong Li",
            "Hao Li",
            "Xianxu Hou",
            "Linlin Shen"
        ],
        "tldr": "The paper introduces GPTFace, a generative pre-training model for facial knowledge learning using web-crawled image-text data and self-supervised tasks like masked image/language modeling and image-text matching, demonstrating strong performance in tasks such as facial attribute classification, expression recognition, and face editing.",
        "tldr_zh": "该论文介绍了 GPTFace，一种用于面部知识学习的生成式预训练模型，利用网络爬取的图像文本数据和自监督任务如掩码图像/语言建模和图像文本匹配，在面部属性分类、表情识别和面部编辑等任务中表现出色。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OmniNWM: Omniscient Driving Navigation World Models",
        "summary": "Autonomous driving world models are expected to work effectively across three\ncore dimensions: state, action, and reward. Existing models, however, are\ntypically restricted to limited state modalities, short video sequences,\nimprecise action control, and a lack of reward awareness. In this paper, we\nintroduce OmniNWM, an omniscient panoramic navigation world model that\naddresses all three dimensions within a unified framework. For state, OmniNWM\njointly generates panoramic videos of RGB, semantics, metric depth, and 3D\noccupancy. A flexible forcing strategy enables high-quality long-horizon\nauto-regressive generation. For action, we introduce a normalized panoramic\nPlucker ray-map representation that encodes input trajectories into pixel-level\nsignals, enabling highly precise and generalizable control over panoramic video\ngeneration. Regarding reward, we move beyond learning reward functions with\nexternal image-based models: instead, we leverage the generated 3D occupancy to\ndirectly define rule-based dense rewards for driving compliance and safety.\nExtensive experiments demonstrate that OmniNWM achieves state-of-the-art\nperformance in video generation, control accuracy, and long-horizon stability,\nwhile providing a reliable closed-loop evaluation framework through\noccupancy-grounded rewards. Project page is available at\nhttps://github.com/Arlo0o/OmniNWM.",
        "url": "http://arxiv.org/abs/2510.18313v1",
        "published_date": "2025-10-21T05:49:01+00:00",
        "updated_date": "2025-10-21T05:49:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bohan Li",
            "Zhuang Ma",
            "Dalong Du",
            "Baorui Peng",
            "Zhujin Liang",
            "Zhenqiang Liu",
            "Chao Ma",
            "Yueming Jin",
            "Hao Zhao",
            "Wenjun Zeng",
            "Xin Jin"
        ],
        "tldr": "OmniNWM is a novel world model for autonomous driving that jointly generates panoramic videos of RGB, semantics, depth, and occupancy, offering precise action control and rule-based rewards for driving compliance and safety.",
        "tldr_zh": "OmniNWM 是一种新的自动驾驶世界模型，可联合生成 RGB、语义、深度和占据的全景视频，为驾驶合规性和安全性提供精确的动作控制和基于规则的奖励。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Chimera: Compositional Image Generation using Part-based Concepting",
        "summary": "Personalized image generative models are highly proficient at synthesizing\nimages from text or a single image, yet they lack explicit control for\ncomposing objects from specific parts of multiple source images without user\nspecified masks or annotations. To address this, we introduce Chimera, a\npersonalized image generation model that generates novel objects by combining\nspecified parts from different source images according to textual instructions.\nTo train our model, we first construct a dataset from a taxonomy built on 464\nunique (part, subject) pairs, which we term semantic atoms. From this, we\ngenerate 37k prompts and synthesize the corresponding images with a\nhigh-fidelity text-to-image model. We train a custom diffusion prior model with\npart-conditional guidance, which steers the image-conditioning features to\nenforce both semantic identity and spatial layout. We also introduce an\nobjective metric PartEval to assess the fidelity and compositional accuracy of\ngeneration pipelines. Human evaluations and our proposed metric show that\nChimera outperforms other baselines by 14% in part alignment and compositional\naccuracy and 21% in visual quality.",
        "url": "http://arxiv.org/abs/2510.18083v1",
        "published_date": "2025-10-20T20:20:47+00:00",
        "updated_date": "2025-10-20T20:20:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shivam Singh",
            "Yiming Chen",
            "Agneet Chatterjee",
            "Amit Raj",
            "James Hays",
            "Yezhou Yang",
            "Chitra Baral"
        ],
        "tldr": "Chimera generates novel images by composing specific parts from multiple source images based on textual instructions, using a custom diffusion prior and a new evaluation metric.",
        "tldr_zh": "Chimera通过文本指令，从多个源图像中组合特定部分来生成新的图像。该方法使用自定义的扩散先验模型和一个新的评估指标。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Few-shot Identity Preserving Attribute Editing for 3D-aware Deep Generative Models",
        "summary": "Identity preserving editing of faces is a generative task that enables\nmodifying the illumination, adding/removing eyeglasses, face aging, editing\nhairstyles, modifying expression etc., while preserving the identity of the\nface. Recent progress in 2D generative models have enabled photorealistic\nediting of faces using simple techniques leveraging the compositionality in\nGANs. However, identity preserving editing for 3D faces with a given set of\nattributes is a challenging task as the generative model must reason about view\nconsistency from multiple poses and render a realistic 3D face. Further, 3D\nportrait editing requires large-scale attribute labelled datasets and presents\na trade-off between editability in low-resolution and inflexibility to editing\nin high resolution. In this work, we aim to alleviate some of the constraints\nin editing 3D faces by identifying latent space directions that correspond to\nphotorealistic edits. To address this, we present a method that builds on\nrecent advancements in 3D-aware deep generative models and 2D portrait editing\ntechniques to perform efficient few-shot identity preserving attribute editing\nfor 3D-aware generative models. We aim to show from experimental results that\nusing just ten or fewer labelled images of an attribute is sufficient to\nestimate edit directions in the latent space that correspond to 3D-aware\nattribute editing. In this work, we leverage an existing face dataset with\nmasks to obtain the synthetic images for few attribute examples required for\nestimating the edit directions. Further, to demonstrate the linearity of edits,\nwe investigate one-shot stylization by performing sequential editing and use\nthe (2D) Attribute Style Manipulation (ASM) technique to investigate a\ncontinuous style manifold for 3D consistent identity preserving face aging.\nCode and results are available at: https://vishal-vinod.github.io/gmpi-edit/",
        "url": "http://arxiv.org/abs/2510.18287v1",
        "published_date": "2025-10-21T04:27:46+00:00",
        "updated_date": "2025-10-21T04:27:46+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Vishal Vinod"
        ],
        "tldr": "This paper presents a method for efficient few-shot identity-preserving attribute editing of 3D-aware deep generative models, requiring as few as ten labeled images per attribute.",
        "tldr_zh": "本文提出了一种高效的少样本身份保持属性编辑方法，用于3D感知深度生成模型，每个属性仅需十个或更少的标记图像。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Beyond Frequency: Scoring-Driven Debiasing for Object Detection via Blueprint-Prompted Image Synthesis",
        "summary": "This paper presents a generation-based debiasing framework for object\ndetection. Prior debiasing methods are often limited by the representation\ndiversity of samples, while naive generative augmentation often preserves the\nbiases it aims to solve. Moreover, our analysis reveals that simply generating\nmore data for rare classes is suboptimal due to two core issues: i) instance\nfrequency is an incomplete proxy for the true data needs of a model, and ii)\ncurrent layout-to-image synthesis lacks the fidelity and control to generate\nhigh-quality, complex scenes. To overcome this, we introduce the representation\nscore (RS) to diagnose representational gaps beyond mere frequency, guiding the\ncreation of new, unbiased layouts. To ensure high-quality synthesis, we replace\nambiguous text prompts with a precise visual blueprint and employ a generative\nalignment strategy, which fosters communication between the detector and\ngenerator. Our method significantly narrows the performance gap for\nunderrepresented object groups, \\eg, improving large/rare instances by 4.4/3.6\nmAP over the baseline, and surpassing prior L2I synthesis models by 15.9 mAP\nfor layout accuracy in generated images.",
        "url": "http://arxiv.org/abs/2510.18229v1",
        "published_date": "2025-10-21T02:19:12+00:00",
        "updated_date": "2025-10-21T02:19:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinhao Cai",
            "Liulei Li",
            "Gensheng Pei",
            "Tao Chen",
            "Jinshan Pan",
            "Yazhou Yao",
            "Wenguan Wang"
        ],
        "tldr": "This paper introduces a generation-based debiasing framework for object detection that uses a representation score (RS) to guide the creation of unbiased layouts and blueprint-prompted image synthesis for high-quality image generation, improving performance for underrepresented object groups.",
        "tldr_zh": "本文介绍了一种基于生成的物体检测去偏框架，该框架使用表征分数（RS）来指导创建无偏布局，并利用基于蓝图提示的图像合成来实现高质量图像生成，从而提高了代表性不足的物体组的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Demystifying Transition Matching: When and Why It Can Beat Flow Matching",
        "summary": "Flow Matching (FM) underpins many state-of-the-art generative models, yet\nrecent results indicate that Transition Matching (TM) can achieve higher\nquality with fewer sampling steps. This work answers the question of when and\nwhy TM outperforms FM. First, when the target is a unimodal Gaussian\ndistribution, we prove that TM attains strictly lower KL divergence than FM for\nfinite number of steps. The improvement arises from stochastic difference\nlatent updates in TM, which preserve target covariance that deterministic FM\nunderestimates. We then characterize convergence rates, showing that TM\nachieves faster convergence than FM under a fixed compute budget, establishing\nits advantage in the unimodal Gaussian setting. Second, we extend the analysis\nto Gaussian mixtures and identify local-unimodality regimes in which the\nsampling dynamics approximate the unimodal case, where TM can outperform FM.\nThe approximation error decreases as the minimal distance between component\nmeans increases, highlighting that TM is favored when the modes are well\nseparated. However, when the target variance approaches zero, each TM update\nconverges to the FM update, and the performance advantage of TM diminishes. In\nsummary, we show that TM outperforms FM when the target distribution has\nwell-separated modes and non-negligible variances. We validate our theoretical\nresults with controlled experiments on Gaussian distributions, and extend the\ncomparison to real-world applications in image and video generation.",
        "url": "http://arxiv.org/abs/2510.17991v1",
        "published_date": "2025-10-20T18:11:29+00:00",
        "updated_date": "2025-10-20T18:11:29+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Jaihoon Kim",
            "Rajarshi Saha",
            "Minhyuk Sung",
            "Youngsuk Park"
        ],
        "tldr": "This paper theoretically and empirically analyzes when Transition Matching (TM) outperforms Flow Matching (FM) in generative models, showing TM's advantage with well-separated modes and non-negligible variance.",
        "tldr_zh": "本文从理论和实验角度分析了生成模型中 Transition Matching (TM) 在何时优于 Flow Matching (FM)，表明 TM 在具有良好分离的模式和不可忽略的方差时具有优势。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]