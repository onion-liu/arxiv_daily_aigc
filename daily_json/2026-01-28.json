[
    {
        "title": "Entropy-Guided k-Guard Sampling for Long-Horizon Autoregressive Video Generation",
        "summary": "Autoregressive (AR) architectures have achieved significant successes in LLMs, inspiring explorations for video generation. In LLMs, top-p/top-k sampling strategies work exceptionally well: language tokens have high semantic density and low redundancy, so a fixed size of token candidates already strikes a balance between semantic accuracy and generation diversity. In contrast, video tokens have low semantic density and high spatio-temporal redundancy. This mismatch makes static top-k/top-p strategies ineffective for video decoders: they either introduce unnecessary randomness for low-uncertainty regions (static backgrounds) or get stuck in early errors for high-uncertainty regions (foreground objects). Prediction errors will accumulate as more frames are generated and eventually severely degrade long-horizon quality. To address this, we propose Entropy-Guided k-Guard (ENkG) sampling, a simple yet effective strategy that adapts sampling to token-wise dispersion, quantified by the entropy of each token's predicted distribution. ENkG uses adaptive token candidate sizes: for low-entropy regions, it employs fewer candidates to suppress redundant noise and preserve structural integrity; for high-entropy regions, it uses more candidates to mitigate error compounding. ENkG is model-agnostic, training-free, and adds negligible overhead. Experiments demonstrate consistent improvements in perceptual quality and structural stability compared to static top-k/top-p strategies.",
        "url": "http://arxiv.org/abs/2601.19488v1",
        "published_date": "2026-01-27T11:19:53+00:00",
        "updated_date": "2026-01-27T11:19:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yizhao Han",
            "Tianxing Shi",
            "Zhao Wang",
            "Zifan Xu",
            "Zhiyuan Pu",
            "Mingxiao Li",
            "Qian Zhang",
            "Wei Yin",
            "Xiao-Xiao Long"
        ],
        "tldr": "This paper introduces Entropy-Guided k-Guard (ENkG) sampling, an adaptive top-k sampling strategy for autoregressive video generation that adjusts the number of candidate tokens based on the entropy of the predicted distribution, improving long-horizon video quality.",
        "tldr_zh": "该论文介绍了熵引导的 k-Guard (ENkG) 采样，一种自适应的 top-k 采样策略，用于自回归视频生成，该策略基于预测分布的熵来调整候选 token 的数量，从而提高长程视频质量。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision",
        "summary": "Despite the significant advancements represented by Vision-Language Models (VLMs), current architectures often exhibit limitations in retaining fine-grained visual information, leading to coarse-grained multimodal comprehension. We attribute this deficiency to a suboptimal training paradigm inherent in prevailing VLMs, which exhibits a text-dominant optimization bias by conceptualizing visual signals merely as passive conditional inputs rather than supervisory targets. To mitigate this, we introduce Youtu-VL, a framework leveraging the Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm, which fundamentally shifts the optimization objective from ``vision-as-input'' to ``vision-as-target.'' By integrating visual tokens directly into the prediction stream, Youtu-VL applies unified autoregressive supervision to both visual details and linguistic content. Furthermore, we extend this paradigm to encompass vision-centric tasks, enabling a standard VLM to perform vision-centric tasks without task-specific additions. Extensive empirical evaluations demonstrate that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, establishing a robust foundation for the development of comprehensive generalist visual agents.",
        "url": "http://arxiv.org/abs/2601.19798v1",
        "published_date": "2026-01-27T17:01:16+00:00",
        "updated_date": "2026-01-27T17:01:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhixiang Wei",
            "Yi Li",
            "Zhehan Kan",
            "Xinghua Jiang",
            "Zuwei Long",
            "Shifeng Liu",
            "Hongze Shen",
            "Wei Liu",
            "Xiaoyu Tan",
            "Haojia Lin",
            "Yubo Zhu",
            "Qianyu Li",
            "Di Yin",
            "Haoyu Cao",
            "Weibo Gu",
            "Xin Li",
            "Yinsong Liu",
            "Deqiang Jiang",
            "Xing Sun",
            "Yunsheng Wu",
            "Mingkong Tang",
            "Shuangyin Liu",
            "Lexiang Tang",
            "Haodong Lin",
            "Junru Lu",
            "Jiarui Qin",
            "Lingfeng Qiao",
            "Ruizhi Qiao",
            "Bo Ke",
            "Jianfeng He",
            "Ke Li",
            "Yangning Li",
            "Yunhang Shen",
            "Mengdan Zhang",
            "Peixian Chen",
            "Kun Yin",
            "Bing Liu",
            "Yunfei Wu",
            "Huang Chen",
            "Zhongpeng Cai",
            "Xiaotian Li"
        ],
        "tldr": "Youtu-VL introduces a Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm to improve VLMs by treating vision as a supervisory target, leading to enhanced multimodal comprehension and performance on both multimodal and vision-centric tasks.",
        "tldr_zh": "Youtu-VL 引入了一种视觉-语言统一自回归监督（VLUAS）范式，通过将视觉视为监督目标来改进视觉-语言模型（VLMs），从而增强多模态理解能力，并提高在多模态任务和以视觉为中心任务中的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance",
        "summary": "3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction.",
        "url": "http://arxiv.org/abs/2601.19785v1",
        "published_date": "2026-01-27T16:47:35+00:00",
        "updated_date": "2026-01-27T16:47:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haozhi Zhu",
            "Miaomiao Zhao",
            "Dingyao Liu",
            "Runze Tian",
            "Yan Zhang",
            "Jie Guo",
            "Fenggen Yu"
        ],
        "tldr": "GeoDiff3D is a self-supervised framework for 3D scene generation that uses coarse geometry and a geometry-constrained 2D diffusion model, achieving high-quality results with low computational cost and reduced dependence on labeled data.",
        "tldr_zh": "GeoDiff3D是一个自监督的3D场景生成框架，它利用粗略的几何形状和一个几何约束的2D扩散模型，以低计算成本和减少对标签数据的依赖来实现高质量的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining",
        "summary": "Recent advances in video-audio (V-A) understanding and generation have increasingly relied on joint V-A embeddings, which serve as the foundation for tasks such as cross-modal retrieval and generation. While prior methods like CAVP effectively model semantic and temporal correspondences between modalities using contrastive objectives, their performance remains suboptimal. A key limitation is the insufficient modeling of the dense, multi-scale nature of both video and audio signals, correspondences often span fine- to coarse-grained spatial-temporal structures, which are underutilized in existing frameworks. To this end, we propose GMS-CAVP, a novel framework that combines Multi-Scale Video-Audio Alignment and Multi-Scale Spatial-Temporal Diffusion-based pretraining objectives to enhance V-A correspondence modeling. First, GMS-CAVP introduces a multi-scale contrastive learning strategy that captures semantic and temporal relations across varying granularities. Second, we go beyond traditional contrastive learning by incorporating a diffusion-based generative objective, enabling modality translation and synthesis between video and audio. This unified discriminative-generative formulation facilitates deeper cross-modal understanding and paves the way for high-fidelity generation. Extensive experiments on VGGSound, AudioSet, and Panda70M demonstrate that GMS-CAVP outperforms previous methods in generation and retrieval.",
        "url": "http://arxiv.org/abs/2601.19606v1",
        "published_date": "2026-01-27T13:43:32+00:00",
        "updated_date": "2026-01-27T13:43:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Shentong Mo",
            "Zehua Chen",
            "Jun Zhu"
        ],
        "tldr": "The paper introduces GMS-CAVP, a novel framework utilizing multi-scale contrastive and diffusion-based generative pretraining for improved video-audio correspondence learning, demonstrating superior performance in generation and retrieval tasks.",
        "tldr_zh": "该论文介绍了GMS-CAVP，一种新颖的框架，采用多尺度对比和基于扩散的生成式预训练来改进视频-音频对应学习，并在生成和检索任务中表现出卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cortex-Grounded Diffusion Models for Brain Image Generation",
        "summary": "Synthetic neuroimaging data can mitigate critical limitations of real-world datasets, including the scarcity of rare phenotypes, domain shifts across scanners, and insufficient longitudinal coverage. However, existing generative models largely rely on weak conditioning signals, such as labels or text, which lack anatomical grounding and often produce biologically implausible outputs. To this end, we introduce Cor2Vox, a cortex-grounded generative framework for brain magnetic resonance image (MRI) synthesis that ties image generation to continuous structural priors of the cerebral cortex. It leverages high-resolution cortical surfaces to guide a 3D shape-to-image Brownian bridge diffusion process, enabling topologically faithful synthesis and precise control over underlying anatomies. To support the generation of new, realistic brain shapes, we developed a large-scale statistical shape model of cortical morphology derived from over 33,000 UK Biobank scans. We validated the fidelity of Cor2Vox based on traditional image quality metrics, advanced cortical surface reconstruction, and whole-brain segmentation quality, outperforming many baseline methods. Across three applications, namely (i) anatomically consistent synthesis, (ii) simulation of progressive gray matter atrophy, and (iii) harmonization of in-house frontotemporal dementia scans with public datasets, Cor2Vox preserved fine-grained cortical morphology at the sub-voxel level, exhibiting remarkable robustness to variations in cortical geometry and disease phenotype without retraining.",
        "url": "http://arxiv.org/abs/2601.19498v1",
        "published_date": "2026-01-27T11:34:43+00:00",
        "updated_date": "2026-01-27T11:34:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Fabian Bongratz",
            "Yitong Li",
            "Sama Elbaroudy",
            "Christian Wachinger"
        ],
        "tldr": "The paper introduces Cor2Vox, a cortex-grounded diffusion model for generating realistic brain MRI images with precise anatomical control using cortical surface priors and a large-scale statistical shape model.",
        "tldr_zh": "该论文介绍了Cor2Vox，一种基于皮层的扩散模型，利用皮层表面先验和大规模统计形状模型生成具有精确解剖结构的逼真脑部MRI图像。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
        "summary": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.",
        "url": "http://arxiv.org/abs/2601.19433v1",
        "published_date": "2026-01-27T10:10:55+00:00",
        "updated_date": "2026-01-27T10:10:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jisheng Chu",
            "Wenrui Li",
            "Rui Zhao",
            "Wangmeng Zuo",
            "Shifeng Chen",
            "Xiaopeng Fan"
        ],
        "tldr": "The paper introduces RoamScene3D, a novel framework for generating immersive 3D scenes from text by using a scene graph based on object relations and a motion-injected inpainting model to overcome limitations of existing 2D diffusion prior methods.",
        "tldr_zh": "该论文介绍了 RoamScene3D，一种通过基于物体关系的场景图和运动注入修复模型从文本生成沉浸式3D场景的新框架，克服了现有2D扩散先验方法的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FBSDiff++: Improved Frequency Band Substitution of Diffusion Features for Efficient and Highly Controllable Text-Driven Image-to-Image Translation",
        "summary": "With large-scale text-to-image (T2I) diffusion models achieving significant advancements in open-domain image creation, increasing attention has been focused on their natural extension to the realm of text-driven image-to-image (I2I) translation, where a source image acts as visual guidance to the generated image in addition to the textual guidance provided by the text prompt. We propose FBSDiff, a novel framework adapting off-the-shelf T2I diffusion model into the I2I paradigm from a fresh frequency-domain perspective. Through dynamic frequency band substitution of diffusion features, FBSDiff realizes versatile and highly controllable text-driven I2I in a plug-and-play manner (without need for model training, fine-tuning, or online optimization), allowing appearance-guided, layout-guided, and contour-guided I2I translation by progressively substituting low-frequency band, mid-frequency band, and high-frequency band of latent diffusion features, respectively. In addition, FBSDiff flexibly enables continuous control over I2I correlation intensity simply by tuning the bandwidth of the substituted frequency band. To further promote image translation efficiency, flexibility, and functionality, we propose FBSDiff++ which improves upon FBSDiff mainly in three aspects: (1) accelerate inference speed by a large margin (8.9$\\times$ speedup in inference) with refined model architecture; (2) improve the Frequency Band Substitution module to allow for input source images of arbitrary resolution and aspect ratio; (3) extend model functionality to enable localized image manipulation and style-specific content creation with only subtle adjustments to the core method. Extensive qualitative and quantitative experiments verify superiority of FBSDiff++ in I2I translation visual quality, efficiency, versatility, and controllability compared to related advanced approaches.",
        "url": "http://arxiv.org/abs/2601.19115v1",
        "published_date": "2026-01-27T02:39:20+00:00",
        "updated_date": "2026-01-27T02:39:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiang Gao",
            "Yunpeng Jia"
        ],
        "tldr": "The paper introduces FBSDiff++, an improved version of FBSDiff, for efficient and highly controllable text-driven image-to-image translation using frequency band substitution in diffusion models, achieving significant improvements in speed, flexibility, and functionality.",
        "tldr_zh": "该论文介绍了FBSDiff++，一个FBSDiff的改进版本，通过在扩散模型中使用频率带替换来实现高效且高度可控的文本驱动的图像到图像转换，在速度、灵活性和功能性方面取得了显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NuiWorld: Exploring a Scalable Framework for End-to-End Controllable World Generation",
        "summary": "World generation is a fundamental capability for applications like video games, simulation, and robotics. However, existing approaches face three main obstacles: controllability, scalability, and efficiency. End-to-end scene generation models have been limited by data scarcity. While object-centric generation approaches rely on fixed resolution representations, degrading fidelity for larger scenes. Training-free approaches, while flexible, are often slow and computationally expensive at inference time. We present NuiWorld, a framework that attempts to address these challenges. To overcome data scarcity, we propose a generative bootstrapping strategy that starts from a few input images. Leveraging recent 3D reconstruction and expandable scene generation techniques, we synthesize scenes of varying sizes and layouts, producing enough data to train an end-to-end model. Furthermore, our framework enables controllability through pseudo sketch labels, and demonstrates a degree of generalization to previously unseen sketches. Our approach represents scenes as a collection of variable scene chunks, which are compressed into a flattened vector-set representation. This significantly reduces the token length for large scenes, enabling consistent geometric fidelity across scenes sizes while improving training and inference efficiency.",
        "url": "http://arxiv.org/abs/2601.19048v1",
        "published_date": "2026-01-27T00:04:02+00:00",
        "updated_date": "2026-01-27T00:04:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Han-Hung Lee",
            "Cheng-Yu Yang",
            "Yu-Lun Liu",
            "Angel X. Chang"
        ],
        "tldr": "NuiWorld introduces a scalable and controllable world generation framework using generative bootstrapping from limited images, expandable scene generation, and a compressed vector representation for improved fidelity and efficiency.",
        "tldr_zh": "NuiWorld 提出了一个可扩展且可控的世界生成框架，该框架利用从有限图像的生成式引导启动、可扩展的场景生成以及压缩的向量表示，以提高保真度和效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiffStyle3D: Consistent 3D Gaussian Stylization via Attention Optimization",
        "summary": "3D style transfer enables the creation of visually expressive 3D content, enriching the visual appearance of 3D scenes and objects. However, existing VGG- and CLIP-based methods struggle to model multi-view consistency within the model itself, while diffusion-based approaches can capture such consistency but rely on denoising directions, leading to unstable training. To address these limitations, we propose DiffStyle3D, a novel diffusion-based paradigm for 3DGS style transfer that directly optimizes in the latent space. Specifically, we introduce an Attention-Aware Loss that performs style transfer by aligning style features in the self-attention space, while preserving original content through content feature alignment. Inspired by the geometric invariance of 3D stylization, we propose a Geometry-Guided Multi-View Consistency method that integrates geometric information into self-attention to enable cross-view correspondence modeling. Based on geometric information, we additionally construct a geometry-aware mask to prevent redundant optimization in overlapping regions across views, which further improves multi-view consistency. Extensive experiments show that DiffStyle3D outperforms state-of-the-art methods, achieving higher stylization quality and visual realism.",
        "url": "http://arxiv.org/abs/2601.19717v1",
        "published_date": "2026-01-27T15:41:11+00:00",
        "updated_date": "2026-01-27T15:41:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yitong Yang",
            "Xuexin Liu",
            "Yinglin Wang",
            "Jing Wang",
            "Hao Dou",
            "Changshuo Wang",
            "Shuting He"
        ],
        "tldr": "DiffStyle3D is a novel diffusion-based method for 3D Gaussian Splatting (3DGS) style transfer that uses attention optimization for improved multi-view consistency and stylization quality by aligning style features in the self-attention space and preserving content.",
        "tldr_zh": "DiffStyle3D是一种新颖的基于扩散的3D高斯风格迁移方法，它通过注意力优化来提高多视图一致性和风格化质量，通过在自注意力空间中对齐风格特征并保留内容来实现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SNR-Edit: Structure-Aware Noise Rectification for Inversion-Free Flow-Based Editing",
        "summary": "Inversion-free image editing using flow-based generative models challenges the prevailing inversion-based pipelines. However, existing approaches rely on fixed Gaussian noise to construct the source trajectory, leading to biased trajectory dynamics and causing structural degradation or quality loss. To address this, we introduce SNR-Edit, a training-free framework achieving faithful Latent Trajectory Correction via adaptive noise control. Mechanistically, SNR-Edit uses structure-aware noise rectification to inject segmentation constraints into the initial noise, anchoring the stochastic component of the source trajectory to the real image's implicit inversion position and reducing trajectory drift during source--target transport. This lightweight modification yields smoother latent trajectories and ensures high-fidelity structural preservation without requiring model tuning or inversion. Across SD3 and FLUX, evaluations on PIE-Bench and SNR-Bench show that SNR-Edit delivers performance on pixel-level metrics and VLM-based scoring, while adding only about 1s overhead per image.",
        "url": "http://arxiv.org/abs/2601.19180v1",
        "published_date": "2026-01-27T04:24:21+00:00",
        "updated_date": "2026-01-27T04:24:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lifan Jiang",
            "Boxi Wu",
            "Yuhang Pei",
            "Tianrun Wu",
            "Yongyuan Chen",
            "Yan Zhao",
            "Shiyu Yu",
            "Deng Cai"
        ],
        "tldr": "The paper introduces SNR-Edit, a training-free framework for inversion-free image editing that improves structural preservation by using structure-aware noise rectification to guide the latent trajectory, achieving better performance with minimal overhead.",
        "tldr_zh": "该论文介绍了一种名为SNR-Edit的免训练框架，用于无反演的图像编辑。该方法通过结构感知的噪声校正来引导潜在轨迹，从而提高结构保存能力，并以最小的开销实现更好的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Diffusion for De-Occlusion: Accessory-Aware Diffusion Inpainting for Robust Ear Biometric Recognition",
        "summary": "Ear occlusions (arising from the presence of ear accessories such as earrings and earphones) can negatively impact performance in ear-based biometric recognition systems, especially in unconstrained imaging circumstances. In this study, we assess the effectiveness of a diffusion-based ear inpainting technique as a pre-processing aid to mitigate the issues of ear accessory occlusions in transformer-based ear recognition systems. Given an input ear image and an automatically derived accessory mask, the inpainting model reconstructs clean and anatomically plausible ear regions by synthesizing missing pixels while preserving local geometric coherence along key ear structures, including the helix, antihelix, concha, and lobule. We evaluate the effectiveness of this pre-processing aid in transformer-based recognition systems for several vision transformer models and different patch sizes for a range of benchmark datasets. Experiments show that diffusion-based inpainting can be a useful pre-processing aid to alleviate ear accessory occlusions to improve overall recognition performance.",
        "url": "http://arxiv.org/abs/2601.19795v1",
        "published_date": "2026-01-27T16:55:35+00:00",
        "updated_date": "2026-01-27T16:55:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Deeksha Arun",
            "Kevin W. Bowyer",
            "Patrick Flynn"
        ],
        "tldr": "This paper explores using diffusion-based inpainting to remove ear accessory occlusions, improving the performance of transformer-based ear biometric recognition systems.",
        "tldr_zh": "本文探讨了使用基于扩散的图像修复技术来移除耳朵饰品遮挡，从而提高基于Transformer的耳部生物特征识别系统的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 5,
        "overall_priority_score": 5
    }
]