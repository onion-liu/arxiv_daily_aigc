[
    {
        "title": "Unified Personalized Understanding, Generating and Editing",
        "summary": "Unified large multimodal models (LMMs) have achieved remarkable progress in general-purpose multimodal understanding and generation. However, they still operate under a ``one-size-fits-all'' paradigm and struggle to model user-specific concepts (e.g., generate a photo of \\texttt{<maeve>}) in a consistent and controllable manner. Existing personalization methods typically rely on external retrieval, which is inefficient and poorly integrated into unified multimodal pipelines. Recent personalized unified models introduce learnable soft prompts to encode concept information, yet they either couple understanding and generation or depend on complex multi-stage training, leading to cross-task interference and ultimately to fuzzy or misaligned personalized knowledge. We present \\textbf{OmniPersona}, an end-to-end personalization framework for unified LMMs that, for the first time, integrates personalized understanding, generation, and image editing within a single architecture. OmniPersona introduces structurally decoupled concept tokens, allocating dedicated subspaces for different tasks to minimize interference, and incorporates an explicit knowledge replay mechanism that propagates personalized attribute knowledge across tasks, enabling consistent personalized behavior. To systematically evaluate unified personalization, we propose \\textbf{\\texttt{OmniPBench}}, extending the public UnifyBench concept set with personalized editing tasks and cross-task evaluation protocols integrating understanding, generation, and editing. Experimental results demonstrate that OmniPersona delivers competitive and robust performance across diverse personalization tasks. We hope OmniPersona will serve as a strong baseline and spur further research on controllable, unified personalization.",
        "url": "http://arxiv.org/abs/2601.06965v1",
        "published_date": "2026-01-11T15:46:34+00:00",
        "updated_date": "2026-01-11T15:46:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Zhong",
            "Tianwei Lin",
            "Ruike Zhu",
            "Yuqian Yuan",
            "Haoyu Zheng",
            "Liang Liang",
            "Wenqiao Zhang",
            "Feifei Shao",
            "Haoyuan Li",
            "Wanggui He",
            "Hao Jiang",
            "Yueting Zhuang"
        ],
        "tldr": "This paper introduces OmniPersona, a unified LMM personalization framework that integrates personalized understanding, generation, and image editing with decoupled concept tokens and knowledge replay, evaluated on a new benchmark. The framework aims to address the limitations of 'one-size-fits-all' LMMs in modeling user-specific concepts consistently and controllably.",
        "tldr_zh": "本文介绍了一种统一的 LMM 个性化框架 OmniPersona，它集成了个性化的理解、生成和图像编辑，通过解耦的概念令牌和知识重放机制，并在新的基准上进行了评估。该框架旨在解决“一刀切”的 LMM 在一致且可控地建模用户特定概念方面的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]