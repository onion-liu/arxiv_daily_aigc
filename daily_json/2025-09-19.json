[
    {
        "title": "AToken: A Unified Tokenizer for Vision",
        "summary": "We present AToken, the first unified visual tokenizer that achieves both\nhigh-fidelity reconstruction and semantic understanding across images, videos,\nand 3D assets. Unlike existing tokenizers that specialize in either\nreconstruction or understanding for single modalities, AToken encodes these\ndiverse visual inputs into a shared 4D latent space, unifying both tasks and\nmodalities in a single framework. Specifically, we introduce a pure transformer\narchitecture with 4D rotary position embeddings to process visual inputs of\narbitrary resolutions and temporal durations. To ensure stable training, we\nintroduce an adversarial-free training objective that combines perceptual and\nGram matrix losses, achieving state-of-the-art reconstruction quality. By\nemploying a progressive training curriculum, AToken gradually expands from\nsingle images, videos, and 3D, and supports both continuous and discrete latent\ntokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01\nrFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9%\nclassification accuracy for 3D. In downstream applications, AToken enables both\nvisual generation tasks (e.g., image generation with continuous and discrete\ntokens, text-to-video generation, image-to-3D synthesis) and understanding\ntasks (e.g., multimodal LLMs), achieving competitive performance across all\nbenchmarks. These results shed light on the next-generation multimodal AI\nsystems built upon unified visual tokenization.",
        "url": "http://arxiv.org/abs/2509.14476v1",
        "published_date": "2025-09-17T23:11:18+00:00",
        "updated_date": "2025-09-17T23:11:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Jiasen Lu",
            "Liangchen Song",
            "Mingze Xu",
            "Byeongjoo Ahn",
            "Yanjun Wang",
            "Chen Chen",
            "Afshin Dehghan",
            "Yinfei Yang"
        ],
        "tldr": "AToken introduces a unified visual tokenizer that encodes images, videos, and 3D assets into a shared latent space, achieving state-of-the-art reconstruction and enabling various downstream generation and understanding tasks.",
        "tldr_zh": "AToken 提出了一个统一的视觉标记器，可以将图像、视频和 3D 资产编码到一个共享的潜在空间中，实现了最先进的重建效果，并能够支持各种下游生成和理解任务。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation",
        "summary": "Recent studies have demonstrated the importance of high-quality visual\nrepresentations in image generation and have highlighted the limitations of\ngenerative models in image understanding. As a generative paradigm originally\ndesigned for natural language, autoregressive models face similar challenges.\nIn this work, we present the first systematic investigation into the mechanisms\nof applying the next-token prediction paradigm to the visual domain. We\nidentify three key properties that hinder the learning of high-level visual\nsemantics: local and conditional dependence, inter-step semantic inconsistency,\nand spatial invariance deficiency. We show that these issues can be effectively\naddressed by introducing self-supervised objectives during training, leading to\na novel training framework, Self-guided Training for AutoRegressive models\n(ST-AR). Without relying on pre-trained representation models, ST-AR\nsignificantly enhances the image understanding ability of autoregressive models\nand leads to improved generation quality. Specifically, ST-AR brings\napproximately 42% FID improvement for LlamaGen-L and 49% FID improvement for\nLlamaGen-XL, while maintaining the same sampling strategy.",
        "url": "http://arxiv.org/abs/2509.15185v1",
        "published_date": "2025-09-18T17:47:40+00:00",
        "updated_date": "2025-09-18T17:47:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyu Yue",
            "Zidong Wang",
            "Yuqing Wang",
            "Wenlong Zhang",
            "Xihui Liu",
            "Wanli Ouyang",
            "Lei Bai",
            "Luping Zhou"
        ],
        "tldr": "This paper introduces a self-supervised training framework (ST-AR) to improve the image understanding ability and generation quality of autoregressive image generation models by addressing limitations in learning high-level visual semantics.",
        "tldr_zh": "本文介绍了一种自监督训练框架 (ST-AR)，通过解决自回归图像生成模型在学习高层视觉语义方面的局限性，来提高其图像理解能力和生成质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance",
        "summary": "Recent video diffusion models demonstrate strong potential in spatial\nintelligence tasks due to their rich latent world priors. However, this\npotential is hindered by their limited controllability and geometric\ninconsistency, creating a gap between their strong priors and their practical\nuse in 3D/4D tasks. As a result, current approaches often rely on retraining or\nfine-tuning, which risks degrading pretrained knowledge and incurs high\ncomputational costs. To address this, we propose WorldForge, a training-free,\ninference-time framework composed of three tightly coupled modules. Intra-Step\nRecursive Refinement introduces a recursive refinement mechanism during\ninference, which repeatedly optimizes network predictions within each denoising\nstep to enable precise trajectory injection. Flow-Gated Latent Fusion leverages\noptical flow similarity to decouple motion from appearance in the latent space\nand selectively inject trajectory guidance into motion-related channels.\nDual-Path Self-Corrective Guidance compares guided and unguided denoising paths\nto adaptively correct trajectory drift caused by noisy or misaligned structural\nsignals. Together, these components inject fine-grained, trajectory-aligned\nguidance without training, achieving both accurate motion control and\nphotorealistic content generation. Extensive experiments across diverse\nbenchmarks validate our method's superiority in realism, trajectory\nconsistency, and visual fidelity. This work introduces a novel plug-and-play\nparadigm for controllable video synthesis, offering a new perspective on\nleveraging generative priors for spatial intelligence.",
        "url": "http://arxiv.org/abs/2509.15130v1",
        "published_date": "2025-09-18T16:40:47+00:00",
        "updated_date": "2025-09-18T16:40:47+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Chenxi Song",
            "Yanming Yang",
            "Tong Zhao",
            "Ruibo Li",
            "Chi Zhang"
        ],
        "tldr": "The paper introduces WorldForge, a training-free framework that enhances the controllability and geometric consistency of video diffusion models for 3D/4D generation through recursive refinement, flow-gated latent fusion, and dual-path self-corrective guidance.",
        "tldr_zh": "该论文介绍了 WorldForge，一个无需训练的框架，通过递归细化、流门控潜在融合和双路径自校正引导，增强了视频扩散模型在 3D/4D 生成中的可控性和几何一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models",
        "summary": "Air pollution remains a critical threat to public health and environmental\nsustainability, yet conventional monitoring systems are often constrained by\nlimited spatial coverage and accessibility. This paper proposes an AI-driven\nagent that predicts ambient air pollution levels from sky images and\nsynthesizes realistic visualizations of pollution scenarios using generative\nmodeling. Our approach combines statistical texture analysis with supervised\nlearning for pollution classification, and leverages vision-language model\n(VLM)-guided image generation to produce interpretable representations of air\nquality conditions. The generated visuals simulate varying degrees of\npollution, offering a foundation for user-facing interfaces that improve\ntransparency and support informed environmental decision-making. These outputs\ncan be seamlessly integrated into intelligent applications aimed at enhancing\nsituational awareness and encouraging behavioral responses based on real-time\nforecasts. We validate our method using a dataset of urban sky images and\ndemonstrate its effectiveness in both pollution level estimation and\nsemantically consistent visual synthesis. The system design further\nincorporates human-centered user experience principles to ensure accessibility,\nclarity, and public engagement in air quality forecasting. To support scalable\nand energy-efficient deployment, future iterations will incorporate a green CNN\narchitecture enhanced with FPGA-based incremental learning, enabling real-time\ninference on edge platforms.",
        "url": "http://arxiv.org/abs/2509.15076v1",
        "published_date": "2025-09-18T15:36:38+00:00",
        "updated_date": "2025-09-18T15:36:38+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Mohammad Saleh Vahdatpour",
            "Maryam Eyvazi",
            "Yanqing Zhang"
        ],
        "tldr": "This paper presents an AI-driven approach to forecast air pollution levels from sky images using vision-language models, generating realistic visualizations of pollution scenarios for enhanced public awareness and decision-making. Future work will use a green CNN architecture for edge deployment.",
        "tldr_zh": "本文提出了一种基于AI的方法，使用视觉-语言模型从天空图像预测空气污染水平，并生成逼真的污染场景可视化效果，以增强公众意识和决策能力。未来的工作将使用绿色CNN架构进行边缘部署。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AutoEdit: Automatic Hyperparameter Tuning for Image Editing",
        "summary": "Recent advances in diffusion models have revolutionized text-guided image\nediting, yet existing editing methods face critical challenges in\nhyperparameter identification. To get the reasonable editing performance, these\nmethods often require the user to brute-force tune multiple interdependent\nhyperparameters, such as inversion timesteps and attention modification,\n\\textit{etc.} This process incurs high computational costs due to the huge\nhyperparameter search space. We consider searching optimal editing's\nhyperparameters as a sequential decision-making task within the diffusion\ndenoising process. Specifically, we propose a reinforcement learning framework,\nwhich establishes a Markov Decision Process that dynamically adjusts\nhyperparameters across denoising steps, integrating editing objectives into a\nreward function. The method achieves time efficiency through proximal policy\noptimization while maintaining optimal hyperparameter configurations.\nExperiments demonstrate significant reduction in search time and computational\noverhead compared to existing brute-force approaches, advancing the practical\ndeployment of a diffusion-based image editing framework in the real world.",
        "url": "http://arxiv.org/abs/2509.15031v1",
        "published_date": "2025-09-18T14:56:50+00:00",
        "updated_date": "2025-09-18T14:56:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chau Pham",
            "Quan Dao",
            "Mahesh Bhosale",
            "Yunjie Tian",
            "Dimitris Metaxas",
            "David Doermann"
        ],
        "tldr": "The paper proposes a reinforcement learning framework (AutoEdit) to automatically tune hyperparameters for text-guided image editing using diffusion models, significantly reducing search time and computational cost.",
        "tldr_zh": "该论文提出了一个强化学习框架 (AutoEdit)，用于自动调整文本引导的图像编辑中扩散模型的超参数，从而显著减少搜索时间和计算成本。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SPATIALGEN: Layout-guided 3D Indoor Scene Generation",
        "summary": "Creating high-fidelity 3D models of indoor environments is essential for\napplications in design, virtual reality, and robotics. However, manual 3D\nmodeling remains time-consuming and labor-intensive. While recent advances in\ngenerative AI have enabled automated scene synthesis, existing methods often\nface challenges in balancing visual quality, diversity, semantic consistency,\nand user control. A major bottleneck is the lack of a large-scale, high-quality\ndataset tailored to this task. To address this gap, we introduce a\ncomprehensive synthetic dataset, featuring 12,328 structured annotated scenes\nwith 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this\ndataset, we present SpatialGen, a novel multi-view multi-modal diffusion model\nthat generates realistic and semantically consistent 3D indoor scenes. Given a\n3D layout and a reference image (derived from a text prompt), our model\nsynthesizes appearance (color image), geometry (scene coordinate map), and\nsemantic (semantic segmentation map) from arbitrary viewpoints, while\npreserving spatial consistency across modalities. SpatialGen consistently\ngenerates superior results to previous methods in our experiments. We are\nopen-sourcing our data and models to empower the community and advance the\nfield of indoor scene understanding and generation.",
        "url": "http://arxiv.org/abs/2509.14981v1",
        "published_date": "2025-09-18T14:12:32+00:00",
        "updated_date": "2025-09-18T14:12:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chuan Fang",
            "Heng Li",
            "Yixun Liang",
            "Jia Zheng",
            "Yongsen Mao",
            "Yuan Liu",
            "Rui Tang",
            "Zihan Zhou",
            "Ping Tan"
        ],
        "tldr": "SpatialGen introduces a large-scale indoor scene dataset and a multi-view multi-modal diffusion model for generating realistic and semantically consistent 3D indoor scenes from layouts and reference images.",
        "tldr_zh": "SpatialGen 提出了一个大规模室内场景数据集和一个多视角多模态扩散模型，用于从布局和参考图像生成逼真且语义一致的 3D 室内场景。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model",
        "summary": "Text to image latent diffusion models have recently advanced medical image\nsynthesis, but applications to 3D CT generation remain limited. Existing\napproaches rely on simplified prompts, neglecting the rich semantic detail in\nfull radiology reports, which reduces text image alignment and clinical\nfidelity. We propose Report2CT, a radiology report conditional latent diffusion\nframework for synthesizing 3D chest CT volumes directly from free text\nradiology reports, incorporating both findings and impression sections using\nmultiple text encoder. Report2CT integrates three pretrained medical text\nencoders (BiomedVLP CXR BERT, MedEmbed, and ClinicalBERT) to capture nuanced\nclinical context. Radiology reports and voxel spacing information condition a\n3D latent diffusion model trained on 20000 CT volumes from the CT RATE dataset.\nModel performance was evaluated using Frechet Inception Distance (FID) for real\nsynthetic distributional similarity and CLIP based metrics for semantic\nalignment, with additional qualitative and quantitative comparisons against\nGenerateCT model. Report2CT generated anatomically consistent CT volumes with\nexcellent visual quality and text image alignment. Multi encoder conditioning\nimproved CLIP scores, indicating stronger preservation of fine grained clinical\ndetails in the free text radiology reports. Classifier free guidance further\nenhanced alignment with only a minor trade off in FID. We ranked first in the\nVLM3D Challenge at MICCAI 2025 on Text Conditional CT Generation and achieved\nstate of the art performance across all evaluation metrics. By leveraging\ncomplete radiology reports and multi encoder text conditioning, Report2CT\nadvances 3D CT synthesis, producing clinically faithful and high quality\nsynthetic data.",
        "url": "http://arxiv.org/abs/2509.14780v1",
        "published_date": "2025-09-18T09:32:23+00:00",
        "updated_date": "2025-09-18T09:32:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sina Amirrajab",
            "Zohaib Salahuddin",
            "Sheng Kuang",
            "Henry C. Woodruff",
            "Philippe Lambin"
        ],
        "tldr": "The paper introduces Report2CT, a novel latent diffusion framework that synthesizes 3D chest CT volumes from full radiology reports using multiple text encoders, achieving state-of-the-art performance in text-conditional CT generation.",
        "tldr_zh": "该论文介绍了Report2CT，一种新型的潜在扩散框架，它使用多个文本编码器从完整的放射学报告中合成3D胸部CT体积，在文本条件CT生成方面实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks",
        "summary": "Current instruction-based image editing (IBIE) methods struggle with\nchallenging editing tasks, as both editing types and sample counts of existing\ndatasets are limited. Moreover, traditional dataset construction often contains\nnoisy image-caption pairs, which may introduce biases and limit model\ncapabilities in complex editing scenarios. To address these limitations, we\nintroduce MultiEdit, a comprehensive dataset featuring over 107K high-quality\nimage editing samples. It encompasses 6 challenging editing tasks through a\ndiverse collection of 18 non-style-transfer editing types and 38 style transfer\noperations, covering a spectrum from sophisticated style transfer to complex\nsemantic operations like person reference editing and in-image text editing. We\nemploy a novel dataset construction pipeline that utilizes two multi-modal\nlarge language models (MLLMs) to generate visual-adaptive editing instructions\nand produce high-fidelity edited images, respectively. Extensive experiments\ndemonstrate that fine-tuning foundational open-source models with our\nMultiEdit-Train set substantially improves models' performance on sophisticated\nediting tasks in our proposed MultiEdit-Test benchmark, while effectively\npreserving their capabilities on the standard editing benchmark. We believe\nMultiEdit provides a valuable resource for advancing research into more diverse\nand challenging IBIE capabilities. Our dataset is available at\nhttps://huggingface.co/datasets/inclusionAI/MultiEdit.",
        "url": "http://arxiv.org/abs/2509.14638v1",
        "published_date": "2025-09-18T05:33:38+00:00",
        "updated_date": "2025-09-18T05:33:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingsong Li",
            "Lin Liu",
            "Hongjun Wang",
            "Haoxing Chen",
            "Xijun Gu",
            "Shizhan Liu",
            "Dong Gong",
            "Junbo Zhao",
            "Zhenzhong Lan",
            "Jianguo Li"
        ],
        "tldr": "The paper introduces MultiEdit, a new instruction-based image editing dataset designed to address the limitations of current datasets concerning task diversity, sample quality, and challenging editing scenarios. They use MLLMs to create high-quality image editing samples and demonstrate improved performance through fine-tuning on their dataset.",
        "tldr_zh": "该论文介绍了MultiEdit，一个新的基于指令的图像编辑数据集，旨在解决当前数据集在任务多样性、样本质量和具有挑战性的编辑场景方面的局限性。他们使用MLLM创建高质量的图像编辑样本，并通过在他们的数据集上进行微调来展示改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation",
        "summary": "In recent years, the underwater image formation model has found extensive use\nin the generation of synthetic underwater data. Although many approaches focus\non scenes primarily affected by discoloration, they often overlook the model's\nability to capture the complex, distance-dependent visibility loss present in\nhighly turbid environments. In this work, we propose an improved synthetic data\ngeneration pipeline that includes the commonly omitted forward scattering term,\nwhile also considering a nonuniform medium. Additionally, we collected the\nBUCKET dataset under controlled turbidity conditions to acquire real turbid\nfootage with the corresponding reference images. Our results demonstrate\nqualitative improvements over the reference model, particularly under\nincreasing turbidity, with a selection rate of 82. 5\\% by survey participants.\nData and code can be accessed on the project page:\nvap.aau.dk/sea-ing-through-scattered-rays.",
        "url": "http://arxiv.org/abs/2509.15011v1",
        "published_date": "2025-09-18T14:42:24+00:00",
        "updated_date": "2025-09-18T14:42:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Vasiliki Ismiroglou",
            "Malte Pedersen",
            "Stefan H. Bengtson",
            "Andreas Aakerberg",
            "Thomas B. Moeslund"
        ],
        "tldr": "This paper introduces an improved underwater image generation pipeline that incorporates forward scattering and non-uniform medium, validated by a newly collected dataset (BUCKET) and user surveys, showing qualitative improvements in high turbidity scenarios.",
        "tldr_zh": "本文介绍了一种改进的水下图像生成流程，加入了前向散射和非均匀介质因素，并通过新收集的数据集(BUCKET)和用户调查进行了验证，展示了在高浑浊度场景下的定性改进。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]