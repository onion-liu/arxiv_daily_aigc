[
    {
        "title": "Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals",
        "summary": "Distribution Matching Distillation (DMD) distills score-based generative\nmodels into efficient one-step generators, without requiring a one-to-one\ncorrespondence with the sampling trajectories of their teachers. However,\nlimited model capacity causes one-step distilled models underperform on complex\ngenerative tasks, e.g., synthesizing intricate object motions in text-to-video\ngeneration. Directly extending DMD to multi-step distillation increases memory\nusage and computational depth, leading to instability and reduced efficiency.\nWhile prior works propose stochastic gradient truncation as a potential\nsolution, we observe that it substantially reduces the generation diversity of\nmulti-step distilled models, bringing it down to the level of their one-step\ncounterparts. To address these limitations, we propose Phased DMD, a multi-step\ndistillation framework that bridges the idea of phase-wise distillation with\nMixture-of-Experts (MoE), reducing learning difficulty while enhancing model\ncapacity. Phased DMD is built upon two key ideas: progressive distribution\nmatching and score matching within subintervals. First, our model divides the\nSNR range into subintervals, progressively refining the model to higher SNR\nlevels, to better capture complex distributions. Next, to ensure the training\nobjective within each subinterval is accurate, we have conducted rigorous\nmathematical derivations. We validate Phased DMD by distilling state-of-the-art\nimage and video generation models, including Qwen-Image (20B parameters) and\nWan2.2 (28B parameters). Experimental results demonstrate that Phased DMD\npreserves output diversity better than DMD while retaining key generative\ncapabilities. We will release our code and models.",
        "url": "http://arxiv.org/abs/2510.27684v1",
        "published_date": "2025-10-31T17:55:10+00:00",
        "updated_date": "2025-10-31T17:55:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangyu Fan",
            "Zesong Qiu",
            "Zhuguanyu Wu",
            "Fanzhou Wang",
            "Zhiqian Lin",
            "Tianxiang Ren",
            "Dahua Lin",
            "Ruihao Gong",
            "Lei Yang"
        ],
        "tldr": "The paper introduces Phased DMD, a multi-step distillation framework for score-based generative models that improves diversity and efficiency by progressive distribution matching and score matching within subintervals, and demonstrates its effectiveness in image and video generation.",
        "tldr_zh": "该论文介绍了 Phased DMD，一种用于基于分数的生成模型的多步蒸馏框架，通过渐进式分布匹配和子区间内的分数匹配来提高多样性和效率，并展示了其在图像和视频生成中的有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Understanding the Implicit User Intention via Reasoning with Large Language Model for Image Editing",
        "summary": "Existing image editing methods can handle simple editing instructions very\nwell. To deal with complex editing instructions, they often need to jointly\nfine-tune the large language models (LLMs) and diffusion models (DMs), which\ninvolves very high computational complexity and training cost. To address this\nissue, we propose a new method, called \\textbf{C}omplex \\textbf{I}mage\n\\textbf{E}diting via \\textbf{L}LM \\textbf{R}easoning (CIELR), which converts a\ncomplex user instruction into a set of simple and explicit editing actions,\neliminating the need for jointly fine-tuning the large language models and\ndiffusion models. Specifically, we first construct a structured semantic\nrepresentation of the input image using foundation models. Then, we introduce\nan iterative update mechanism that can progressively refine this\nrepresentation, obtaining a fine-grained visual representation of the image\nscene. This allows us to perform complex and flexible image editing tasks.\nExtensive experiments on the SmartEdit Reasoning Scenario Set show that our\nmethod surpasses the previous state-of-the-art by 9.955 dB in PSNR, indicating\nits superior preservation of regions that should remain consistent. Due to the\nlimited number of samples of public datasets of complex image editing with\nreasoning, we construct a benchmark named CIEBench, containing 86 image\nsamples, together with a metric specifically for reasoning-based image editing.\nCIELR also outperforms previous methods on this benchmark. The code and dataset\nare available at\n\\href{https://github.com/Jia-shao/Reasoning-Editing}{https://github.com/Jia-shao/Reasoning-Editing}.",
        "url": "http://arxiv.org/abs/2510.27335v1",
        "published_date": "2025-10-31T10:06:28+00:00",
        "updated_date": "2025-10-31T10:06:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yijia Wang",
            "Yiqing Shen",
            "Weiming Chen",
            "Zhihai He"
        ],
        "tldr": "The paper introduces CIELR, a method for complex image editing that uses LLM reasoning to convert complex instructions into simple actions, avoiding joint fine-tuning of LLMs and diffusion models. It outperforms SOTA on a new reasoning-based image editing benchmark, CIEBench.",
        "tldr_zh": "本文提出了一种名为CIELR的复杂图像编辑方法，该方法利用大型语言模型（LLM）的推理能力，将复杂的指令转化为简单的编辑动作，从而避免了大型语言模型和扩散模型的联合微调。该方法在新构建的基于推理的图像编辑基准测试CIEBench上，性能优于现有技术。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance Acceleration of Generative Diffusion Models",
        "summary": "Diffusion models have emerged as state-of-the-art in image generation, but\ntheir practical deployment is hindered by the significant computational cost of\ntheir iterative denoising process. While existing caching techniques can\naccelerate inference, they often create a challenging trade-off between speed\nand fidelity, suffering from quality degradation and high computational\noverhead. To address these limitations, we introduce H2-Cache, a novel\nhierarchical caching mechanism designed for modern generative diffusion model\narchitectures. Our method is founded on the key insight that the denoising\nprocess can be functionally separated into a structure-defining stage and a\ndetail-refining stage. H2-cache leverages this by employing a dual-threshold\nsystem, using independent thresholds to selectively cache each stage. To ensure\nthe efficiency of our dual-check approach, we introduce pooled feature\nsummarization (PFS), a lightweight technique for robust and fast similarity\nestimation. Extensive experiments on the Flux architecture demonstrate that\nH2-cache achieves significant acceleration (up to 5.08x) while maintaining\nimage quality nearly identical to the baseline, quantitatively and\nqualitatively outperforming existing caching methods. Our work presents a\nrobust and practical solution that effectively resolves the speed-quality\ndilemma, significantly lowering the barrier for the real-world application of\nhigh-fidelity diffusion models. Source code is available at\nhttps://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.",
        "url": "http://arxiv.org/abs/2510.27171v1",
        "published_date": "2025-10-31T04:47:14+00:00",
        "updated_date": "2025-10-31T04:47:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mingyu Sung",
            "Il-Min Kim",
            "Sangseok Yun",
            "Jae-Mo Kang"
        ],
        "tldr": "The paper introduces H2-Cache, a hierarchical dual-stage caching mechanism with pooled feature summarization (PFS) to accelerate generative diffusion models while maintaining image quality, achieving up to 5.08x speedup on the Flux architecture.",
        "tldr_zh": "该论文介绍了H2-Cache，一种分层双阶段缓存机制，结合池化特征汇总（PFS），用于加速生成式扩散模型，同时保持图像质量，在Flux架构上实现了高达5.08倍的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "DANCER: Dance ANimation via Condition Enhancement and Rendering with diffusion model",
        "summary": "Recently, diffusion models have shown their impressive ability in visual\ngeneration tasks. Besides static images, more and more research attentions have\nbeen drawn to the generation of realistic videos. The video generation not only\nhas a higher requirement for the quality, but also brings a challenge in\nensuring the video continuity. Among all the video generation tasks,\nhuman-involved contents, such as human dancing, are even more difficult to\ngenerate due to the high degrees of freedom associated with human motions. In\nthis paper, we propose a novel framework, named as DANCER (Dance ANimation via\nCondition Enhancement and Rendering with Diffusion Model), for realistic\nsingle-person dance synthesis based on the most recent stable video diffusion\nmodel. As the video generation is generally guided by a reference image and a\nvideo sequence, we introduce two important modules into our framework to fully\nbenefit from the two inputs. More specifically, we design an Appearance\nEnhancement Module (AEM) to focus more on the details of the reference image\nduring the generation, and extend the motion guidance through a Pose Rendering\nModule (PRM) to capture pose conditions from extra domains. To further improve\nthe generation capability of our model, we also collect a large amount of video\ndata from Internet, and generate a novel datasetTikTok-3K to enhance the model\ntraining. The effectiveness of the proposed model has been evaluated through\nextensive experiments on real-world datasets, where the performance of our\nmodel is superior to that of the state-of-the-art methods. All the data and\ncodes will be released upon acceptance.",
        "url": "http://arxiv.org/abs/2510.27169v1",
        "published_date": "2025-10-31T04:42:08+00:00",
        "updated_date": "2025-10-31T04:42:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yucheng Xing",
            "Jinxing Yin",
            "Xiaodong Liu"
        ],
        "tldr": "The paper introduces DANCER, a diffusion-based framework for realistic single-person dance synthesis, utilizing appearance and pose enhancement modules and trained on a new large-scale dataset, demonstrating superior performance compared to state-of-the-art methods.",
        "tldr_zh": "该论文介绍了DANCER，一个基于扩散模型的逼真单人舞蹈合成框架，利用外观和姿势增强模块，并在一个新的大规模数据集上进行训练，展示了优于现有技术水平的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "HiGS: Hierarchical Generative Scene Framework for Multi-Step Associative Semantic Spatial Composition",
        "summary": "Three-dimensional scene generation holds significant potential in gaming,\nfilm, and virtual reality. However, most existing methods adopt a single-step\ngeneration process, making it difficult to balance scene complexity with\nminimal user input. Inspired by the human cognitive process in scene modeling,\nwhich progresses from global to local, focuses on key elements, and completes\nthe scene through semantic association, we propose HiGS, a hierarchical\ngenerative framework for multi-step associative semantic spatial composition.\nHiGS enables users to iteratively expand scenes by selecting key semantic\nobjects, offering fine-grained control over regions of interest while the model\ncompletes peripheral areas automatically. To support structured and coherent\ngeneration, we introduce the Progressive Hierarchical Spatial-Semantic Graph\n(PHiSSG), which dynamically organizes spatial relationships and semantic\ndependencies across the evolving scene structure. PHiSSG ensures spatial and\ngeometric consistency throughout the generation process by maintaining a\none-to-one mapping between graph nodes and generated objects and supporting\nrecursive layout optimization. Experiments demonstrate that HiGS outperforms\nsingle-stage methods in layout plausibility, style consistency, and user\npreference, offering a controllable and extensible paradigm for efficient 3D\nscene construction.",
        "url": "http://arxiv.org/abs/2510.27148v1",
        "published_date": "2025-10-31T03:50:47+00:00",
        "updated_date": "2025-10-31T03:50:47+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Jiacheng Hong",
            "Kunzhen Wu",
            "Mingrui Yu",
            "Yichao Gu",
            "Shengze Xue",
            "Shuangjiu Xiao",
            "Deli Dong"
        ],
        "tldr": "The paper introduces HiGS, a hierarchical generative framework for 3D scene creation that allows users to iteratively build scenes with fine-grained control and automatic peripheral completion, facilitated by the Progressive Hierarchical Spatial-Semantic Graph (PHiSSG).",
        "tldr_zh": "该论文介绍了一种用于3D场景创建的分层生成框架HiGS，它允许用户通过细粒度的控制和自动完成外围环境的方式迭代地构建场景，该框架由渐进式分层空间语义图（PHiSSG）提供支持。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "E-MMDiT: Revisiting Multimodal Diffusion Transformer Design for Fast Image Synthesis under Limited Resources",
        "summary": "Diffusion models have shown strong capabilities in generating high-quality\nimages from text prompts. However, these models often require large-scale\ntraining data and significant computational resources to train, or suffer from\nheavy structure with high latency. To this end, we propose Efficient Multimodal\nDiffusion Transformer (E-MMDiT), an efficient and lightweight multimodal\ndiffusion model with only 304M parameters for fast image synthesis requiring\nlow training resources. We provide an easily reproducible baseline with\ncompetitive results. Our model for 512px generation, trained with only 25M\npublic data in 1.5 days on a single node of 8 AMD MI300X GPUs, achieves 0.66 on\nGenEval and easily reaches to 0.72 with some post-training techniques such as\nGRPO. Our design philosophy centers on token reduction as the computational\ncost scales significantly with the token count. We adopt a highly compressive\nvisual tokenizer to produce a more compact representation and propose a novel\nmulti-path compression module for further compression of tokens. To enhance our\ndesign, we introduce Position Reinforcement, which strengthens positional\ninformation to maintain spatial coherence, and Alternating Subregion Attention\n(ASA), which performs attention within subregions to further reduce\ncomputational cost. In addition, we propose AdaLN-affine, an efficient\nlightweight module for computing modulation parameters in transformer blocks.\nOur code is available at https://github.com/AMD-AGI/Nitro-E and we hope E-MMDiT\nserves as a strong and practical baseline for future research and contributes\nto democratization of generative AI models.",
        "url": "http://arxiv.org/abs/2510.27135v1",
        "published_date": "2025-10-31T03:13:08+00:00",
        "updated_date": "2025-10-31T03:13:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tong Shen",
            "Jingai Yu",
            "Dong Zhou",
            "Dong Li",
            "Emad Barsoum"
        ],
        "tldr": "The paper introduces E-MMDiT, an efficient multimodal diffusion transformer for fast image synthesis using limited resources, achieving competitive results with a relatively small model trained on public data within a short timeframe.",
        "tldr_zh": "该论文介绍了E-MMDiT，一种高效的多模态扩散Transformer，用于在有限资源下快速图像合成，通过在一个短时间内使用公共数据训练的相对较小的模型实现了具有竞争力的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]