[
    {
        "title": "Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation",
        "summary": "Recent advances in video generation have enabled high-fidelity video\nsynthesis from user provided prompts. However, existing models and benchmarks\nfail to capture the complexity and requirements of professional video\ngeneration. Towards that goal, we introduce Stable Cinemetrics, a structured\nevaluation framework that formalizes filmmaking controls into four\ndisentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera.\nTogether, these taxonomies define 76 fine-grained control nodes grounded in\nindustry practices. Using these taxonomies, we construct a benchmark of prompts\naligned with professional use cases and develop an automated pipeline for\nprompt categorization and question generation, enabling independent evaluation\nof each control dimension. We conduct a large-scale human study spanning 10+\nmodels and 20K videos, annotated by a pool of 80+ film professionals. Our\nanalysis, both coarse and fine-grained reveal that even the strongest current\nmodels exhibit significant gaps, particularly in Events and Camera-related\ncontrols. To enable scalable evaluation, we train an automatic evaluator, a\nvision-language model aligned with expert annotations that outperforms existing\nzero-shot baselines. SCINE is the first approach to situate professional video\ngeneration within the landscape of video generative models, introducing\ntaxonomies centered around cinematic controls and supporting them with\nstructured evaluation pipelines and detailed analyses to guide future research.",
        "url": "http://arxiv.org/abs/2509.26555v1",
        "published_date": "2025-09-30T17:22:18+00:00",
        "updated_date": "2025-09-30T17:22:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Agneet Chatterjee",
            "Rahim Entezari",
            "Maksym Zhuravinskyi",
            "Maksim Lapin",
            "Reshinth Adithyan",
            "Amit Raj",
            "Chitta Baral",
            "Yezhou Yang",
            "Varun Jampani"
        ],
        "tldr": "The paper introduces Stable Cinemetrics, a structured evaluation framework for professional video generation, focusing on cinematic controls and benchmarked against existing models with human and automated evaluation pipelines, revealing significant gaps in current models.",
        "tldr_zh": "该论文介绍了Stable Cinemetrics，一个用于专业视频生成的结构化评估框架，重点关注电影制作控制，并针对现有模型进行了基准测试，使用人工和自动评估流程，揭示了当前模型中的重大缺陷。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "EchoGen: Generating Visual Echoes in Any Scene via Feed-Forward Subject-Driven Auto-Regressive Model",
        "summary": "Subject-driven generation is a critical task in creative AI; yet current\nstate-of-the-art methods present a stark trade-off. They either rely on\ncomputationally expensive, per-subject fine-tuning, sacrificing efficiency and\nzero-shot capability, or employ feed-forward architectures built on diffusion\nmodels, which are inherently plagued by slow inference speeds. Visual\nAuto-Regressive (VAR) models are renowned for their rapid sampling speeds and\nstrong generative quality, making them an ideal yet underexplored foundation\nfor resolving this tension. To bridge this gap, we introduce EchoGen, a\npioneering framework that empowers VAR models with subject-driven generation\ncapabilities. The core design of EchoGen is an effective dual-path injection\nstrategy that disentangles a subject's high-level semantic identity from its\nlow-level fine-grained details, enabling enhanced controllability and fidelity.\nWe employ a semantic encoder to extract the subject's abstract identity, which\nis injected through decoupled cross-attention to guide the overall composition.\nConcurrently, a content encoder captures intricate visual details, which are\nintegrated via a multi-modal attention mechanism to ensure high-fidelity\ntexture and structural preservation. To the best of our knowledge, EchoGen is\nthe first feed-forward subject-driven framework built upon VAR models. Both\nquantitative and qualitative results substantiate our design, demonstrating\nthat EchoGen achieves subject fidelity and image quality comparable to\nstate-of-the-art diffusion-based methods with significantly lower sampling\nlatency. Code and models will be released soon.",
        "url": "http://arxiv.org/abs/2509.26127v1",
        "published_date": "2025-09-30T11:45:48+00:00",
        "updated_date": "2025-09-30T11:45:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruixiao Dong",
            "Zhendong Wang",
            "Keli Liu",
            "Li Li",
            "Ying Chen",
            "Kai Li",
            "Daowen Li",
            "Houqiang Li"
        ],
        "tldr": "EchoGen introduces a novel feed-forward subject-driven image generation framework based on Visual Auto-Regressive models, achieving comparable quality to diffusion models but with significantly faster inference.",
        "tldr_zh": "EchoGen 提出了一种基于视觉自回归模型的新型前馈主题驱动图像生成框架，实现了与扩散模型相当的质量，但推理速度明显更快。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "EVODiff: Entropy-aware Variance Optimized Diffusion Inference",
        "summary": "Diffusion models (DMs) excel in image generation, but suffer from slow\ninference and the training-inference discrepancies. Although gradient-based\nsolvers like DPM-Solver accelerate the denoising inference, they lack\ntheoretical foundations in information transmission efficiency. In this work,\nwe introduce an information-theoretic perspective on the inference processes of\nDMs, revealing that successful denoising fundamentally reduces conditional\nentropy in reverse transitions. This principle leads to our key insights into\nthe inference processes: (1) data prediction parameterization outperforms its\nnoise counterpart, and (2) optimizing conditional variance offers a\nreference-free way to minimize both transition and reconstruction errors. Based\non these insights, we propose an entropy-aware variance optimized method for\nthe generative process of DMs, called EVODiff, which systematically reduces\nuncertainty by optimizing conditional entropy during denoising. Extensive\nexperiments on DMs validate our insights and demonstrate that our method\nsignificantly and consistently outperforms state-of-the-art (SOTA)\ngradient-based solvers. For example, compared to the DPM-Solver++, EVODiff\nreduces the reconstruction error by up to 45.5\\% (FID improves from 5.10 to\n2.78) at 10 function evaluations (NFE) on CIFAR-10, cuts the NFE cost by 25\\%\n(from 20 to 15 NFE) for high-quality samples on ImageNet-256, and improves\ntext-to-image generation while reducing artifacts. Code is available at\nhttps://github.com/ShiguiLi/EVODiff.",
        "url": "http://arxiv.org/abs/2509.26096v1",
        "published_date": "2025-09-30T11:15:07+00:00",
        "updated_date": "2025-09-30T11:15:07+00:00",
        "categories": [
            "cs.CV",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.OC",
            "stat.ML"
        ],
        "authors": [
            "Shigui Li",
            "Wei Chen",
            "Delu Zeng"
        ],
        "tldr": "The paper introduces EVODiff, a diffusion model inference method that optimizes conditional entropy during denoising, leading to significant performance improvements over state-of-the-art gradient-based solvers in image generation tasks.",
        "tldr_zh": "这篇论文介绍了EVODiff，一种扩散模型推理方法，通过优化去噪过程中的条件熵来显著提升图像生成任务中的性能，优于现有的基于梯度的求解器。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation",
        "summary": "Image-to-video generation has made remarkable progress with the advancements\nin diffusion models, yet generating videos with realistic motion remains highly\nchallenging. This difficulty arises from the complexity of accurately modeling\nmotion, which involves capturing physical constraints, object interactions, and\ndomain-specific dynamics that are not easily generalized across diverse\nscenarios. To address this, we propose MotionRAG, a retrieval-augmented\nframework that enhances motion realism by adapting motion priors from relevant\nreference videos through Context-Aware Motion Adaptation (CAMA). The key\ntechnical innovations include: (i) a retrieval-based pipeline extracting\nhigh-level motion features using video encoder and specialized resamplers to\ndistill semantic motion representations; (ii) an in-context learning approach\nfor motion adaptation implemented through a causal transformer architecture;\n(iii) an attention-based motion injection adapter that seamlessly integrates\ntransferred motion features into pretrained video diffusion models. Extensive\nexperiments demonstrate that our method achieves significant improvements\nacross multiple domains and various base models, all with negligible\ncomputational overhead during inference. Furthermore, our modular design\nenables zero-shot generalization to new domains by simply updating the\nretrieval database without retraining any components. This research enhances\nthe core capability of video generation systems by enabling the effective\nretrieval and transfer of motion priors, facilitating the synthesis of\nrealistic motion dynamics.",
        "url": "http://arxiv.org/abs/2509.26391v1",
        "published_date": "2025-09-30T15:26:04+00:00",
        "updated_date": "2025-09-30T15:26:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenhui Zhu",
            "Yilu Wu",
            "Shuai Wang",
            "Gangshan Wu",
            "Limin Wang"
        ],
        "tldr": "The paper introduces MotionRAG, a retrieval-augmented framework for image-to-video generation that enhances motion realism by retrieving and adapting motion priors from reference videos. It claims improvements in motion realism across domains with minimal overhead and zero-shot generalization capabilities.",
        "tldr_zh": "该论文介绍了一种名为MotionRAG的检索增强框架，用于image-to-video生成，通过检索和调整参考视频的运动先验来提高运动的真实感。它声称在多个领域内提高了运动的真实感，且开销极小，并具备零样本泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Go with Your Gut: Scaling Confidence for Autoregressive Image Generation",
        "summary": "Test-time scaling (TTS) has demonstrated remarkable success in enhancing\nlarge language models, yet its application to next-token prediction (NTP)\nautoregressive (AR) image generation remains largely uncharted. Existing TTS\napproaches for visual AR (VAR), which rely on frequent partial decoding and\nexternal reward models, are ill-suited for NTP-based image generation due to\nthe inherent incompleteness of intermediate decoding results. To bridge this\ngap, we introduce ScalingAR, the first TTS framework specifically designed for\nNTP-based AR image generation that eliminates the need for early decoding or\nauxiliary rewards. ScalingAR leverages token entropy as a novel signal in\nvisual token generation and operates at two complementary scaling levels: (i)\nProfile Level, which streams a calibrated confidence state by fusing intrinsic\nand conditional signals; and (ii) Policy Level, which utilizes this state to\nadaptively terminate low-confidence trajectories and dynamically schedule\nguidance for phase-appropriate conditioning strength. Experiments on both\ngeneral and compositional benchmarks show that ScalingAR (1) improves base\nmodels by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces\nvisual token consumption by 62.0% while outperforming baselines, and (3)\nsuccessfully enhances robustness, mitigating performance drops by 26.0% in\nchallenging scenarios.",
        "url": "http://arxiv.org/abs/2509.26376v1",
        "published_date": "2025-09-30T15:08:25+00:00",
        "updated_date": "2025-09-30T15:08:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Harold Haodong Chen",
            "Xianfeng Wu",
            "Wen-Jie Shu",
            "Rongjin Guo",
            "Disen Lan",
            "Harry Yang",
            "Ying-Cong Chen"
        ],
        "tldr": "The paper introduces ScalingAR, a novel test-time scaling framework for autoregressive image generation that uses token entropy to improve performance, reduce token consumption, and enhance robustness without relying on partial decoding or reward models.",
        "tldr_zh": "该论文介绍了ScalingAR，一种新颖的用于自回归图像生成的测试时缩放框架，它使用token熵来提高性能、减少token消耗并提升鲁棒性，且不依赖于部分解码或奖励模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing",
        "summary": "Recently, we have witnessed great progress in image editing with natural\nlanguage instructions. Several closed-source models like GPT-Image-1, Seedream,\nand Google-Nano-Banana have shown highly promising progress. However, the\nopen-source models are still lagging. The main bottleneck is the lack of a\nreliable reward model to scale up high-quality synthetic training data. To\naddress this critical bottleneck, we built \\mname, trained with our new\nlarge-scale human preference dataset, meticulously annotated by trained experts\nfollowing a rigorous protocol containing over 200K preference pairs. \\mname\ndemonstrates superior alignment with human preferences in instruction-guided\nimage editing tasks. Experiments show that \\mname achieves state-of-the-art\nhuman correlation on established benchmarks such as GenAI-Bench, AURORA-Bench,\nImagenHub, and our new \\benchname, outperforming a wide range of VLM-as-judge\nmodels. Furthermore, we use \\mname to select a high-quality subset from the\nexisting noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected\nsubset, which shows significant improvement over training on the full set. This\ndemonstrates \\mname's ability to serve as a reward model to scale up\nhigh-quality training data for image editing. Furthermore, its strong alignment\nsuggests potential for advanced applications like reinforcement learning-based\npost-training and test-time scaling of image editing models. \\mname with its\ntraining dataset will be released to help the community build more high-quality\nimage editing training datasets.",
        "url": "http://arxiv.org/abs/2509.26346v1",
        "published_date": "2025-09-30T14:51:04+00:00",
        "updated_date": "2025-09-30T14:51:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Keming Wu",
            "Sicong Jiang",
            "Max Ku",
            "Ping Nie",
            "Minghao Liu",
            "Wenhu Chen"
        ],
        "tldr": "The paper introduces EditReward, a human-aligned reward model for instruction-guided image editing, addressing the lack of reliable reward models for scaling up high-quality training data in open-source models.",
        "tldr_zh": "该论文介绍了 EditReward，一个针对指令引导的图像编辑的人类对齐奖励模型，解决了开源模型中缺乏可靠的奖励模型来扩大高质量训练数据的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Continual Expansion of Data Coverage: Automatic Text-guided Edge-case Synthesis",
        "summary": "The performance of deep neural networks is strongly influenced by the quality\nof their training data. However, mitigating dataset bias by manually curating\nchallenging edge cases remains a major bottleneck. To address this, we propose\nan automated pipeline for text-guided edge-case synthesis. Our approach employs\na Large Language Model, fine-tuned via preference learning, to rephrase image\ncaptions into diverse textual prompts that steer a Text-to-Image model toward\ngenerating difficult visual scenarios. Evaluated on the FishEye8K object\ndetection benchmark, our method achieves superior robustness, surpassing both\nnaive augmentation and manually engineered prompts. This work establishes a\nscalable framework that shifts data curation from manual effort to automated,\ntargeted synthesis, offering a promising direction for developing more reliable\nand continuously improving AI systems. Code is available at\nhttps://github.com/gokyeongryeol/ATES.",
        "url": "http://arxiv.org/abs/2509.26158v1",
        "published_date": "2025-09-30T12:11:25+00:00",
        "updated_date": "2025-09-30T12:11:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kyeongryeol Go"
        ],
        "tldr": "The paper introduces an automated pipeline that uses a fine-tuned Large Language Model to generate diverse textual prompts, guiding Text-to-Image models to synthesize challenging edge cases for improving dataset robustness and object detection.",
        "tldr_zh": "该论文介绍了一种自动流程，使用微调的大型语言模型生成多样化的文本提示，引导文本到图像模型合成具有挑战性的边缘案例，以提高数据集的鲁棒性和对象检测性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Text-to-Scene with Large Reasoning Models",
        "summary": "Prompt-driven scene synthesis allows users to generate complete 3D\nenvironments from textual descriptions. Current text-to-scene methods often\nstruggle with complex geometries and object transformations, and tend to show\nweak adherence to complex instructions. We address these limitations by\nintroducing Reason-3D, a text-to-scene model powered by large reasoning models\n(LRMs). Reason-3D integrates object retrieval using captions covering physical,\nfunctional, and contextual attributes. Reason-3D then places the selected\nobjects based on implicit and explicit layout constraints, and refines their\npositions with collision-aware spatial reasoning. Evaluated on instructions\nranging from simple to complex indoor configurations, Reason-3D significantly\noutperforms previous methods in human-rated visual fidelity, adherence to\nconstraints, and asset retrieval quality. Beyond its contribution to the field\nof text-to-scene generation, our work showcases the advanced spatial reasoning\nabilities of modern LRMs. Additionally, we release the codebase to further the\nresearch in object retrieval and placement with LRMs.",
        "url": "http://arxiv.org/abs/2509.26091v1",
        "published_date": "2025-09-30T11:08:11+00:00",
        "updated_date": "2025-09-30T11:08:11+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Frédéric Berdoz",
            "Luca A. Lanzendörfer",
            "Nick Tuninga",
            "Roger Wattenhofer"
        ],
        "tldr": "The paper introduces Reason-3D, a text-to-scene model leveraging large reasoning models for improved 3D environment generation from textual descriptions, achieving better visual fidelity and constraint adherence.",
        "tldr_zh": "该论文介绍了Reason-3D，一个文本到场景的模型，利用大型推理模型从文本描述中生成更好的3D环境，从而实现更好的视觉保真度和约束遵守。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VRWKV-Editor: Reducing quadratic complexity in transformer-based video editing",
        "summary": "In light of recent progress in video editing, deep learning models focusing\non both spatial and temporal dependencies have emerged as the primary method.\nHowever, these models suffer from the quadratic computational complexity of\ntraditional attention mechanisms, making them difficult to adapt to\nlong-duration and high-resolution videos. This limitation restricts their\napplicability in practical contexts such as real-time video processing. To\ntackle this challenge, we introduce a method to reduce both time and space\ncomplexity of these systems by proposing VRWKV-Editor, a novel video editing\nmodel that integrates a linear spatio-temporal aggregation module into\nvideo-based diffusion models. VRWKV-Editor leverages bidirectional weighted\nkey-value recurrence mechanism of the RWKV transformer to capture global\ndependencies while preserving temporal coherence, achieving linear complexity\nwithout sacrificing quality. Extensive experiments demonstrate that the\nproposed method achieves up to 3.7x speedup and 60% lower memory usage compared\nto state-of-the-art diffusion-based video editing methods, while maintaining\ncompetitive performance in frame consistency and text alignment. Furthermore, a\ncomparative analysis we conducted on videos with different sequence lengths\nconfirms that the gap in editing speed between our approach and architectures\nwith self-attention becomes more significant with long videos.",
        "url": "http://arxiv.org/abs/2509.25998v1",
        "published_date": "2025-09-30T09:30:23+00:00",
        "updated_date": "2025-09-30T09:30:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Abdelilah Aitrouga",
            "Youssef Hmamouche",
            "Amal El Fallah Seghrouchni"
        ],
        "tldr": "The paper introduces VRWKV-Editor, a video editing model that uses a linear spatio-temporal aggregation module based on the RWKV transformer to reduce computational complexity for long-duration, high-resolution videos, achieving significant speedups and reduced memory usage.",
        "tldr_zh": "该论文介绍了VRWKV-Editor，一种视频编辑模型，它使用基于RWKV Transformer的线性时空聚合模块来降低长时高分辨率视频的计算复杂度，从而显著提高速度并减少内存使用。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning",
        "summary": "The \"thinking with images\" paradigm represents a pivotal shift in the\nreasoning of Vision Language Models (VLMs), moving from text-dominant\nchain-of-thought to image-interactive reasoning. By invoking visual tools or\ngenerating intermediate visual representations, VLMs can iteratively attend to\nfine-grained regions, enabling deeper image understanding and more faithful\nmultimodal reasoning. As an emerging paradigm, however, it still leaves\nsubstantial room for exploration in data construction accuracy, structural\ndesign, and broader application scenarios, which offer rich opportunities for\nadvancing multimodal reasoning. To further advance this line of work, we\npresent DeepSketcher, a comprehensive suite comprising both an image-text\ninterleaved dataset and a self-contained model. The dataset contains 31k\nchain-of-thought (CoT) reasoning trajectories with diverse tool calls and\nresulting edited images, covering a wide range of data types and manipulation\ninstructions with high annotation accuracy. Building on this resource, we\ndesign a model that performs interleaved image-text reasoning and natively\ngenerates \"visual thoughts\" by operating directly in the visual embedding\nspace, rather than invoking external tools and repeatedly re-encoding generated\nimages. This design enables tool-free and more flexible \"thinking with images\".\nExtensive experiments on multimodal reasoning benchmarks demonstrate strong\nperformance, validating both the utility of the dataset and the effectiveness\nof the model design.",
        "url": "http://arxiv.org/abs/2509.25866v1",
        "published_date": "2025-09-30T07:02:01+00:00",
        "updated_date": "2025-09-30T07:02:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chi Zhang",
            "Haibo Qiu",
            "Qiming Zhang",
            "Zhixiong Zeng",
            "Lin Ma",
            "Jing Zhang"
        ],
        "tldr": "The paper introduces DeepSketcher, a dataset and model for multimodal reasoning that internalizes visual manipulation by generating \"visual thoughts\" in the visual embedding space, achieving strong performance on multimodal benchmarks.",
        "tldr_zh": "该论文介绍了 DeepSketcher，一个用于多模态推理的数据集和模型，通过在视觉嵌入空间中生成“视觉思想”来实现视觉操作的内部化，并在多模态基准测试中取得了优异的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vector sketch animation generation with differentialable motion trajectories",
        "summary": "Sketching is a direct and inexpensive means of visual expression. Though\nimage-based sketching has been well studied, video-based sketch animation\ngeneration is still very challenging due to the temporal coherence requirement.\nIn this paper, we propose a novel end-to-end automatic generation approach for\nvector sketch animation. To solve the flickering issue, we introduce a\nDifferentiable Motion Trajectory (DMT) representation that describes the\nframe-wise movement of stroke control points using differentiable\npolynomial-based trajectories. DMT enables global semantic gradient propagation\nacross multiple frames, significantly improving the semantic consistency and\ntemporal coherence, and producing high-framerate output. DMT employs a\nBernstein basis to balance the sensitivity of polynomial parameters, thus\nachieving more stable optimization. Instead of implicit fields, we introduce\nsparse track points for explicit spatial modeling, which improves efficiency\nand supports long-duration video processing. Evaluations on DAVIS and LVOS\ndatasets demonstrate the superiority of our approach over SOTA methods.\nCross-domain validation on 3D models and text-to-video data confirms the\nrobustness and compatibility of our approach.",
        "url": "http://arxiv.org/abs/2509.25857v1",
        "published_date": "2025-09-30T06:53:04+00:00",
        "updated_date": "2025-09-30T06:53:04+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Xinding Zhu",
            "Xinye Yang",
            "Shuyang Zheng",
            "Zhexin Zhang",
            "Fei Gao",
            "Jing Huang",
            "Jiazhou Chen"
        ],
        "tldr": "This paper introduces a novel end-to-end approach for generating vector sketch animations with improved temporal coherence and stability using Differentiable Motion Trajectories (DMT). The method leverages differentiable polynomial-based trajectories and sparse track points for efficient, long-duration video processing.",
        "tldr_zh": "本文提出了一种新的端到端方法，用于生成矢量草图动画，该方法使用可微运动轨迹（DMT）提高了时间连贯性和稳定性。该方法利用可微的基于多项式的轨迹和稀疏的跟踪点，以实现高效的长时间视频处理。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Editable Noise Map Inversion: Encoding Target-image into Noise For High-Fidelity Image Manipulation",
        "summary": "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality and diverse images. Building on these advancements, diffusion\nmodels have also demonstrated exceptional performance in text-guided image\nediting. A key strategy for effective image editing involves inverting the\nsource image into editable noise maps associated with the target image.\nHowever, previous inversion methods face challenges in adhering closely to the\ntarget text prompt. The limitation arises because inverted noise maps, while\nenabling faithful reconstruction of the source image, restrict the flexibility\nneeded for desired edits. To overcome this issue, we propose Editable Noise Map\nInversion (ENM Inversion), a novel inversion technique that searches for\noptimal noise maps to ensure both content preservation and editability. We\nanalyze the properties of noise maps for enhanced editability. Based on this\nanalysis, our method introduces an editable noise refinement that aligns with\nthe desired edits by minimizing the difference between the reconstructed and\nedited noise maps. Extensive experiments demonstrate that ENM Inversion\noutperforms existing approaches across a wide range of image editing tasks in\nboth preservation and edit fidelity with target prompts. Our approach can also\nbe easily applied to video editing, enabling temporal consistency and content\nmanipulation across frames.",
        "url": "http://arxiv.org/abs/2509.25776v1",
        "published_date": "2025-09-30T04:44:53+00:00",
        "updated_date": "2025-09-30T04:44:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mingyu Kang",
            "Yong Suk Choi"
        ],
        "tldr": "This paper introduces a novel noise map inversion technique, ENM Inversion, for improved text-guided image editing, addressing limitations in existing methods regarding target text prompt adherence by balancing content preservation and editability. It shows improvements in image and video editing.",
        "tldr_zh": "该论文提出了一种新型的噪声图反演技术 ENM Inversion，用于改进文本引导的图像编辑，通过平衡内容保留和可编辑性，解决了现有方法在目标文本提示依从性方面的局限性。它展示了在图像和视频编辑方面的改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models",
        "summary": "While reinforcement learning has advanced the alignment of text-to-image\n(T2I) models, state-of-the-art policy gradient methods are still hampered by\ntraining instability and high variance, hindering convergence speed and\ncompromising image quality. Our analysis identifies a key cause of this\ninstability: disproportionate credit assignment, in which the mathematical\nstructure of the generative sampler produces volatile and non-proportional\nfeedback across timesteps. To address this, we introduce Proportionate Credit\nPolicy Optimization (PCPO), a framework that enforces proportional credit\nassignment through a stable objective reformulation and a principled\nreweighting of timesteps. This correction stabilizes the training process,\nleading to significantly accelerated convergence and superior image quality.\nThe improvement in quality is a direct result of mitigating model collapse, a\ncommon failure mode in recursive training. PCPO substantially outperforms\nexisting policy gradient baselines on all fronts, including the\nstate-of-the-art DanceGRPO.",
        "url": "http://arxiv.org/abs/2509.25774v1",
        "published_date": "2025-09-30T04:43:58+00:00",
        "updated_date": "2025-09-30T04:43:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jeongjae Lee",
            "Jong Chul Ye"
        ],
        "tldr": "The paper introduces PCPO, a reinforcement learning framework that addresses the instability and high variance in aligning text-to-image models by enforcing proportionate credit assignment, leading to accelerated convergence and improved image quality.",
        "tldr_zh": "该论文介绍了一种名为PCPO的强化学习框架，通过强制执行比例信用分配来解决文本到图像模型对齐中的不稳定性和高方差问题，从而加速收敛并提高图像质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Importance Sampling for Multi-Negative Multimodal Direct Preference Optimization",
        "summary": "Direct Preference Optimization (DPO) has recently been extended from\ntext-only models to vision-language models. However, existing methods rely on\noversimplified pairwise comparisons, generating a single negative image via\nbasic perturbations or similarity-based retrieval, which fail to capture the\ncomplex nature of multimodal preferences, inducing optimization bias and\nhallucinations. To address this issue, we propose MISP-DPO, the first framework\nto incorporate multiple, semantically diverse negative images in multimodal DPO\nvia the Plackett-Luce model. Our method embeds prompts and candidate images in\nCLIP (Contrastive Language-Image Pretraining) space and applies a sparse\nautoencoder to uncover semantic deviations into interpretable factors. Negative\nsamples are selected based on reconstruction difficulty, semantic deviation\nfrom the positive, and mutual diversity, yielding broader and more informative\nsupervision. To handle multi-negative comparisons, we adopt a Plackett-Luce\nobjective and introduce an importance sampling strategy that improves training\nefficiency. Experiments across five diverse benchmarks demonstrate that\nMISP-DPO consistently improves multimodal alignment over prior methods,\nvalidating the effectiveness of semantic-aware, multi-negative sampling in\npreference-based learning.",
        "url": "http://arxiv.org/abs/2509.25717v1",
        "published_date": "2025-09-30T03:24:09+00:00",
        "updated_date": "2025-09-30T03:24:09+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Xintong Li",
            "Chuhan Wang",
            "Junda Wu",
            "Rohan Surana",
            "Tong Yu",
            "Julian McAuley",
            "Jingbo Shang"
        ],
        "tldr": "The paper introduces MISP-DPO, a novel framework for multimodal direct preference optimization that uses multiple semantically diverse negative images selected based on CLIP embeddings and a sparse autoencoder to improve training efficiency and multimodal alignment.",
        "tldr_zh": "该论文介绍了MISP-DPO，一种新颖的多模态直接偏好优化框架，它使用多个语义上不同的负样本图像，这些负样本基于CLIP嵌入和稀疏自动编码器选择，以提高训练效率和多模态对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IRIS: Intrinsic Reward Image Synthesis",
        "summary": "Despite the success of Reinforcement Learning from Human Feedback (RLHF) in\nlanguage reasoning, its application to autoregressive Text-to-Image (T2I)\ngeneration is often constrained by the limited availability of human preference\ndata. This paper explores how an autoregressive T2I model can learn from\ninternal signals without relying on external rewards or labeled data. Contrary\nto recent findings in text generation, we show that maximizing\nself-uncertainty, rather than self-certainty, improves image generation. We\nobserve that this is because autoregressive T2I models with low uncertainty\ntend to generate simple and uniform images, which are less aligned with human\npreferences. Based on these observations, we propose IRIS (Intrinsic Reward\nImage Synthesis), the first framework to improve autoregressive T2I models with\nreinforcement learning using only an intrinsic reward. Empirical results\ndemonstrate that applying IRIS to autoregressive T2I models achieves\nperformance that is competitive with or superior to external rewards.",
        "url": "http://arxiv.org/abs/2509.25562v1",
        "published_date": "2025-09-29T22:38:25+00:00",
        "updated_date": "2025-09-29T22:38:25+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yihang Chen",
            "Yuanhao Ban",
            "Yunqi Hong",
            "Cho-Jui Hsieh"
        ],
        "tldr": "The paper introduces IRIS, a reinforcement learning framework that improves autoregressive text-to-image models using only an intrinsic reward based on self-uncertainty, achieving performance comparable to or better than methods using external rewards.",
        "tldr_zh": "该论文介绍了IRIS，一个使用内在奖励的强化学习框架，通过基于自不确定性的内在奖励来改进自回归文本到图像模型，其性能与使用外部奖励的方法相当或更好。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiffCamera: Arbitrary Refocusing on Images",
        "summary": "The depth-of-field (DoF) effect, which introduces aesthetically pleasing\nblur, enhances photographic quality but is fixed and difficult to modify once\nthe image has been created. This becomes problematic when the applied blur is\nundesirable~(e.g., the subject is out of focus). To address this, we propose\nDiffCamera, a model that enables flexible refocusing of a created image\nconditioned on an arbitrary new focus point and a blur level. Specifically, we\ndesign a diffusion transformer framework for refocusing learning. However, the\ntraining requires pairs of data with different focus planes and bokeh levels in\nthe same scene, which are hard to acquire. To overcome this limitation, we\ndevelop a simulation-based pipeline to generate large-scale image pairs with\nvarying focus planes and bokeh levels. With the simulated data, we find that\ntraining with only a vanilla diffusion objective often leads to incorrect DoF\nbehaviors due to the complexity of the task. This requires a stronger\nconstraint during training. Inspired by the photographic principle that photos\nof different focus planes can be linearly blended into a multi-focus image, we\npropose a stacking constraint during training to enforce precise DoF\nmanipulation. This constraint enhances model training by imposing physically\ngrounded refocusing behavior that the focusing results should be faithfully\naligned with the scene structure and the camera conditions so that they can be\ncombined into the correct multi-focus image. We also construct a benchmark to\nevaluate the effectiveness of our refocusing model. Extensive experiments\ndemonstrate that DiffCamera supports stable refocusing across a wide range of\nscenes, providing unprecedented control over DoF adjustments for photography\nand generative AI applications.",
        "url": "http://arxiv.org/abs/2509.26599v1",
        "published_date": "2025-09-30T17:48:23+00:00",
        "updated_date": "2025-09-30T17:48:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiyang Wang",
            "Xi Chen",
            "Xiaogang Xu",
            "Yu Liu",
            "Hengshuang Zhao"
        ],
        "tldr": "The paper introduces DiffCamera, a diffusion transformer model, for arbitrary image refocusing by generating training data via simulation and enforcing a stacking constraint to improve depth-of-field manipulation.",
        "tldr_zh": "该论文介绍了DiffCamera，一个扩散transformer模型，通过模拟生成训练数据并强制执行堆叠约束来改善景深操作，从而实现任意图像的重新聚焦。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "3DiFACE: Synthesizing and Editing Holistic 3D Facial Animation",
        "summary": "Creating personalized 3D animations with precise control and realistic head\nmotions remains challenging for current speech-driven 3D facial animation\nmethods. Editing these animations is especially complex and time consuming,\nrequires precise control and typically handled by highly skilled animators.\nMost existing works focus on controlling style or emotion of the synthesized\nanimation and cannot edit/regenerate parts of an input animation. They also\noverlook the fact that multiple plausible lip and head movements can match the\nsame audio input. To address these challenges, we present 3DiFACE, a novel\nmethod for holistic speech-driven 3D facial animation. Our approach produces\ndiverse plausible lip and head motions for a single audio input and allows for\nediting via keyframing and interpolation. Specifically, we propose a\nfully-convolutional diffusion model that can leverage the viseme-level\ndiversity in our training corpus. Additionally, we employ a speaking-style\npersonalization and a novel sparsely-guided motion diffusion to enable precise\ncontrol and editing. Through quantitative and qualitative evaluations, we\ndemonstrate that our method is capable of generating and editing diverse\nholistic 3D facial animations given a single audio input, with control between\nhigh fidelity and diversity. Code and models are available here:\nhttps://balamuruganthambiraja.github.io/3DiFACE",
        "url": "http://arxiv.org/abs/2509.26233v1",
        "published_date": "2025-09-30T13:30:01+00:00",
        "updated_date": "2025-09-30T13:30:01+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Balamurugan Thambiraja",
            "Malte Prinzler",
            "Sadegh Aliakbarian",
            "Darren Cosker",
            "Justus Thies"
        ],
        "tldr": "The paper introduces 3DiFACE, a novel fully-convolutional diffusion model for speech-driven 3D facial animation that generates diverse plausible lip and head motions with editing capabilities through keyframing and interpolation.",
        "tldr_zh": "该论文介绍了 3DiFACE，一种新颖的全卷积扩散模型，用于语音驱动的 3D 面部动画，该模型能够生成多样且合理的唇部和头部运动，并通过关键帧和插值进行编辑。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GaussEdit: Adaptive 3D Scene Editing with Text and Image Prompts",
        "summary": "This paper presents GaussEdit, a framework for adaptive 3D scene editing\nguided by text and image prompts. GaussEdit leverages 3D Gaussian Splatting as\nits backbone for scene representation, enabling convenient Region of Interest\nselection and efficient editing through a three-stage process. The first stage\ninvolves initializing the 3D Gaussians to ensure high-quality edits. The second\nstage employs an Adaptive Global-Local Optimization strategy to balance global\nscene coherence and detailed local edits and a category-guided regularization\ntechnique to alleviate the Janus problem. The final stage enhances the texture\nof the edited objects using a sophisticated image-to-image synthesis technique,\nensuring that the results are visually realistic and align closely with the\ngiven prompts. Our experimental results demonstrate that GaussEdit surpasses\nexisting methods in editing accuracy, visual fidelity, and processing speed. By\nsuccessfully embedding user-specified concepts into 3D scenes, GaussEdit is a\npowerful tool for detailed and user-driven 3D scene editing, offering\nsignificant improvements over traditional methods.",
        "url": "http://arxiv.org/abs/2509.26055v1",
        "published_date": "2025-09-30T10:31:31+00:00",
        "updated_date": "2025-09-30T10:31:31+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhenyu Shu",
            "Junlong Yu",
            "Kai Chao",
            "Shiqing Xin",
            "Ligang Liu"
        ],
        "tldr": "GaussEdit is a novel framework for 3D scene editing using Gaussian Splatting, guided by text and image prompts, and featuring adaptive optimization and texture enhancement for realistic results.",
        "tldr_zh": "GaussEdit是一个新颖的3D场景编辑框架，它使用高斯溅射，并以文本和图像提示作为指导，具有自适应优化和纹理增强功能，可实现逼真的效果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Training-Free Reward-Guided Image Editing via Trajectory Optimal Control",
        "summary": "Recent advancements in diffusion and flow-matching models have demonstrated\nremarkable capabilities in high-fidelity image synthesis. A prominent line of\nresearch involves reward-guided guidance, which steers the generation process\nduring inference to align with specific objectives. However, leveraging this\nreward-guided approach to the task of image editing, which requires preserving\nthe semantic content of the source image while enhancing a target reward, is\nlargely unexplored. In this work, we introduce a novel framework for\ntraining-free, reward-guided image editing. We formulate the editing process as\na trajectory optimal control problem where the reverse process of a diffusion\nmodel is treated as a controllable trajectory originating from the source\nimage, and the adjoint states are iteratively updated to steer the editing\nprocess. Through extensive experiments across distinct editing tasks, we\ndemonstrate that our approach significantly outperforms existing\ninversion-based training-free guidance baselines, achieving a superior balance\nbetween reward maximization and fidelity to the source image without reward\nhacking.",
        "url": "http://arxiv.org/abs/2509.25845v1",
        "published_date": "2025-09-30T06:34:37+00:00",
        "updated_date": "2025-09-30T06:34:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jinho Chang",
            "Jaemin Kim",
            "Jong Chul Ye"
        ],
        "tldr": "This paper introduces a training-free framework for reward-guided image editing using trajectory optimal control with diffusion models, demonstrating improved performance over existing methods in balancing reward maximization and source image fidelity.",
        "tldr_zh": "本文提出了一种基于轨迹最优控制的免训练奖励引导图像编辑框架，该框架使用扩散模型，并在奖励最大化和源图像保真度之间取得了比现有方法更好的平衡。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LaTo: Landmark-tokenized Diffusion Transformer for Fine-grained Human Face Editing",
        "summary": "Recent multimodal models for instruction-based face editing enable semantic\nmanipulation but still struggle with precise attribute control and identity\npreservation. Structural facial representations such as landmarks are effective\nfor intermediate supervision, yet most existing methods treat them as rigid\ngeometric constraints, which can degrade identity when conditional landmarks\ndeviate significantly from the source (e.g., large expression or pose changes,\ninaccurate landmark estimates). To address these limitations, we propose LaTo,\na landmark-tokenized diffusion transformer for fine-grained,\nidentity-preserving face editing. Our key innovations include: (1) a landmark\ntokenizer that directly quantizes raw landmark coordinates into discrete facial\ntokens, obviating the need for dense pixel-wise correspondence; (2) a\nlocation-mapping positional encoding that integrates facial and image tokens\nfor unified processing, enabling flexible yet decoupled geometry-appearance\ninteractions with high efficiency and strong identity preservation; and (3) a\nlandmark predictor that leverages vision-language models to infer target\nlandmarks from instructions and source images, whose structured\nchain-of-thought improves estimation accuracy and interactive control. To\nmitigate data scarcity, we curate HFL-150K, to our knowledge the largest\nbenchmark for this task, containing over 150K real face pairs with fine-grained\ninstructions. Extensive experiments show that LaTo outperforms state-of-the-art\nmethods by 7.8% in identity preservation and 4.6% in semantic consistency. Code\nand dataset will be made publicly available upon acceptance.",
        "url": "http://arxiv.org/abs/2509.25731v1",
        "published_date": "2025-09-30T03:40:27+00:00",
        "updated_date": "2025-09-30T03:40:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenghao Zhang",
            "Ziying Zhang",
            "Junchao Liao",
            "Xiangyu Meng",
            "Qiang Hu",
            "Siyu Zhu",
            "Xiaoyun Zhang",
            "Long Qin",
            "Weizhi Wang"
        ],
        "tldr": "This paper introduces LaTo, a landmark-tokenized diffusion transformer for fine-grained face editing that improves identity preservation and semantic consistency by discretizing landmarks and using a location-mapping positional encoding. They also introduce a new large dataset for this task.",
        "tldr_zh": "本文介绍了一种名为LaTo的地标标记化扩散Transformer，用于精细的人脸编辑，通过离散化地标并使用位置映射位置编码来提高身份保持和语义一致性。他们还为此任务引入了一个新的大型数据集。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Reweighted Flow Matching via Unbalanced OT for Label-free Long-tailed Generation",
        "summary": "Flow matching has recently emerged as a powerful framework for\ncontinuous-time generative modeling. However, when applied to long-tailed\ndistributions, standard flow matching suffers from majority bias, producing\nminority modes with low fidelity and failing to match the true class\nproportions. In this work, we propose Unbalanced Optimal Transport Reweighted\nFlow Matching (UOT-RFM), a novel framework for generative modeling under\nclass-imbalanced (long-tailed) distributions that operates without any class\nlabel information. Our method constructs the conditional vector field using\nmini-batch Unbalanced Optimal Transport (UOT) and mitigates majority bias\nthrough a principled inverse reweighting strategy. The reweighting relies on a\nlabel-free majority score, defined as the density ratio between the target\ndistribution and the UOT marginal. This score quantifies the degree of majority\nbased on the geometric structure of the data, without requiring class labels.\nBy incorporating this score into the training objective, UOT-RFM theoretically\nrecovers the target distribution with first-order correction ($k=1$) and\nempirically improves tail-class generation through higher-order corrections ($k\n> 1$). Our model outperforms existing flow matching baselines on long-tailed\nbenchmarks, while maintaining competitive performance on balanced datasets.",
        "url": "http://arxiv.org/abs/2509.25713v1",
        "published_date": "2025-09-30T03:19:52+00:00",
        "updated_date": "2025-09-30T03:19:52+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Hyunsoo Song",
            "Minjung Gim",
            "Jaewoong Choi"
        ],
        "tldr": "This paper introduces Unbalanced Optimal Transport Reweighted Flow Matching (UOT-RFM) for long-tailed generative modeling without class labels, addressing majority bias and improving tail-class generation.",
        "tldr_zh": "本文提出了一种用于无类别标签的长尾生成建模的非平衡最优传输重加权流匹配（UOT-RFM）方法，解决了多数偏差问题，并改进了尾部类别生成。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Editing Physiological Signals in Videos Using Latent Representations",
        "summary": "Camera-based physiological signal estimation provides a non-contact and\nconvenient means to monitor Heart Rate (HR). However, the presence of vital\nsignals in facial videos raises significant privacy concerns, as they can\nreveal sensitive personal information related to the health and emotional\nstates of an individual. To address this, we propose a learned framework that\nedits physiological signals in videos while preserving visual fidelity. First,\nwe encode an input video into a latent space via a pretrained 3D Variational\nAutoencoder (3D VAE), while a target HR prompt is embedded through a frozen\ntext encoder. We fuse them using a set of trainable spatio-temporal layers with\nAdaptive Layer Normalizations (AdaLN) to capture the strong temporal coherence\nof remote Photoplethysmography (rPPG) signals. We apply Feature-wise Linear\nModulation (FiLM) in the decoder with a fine-tuned output layer to avoid the\ndegradation of physiological signals during reconstruction, enabling accurate\nphysiological modulation in the reconstructed video. Empirical results show\nthat our method preserves visual quality with an average PSNR of 38.96 dB and\nSSIM of 0.98 on selected datasets, while achieving an average HR modulation\nerror of 10.00 bpm MAE and 10.09% MAPE using a state-of-the-art rPPG estimator.\nOur design's controllable HR editing is useful for applications such as\nanonymizing biometric signals in real videos or synthesizing realistic videos\nwith desired vital signs.",
        "url": "http://arxiv.org/abs/2509.25348v1",
        "published_date": "2025-09-29T18:02:50+00:00",
        "updated_date": "2025-09-29T18:02:50+00:00",
        "categories": [
            "cs.CV",
            "cs.HC",
            "cs.MM"
        ],
        "authors": [
            "Tianwen Zhou",
            "Akshay Paruchuri",
            "Josef Spjut",
            "Kaan Akşit"
        ],
        "tldr": "This paper introduces a framework to edit physiological signals (Heart Rate) in videos by encoding the video into a latent space and modulating it with a target HR prompt, while preserving visual fidelity.",
        "tldr_zh": "本文介绍了一个框架，通过将视频编码到潜在空间并使用目标心率提示进行调制，来编辑视频中的生理信号（心率），同时保持视觉保真度。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Dragging with Geometry: From Pixels to Geometry-Guided Image Editing",
        "summary": "Interactive point-based image editing serves as a controllable editor,\nenabling precise and flexible manipulation of image content. However, most\ndrag-based methods operate primarily on the 2D pixel plane with limited use of\n3D cues. As a result, they often produce imprecise and inconsistent edits,\nparticularly in geometry-intensive scenarios such as rotations and perspective\ntransformations. To address these limitations, we propose a novel\ngeometry-guided drag-based image editing method - GeoDrag, which addresses\nthree key challenges: 1) incorporating 3D geometric cues into pixel-level\nediting, 2) mitigating discontinuities caused by geometry-only guidance, and 3)\nresolving conflicts arising from multi-point dragging. Built upon a unified\ndisplacement field that jointly encodes 3D geometry and 2D spatial priors,\nGeoDrag enables coherent, high-fidelity, and structure-consistent editing in a\nsingle forward pass. In addition, a conflict-free partitioning strategy is\nintroduced to isolate editing regions, effectively preventing interference and\nensuring consistency. Extensive experiments across various editing scenarios\nvalidate the effectiveness of our method, showing superior precision,\nstructural consistency, and reliable multi-point editability. The code will be\navailable on https://github.com/xinyu-pu/GeoDrag .",
        "url": "http://arxiv.org/abs/2509.25740v1",
        "published_date": "2025-09-30T03:53:11+00:00",
        "updated_date": "2025-09-30T03:53:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyu Pu",
            "Hongsong Wang",
            "Jie Gui",
            "Pan Zhou"
        ],
        "tldr": "The paper proposes GeoDrag, a geometry-guided drag-based image editing method that incorporates 3D geometric cues to improve the precision and consistency of image edits, especially in geometry-intensive scenarios.",
        "tldr_zh": "该论文提出了一种名为GeoDrag的几何引导的拖拽式图像编辑方法，该方法融合了3D几何线索，提高了图像编辑的精确性和一致性，尤其是在几何密集型场景中。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]