[
    {
        "title": "PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation",
        "summary": "Generating high-quality 360{\\deg} panoramic videos remains a significant\nchallenge due to the fundamental differences between panoramic and traditional\nperspective-view projections. While perspective videos rely on a single\nviewpoint with a limited field of view, panoramic content requires rendering\nthe full surrounding environment, making it difficult for standard video\ngeneration models to adapt. Existing solutions often introduce complex\narchitectures or large-scale training, leading to inefficiency and suboptimal\nresults. Motivated by the success of Low-Rank Adaptation (LoRA) in style\ntransfer tasks, we propose treating panoramic video generation as an adaptation\nproblem from perspective views. Through theoretical analysis, we demonstrate\nthat LoRA can effectively model the transformation between these projections\nwhen its rank exceeds the degrees of freedom in the task. Our approach\nefficiently fine-tunes a pretrained video diffusion model using only\napproximately 1,000 videos while achieving high-quality panoramic generation.\nExperimental results demonstrate that our method maintains proper projection\ngeometry and surpasses previous state-of-the-art approaches in visual quality,\nleft-right consistency, and motion diversity.",
        "url": "http://arxiv.org/abs/2509.11092v1",
        "published_date": "2025-09-14T05:05:27+00:00",
        "updated_date": "2025-09-14T05:05:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zeyu Dong",
            "Yuyang Yin",
            "Yuqi Li",
            "Eric Li",
            "Hao-Xiang Guo",
            "Yikai Wang"
        ],
        "tldr": "The paper introduces PanoLora, a LoRA-based approach for efficiently adapting pre-trained video diffusion models to generate high-quality 360° panoramic videos. It claims to surpass existing methods in visual quality, consistency, and motion diversity while using a relatively small training dataset.",
        "tldr_zh": "该论文提出 PanoLora，一种基于 LoRA 的方法，用于高效调整预训练的视频扩散模型，生成高质量的 360° 全景视频。 它声称在使用相对较小训练数据集的同时，在视觉质量、一致性和运动多样性方面超越了现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation",
        "summary": "Skin tone recognition and generation play important roles in model fairness,\nhealthcare, and generative AI, yet they remain challenging due to the lack of\ncomprehensive datasets and robust methodologies. Compared to other human image\nanalysis tasks, state-of-the-art large multimodal models (LMMs) and image\ngeneration models struggle to recognize and synthesize skin tones accurately.\nTo address this, we introduce TrueSkin, a dataset with 7299 images\nsystematically categorized into 6 classes, collected under diverse lighting\nconditions, camera angles, and capture settings. Using TrueSkin, we benchmark\nexisting recognition and generation approaches, revealing substantial biases:\nLMMs tend to misclassify intermediate skin tones as lighter ones, whereas\ngenerative models struggle to accurately produce specified skin tones when\ninfluenced by inherent biases from unrelated attributes in the prompts, such as\nhairstyle or environmental context. We further demonstrate that training a\nrecognition model on TrueSkin improves classification accuracy by more than\n20\\% compared to LMMs and conventional approaches, and fine-tuning with\nTrueSkin significantly improves skin tone fidelity in image generation models.\nOur findings highlight the need for comprehensive datasets like TrueSkin, which\nnot only serves as a benchmark for evaluating existing models but also provides\na valuable training resource to enhance fairness and accuracy in skin tone\nrecognition and generation tasks.",
        "url": "http://arxiv.org/abs/2509.10980v1",
        "published_date": "2025-09-13T20:58:09+00:00",
        "updated_date": "2025-09-13T20:58:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoming Lu"
        ],
        "tldr": "This paper introduces TrueSkin, a new dataset for skin tone recognition and generation, and demonstrates its effectiveness in improving the fairness and accuracy of LMMs and generative models.",
        "tldr_zh": "本文介绍了TrueSkin，一个用于肤色识别和生成的新数据集，并展示了它在提高大型多模态模型和生成模型的公平性和准确性方面的有效性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation",
        "summary": "In the realm of image generation, the quest for realism and customization has\nnever been more pressing. While existing methods like concept sliders have made\nstrides, they often falter when it comes to no-AIGC images, particularly images\ncaptured in real world settings. To bridge this gap, we introduce Beyond\nSliders, an innovative framework that integrates GANs and diffusion models to\nfacilitate sophisticated image manipulation across diverse image categories.\nImproved upon concept sliders, our method refines the image through fine\ngrained guidance both textual and visual in an adversarial manner, leading to a\nmarked enhancement in image quality and realism. Extensive experimental\nvalidation confirms the robustness and versatility of Beyond Sliders across a\nspectrum of applications.",
        "url": "http://arxiv.org/abs/2509.11213v1",
        "published_date": "2025-09-14T10:48:37+00:00",
        "updated_date": "2025-09-14T10:48:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yufei Tang",
            "Daiheng Gao",
            "Pingyu Wu",
            "Wenbo Zhou",
            "Bang Zhang",
            "Weiming Zhang"
        ],
        "tldr": "The paper introduces \"Beyond Sliders,\" a GAN and diffusion model-based framework for improved image manipulation, particularly for real-world images, addressing the limitations of existing concept slider methods.",
        "tldr_zh": "本文介绍了一个名为“超越滑块”的框架，该框架结合了GAN和扩散模型，用于改进图像处理，尤其是在真实世界图像中，解决了现有概念滑块方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild",
        "summary": "We propose a pipeline to extract and reconstruct dynamic 3D smoke assets from\na single in-the-wild video, and further integrate interactive simulation for\nsmoke design and editing. Recent developments in 3D vision have significantly\nimproved reconstructing and rendering fluid dynamics, supporting realistic and\ntemporally consistent view synthesis. However, current fluid reconstructions\nrely heavily on carefully controlled clean lab environments, whereas real-world\nvideos captured in the wild are largely underexplored. We pinpoint three key\nchallenges of reconstructing smoke in real-world videos and design targeted\ntechniques, including smoke extraction with background removal, initialization\nof smoke particles and camera poses, and inferring multi-view videos. Our\nmethod not only outperforms previous reconstruction and generation methods with\nhigh-quality smoke reconstructions (+2.22 average PSNR on wild videos), but\nalso enables diverse and realistic editing of fluid dynamics by simulating our\nsmoke assets. We provide our models, data, and 4D smoke assets at\n[https://autumnyq.github.io/WildSmoke](https://autumnyq.github.io/WildSmoke).",
        "url": "http://arxiv.org/abs/2509.11114v1",
        "published_date": "2025-09-14T06:06:42+00:00",
        "updated_date": "2025-09-14T06:06:42+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yuqiu Liu",
            "Jialin Song",
            "Manolis Savva",
            "Wuyang Chen"
        ],
        "tldr": "This paper introduces a pipeline to extract and reconstruct dynamic 3D smoke assets from single in-the-wild videos, enabling interactive simulation and editing, and outperforms previous methods in reconstruction quality.",
        "tldr_zh": "该论文提出了一个从单个真实场景视频中提取和重建动态3D烟雾资产的流程，从而能够进行交互式模拟和编辑， 并且在重建质量上优于先前的方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]