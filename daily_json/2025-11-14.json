[
    {
        "title": "Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation",
        "summary": "Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.\n  Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.",
        "url": "http://arxiv.org/abs/2511.10547v1",
        "published_date": "2025-11-13T17:48:38+00:00",
        "updated_date": "2025-11-14T01:57:27+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Isabela Albuquerque",
            "Ira Ktena",
            "Olivia Wiles",
            "Ivana Kajić",
            "Amal Rannen-Triki",
            "Cristina Vasconcelos",
            "Aida Nematzadeh"
        ],
        "tldr": "This paper introduces a human evaluation framework and dataset for benchmarking diversity in text-to-image models, addressing the problem of homogeneity in generated outputs.",
        "tldr_zh": "本文介绍了一种用于评估文本到图像模型中多样性的框架和数据集，该框架通过人类评估来解决生成输出同质性的问题。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VLF-MSC: Vision-Language Feature-Based Multimodal Semantic Communication System",
        "summary": "We propose Vision-Language Feature-based Multimodal Semantic Communication (VLF-MSC), a unified system that transmits a single compact vision-language representation to support both image and text generation at the receiver. Unlike existing semantic communication techniques that process each modality separately, VLF-MSC employs a pre-trained vision-language model (VLM) to encode the source image into a vision-language semantic feature (VLF), which is transmitted over the wireless channel. At the receiver, a decoder-based language model and a diffusion-based image generator are both conditioned on the VLF to produce a descriptive text and a semantically aligned image. This unified representation eliminates the need for modality-specific streams or retransmissions, improving spectral efficiency and adaptability. By leveraging foundation models, the system achieves robustness to channel noise while preserving semantic fidelity. Experiments demonstrate that VLF-MSC outperforms text-only and image-only baselines, achieving higher semantic accuracy for both modalities under low SNR with significantly reduced bandwidth.",
        "url": "http://arxiv.org/abs/2511.10074v1",
        "published_date": "2025-11-13T08:29:32+00:00",
        "updated_date": "2025-11-14T01:30:32+00:00",
        "categories": [
            "cs.CV",
            "eess.SY"
        ],
        "authors": [
            "Gwangyeon Ahn",
            "Jiwan Seo",
            "Joonhyuk Kang"
        ],
        "tldr": "The paper introduces VLF-MSC, a system which uses a pre-trained vision-language model to encode an image into a unified vision-language feature representation for transmission, enabling both text and image generation at the receiver, improving spectral efficiency and robustness.",
        "tldr_zh": "该论文提出了一种名为VLF-MSC的系统，它使用预训练的视觉-语言模型将图像编码成统一的视觉-语言特征表示进行传输，从而在接收端实现文本和图像生成，提高了频谱效率和鲁棒性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control",
        "summary": "Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.",
        "url": "http://arxiv.org/abs/2511.09715v1",
        "published_date": "2025-11-12T20:21:37+00:00",
        "updated_date": "2025-11-14T01:05:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Arman Zarei",
            "Samyadeep Basu",
            "Mobina Pournemat",
            "Sayan Nag",
            "Ryan Rossi",
            "Soheil Feizi"
        ],
        "tldr": "The paper introduces SliderEdit, a framework for continuous, fine-grained control in instruction-based image editing models using globally trained sliders, improving edit controllability and visual consistency without requiring retraining for each attribute.",
        "tldr_zh": "该论文介绍了SliderEdit，一个用于基于指令的图像编辑模型中的连续、细粒度控制的框架，它使用全局训练的滑块，在提高编辑可控性和视觉一致性的同时，无需为每个属性重新训练。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space",
        "summary": "Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.",
        "url": "http://arxiv.org/abs/2511.10555v1",
        "published_date": "2025-11-13T17:56:10+00:00",
        "updated_date": "2025-11-14T01:57:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Huijie Liu",
            "Shuhao Cui",
            "Haoxiang Cao",
            "Shuai Ma",
            "Kai Wu",
            "Guoliang Kang"
        ],
        "tldr": "The paper introduces CoTyle, a novel open-source method for code-to-style image generation, which generates images with consistent visual styles conditioned solely on a numerical style code using a discrete style codebook and a text-to-image diffusion model.",
        "tldr_zh": "该论文介绍了CoTyle，一种新的代码到风格的图像生成开源方法，它仅使用数字风格代码作为条件，通过离散风格代码本和文本到图像扩散模型生成具有一致视觉风格的图像。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LoG3D: Ultra-High-Resolution 3D Shape Modeling via Local-to-Global Partitioning",
        "summary": "Generating high-fidelity 3D contents remains a fundamental challenge due to the complexity of representing arbitrary topologies-such as open surfaces and intricate internal structures-while preserving geometric details. Prevailing methods based on signed distance fields (SDFs) are hampered by costly watertight preprocessing and struggle with non-manifold geometries, while point-cloud representations often suffer from sampling artifacts and surface discontinuities. To overcome these limitations, we propose a novel 3D variational autoencoder (VAE) framework built upon unsigned distance fields (UDFs)-a more robust and computationally efficient representation that naturally handles complex and incomplete shapes. Our core innovation is a local-to-global (LoG) architecture that processes the UDF by partitioning it into uniform subvolumes, termed UBlocks. This architecture couples 3D convolutions for capturing local detail with sparse transformers for enforcing global coherence. A Pad-Average strategy further ensures smooth transitions at subvolume boundaries during reconstruction. This modular design enables seamless scaling to ultra-high resolutions up to 2048^3-a regime previously unattainable for 3D VAEs. Experiments demonstrate state-of-the-art performance in both reconstruction accuracy and generative quality, yielding superior surface smoothness and geometric flexibility.",
        "url": "http://arxiv.org/abs/2511.10040v1",
        "published_date": "2025-11-13T07:34:43+00:00",
        "updated_date": "2025-11-14T01:28:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinran Yang",
            "Shuichang Lai",
            "Jiangjing Lyu",
            "Hongjie Li",
            "Bowen Pan",
            "Yuanqi Li",
            "Jie Guo",
            "Zhou Zhengkang",
            "Yanwen Guo"
        ],
        "tldr": "The paper introduces LoG3D, a novel VAE framework using unsigned distance fields (UDFs) and a local-to-global architecture for generating high-resolution 3D shapes, achieving state-of-the-art results in reconstruction accuracy and generative quality.",
        "tldr_zh": "该论文介绍了LoG3D，一种新颖的VAE框架，它使用无符号距离场（UDF）和局部到全局的架构来生成高分辨率3D形状，并在重建精度和生成质量方面取得了最先进的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]