[
    {
        "title": "On Flow Matching KL Divergence",
        "summary": "We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler\n(KL) divergence of the flow-matching distribution approximation. In particular,\nif the $L_2$ flow-matching loss is bounded by $\\epsilon^2 > 0$, then the KL\ndivergence between the true data distribution and the estimated distribution is\nbounded by $A_1 \\epsilon + A_2 \\epsilon^2$. Here, the constants $A_1$ and $A_2$\ndepend only on the regularities of the data and velocity fields. Consequently,\nthis bound implies statistical convergence rates of Flow Matching Transformers\nunder the Total Variation (TV) distance. We show that, flow matching achieves\nnearly minimax-optimal efficiency in estimating smooth distributions. Our\nresults make the statistical efficiency of flow matching comparable to that of\ndiffusion models under the TV distance. Numerical studies on synthetic and\nlearned velocities corroborate our theory.",
        "url": "http://arxiv.org/abs/2511.05480v1",
        "published_date": "2025-11-07T18:47:46+00:00",
        "updated_date": "2025-11-07T18:47:46+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Maojiang Su",
            "Jerry Yao-Chieh Hu",
            "Sophia Pi",
            "Han Liu"
        ],
        "tldr": "The paper provides a non-asymptotic upper bound on the KL divergence for flow-matching distribution approximation, showing statistical convergence and efficiency comparable to diffusion models under the Total Variation distance.",
        "tldr_zh": "该论文推导了流匹配分布近似的KL散度的确定性非渐近上界，表明其统计收敛性和效率与扩散模型相当，衡量标准为总变差距离。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]