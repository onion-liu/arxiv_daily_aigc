[
    {
        "title": "Lynx: Towards High-Fidelity Personalized Video Generation",
        "summary": "We present Lynx, a high-fidelity model for personalized video synthesis from\na single input image. Built on an open-source Diffusion Transformer (DiT)\nfoundation model, Lynx introduces two lightweight adapters to ensure identity\nfidelity. The ID-adapter employs a Perceiver Resampler to convert\nArcFace-derived facial embeddings into compact identity tokens for\nconditioning, while the Ref-adapter integrates dense VAE features from a frozen\nreference pathway, injecting fine-grained details across all transformer layers\nthrough cross-attention. These modules collectively enable robust identity\npreservation while maintaining temporal coherence and visual realism. Through\nevaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which\nyielded 800 test cases, Lynx has demonstrated superior face resemblance,\ncompetitive prompt following, and strong video quality, thereby advancing the\nstate of personalized video generation.",
        "url": "http://arxiv.org/abs/2509.15496v1",
        "published_date": "2025-09-19T00:31:57+00:00",
        "updated_date": "2025-09-19T00:31:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shen Sang",
            "Tiancheng Zhi",
            "Tianpei Gu",
            "Jing Liu",
            "Linjie Luo"
        ],
        "tldr": "The paper introduces Lynx, a high-fidelity personalized video generation model leveraging Diffusion Transformers and lightweight adapters for enhanced identity preservation and video quality with demonstrated improvement over existing methods.",
        "tldr_zh": "该论文介绍了 Lynx，一个高保真个性化视频生成模型，它利用扩散变换器和轻量级适配器来增强身份保持和视频质量，并展示了相对于现有方法的改进。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer",
        "summary": "Unified multimodal Large Language Models (LLMs) that can both understand and\ngenerate visual content hold immense potential. However, existing open-source\nmodels often suffer from a performance trade-off between these capabilities. We\npresent Manzano, a simple and scalable unified framework that substantially\nreduces this tension by coupling a hybrid image tokenizer with a well-curated\ntraining recipe. A single shared vision encoder feeds two lightweight adapters\nthat produce continuous embeddings for image-to-text understanding and discrete\ntokens for text-to-image generation within a common semantic space. A unified\nautoregressive LLM predicts high-level semantics in the form of text and image\ntokens, with an auxiliary diffusion decoder subsequently translating the image\ntokens into pixels. The architecture, together with a unified training recipe\nover understanding and generation data, enables scalable joint learning of both\ncapabilities. Manzano achieves state-of-the-art results among unified models,\nand is competitive with specialist models, particularly on text-rich\nevaluation. Our studies show minimal task conflicts and consistent gains from\nscaling model size, validating our design choice of a hybrid tokenizer.",
        "url": "http://arxiv.org/abs/2509.16197v1",
        "published_date": "2025-09-19T17:58:00+00:00",
        "updated_date": "2025-09-19T17:58:00+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Yanghao Li",
            "Rui Qian",
            "Bowen Pan",
            "Haotian Zhang",
            "Haoshuo Huang",
            "Bowen Zhang",
            "Jialing Tong",
            "Haoxuan You",
            "Xianzhi Du",
            "Zhe Gan",
            "Hyunjik Kim",
            "Chao Jia",
            "Zhenbang Wang",
            "Yinfei Yang",
            "Mingfei Gao",
            "Zi-Yi Dou",
            "Wenze Hu",
            "Chang Gao",
            "Dongxu Li",
            "Philipp Dufter",
            "Zirui Wang",
            "Guoli Yin",
            "Zhengdong Zhang",
            "Chen Chen",
            "Yang Zhao",
            "Ruoming Pang",
            "Zhifeng Chen"
        ],
        "tldr": "The paper introduces Manzano, a unified multimodal LLM that uses a hybrid vision tokenizer and a tailored training recipe to achieve state-of-the-art performance in both image understanding and generation, while mitigating performance trade-offs.",
        "tldr_zh": "该论文介绍了Manzano，一种统一的多模态LLM，它使用混合视觉标记器和定制的训练配方，在图像理解和生成方面都实现了最先进的性能，同时减轻了性能权衡。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models",
        "summary": "Text-to-Image (T2I) models have recently achieved remarkable success in\ngenerating images from textual descriptions. However, challenges still persist\nin accurately rendering complex scenes where actions and interactions form the\nprimary semantic focus. Our key observation in this work is that T2I models\nfrequently struggle to capture nuanced and often implicit attributes inherent\nin action depiction, leading to generating images that lack key contextual\ndetails. To enable systematic evaluation, we introduce AcT2I, a benchmark\ndesigned to evaluate the performance of T2I models in generating images from\naction-centric prompts. We experimentally validate that leading T2I models do\nnot fare well on AcT2I. We further hypothesize that this shortcoming arises\nfrom the incomplete representation of the inherent attributes and contextual\ndependencies in the training corpora of existing T2I models. We build upon this\nby developing a training-free, knowledge distillation technique utilizing Large\nLanguage Models to address this limitation. Specifically, we enhance prompts by\nincorporating dense information across three dimensions, observing that\ninjecting prompts with temporal details significantly improves image generation\naccuracy, with our best model achieving an increase of 72%. Our findings\nhighlight the limitations of current T2I methods in generating images that\nrequire complex reasoning and demonstrate that integrating linguistic knowledge\nin a systematic way can notably advance the generation of nuanced and\ncontextually accurate images.",
        "url": "http://arxiv.org/abs/2509.16141v1",
        "published_date": "2025-09-19T16:41:39+00:00",
        "updated_date": "2025-09-19T16:41:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vatsal Malaviya",
            "Agneet Chatterjee",
            "Maitreya Patel",
            "Yezhou Yang",
            "Chitta Baral"
        ],
        "tldr": "The paper introduces AcT2I, a benchmark for evaluating Text-to-Image models on action depiction, and proposes a knowledge distillation technique using LLMs to improve performance, achieving significant accuracy gains by incorporating temporal information in prompts.",
        "tldr_zh": "该论文介绍了AcT2I，一个用于评估文本到图像模型在动作描述上的基准，并提出了一种利用大型语言模型的知识蒸馏技术来提高性能，通过在提示中加入时间信息，实现了显著的准确率提升。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dynamic Classifier-Free Diffusion Guidance via Online Feedback",
        "summary": "Classifier-free guidance (CFG) is a cornerstone of text-to-image diffusion\nmodels, yet its effectiveness is limited by the use of static guidance scales.\nThis \"one-size-fits-all\" approach fails to adapt to the diverse requirements of\ndifferent prompts; moreover, prior solutions like gradient-based correction or\nfixed heuristic schedules introduce additional complexities and fail to\ngeneralize. In this work, we challeng this static paradigm by introducing a\nframework for dynamic CFG scheduling. Our method leverages online feedback from\na suite of general-purpose and specialized small-scale latent-space\nevaluations, such as CLIP for alignment, a discriminator for fidelity and a\nhuman preference reward model, to assess generation quality at each step of the\nreverse diffusion process. Based on this feedback, we perform a greedy search\nto select the optimal CFG scale for each timestep, creating a unique guidance\nschedule tailored to every prompt and sample. We demonstrate the effectiveness\nof our approach on both small-scale models and the state-of-the-art Imagen 3,\nshowing significant improvements in text alignment, visual quality, text\nrendering and numerical reasoning. Notably, when compared against the default\nImagen 3 baseline, our method achieves up to 53.8% human preference win-rate\nfor overall preference, a figure that increases up to to 55.5% on prompts\ntargeting specific capabilities like text rendering. Our work establishes that\nthe optimal guidance schedule is inherently dynamic and prompt-dependent, and\nprovides an efficient and generalizable framework to achieve it.",
        "url": "http://arxiv.org/abs/2509.16131v1",
        "published_date": "2025-09-19T16:27:19+00:00",
        "updated_date": "2025-09-19T16:27:19+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Pinelopi Papalampidi",
            "Olivia Wiles",
            "Ira Ktena",
            "Aleksandar Shtedritski",
            "Emanuele Bugliarello",
            "Ivana Kajic",
            "Isabela Albuquerque",
            "Aida Nematzadeh"
        ],
        "tldr": "The paper introduces a dynamic classifier-free guidance (CFG) method for text-to-image diffusion models that uses online feedback from latent-space evaluations to optimize the CFG scale at each timestep, resulting in improved generation quality and human preference win-rates.",
        "tldr_zh": "该论文介绍了一种动态的无分类器引导（CFG）方法，用于文本到图像的扩散模型，该方法利用来自潜在空间评估的在线反馈来优化每个时间步的CFG尺度，从而提高生成质量和人类偏好胜率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process",
        "summary": "Online reinforcement learning (RL) has been central to post-training language\nmodels, but its extension to diffusion models remains challenging due to\nintractable likelihoods. Recent works discretize the reverse sampling process\nto enable GRPO-style training, yet they inherit fundamental drawbacks,\nincluding solver restrictions, forward-reverse inconsistency, and complicated\nintegration with classifier-free guidance (CFG). We introduce Diffusion\nNegative-aware FineTuning (DiffusionNFT), a new online RL paradigm that\noptimizes diffusion models directly on the forward process via flow matching.\nDiffusionNFT contrasts positive and negative generations to define an implicit\npolicy improvement direction, naturally incorporating reinforcement signals\ninto the supervised learning objective. This formulation enables training with\narbitrary black-box solvers, eliminates the need for likelihood estimation, and\nrequires only clean images rather than sampling trajectories for policy\noptimization. DiffusionNFT is up to $25\\times$ more efficient than FlowGRPO in\nhead-to-head comparisons, while being CFG-free. For instance, DiffusionNFT\nimproves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO\nachieves 0.95 with over 5k steps and additional CFG employment. By leveraging\nmultiple reward models, DiffusionNFT significantly boosts the performance of\nSD3.5-Medium in every benchmark tested.",
        "url": "http://arxiv.org/abs/2509.16117v1",
        "published_date": "2025-09-19T16:09:33+00:00",
        "updated_date": "2025-09-19T16:09:33+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Kaiwen Zheng",
            "Huayu Chen",
            "Haotian Ye",
            "Haoxiang Wang",
            "Qinsheng Zhang",
            "Kai Jiang",
            "Hang Su",
            "Stefano Ermon",
            "Jun Zhu",
            "Ming-Yu Liu"
        ],
        "tldr": "The paper introduces DiffusionNFT, a new online reinforcement learning paradigm for diffusion models that optimizes directly on the forward process using flow matching, achieving significant efficiency gains over existing methods like FlowGRPO.",
        "tldr_zh": "该论文介绍了一种新的扩散模型在线强化学习范式 DiffusionNFT，它利用流动匹配直接在正向过程中进行优化，与现有方法 FlowGRPO 相比，效率显著提高。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation",
        "summary": "Score Distillation Sampling (SDS) enables high-quality text-to-3D generation\nby supervising 3D models through the denoising of multi-view 2D renderings,\nusing a pretrained text-to-image diffusion model to align with the input prompt\nand ensure 3D consistency. However, existing SDS-based methods face two\nfundamental limitations: (1) their reliance on CLIP-style text encoders leads\nto coarse semantic alignment and struggles with fine-grained prompts; and (2)\n2D diffusion priors lack explicit 3D spatial constraints, resulting in\ngeometric inconsistencies and inaccurate object relationships in multi-object\nscenes. To address these challenges, we propose VLM3D, a novel text-to-3D\ngeneration framework that integrates large vision-language models (VLMs) into\nthe SDS pipeline as differentiable semantic and spatial priors. Unlike standard\ntext-to-image diffusion priors, VLMs leverage rich language-grounded\nsupervision that enables fine-grained prompt alignment. Moreover, their\ninherent vision language modeling provides strong spatial understanding, which\nsignificantly enhances 3D consistency for single-object generation and improves\nrelational reasoning in multi-object scenes. We instantiate VLM3D based on the\nopen-source Qwen2.5-VL model and evaluate it on the GPTeval3D benchmark.\nExperiments across diverse objects and complex scenes show that VLM3D\nsignificantly outperforms prior SDS-based methods in semantic fidelity,\ngeometric coherence, and spatial correctness.",
        "url": "http://arxiv.org/abs/2509.15772v1",
        "published_date": "2025-09-19T08:54:52+00:00",
        "updated_date": "2025-09-19T08:54:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weimin Bai",
            "Yubo Li",
            "Weijian Luo",
            "Wenzheng Chen",
            "He Sun"
        ],
        "tldr": "The paper introduces VLM3D, a novel text-to-3D generation framework utilizing Vision-Language Models (VLMs) within the Score Distillation Sampling (SDS) pipeline to enhance semantic fidelity, geometric coherence, and spatial correctness in generated 3D models, particularly for complex scenes and fine-grained prompts.",
        "tldr_zh": "该论文介绍了一种名为 VLM3D 的新型文本到 3D 生成框架，该框架利用视觉语言模型 (VLM) 在分数蒸馏采样 (SDS) 管道中来提高生成 3D 模型中的语义保真度、几何一致性和空间正确性，尤其是在复杂场景和细粒度提示的情况下。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification",
        "summary": "Generative modeling, representation learning, and classification are three\ncore problems in machine learning (ML), yet their state-of-the-art (SoTA)\nsolutions remain largely disjoint. In this paper, we ask: Can a unified\nprinciple address all three? Such unification could simplify ML pipelines and\nfoster greater synergy across tasks. We introduce Latent Zoning Network (LZN)\nas a step toward this goal. At its core, LZN creates a shared Gaussian latent\nspace that encodes information across all tasks. Each data type (e.g., images,\ntext, labels) is equipped with an encoder that maps samples to disjoint latent\nzones, and a decoder that maps latents back to data. ML tasks are expressed as\ncompositions of these encoders and decoders: for example, label-conditional\nimage generation uses a label encoder and image decoder; image embedding uses\nan image encoder; classification uses an image encoder and label decoder. We\ndemonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN\ncan enhance existing models (image generation): When combined with the SoTA\nRectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without\nmodifying the training objective. (2) LZN can solve tasks independently\n(representation learning): LZN can implement unsupervised representation\nlearning without auxiliary loss functions, outperforming the seminal MoCo and\nSimCLR methods by 9.3% and 0.2%, respectively, on downstream linear\nclassification on ImageNet. (3) LZN can solve multiple tasks simultaneously\n(joint generation and classification): With image and label encoders/decoders,\nLZN performs both tasks jointly by design, improving FID and achieving SoTA\nclassification accuracy on CIFAR10. The code and trained models are available\nat https://github.com/microsoft/latent-zoning-networks. The project website is\nat https://zinanlin.me/blogs/latent_zoning_networks.html.",
        "url": "http://arxiv.org/abs/2509.15591v1",
        "published_date": "2025-09-19T04:47:16+00:00",
        "updated_date": "2025-09-19T04:47:16+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Zinan Lin",
            "Enshu Liu",
            "Xuefei Ning",
            "Junyi Zhu",
            "Wenyu Wang",
            "Sergey Yekhanin"
        ],
        "tldr": "The paper introduces Latent Zoning Network (LZN), a unified framework using shared latent spaces to address generative modeling, representation learning, and classification, achieving state-of-the-art results in several tasks.",
        "tldr_zh": "该论文介绍了潜在分区网络 (LZN)，这是一个统一的框架，使用共享潜在空间来解决生成建模、表征学习和分类问题，并在多个任务中取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models",
        "summary": "World models allow agents to simulate the consequences of actions in imagined\nenvironments for planning, control, and long-horizon decision-making. However,\nexisting autoregressive world models struggle with visually coherent\npredictions due to disrupted spatial structure, inefficient decoding, and\ninadequate motion modeling. In response, we propose \\textbf{S}cale-wise\n\\textbf{A}utoregression with \\textbf{M}otion \\textbf{P}r\\textbf{O}mpt\n(\\textbf{SAMPO}), a hybrid framework that combines visual autoregressive\nmodeling for intra-frame generation with causal modeling for next-frame\ngeneration. Specifically, SAMPO integrates temporal causal decoding with\nbidirectional spatial attention, which preserves spatial locality and supports\nparallel decoding within each scale. This design significantly enhances both\ntemporal consistency and rollout efficiency. To further improve dynamic scene\nunderstanding, we devise an asymmetric multi-scale tokenizer that preserves\nspatial details in observed frames and extracts compact dynamic representations\nfor future frames, optimizing both memory usage and model performance.\nAdditionally, we introduce a trajectory-aware motion prompt module that injects\nspatiotemporal cues about object and robot trajectories, focusing attention on\ndynamic regions and improving temporal consistency and physical realism.\nExtensive experiments show that SAMPO achieves competitive performance in\naction-conditioned video prediction and model-based control, improving\ngeneration quality with 4.4$\\times$ faster inference. We also evaluate SAMPO's\nzero-shot generalization and scaling behavior, demonstrating its ability to\ngeneralize to unseen tasks and benefit from larger model sizes.",
        "url": "http://arxiv.org/abs/2509.15536v1",
        "published_date": "2025-09-19T02:41:37+00:00",
        "updated_date": "2025-09-19T02:41:37+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Sen Wang",
            "Jingyi Tian",
            "Le Wang",
            "Zhimin Liao",
            "Jiayi Li",
            "Huaiyi Dong",
            "Kun Xia",
            "Sanping Zhou",
            "Wei Tang",
            "Hua Gang"
        ],
        "tldr": "The paper introduces SAMPO, a novel autoregressive world model that uses scale-wise autoregression with motion prompts for more coherent and efficient video prediction, particularly in action-conditioned scenarios.",
        "tldr_zh": "该论文介绍了SAMPO，一种新型自回归世界模型，它使用具有运动提示的尺度自回归来实现更连贯、高效的视频预测，尤其是在动作条件场景中。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data",
        "summary": "Recent successful video generation systems that predict and create realistic\nautomotive driving scenes from short video inputs assign tokenization, future\nstate prediction (world model), and video decoding to dedicated models. These\napproaches often utilize large models that require significant training\nresources, offer limited insight into design choices, and lack publicly\navailable code and datasets. In this work, we address these deficiencies and\npresent OpenViGA, an open video generation system for automotive driving\nscenes. Our contributions are: Unlike several earlier works for video\ngeneration, such as GAIA-1, we provide a deep analysis of the three components\nof our system by separate quantitative and qualitative evaluation: Image\ntokenizer, world model, video decoder. Second, we purely build upon powerful\npre-trained open source models from various domains, which we fine-tune by\npublicly available automotive data (BDD100K) on GPU hardware at academic scale.\nThird, we build a coherent video generation system by streamlining interfaces\nof our components. Fourth, due to public availability of the underlying models\nand data, we allow full reproducibility. Finally, we also publish our code and\nmodels on Github. For an image size of 256x256 at 4 fps we are able to predict\nrealistic driving scene videos frame-by-frame with only one frame of\nalgorithmic latency.",
        "url": "http://arxiv.org/abs/2509.15479v1",
        "published_date": "2025-09-18T22:54:13+00:00",
        "updated_date": "2025-09-18T22:54:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Björn Möller",
            "Zhengyang Li",
            "Malte Stelzer",
            "Thomas Graave",
            "Fabian Bettels",
            "Muaaz Ataya",
            "Tim Fingscheidt"
        ],
        "tldr": "OpenViGA is an open-source video generation system for automotive driving scenes, built upon publicly available pre-trained models and datasets, offering reproducibility and detailed component analysis.",
        "tldr_zh": "OpenViGA 是一个开源的汽车驾驶场景视频生成系统，它基于公开的预训练模型和数据集构建，提供可重复性和详细的组件分析。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation",
        "summary": "Text-to-image diffusion models achieve impressive realism but often suffer\nfrom compositional failures on prompts with multiple objects, attributes, and\nspatial relations, resulting in cross-token interference where entities\nentangle, attributes mix across objects, and spatial cues are violated. To\naddress these failures, we propose MaskAttn-SDXL,a region-level gating\nmechanism applied to the cross-attention logits of Stable Diffusion XL(SDXL)'s\nUNet. MaskAttn-SDXL learns a binary mask per layer, injecting it into each\ncross-attention logit map before softmax to sparsify token-to-latent\ninteractions so that only semantically relevant connections remain active. The\nmethod requires no positional encodings, auxiliary tokens, or external region\nmasks, and preserves the original inference path with negligible overhead. In\npractice, our model improves spatial compliance and attribute binding in\nmulti-object prompts while preserving overall image quality and diversity.\nThese findings demonstrate that logit-level maksed cross-attention is an\ndata-efficient primitve for enforcing compositional control, and our method\nthus serves as a practical extension for spatial control in text-to-image\ngeneration.",
        "url": "http://arxiv.org/abs/2509.15357v1",
        "published_date": "2025-09-18T18:57:47+00:00",
        "updated_date": "2025-09-18T18:57:47+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yu Chang",
            "Jiahao Chen",
            "Anzhe Cheng",
            "Paul Bogdan"
        ],
        "tldr": "The paper proposes MaskAttn-SDXL, a region-level gating mechanism for Stable Diffusion XL that uses masked cross-attention logits to improve compositional control in text-to-image generation, particularly for multi-object prompts.",
        "tldr_zh": "该论文提出了 MaskAttn-SDXL，一种用于 Stable Diffusion XL 的区域级门控机制，它使用 masked cross-attention logits 来提高文本到图像生成中的组合控制，特别是对于多对象提示。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition",
        "summary": "Diffusion models have achieved remarkable success in image generation but\ntheir practical application is often hindered by the slow sampling speed. Prior\nefforts of improving efficiency primarily focus on compressing models or\nreducing the total number of denoising steps, largely neglecting the\npossibility to leverage multiple input resolutions in the generation process.\nIn this work, we propose LowDiff, a novel and efficient diffusion framework\nbased on a cascaded approach by generating increasingly higher resolution\noutputs. Besides, LowDiff employs a unified model to progressively refine\nimages from low resolution to the desired resolution. With the proposed\narchitecture design and generation techniques, we achieve comparable or even\nsuperior performance with much fewer high-resolution sampling steps. LowDiff is\napplicable to diffusion models in both pixel space and latent space. Extensive\nexperiments on both conditional and unconditional generation tasks across\nCIFAR-10, FFHQ and ImageNet demonstrate the effectiveness and generality of our\nmethod. Results show over 50% throughput improvement across all datasets and\nsettings while maintaining comparable or better quality. On unconditional\nCIFAR-10, LowDiff achieves an FID of 2.11 and IS of 9.87, while on conditional\nCIFAR-10, an FID of 1.94 and IS of 10.03. On FFHQ 64x64, LowDiff achieves an\nFID of 2.43, and on ImageNet 256x256, LowDiff built on LightningDiT-B/1\nproduces high-quality samples with a FID of 4.00 and an IS of 195.06, together\nwith substantial efficiency gains.",
        "url": "http://arxiv.org/abs/2509.15342v1",
        "published_date": "2025-09-18T18:31:56+00:00",
        "updated_date": "2025-09-18T18:31:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiuyi Xu",
            "Qing Jin",
            "Meida Chen",
            "Andrew Feng",
            "Yang Sui",
            "Yangming Shi"
        ],
        "tldr": "The paper introduces LowDiff, a novel diffusion framework that leverages a cascaded approach with a unified model to efficiently generate high-resolution images from low-resolution conditions, achieving significant throughput improvements while maintaining comparable or superior quality.",
        "tldr_zh": "该论文介绍了LowDiff，一种新颖的扩散框架，它利用级联方法和统一模型，从低分辨率条件高效生成高分辨率图像，在保持相当或更好质量的同时，实现了显著的吞吐量提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Kuramoto Orientation Diffusion Models",
        "summary": "Orientation-rich images, such as fingerprints and textures, often exhibit\ncoherent angular directional patterns that are challenging to model using\nstandard generative approaches based on isotropic Euclidean diffusion.\nMotivated by the role of phase synchronization in biological systems, we\npropose a score-based generative model built on periodic domains by leveraging\nstochastic Kuramoto dynamics in the diffusion process. In neural and physical\nsystems, Kuramoto models capture synchronization phenomena across coupled\noscillators -- a behavior that we re-purpose here as an inductive bias for\nstructured image generation. In our framework, the forward process performs\n\\textit{synchronization} among phase variables through globally or locally\ncoupled oscillator interactions and attraction to a global reference phase,\ngradually collapsing the data into a low-entropy von Mises distribution. The\nreverse process then performs \\textit{desynchronization}, generating diverse\npatterns by reversing the dynamics with a learned score function. This approach\nenables structured destruction during forward diffusion and a hierarchical\ngeneration process that progressively refines global coherence into fine-scale\ndetails. We implement wrapped Gaussian transition kernels and periodicity-aware\nnetworks to account for the circular geometry. Our method achieves competitive\nresults on general image benchmarks and significantly improves generation\nquality on orientation-dense datasets like fingerprints and textures.\nUltimately, this work demonstrates the promise of biologically inspired\nsynchronization dynamics as structured priors in generative modeling.",
        "url": "http://arxiv.org/abs/2509.15328v1",
        "published_date": "2025-09-18T18:18:49+00:00",
        "updated_date": "2025-09-18T18:18:49+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "q-bio.NC"
        ],
        "authors": [
            "Yue Song",
            "T. Anderson Keller",
            "Sevan Brodjian",
            "Takeru Miyato",
            "Yisong Yue",
            "Pietro Perona",
            "Max Welling"
        ],
        "tldr": "The paper introduces a novel score-based generative model, Kuramoto Orientation Diffusion Models, inspired by biological synchronization, to generate orientation-rich images such as fingerprints and textures more effectively than standard methods.",
        "tldr_zh": "该论文介绍了一种新颖的基于分数的生成模型，Kuramoto Orientation Diffusion Models，灵感来自生物同步，旨在比标准方法更有效地生成方向丰富的图像，如指纹和纹理。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]