[
    {
        "title": "EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing",
        "summary": "High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.",
        "url": "http://arxiv.org/abs/2602.15031v1",
        "published_date": "2026-02-16T18:59:58+00:00",
        "updated_date": "2026-02-16T18:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yehonathan Litman",
            "Shikun Liu",
            "Dario Seyb",
            "Nicholas Milef",
            "Yang Zhou",
            "Carl Marshall",
            "Shubham Tulsiani",
            "Caleb Leak"
        ],
        "tldr": "EditCtrl introduces a computationally efficient video editing framework that focuses computation on masked tokens for local edits and uses a lightweight global context embedder for video consistency, achieving significant speedups and improved quality compared to existing methods.",
        "tldr_zh": "EditCtrl 提出了一种计算高效的视频编辑框架，该框架将计算集中在被遮蔽的 tokens 上进行局部编辑，并使用轻量级的全局上下文嵌入器来保证视频一致性，与现有方法相比，实现了显著的加速和质量提升。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Spanning the Visual Analogy Space with a Weight Basis of LoRAs",
        "summary": "Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet $\\{\\mathbf{a}$, $\\mathbf{a}'$, $\\mathbf{b}\\}$, the goal is to generate $\\mathbf{b}'$ such that $\\mathbf{a} : \\mathbf{a}' :: \\mathbf{b} : \\mathbf{b}'$. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a \"space of LoRAs\". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb",
        "url": "http://arxiv.org/abs/2602.15727v1",
        "published_date": "2026-02-17T17:02:38+00:00",
        "updated_date": "2026-02-17T17:02:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.GR",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Hila Manor",
            "Rinon Gal",
            "Haggai Maron",
            "Tomer Michaeli",
            "Gal Chechik"
        ],
        "tldr": "This paper introduces LoRWeB, a novel approach for visual analogy learning that dynamically composes learned LoRA transformation primitives using a learnable basis, achieving state-of-the-art performance and improved generalization.",
        "tldr_zh": "本文介绍了LoRWeB，一种新颖的视觉类比学习方法，利用可学习的基动态组合LoRA变换原语，实现了最先进的性能并提高了泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dynamic Training-Free Fusion of Subject and Style LoRAs",
        "summary": "Recent studies have explored the combination of multiple LoRAs to simultaneously generate user-specified subjects and styles. However, most existing approaches fuse LoRA weights using static statistical heuristics that deviate from LoRA's original purpose of learning adaptive feature adjustments and ignore the randomness of sampled inputs. To address this, we propose a dynamic training-free fusion framework that operates throughout the generation process. During the forward pass, at each LoRA-applied layer, we dynamically compute the KL divergence between the base model's original features and those produced by subject and style LoRAs, respectively, and adaptively select the most appropriate weights for fusion. In the reverse denoising stage, we further refine the generation trajectory by dynamically applying gradient-based corrections derived from objective metrics such as CLIP and DINO scores, providing continuous semantic and stylistic guidance. By integrating these two complementary mechanisms-feature-level selection and metric-guided latent adjustment-across the entire diffusion timeline, our method dynamically achieves coherent subject-style synthesis without any retraining. Extensive experiments across diverse subject-style combinations demonstrate that our approach consistently outperforms state-of-the-art LoRA fusion methods both qualitatively and quantitatively.",
        "url": "http://arxiv.org/abs/2602.15539v1",
        "published_date": "2026-02-17T12:42:30+00:00",
        "updated_date": "2026-02-17T12:42:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.SC"
        ],
        "authors": [
            "Qinglong Cao",
            "Yuntian Chen",
            "Chao Ma",
            "Xiaokang Yang"
        ],
        "tldr": "This paper proposes a dynamic, training-free LoRA fusion method for subject and style image generation that adaptively selects LoRA weights based on feature-level KL divergence and refines the generation trajectory with metric-guided gradient corrections.",
        "tldr_zh": "本文提出了一种动态的，免训练的LoRA融合方法，用于主体和风格图像生成，该方法基于特征级别的KL散度自适应地选择LoRA权重，并利用指标引导的梯度校正来改进生成轨迹。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schrödinger Bridge Matching",
        "summary": "Diffusion models often yield highly curved trajectories and noisy score targets due to an uninformative, memoryless forward process that induces independent data-noise coupling. We propose Adjoint Schrödinger Bridge Matching (ASBM), a generative modeling framework that recovers optimal trajectories in high dimensions via two stages. First, we view the Schrödinger Bridge (SB) forward dynamic as a coupling construction problem and learn it through a data-to-energy sampling perspective that transports data to an energy-defined prior. Then, we learn the backward generative dynamic with a simple matching loss supervised by the induced optimal coupling. By operating in a non-memoryless regime, ASBM produces significantly straighter and more efficient sampling paths. Compared to prior works, ASBM scales to high-dimensional data with notably improved stability and efficiency. Extensive experiments on image generation show that ASBM improves fidelity with fewer sampling steps. We further showcase the effectiveness of our optimal trajectory via distillation to a one-step generator.",
        "url": "http://arxiv.org/abs/2602.15396v1",
        "published_date": "2026-02-17T07:06:20+00:00",
        "updated_date": "2026-02-17T07:06:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jeongwoo Shin",
            "Jinhwan Sul",
            "Joonseok Lee",
            "Jaewong Choi",
            "Jaemoo Choi"
        ],
        "tldr": "The paper introduces Adjoint Schrödinger Bridge Matching (ASBM), a method for generative modeling that learns optimal data trajectories using a non-memoryless forward process, resulting in improved fidelity and efficiency, especially for high-dimensional image generation.",
        "tldr_zh": "该论文介绍了伴随薛定谔桥匹配（ASBM），一种生成建模方法，它使用非无记忆前向过程来学习最佳的数据轨迹，尤其是在高维图像生成上，从而提高了保真度和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GMAIL: Generative Modality Alignment for generated Image Learning",
        "summary": "Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves performance on image captioning, zero-shot image retrieval, zero-shot image classification, and long caption retrieval tasks. It also shows positive generated data scaling trends and notable enhancements in the captioning performance of the large multimodal model, LLaVA.",
        "url": "http://arxiv.org/abs/2602.15368v1",
        "published_date": "2026-02-17T05:40:25+00:00",
        "updated_date": "2026-02-17T05:40:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Shentong Mo",
            "Sukmin Yun"
        ],
        "tldr": "The paper introduces GMAIL, a framework that aligns generated images with real images in a latent space to improve the performance of vision-language models trained with synthetic data, showing significant improvements across various tasks.",
        "tldr_zh": "该论文介绍了GMAIL，一个将生成的图像与真实图像在潜在空间中对齐的框架，旨在提升使用合成数据训练的视觉-语言模型的性能，并在各项任务中显示出显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Consistency-Preserving Diverse Video Generation",
        "summary": "Text-to-video generation is expensive, so only a few samples are typically produced per prompt. In this low-sample regime, maximizing the value of each batch requires high cross-video diversity. Recent methods improve diversity for image generation, but for videos they often degrade within-video temporal consistency and require costly backpropagation through a video decoder. We propose a joint-sampling framework for flow-matching video generators that improves batch diversity while preserving temporal consistency. Our approach applies diversity-driven updates and then removes only the components that would decrease a temporal-consistency objective. To avoid image-space gradients, we compute both objectives with lightweight latent-space models, avoiding video decoding and decoder backpropagation. Experiments on a state-of-the-art text-to-video flow-matching model show diversity comparable to strong joint-sampling baselines while substantially improving temporal consistency and color naturalness. Code will be released.",
        "url": "http://arxiv.org/abs/2602.15287v1",
        "published_date": "2026-02-17T01:12:20+00:00",
        "updated_date": "2026-02-17T01:12:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinshuang Liu",
            "Runfa Blark Li",
            "Truong Nguyen"
        ],
        "tldr": "This paper introduces a joint-sampling framework for text-to-video generation that improves batch diversity and temporal consistency without requiring costly backpropagation through a video decoder.",
        "tldr_zh": "本文提出了一种文本到视频生成的联合采样框架，该框架在提高批次多样性和时间一致性的同时，避免了通过视频解码器进行代价高昂的反向传播。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Image Generation with a Sphere Encoder",
        "summary": "We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .",
        "url": "http://arxiv.org/abs/2602.15030v1",
        "published_date": "2026-02-16T18:59:57+00:00",
        "updated_date": "2026-02-16T18:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaiyu Yue",
            "Menglin Jia",
            "Ji Hou",
            "Tom Goldstein"
        ],
        "tldr": "The paper introduces a Sphere Encoder for single-pass image generation, achieving competitive results with diffusion models at a fraction of the inference cost by mapping images onto a spherical latent space.",
        "tldr_zh": "该论文介绍了一种用于单次图像生成的球形编码器（Sphere Encoder），通过将图像映射到球形潜在空间，以远低于扩散模型的推理成本实现了与之媲美的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]