[
    {
        "title": "PanFlow: Decoupled Motion Control for Panoramic Video Generation",
        "summary": "Panoramic video generation has attracted growing attention due to its applications in virtual reality and immersive media. However, existing methods lack explicit motion control and struggle to generate scenes with large and complex motions. We propose PanFlow, a novel approach that exploits the spherical nature of panoramas to decouple the highly dynamic camera rotation from the input optical flow condition, enabling more precise control over large and dynamic motions. We further introduce a spherical noise warping strategy to promote loop consistency in motion across panorama boundaries. To support effective training, we curate a large-scale, motion-rich panoramic video dataset with frame-level pose and flow annotations. We also showcase the effectiveness of our method in various applications, including motion transfer and video editing. Extensive experiments demonstrate that PanFlow significantly outperforms prior methods in motion fidelity, visual quality, and temporal coherence. Our code, dataset, and models are available at https://github.com/chengzhag/PanFlow.",
        "url": "http://arxiv.org/abs/2512.00832v1",
        "published_date": "2025-11-30T11:03:31+00:00",
        "updated_date": "2025-11-30T11:03:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cheng Zhang",
            "Hanwen Liang",
            "Donny Y. Chen",
            "Qianyi Wu",
            "Konstantinos N. Plataniotis",
            "Camilo Cruz Gambardella",
            "Jianfei Cai"
        ],
        "tldr": "PanFlow introduces a novel method for panoramic video generation with decoupled motion control and a spherical noise warping strategy, achieving superior performance in motion fidelity and visual quality, supported by a new large-scale dataset.",
        "tldr_zh": "PanFlow 提出了一种新颖的全景视频生成方法，通过解耦运动控制和球形噪声扭曲策略，在运动保真度和视觉质量方面实现了卓越的性能，并由一个新的大规模数据集支持。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Charts Are Not Images: On the Challenges of Scientific Chart Editing",
        "summary": "Generative models, such as diffusion and autoregressive approaches, have demonstrated impressive capabilities in editing natural images. However, applying these tools to scientific charts rests on a flawed assumption: a chart is not merely an arrangement of pixels but a visual representation of structured data governed by a graphical grammar. Consequently, chart editing is not a pixel-manipulation task but a structured transformation problem. To address this fundamental mismatch, we introduce \\textit{FigEdit}, a large-scale benchmark for scientific figure editing comprising over 30,000 samples. Grounded in real-world data, our benchmark is distinguished by its diversity, covering 10 distinct chart types and a rich vocabulary of complex editing instructions. The benchmark is organized into five distinct and progressively challenging tasks: single edits, multi edits, conversational edits, visual-guidance-based edits, and style transfer. Our evaluation of a range of state-of-the-art models on this benchmark reveals their poor performance on scientific figures, as they consistently fail to handle the underlying structured transformations required for valid edits. Furthermore, our analysis indicates that traditional evaluation metrics (e.g., SSIM, PSNR) have limitations in capturing the semantic correctness of chart edits. Our benchmark demonstrates the profound limitations of pixel-level manipulation and provides a robust foundation for developing and evaluating future structure-aware models. By releasing \\textit{FigEdit} (https://github.com/adobe-research/figure-editing), we aim to enable systematic progress in structure-aware figure editing, provide a common ground for fair comparison, and encourage future research on models that understand both the visual and semantic layers of scientific charts.",
        "url": "http://arxiv.org/abs/2512.00752v1",
        "published_date": "2025-11-30T06:13:48+00:00",
        "updated_date": "2025-11-30T06:13:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shawn Li",
            "Ryan Rossi",
            "Sungchul Kim",
            "Sunav Choudhary",
            "Franck Dernoncourt",
            "Puneet Mathur",
            "Zhengzhong Tu",
            "Yue Zhao"
        ],
        "tldr": "The paper introduces FigEdit, a large-scale benchmark for scientific figure editing, highlighting the limitations of current generative models that treat charts as mere pixel arrangements and advocating for structure-aware approaches.",
        "tldr_zh": "该论文介绍了 FigEdit，一个用于科学图表编辑的大规模基准，强调了当前生成模型将图表视为像素排列的局限性，并提倡结构感知的处理方法。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-GRPO: Multi-Group Advantage Estimation for Text-to-Image Generation with Tree-Based Trajectories and Multiple Rewards",
        "summary": "Recently, Group Relative Policy Optimization (GRPO) has shown promising potential for aligning text-to-image (T2I) models, yet existing GRPO-based methods suffer from two critical limitations. (1) \\textit{Shared credit assignment}: trajectory-level advantages derived from group-normalized sparse terminal rewards are uniformly applied across timesteps, failing to accurately estimate the potential of early denoising steps with vast exploration spaces. (2) \\textit{Reward-mixing}: predefined weights for combining multi-objective rewards (e.g., text accuracy, visual quality, text color)--which have mismatched scales and variances--lead to unstable gradients and conflicting updates. To address these issues, we propose \\textbf{Multi-GRPO}, a multi-group advantage estimation framework with two orthogonal grouping mechanisms. For better credit assignment, we introduce tree-based trajectories inspired by Monte Carlo Tree Search: branching trajectories at selected early denoising steps naturally forms \\emph{temporal groups}, enabling accurate advantage estimation for early steps via descendant leaves while amortizing computation through shared prefixes. For multi-objective optimization, we introduce \\emph{reward-based grouping} to compute advantages for each reward function \\textit{independently} before aggregation, disentangling conflicting signals. To facilitate evaluation of multiple objective alignment, we curate \\textit{OCR-Color-10}, a visual text rendering dataset with explicit color constraints. Across the single-reward \\textit{PickScore-25k} and multi-objective \\textit{OCR-Color-10} benchmarks, Multi-GRPO achieves superior stability and alignment performance, effectively balancing conflicting objectives. Code will be publicly available at \\href{https://github.com/fikry102/Multi-GRPO}{https://github.com/fikry102/Multi-GRPO}.",
        "url": "http://arxiv.org/abs/2512.00743v1",
        "published_date": "2025-11-30T05:44:35+00:00",
        "updated_date": "2025-11-30T05:44:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiang Lyu",
            "Zicong Chen",
            "Chongxiao Wang",
            "Haolin Shi",
            "Shibo Gao",
            "Ran Piao",
            "Youwei Zeng",
            "Jianlou Si",
            "Fei Ding",
            "Jing Li",
            "Chun Pong Lau",
            "Weiqiang Wang"
        ],
        "tldr": "The paper introduces Multi-GRPO, a novel framework for text-to-image generation that improves credit assignment and multi-objective reward handling through tree-based trajectories and reward-based grouping, demonstrating superior performance on single- and multi-objective benchmarks.",
        "tldr_zh": "该论文介绍了Multi-GRPO，一种用于文本到图像生成的新框架，通过基于树的轨迹和基于奖励的分组改进了信用分配和多目标奖励处理，并在单目标和多目标基准测试中展示了卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer",
        "summary": "Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a training-free text-driven 4D editing framework leveraging Multimodal Diffusion Transformer (MM-DiT) and 4DGS. This mechanism consists of Spatio-Temporal Sub-Grid Attention (STGA) for locally consistent cross-view and temporal fusion, and Context Token Propagation (CTP) for global propagation via token inheritance and optical-flow-guided token replacement. Together, these components allow Dynamic-eDiTor to perform seamless, globally consistent multi-view video without additional training and directly optimize pre-trained source 4DGS. Extensive experiments on multi-view video dataset DyNeRF demonstrate that our method achieves superior editing fidelity and both multi-view and temporal consistency prior approaches. Project page for results and code: https://di-lee.github.io/dynamic-eDiTor/",
        "url": "http://arxiv.org/abs/2512.00677v1",
        "published_date": "2025-11-30T00:18:46+00:00",
        "updated_date": "2025-11-30T00:18:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dong In Lee",
            "Hyungjun Doh",
            "Seunggeun Chi",
            "Runlin Duan",
            "Sangpil Kim",
            "Karthik Ramani"
        ],
        "tldr": "The paper introduces Dynamic-eDiTor, a training-free framework for text-driven 4D scene editing using a Multimodal Diffusion Transformer and 4DGS, ensuring multi-view and temporal consistency.",
        "tldr_zh": "该论文介绍了Dynamic-eDiTor，一个无需训练的框架，用于文本驱动的4D场景编辑，利用多模态扩散Transformer和4DGS，确保多视角和时间一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]