[
    {
        "title": "EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning",
        "summary": "Recent advances in foundation models highlight a clear trend toward\nunification and scaling, showing emergent capabilities across diverse domains.\nWhile image generation and editing have rapidly transitioned from task-specific\nto unified frameworks, video generation and editing remain fragmented due to\narchitectural limitations and data scarcity. In this work, we introduce\nEditVerse, a unified framework for image and video generation and editing\nwithin a single model. By representing all modalities, i.e., text, image, and\nvideo, as a unified token sequence, EditVerse leverages self-attention to\nachieve robust in-context learning, natural cross-modal knowledge transfer, and\nflexible handling of inputs and outputs with arbitrary resolutions and\ndurations. To address the lack of video editing training data, we design a\nscalable data pipeline that curates 232K video editing samples and combines\nthem with large-scale image and video datasets for joint training. Furthermore,\nwe present EditVerseBench, the first benchmark for instruction-based video\nediting covering diverse tasks and resolutions. Extensive experiments and user\nstudies demonstrate that EditVerse achieves state-of-the-art performance,\nsurpassing existing open-source and commercial models, while exhibiting\nemergent editing and generation abilities across modalities.",
        "url": "http://arxiv.org/abs/2509.20360v1",
        "published_date": "2025-09-24T17:59:30+00:00",
        "updated_date": "2025-09-24T17:59:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuan Ju",
            "Tianyu Wang",
            "Yuqian Zhou",
            "He Zhang",
            "Qing Liu",
            "Nanxuan Zhao",
            "Zhifei Zhang",
            "Yijun Li",
            "Yuanhao Cai",
            "Shaoteng Liu",
            "Daniil Pakhomov",
            "Zhe Lin",
            "Soo Ye Kim",
            "Qiang Xu"
        ],
        "tldr": "The paper introduces EditVerse, a unified framework for image and video generation and editing using in-context learning and a large curated dataset, achieving state-of-the-art performance across modalities.",
        "tldr_zh": "该论文介绍了EditVerse，一个统一的图像和视频生成与编辑框架，它利用上下文学习和一个大型的整理数据集，在各种模态上实现了最先进的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation",
        "summary": "Existing video generation models excel at producing photo-realistic videos\nfrom text or images, but often lack physical plausibility and 3D\ncontrollability. To overcome these limitations, we introduce PhysCtrl, a novel\nframework for physics-grounded image-to-video generation with physical\nparameters and force control. At its core is a generative physics network that\nlearns the distribution of physical dynamics across four materials (elastic,\nsand, plasticine, and rigid) via a diffusion model conditioned on physics\nparameters and applied forces. We represent physical dynamics as 3D point\ntrajectories and train on a large-scale synthetic dataset of 550K animations\ngenerated by physics simulators. We enhance the diffusion model with a novel\nspatiotemporal attention block that emulates particle interactions and\nincorporates physics-based constraints during training to enforce physical\nplausibility. Experiments show that PhysCtrl generates realistic,\nphysics-grounded motion trajectories which, when used to drive image-to-video\nmodels, yield high-fidelity, controllable videos that outperform existing\nmethods in both visual quality and physical plausibility. Project Page:\nhttps://cwchenwang.github.io/physctrl",
        "url": "http://arxiv.org/abs/2509.20358v1",
        "published_date": "2025-09-24T17:58:04+00:00",
        "updated_date": "2025-09-24T17:58:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Wang",
            "Chuhao Chen",
            "Yiming Huang",
            "Zhiyang Dou",
            "Yuan Liu",
            "Jiatao Gu",
            "Lingjie Liu"
        ],
        "tldr": "PhysCtrl is a framework for generating physically plausible and controllable videos by learning physics dynamics with a diffusion model, using physics parameters and applied forces as control signals. It outperforms existing methods in visual quality and physical plausibility.",
        "tldr_zh": "PhysCtrl是一个用于生成物理上合理且可控视频的框架，它通过扩散模型学习物理动力学，并使用物理参数和外力作为控制信号。在视觉质量和物理合理性方面，该方法优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "4D Driving Scene Generation With Stereo Forcing",
        "summary": "Current generative models struggle to synthesize dynamic 4D driving scenes\nthat simultaneously support temporal extrapolation and spatial novel view\nsynthesis (NVS) without per-scene optimization. Bridging generation and novel\nview synthesis remains a major challenge. We present PhiGenesis, a unified\nframework for 4D scene generation that extends video generation techniques with\ngeometric and temporal consistency. Given multi-view image sequences and camera\nparameters, PhiGenesis produces temporally continuous 4D Gaussian splatting\nrepresentations along target 3D trajectories. In its first stage, PhiGenesis\nleverages a pre-trained video VAE with a novel range-view adapter to enable\nfeed-forward 4D reconstruction from multi-view images. This architecture\nsupports single-frame or video inputs and outputs complete 4D scenes including\ngeometry, semantics, and motion. In the second stage, PhiGenesis introduces a\ngeometric-guided video diffusion model, using rendered historical 4D scenes as\npriors to generate future views conditioned on trajectories. To address\ngeometric exposure bias in novel views, we propose Stereo Forcing, a novel\nconditioning strategy that integrates geometric uncertainty during denoising.\nThis method enhances temporal coherence by dynamically adjusting generative\ninfluence based on uncertainty-aware perturbations. Our experimental results\ndemonstrate that our method achieves state-of-the-art performance in both\nappearance and geometric reconstruction, temporal generation and novel view\nsynthesis (NVS) tasks, while simultaneously delivering competitive performance\nin downstream evaluations. Homepage is at\n\\href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}.",
        "url": "http://arxiv.org/abs/2509.20251v1",
        "published_date": "2025-09-24T15:37:17+00:00",
        "updated_date": "2025-09-24T15:37:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Lu",
            "Zhuang Ma",
            "Guangfeng Jiang",
            "Wenhang Ge",
            "Bohan Li",
            "Yuzhan Cai",
            "Wenzhao Zheng",
            "Yunpeng Zhang",
            "Yingcong Chen"
        ],
        "tldr": "The paper introduces PhiGenesis, a unified framework for 4D driving scene generation using video generation techniques with geometric and temporal consistency, achieving state-of-the-art performance in multiple tasks.",
        "tldr_zh": "该论文介绍了一种名为PhiGenesis的统一框架，用于生成具有几何和时间一致性的4D驾驶场景，该框架利用视频生成技术，并在多项任务中达到了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion",
        "summary": "Recently, camera-controlled video generation has seen rapid development,\noffering more precise control over video generation. However, existing methods\npredominantly focus on camera control in perspective projection video\ngeneration, while geometrically consistent panoramic video generation remains\nchallenging. This limitation is primarily due to the inherent complexities in\npanoramic pose representation and spherical projection. To address this issue,\nwe propose CamPVG, the first diffusion-based framework for panoramic video\ngeneration guided by precise camera poses. We achieve camera position encoding\nfor panoramic images and cross-view feature aggregation based on spherical\nprojection. Specifically, we propose a panoramic Pl\\\"ucker embedding that\nencodes camera extrinsic parameters through spherical coordinate\ntransformation. This pose encoder effectively captures panoramic geometry,\novercoming the limitations of traditional methods when applied to\nequirectangular projections. Additionally, we introduce a spherical epipolar\nmodule that enforces geometric constraints through adaptive attention masking\nalong epipolar lines. This module enables fine-grained cross-view feature\naggregation, substantially enhancing the quality and consistency of generated\npanoramic videos. Extensive experiments demonstrate that our method generates\nhigh-quality panoramic videos consistent with camera trajectories, far\nsurpassing existing methods in panoramic video generation.",
        "url": "http://arxiv.org/abs/2509.19979v1",
        "published_date": "2025-09-24T10:34:24+00:00",
        "updated_date": "2025-09-24T10:34:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenhao Ji",
            "Chaohui Yu",
            "Junyao Gao",
            "Fan Wang",
            "Cairong Zhao"
        ],
        "tldr": "The paper introduces CamPVG, a diffusion-based framework for generating panoramic videos controlled by camera poses, employing a panoramic Plücker embedding and a spherical epipolar module for enhanced geometric consistency and quality.",
        "tldr_zh": "本文介绍了CamPVG，一个基于扩散模型的全景视频生成框架，通过相机姿态控制，并采用全景Plücker嵌入和球面对极模块来增强几何一致性和质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding",
        "summary": "Audio-driven talking face generation has received growing interest,\nparticularly for applications requiring expressive and natural human-avatar\ninteraction. However, most existing emotion-aware methods rely on a single\nmodality (either audio or image) for emotion embedding, limiting their ability\nto capture nuanced affective cues. Additionally, most methods condition on a\nsingle reference image, restricting the model's ability to represent dynamic\nchanges in actions or attributes across time. To address these issues, we\nintroduce SynchroRaMa, a novel framework that integrates a multi-modal emotion\nembedding by combining emotional signals from text (via sentiment analysis) and\naudio (via speech-based emotion recognition and audio-derived valence-arousal\nfeatures), enabling the generation of talking face videos with richer and more\nauthentic emotional expressiveness and fidelity. To ensure natural head motion\nand accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M)\nmodule that generates motion frames aligned with the input audio. Finally,\nSynchroRaMa incorporates scene descriptions generated by Large Language Model\n(LLM) as additional textual input, enabling it to capture dynamic actions and\nhigh-level semantic attributes. Conditioning the model on both visual and\ntextual cues enhances temporal consistency and visual realism. Quantitative and\nqualitative experiments on benchmark datasets demonstrate that SynchroRaMa\noutperforms the state-of-the-art, achieving improvements in image quality,\nexpression preservation, and motion realism. A user study further confirms that\nSynchroRaMa achieves higher subjective ratings than competing methods in\noverall naturalness, motion diversity, and video smoothness. Our project page\nis available at <https://novicemm.github.io/synchrorama>.",
        "url": "http://arxiv.org/abs/2509.19965v1",
        "published_date": "2025-09-24T10:21:29+00:00",
        "updated_date": "2025-09-24T10:21:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Phyo Thet Yee",
            "Dimitrios Kollias",
            "Sudeepta Mishra",
            "Abhinav Dhall"
        ],
        "tldr": "SynchroRaMa is a novel talking face generation framework that uses multi-modal emotion embedding (text and audio), audio-to-motion, and LLM-generated scene descriptions to generate emotionally expressive and natural videos with improved lip synchronization and head motion.",
        "tldr_zh": "SynchroRaMa是一个新颖的口型同步说话人脸生成框架，它利用多模态情感嵌入（文本和音频）、音频到运动转换以及LLM生成的场景描述，生成具有更丰富情感表达和自然感的视频，并改进了口型同步和头部运动。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes",
        "summary": "This paper presents GS-RoadPatching, an inpainting method for driving scene\ncompletion by referring to completely reconstructed regions, which are\nrepresented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting\nmethods that perform generative completion relying on 2D perspective-view-based\ndiffusion or GAN models to predict limited appearance or depth cues for missing\nregions, our approach enables substitutional scene inpainting and editing\ndirectly through the 3DGS modality, extricating it from requiring\nspatial-temporal consistency of 2D cross-modals and eliminating the need for\ntime-intensive retraining of Gaussians. Our key insight is that the highly\nrepetitive patterns in driving scenes often share multi-modal similarities\nwithin the implicit 3DGS feature space and are particularly suitable for\nstructural matching to enable effective 3DGS-based substitutional inpainting.\nPractically, we construct feature-embedded 3DGS scenes to incorporate a patch\nmeasurement method for abstracting local context at different scales and,\nsubsequently, propose a structural search method to find candidate patches in\n3D space effectively. Finally, we propose a simple yet effective\nsubstitution-and-fusion optimization for better visual harmony. We conduct\nextensive experiments on multiple publicly available datasets to demonstrate\nthe effectiveness and efficiency of our proposed method in driving scenes, and\nthe results validate that our method achieves state-of-the-art performance\ncompared to the baseline methods in terms of both quality and interoperability.\nAdditional experiments in general scenes also demonstrate the applicability of\nthe proposed 3D inpainting strategy. The project page and code are available\nat: https://shanzhaguoo.github.io/GS-RoadPatching/",
        "url": "http://arxiv.org/abs/2509.19937v1",
        "published_date": "2025-09-24T09:44:37+00:00",
        "updated_date": "2025-09-24T09:44:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guo Chen",
            "Jiarun Liu",
            "Sicong Du",
            "Chenming Wu",
            "Deqi Li",
            "Shi-Sheng Huang",
            "Guofeng Zhang",
            "Sheng Yang"
        ],
        "tldr": "GS-RoadPatching introduces a 3DGS inpainting method for driving scenes, utilizing substitutional scene inpainting via 3D searching and placing, achieving state-of-the-art performance.",
        "tldr_zh": "GS-RoadPatching 提出了一种用于驾驶场景的 3DGS 修复方法，通过 3D 搜索和放置实现替代场景修复，并达到最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CAMILA: Context-Aware Masking for Image Editing with Language Alignment",
        "summary": "Text-guided image editing has been allowing users to transform and synthesize\nimages through natural language instructions, offering considerable\nflexibility. However, most existing image editing models naively attempt to\nfollow all user instructions, even if those instructions are inherently\ninfeasible or contradictory, often resulting in nonsensical output. To address\nthese challenges, we propose a context-aware method for image editing named as\nCAMILA (Context-Aware Masking for Image Editing with Language Alignment).\nCAMILA is designed to validate the contextual coherence between instructions\nand the image, ensuring that only relevant edits are applied to the designated\nregions while ignoring non-executable instructions. For comprehensive\nevaluation of this new method, we constructed datasets for both single- and\nmulti-instruction image editing, incorporating the presence of infeasible\nrequests. Our method achieves better performance and higher semantic alignment\nthan state-of-the-art models, demonstrating its effectiveness in handling\ncomplex instruction challenges while preserving image integrity.",
        "url": "http://arxiv.org/abs/2509.19731v1",
        "published_date": "2025-09-24T03:20:44+00:00",
        "updated_date": "2025-09-24T03:20:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyunseung Kim",
            "Chiho Choi",
            "Srikanth Malla",
            "Sai Prahladh Padmanabhan",
            "Saurabh Bagchi",
            "Joon Hee Choi"
        ],
        "tldr": "CAMILA is a context-aware image editing method that validates instruction coherence with the image, selectively applying edits and handling infeasible requests, leading to improved performance and semantic alignment.",
        "tldr_zh": "CAMILA 是一种上下文感知的图像编辑方法，它验证指令与图像的一致性，选择性地应用编辑并处理不可行的请求，从而提高性能和语义对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]