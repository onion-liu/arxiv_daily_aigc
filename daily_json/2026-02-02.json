[
    {
        "title": "Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning",
        "summary": "A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the \"creative essence\" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar (\"G\"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.",
        "url": "http://arxiv.org/abs/2602.01335v1",
        "published_date": "2026-02-01T17:01:36+00:00",
        "updated_date": "2026-02-01T17:01:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yu Xu",
            "Yuxin Zhang",
            "Juan Cao",
            "Lin Gao",
            "Chunyu Wang",
            "Oliver Deussen",
            "Tong-Yee Lee",
            "Fan Tang"
        ],
        "tldr": "The paper introduces Visual Metaphor Transfer (VMT), a new task where a model autonomously transfers the core concept of a reference image onto a user-defined subject using a schema-driven, multi-agent framework inspired by Conceptual Blending Theory. Experiments show the method outperforms SOTA baselines.",
        "tldr_zh": "本文提出了视觉隐喻迁移（VMT）任务，模型使用受概念混合理论启发的模式驱动多智能体框架，自主地将参考图像的核心概念转移到用户定义的主题上。实验表明该方法优于SOTA基线。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FlowCast: Trajectory Forecasting for Scalable Zero-Cost Speculative Flow Matching",
        "summary": "Flow Matching (FM) has recently emerged as a powerful approach for high-quality visual generation. However, their prohibitively slow inference due to a large number of denoising steps limits their potential use in real-time or interactive applications. Existing acceleration methods, like distillation, truncation, or consistency training, either degrade quality, incur costly retraining, or lack generalization. We propose FlowCast, a training-free speculative generation framework that accelerates inference by exploiting the fact that FM models are trained to preserve constant velocity. FlowCast speculates future velocity by extrapolating current velocity without incurring additional time cost, and accepts it if it is within a mean-squared error threshold. This constant-velocity forecasting allows redundant steps in stable regions to be aggressively skipped while retaining precision in complex ones. FlowCast is a plug-and-play framework that integrates seamlessly with any FM model and requires no auxiliary networks. We also present a theoretical analysis and bound the worst-case deviation between speculative and full FM trajectories. Empirical evaluations demonstrate that FlowCast achieves $>2.5\\times$ speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss as compared to standard full generation.",
        "url": "http://arxiv.org/abs/2602.01329v1",
        "published_date": "2026-02-01T16:50:15+00:00",
        "updated_date": "2026-02-01T16:50:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Divya Jyoti Bajpai",
            "Shubham Agarwal",
            "Apoorv Saxena",
            "Kuldeep Kulkarni",
            "Subrata Mitra",
            "Manjesh Kumar Hanawal"
        ],
        "tldr": "FlowCast is a training-free speculative generation framework that accelerates Flow Matching inference by extrapolating velocity, achieving significant speedups in image and video generation without quality loss.",
        "tldr_zh": "FlowCast是一个无需训练的推测性生成框架，通过外推速度来加速Flow Matching的推理，在图像和视频生成中实现了显著的加速，且没有质量损失。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ReDiStory: Region-Disentangled Diffusion for Consistent Visual Story Generation",
        "summary": "Generating coherent visual stories requires maintaining subject identity across multiple images while preserving frame-specific semantics. Recent training-free methods concatenate identity and frame prompts into a unified representation, but this often introduces inter-frame semantic interference that weakens identity preservation in complex stories. We propose ReDiStory, a training-free framework that improves multi-frame story generation via inference-time prompt embedding reorganization. ReDiStory explicitly decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames. This reduces cross-frame interference without modifying diffusion parameters or requiring additional supervision. Under identical diffusion backbones and inference settings, ReDiStory improves identity consistency while maintaining prompt fidelity. Experiments on the ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics. Code is available at: https://github.com/YuZhenyuLindy/ReDiStory",
        "url": "http://arxiv.org/abs/2602.01303v1",
        "published_date": "2026-02-01T16:04:40+00:00",
        "updated_date": "2026-02-01T16:04:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ayushman Sarkar",
            "Zhenyu Yu",
            "Chu Chen",
            "Wei Tang",
            "Kangning Cui",
            "Mohd Yamani Idna Idris"
        ],
        "tldr": "The paper presents ReDiStory, a training-free framework for consistent visual story generation that improves identity preservation by explicitly decomposing and decorrelating text embeddings at inference time to reduce cross-frame semantic interference.",
        "tldr_zh": "该论文提出了ReDiStory，一个用于一致视觉故事生成的免训练框架。它通过在推理时显式分解和解耦文本嵌入来减少跨帧语义干扰，从而提高身份保持能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models",
        "summary": "Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.",
        "url": "http://arxiv.org/abs/2602.01289v1",
        "published_date": "2026-02-01T15:45:07+00:00",
        "updated_date": "2026-02-01T15:45:07+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Dung Anh Hoang",
            "Cuong Pham anh Trung Le",
            "Jianfei Cai",
            "Toan Do"
        ],
        "tldr": "This paper introduces a novel post-training quantization (PTQ) method for diffusion models that learns to assign optimal weights to calibration samples to align gradients across timesteps, improving quantization performance.",
        "tldr_zh": "本文提出了一种新的扩散模型后训练量化(PTQ)方法，该方法学习为校准样本分配最佳权重，以对齐各个时间步长的梯度，从而提高量化性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers",
        "summary": "Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.",
        "url": "http://arxiv.org/abs/2602.01077v1",
        "published_date": "2026-02-01T07:47:06+00:00",
        "updated_date": "2026-02-01T07:47:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haopeng Li",
            "Shitong Shao",
            "Wenliang Zhong",
            "Zikai Zhou",
            "Lichen Bai",
            "Hui Xiong",
            "Zeke Xie"
        ],
        "tldr": "The paper introduces Piecewise Sparse Attention (PISA), a training-free method for efficient diffusion transformers in video and image generation, achieving speedups by approximating non-critical attention blocks using Taylor expansion while maintaining high quality.",
        "tldr_zh": "该论文介绍了一种名为分段稀疏注意力（PISA）的免训练方法，用于高效地进行视频和图像生成中的扩散变换，通过使用泰勒展开逼近非关键注意力块来实现加速，同时保持高质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models",
        "summary": "Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/",
        "url": "http://arxiv.org/abs/2602.00883v1",
        "published_date": "2026-01-31T20:08:46+00:00",
        "updated_date": "2026-01-31T20:08:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Alicja Polowczyk",
            "Agnieszka Polowczyk",
            "Piotr Borycki",
            "Joanna Waczyńska",
            "Jacek Tabor",
            "Przemysław Spurek"
        ],
        "tldr": "DIAMOND is a training-free inference method for flow matching and diffusion models that mitigates visual artifacts by actively steering the generative process, leading to higher fidelity images without retraining.",
        "tldr_zh": "\"DIAMOND\"是一种无需训练的推理方法，适用于流匹配和扩散模型，通过主动引导生成过程来减轻视觉伪影，从而无需重新训练即可获得更高保真度的图像。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation",
        "summary": "Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics.",
        "url": "http://arxiv.org/abs/2602.01352v1",
        "published_date": "2026-02-01T17:42:53+00:00",
        "updated_date": "2026-02-01T17:42:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xingzu Zhan",
            "Chen Xie",
            "Honghang Chen",
            "Yixun Lin",
            "Xiaochun Mai"
        ],
        "tldr": "The paper introduces T2M Mamba, a new approach for text-to-motion generation addressing limitations in existing models by considering the coupling between motion periodicity and keyframe saliency and improving robustness to paraphrases through a Periodicity-Saliency Aware Mamba and a Periodic Differential Cross-modal Alignment Module (PDCAM).",
        "tldr_zh": "本文介绍了一种新的文本到动作生成方法 T2M Mamba，通过考虑运动周期性和关键帧显著性之间的耦合以及通过周期性-显著性感知 Mamba 和周期性差分跨模态对齐模块 (PDCAM) 提高对释义的鲁棒性，从而解决现有模型的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Adaptive Visual Autoregressive Acceleration via Dual-Linkage Entropy Analysis",
        "summary": "Visual AutoRegressive modeling (VAR) suffers from substantial computational cost due to the massive token count involved. Failing to account for the continuous evolution of modeling dynamics, existing VAR token reduction methods face three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, thereby leaving significant acceleration potential untapped. Since entropy variation intrinsically reflects the transition of predictive uncertainty, it offers a principled measure to capture modeling dynamics evolution. Therefore, we propose NOVA, a training-free token reduction acceleration framework for VAR models via entropy analysis. NOVA adaptively determines the acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, NOVA dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing the cache derived from the residuals at the prior scale to accelerate inference and maintain generation quality. Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework.",
        "url": "http://arxiv.org/abs/2602.01345v1",
        "published_date": "2026-02-01T17:29:42+00:00",
        "updated_date": "2026-02-01T17:29:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Zhang",
            "Jingyi Liu",
            "Feng Liu",
            "Duoqian Miao",
            "Qi Zhang",
            "Kexue Fu",
            "Changwei Wang",
            "Longbing Cao"
        ],
        "tldr": "This paper introduces NOVA, a training-free token reduction framework for Visual AutoRegressive models that adaptively accelerates inference by analyzing entropy variation to prune low-entropy tokens, enhancing efficiency without sacrificing generation quality.",
        "tldr_zh": "本文介绍了一种名为NOVA的视觉自回归模型（VAR）的免训练令牌缩减框架。通过分析熵变，自适应地加速推理过程，从而裁剪低熵令牌，提高效率且不牺牲生成质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]