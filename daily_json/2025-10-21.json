[
    {
        "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing",
        "summary": "Recent advances in training-free attention control methods have enabled\nflexible and efficient text-guided editing capabilities for existing generation\nmodels. However, current approaches struggle to simultaneously deliver strong\nediting strength while preserving consistency with the source. This limitation\nbecomes particularly critical in multi-round and video editing, where visual\nerrors can accumulate over time. Moreover, most existing methods enforce global\nconsistency, which limits their ability to modify individual attributes such as\ntexture while preserving others, thereby hindering fine-grained editing.\nRecently, the architectural shift from U-Net to MM-DiT has brought significant\nimprovements in generative performance and introduced a novel mechanism for\nintegrating text and vision modalities. These advancements pave the way for\novercoming challenges that previous methods failed to resolve. Through an\nin-depth analysis of MM-DiT, we identify three key insights into its attention\nmechanisms. Building on these, we propose ConsistEdit, a novel attention\ncontrol method specifically tailored for MM-DiT. ConsistEdit incorporates\nvision-only attention control, mask-guided pre-attention fusion, and\ndifferentiated manipulation of the query, key, and value tokens to produce\nconsistent, prompt-aligned edits. Extensive experiments demonstrate that\nConsistEdit achieves state-of-the-art performance across a wide range of image\nand video editing tasks, including both structure-consistent and\nstructure-inconsistent scenarios. Unlike prior methods, it is the first\napproach to perform editing across all inference steps and attention layers\nwithout handcraft, significantly enhancing reliability and consistency, which\nenables robust multi-round and multi-region editing. Furthermore, it supports\nprogressive adjustment of structural consistency, enabling finer control.",
        "url": "http://arxiv.org/abs/2510.17803v1",
        "published_date": "2025-10-20T17:59:52+00:00",
        "updated_date": "2025-10-20T17:59:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixin Yin",
            "Ling-Hao Chen",
            "Lionel Ni",
            "Xili Dai"
        ],
        "tldr": "The paper introduces ConsistEdit, a training-free attention control method tailored for MM-DiT, enabling consistent and precise text-guided image and video editing with fine-grained control and state-of-the-art performance.",
        "tldr_zh": "该论文介绍了ConsistEdit，一种为MM-DiT量身定制的无需训练的注意力控制方法，能够实现一致且精确的文本引导图像和视频编辑，具有细粒度控制和最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Can Image-To-Video Models Simulate Pedestrian Dynamics?",
        "summary": "Recent high-performing image-to-video (I2V) models based on variants of the\ndiffusion transformer (DiT) have displayed remarkable inherent world-modeling\ncapabilities by virtue of training on large scale video datasets. We\ninvestigate whether these models can generate realistic pedestrian movement\npatterns in crowded public scenes. Our framework conditions I2V models on\nkeyframes extracted from pedestrian trajectory benchmarks, then evaluates their\ntrajectory prediction performance using quantitative measures of pedestrian\ndynamics.",
        "url": "http://arxiv.org/abs/2510.17731v1",
        "published_date": "2025-10-20T16:44:40+00:00",
        "updated_date": "2025-10-20T16:44:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aaron Appelle",
            "Jerome P. Lynch"
        ],
        "tldr": "The paper explores the ability of image-to-video diffusion transformer models to simulate realistic pedestrian movement in crowded scenes by conditioning them on keyframes from trajectory benchmarks and evaluating their trajectory prediction performance.",
        "tldr_zh": "该论文研究了基于图像到视频扩散变换器模型在模拟拥挤场景中行人运动的真实性。通过使用行人轨迹基准数据集中的关键帧作为条件，并评估其轨迹预测性能来实现。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver",
        "summary": "While diffusion models achieve state-of-the-art generation quality, they\nstill suffer from computationally expensive sampling. Recent works address this\nissue with gradient-based optimization methods that distill a few-step ODE\ndiffusion solver from the full sampling process, reducing the number of\nfunction evaluations from dozens to just a few. However, these approaches often\nrely on intricate training techniques and do not explicitly focus on preserving\nfine-grained details. In this paper, we introduce the Generalized Solver: a\nsimple parameterization of the ODE sampler that does not require additional\ntraining tricks and improves quality over existing approaches. We further\ncombine the original distillation loss with adversarial training, which\nmitigates artifacts and enhances detail fidelity. We call the resulting method\nthe Generalized Adversarial Solver and demonstrate its superior performance\ncompared to existing solver training methods under similar resource\nconstraints. Code is available at https://github.com/3145tttt/GAS.",
        "url": "http://arxiv.org/abs/2510.17699v1",
        "published_date": "2025-10-20T16:14:38+00:00",
        "updated_date": "2025-10-20T16:14:38+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Aleksandr Oganov",
            "Ilya Bykov",
            "Eva Neudachina",
            "Mishan Aliev",
            "Alexander Tolmachev",
            "Alexander Sidorov",
            "Aleksandr Zuev",
            "Andrey Okhotin",
            "Denis Rakitin",
            "Aibek Alanov"
        ],
        "tldr": "The paper introduces a novel, training-trick-free ODE solver parameterization for diffusion models, named Generalized Adversarial Solver (GAS), which aims to improve sampling quality and reduce artifacts compared to existing methods by combining distillation loss and adversarial training.",
        "tldr_zh": "该论文介绍了一种新型的、无需训练技巧的扩散模型ODE求解器参数化方法，名为广义对抗求解器（GAS），旨在通过结合蒸馏损失和对抗训练，提高采样质量并减少伪影，优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PICABench: How Far Are We from Physically Realistic Image Editing?",
        "summary": "Image editing has achieved remarkable progress recently. Modern editing\nmodels could already follow complex instructions to manipulate the original\ncontent. However, beyond completing the editing instructions, the accompanying\nphysical effects are the key to the generation realism. For example, removing\nan object should also remove its shadow, reflections, and interactions with\nnearby objects. Unfortunately, existing models and benchmarks mainly focus on\ninstruction completion but overlook these physical effects. So, at this moment,\nhow far are we from physically realistic image editing? To answer this, we\nintroduce PICABench, which systematically evaluates physical realism across\neight sub-dimension (spanning optics, mechanics, and state transitions) for\nmost of the common editing operations (add, remove, attribute change, etc). We\nfurther propose the PICAEval, a reliable evaluation protocol that uses\nVLM-as-a-judge with per-case, region-level human annotations and questions.\nBeyond benchmarking, we also explore effective solutions by learning physics\nfrom videos and construct a training dataset PICA-100K. After evaluating most\nof the mainstream models, we observe that physical realism remains a\nchallenging problem with large rooms to explore. We hope that our benchmark and\nproposed solutions can serve as a foundation for future work moving from naive\ncontent editing toward physically consistent realism.",
        "url": "http://arxiv.org/abs/2510.17681v1",
        "published_date": "2025-10-20T15:53:57+00:00",
        "updated_date": "2025-10-20T15:53:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuandong Pu",
            "Le Zhuo",
            "Songhao Han",
            "Jinbo Xing",
            "Kaiwen Zhu",
            "Shuo Cao",
            "Bin Fu",
            "Si Liu",
            "Hongsheng Li",
            "Yu Qiao",
            "Wenlong Zhang",
            "Xi Chen",
            "Yihao Liu"
        ],
        "tldr": "The paper introduces PICABench, a new benchmark for evaluating the physical realism of image editing models, highlighting the gap between instruction completion and physically plausible results. It also explores solutions and provides a dataset to encourage research in this area.",
        "tldr_zh": "该论文介绍了PICABench，一个新的用于评估图像编辑模型物理真实性的基准，突出了指令完成和物理上合理的结果之间的差距。它还探索了解决方案并提供了一个数据集，以鼓励在该领域的研究。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input",
        "summary": "Human communication combines speech with expressive nonverbal cues such as\nhand gestures that serve manifold communicative functions. Yet, current\ngenerative gesture generation approaches are restricted to simple, repetitive\nbeat gestures that accompany the rhythm of speaking but do not contribute to\ncommunicating semantic meaning. This paper tackles a core challenge in\nco-speech gesture synthesis: generating iconic or deictic gestures that are\nsemantically coherent with a verbal utterance. Such gestures cannot be derived\nfrom language input alone, which inherently lacks the visual meaning that is\noften carried autonomously by gestures. We therefore introduce a zero-shot\nsystem that generates gestures from a given language input and additionally is\ninformed by imagistic input, without manual annotation or human intervention.\nOur method integrates an image analysis pipeline that extracts key object\nproperties such as shape, symmetry, and alignment, together with a semantic\nmatching module that links these visual details to spoken text. An inverse\nkinematics engine then synthesizes iconic and deictic gestures and combines\nthem with co-generated natural beat gestures for coherent multimodal\ncommunication. A comprehensive user study demonstrates the effectiveness of our\napproach. In scenarios where speech alone was ambiguous, gestures generated by\nour system significantly improved participants' ability to identify object\nproperties, confirming their interpretability and communicative value. While\nchallenges remain in representing complex shapes, our results highlight the\nimportance of context-aware semantic gestures for creating expressive and\ncollaborative virtual agents or avatars, marking a substantial step forward\ntowards efficient and robust, embodied human-agent interaction. More\ninformation and example videos are available here:\nhttps://review-anon-io.github.io/ImaGGen.github.io/",
        "url": "http://arxiv.org/abs/2510.17617v1",
        "published_date": "2025-10-20T15:01:56+00:00",
        "updated_date": "2025-10-20T15:01:56+00:00",
        "categories": [
            "cs.HC",
            "cs.CV"
        ],
        "authors": [
            "Hendric Voss",
            "Stefan Kopp"
        ],
        "tldr": "The paper introduces ImaGGen, a zero-shot system for generating co-speech iconic and deictic gestures grounded in both language and image inputs, significantly improving object property identification in user studies.",
        "tldr_zh": "该论文介绍了ImaGGen，一个零样本系统，用于生成基于语言和图像输入的、与语音同步的标志性和指示性手势，并在用户研究中显著提高了物体属性识别能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models",
        "summary": "In recent years, large-scale generative models for visual content\n(\\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable\nprogress. However, training large-scale video generation models remains\nparticularly challenging and resource-intensive due to cross-modal text-video\nalignment, the long sequences involved, and the complex spatiotemporal\ndependencies. To address these challenges, we present a training framework that\noptimizes four pillars: (i) data processing, (ii) model architecture, (iii)\ntraining strategy, and (iv) infrastructure for large-scale video generation\nmodels. These optimizations delivered significant efficiency gains and\nperformance improvements across all stages of data preprocessing, video\ncompression, parameter scaling, curriculum-based pretraining, and\nalignment-focused post-training. Our resulting model, MUG-V 10B, matches recent\nstate-of-the-art video generators overall and, on e-commerce-oriented video\ngeneration tasks, surpasses leading open-source baselines in human evaluations.\nMore importantly, we open-source the complete stack, including model weights,\nMegatron-Core-based large-scale training code, and inference pipelines for\nvideo generation and enhancement. To our knowledge, this is the first public\nrelease of large-scale video generation training code that exploits\nMegatron-Core to achieve high training efficiency and near-linear multi-node\nscaling, details are available in\n\\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.",
        "url": "http://arxiv.org/abs/2510.17519v1",
        "published_date": "2025-10-20T13:20:37+00:00",
        "updated_date": "2025-10-20T13:20:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yongshun Zhang",
            "Zhongyi Fan",
            "Yonghang Zhang",
            "Zhangzikang Li",
            "Weifeng Chen",
            "Zhongwei Feng",
            "Chaoyue Wang",
            "Peng Hou",
            "Anxiang Zeng"
        ],
        "tldr": "The paper introduces MUG-V 10B, a highly efficient training pipeline for large video generation models, and open-sources the complete stack, including model weights and training code.",
        "tldr_zh": "该论文介绍了MUG-V 10B，一个用于大型视频生成模型的高效训练管道，并开源了包括模型权重和训练代码在内的完整堆栈。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models",
        "summary": "Recent advances in video diffusion models have significantly enhanced\ntext-to-video generation, particularly through alignment tuning using reward\nmodels trained on human preferences. While these methods improve visual\nquality, they can unintentionally encode and amplify social biases. To\nsystematically trace how such biases evolve throughout the alignment pipeline,\nwe introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating\nsocial representation in video generation. Grounded in established social bias\ntaxonomies, VideoBiasEval employs an event-based prompting strategy to\ndisentangle semantic content (actions and contexts) from actor attributes\n(gender and ethnicity). It further introduces multi-granular metrics to\nevaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,\n(3) distributional shifts in social attributes across model variants, and (4)\nthe temporal persistence of bias within videos. Using this framework, we\nconduct the first end-to-end analysis connecting biases in human preference\ndatasets, their amplification in reward models, and their propagation through\nalignment-tuned video diffusion models. Our results reveal that alignment\ntuning not only strengthens representational biases but also makes them\ntemporally stable, producing smoother yet more stereotyped portrayals. These\nfindings highlight the need for bias-aware evaluation and mitigation throughout\nthe alignment process to ensure fair and socially responsible video generation.",
        "url": "http://arxiv.org/abs/2510.17247v1",
        "published_date": "2025-10-20T07:37:43+00:00",
        "updated_date": "2025-10-20T07:37:43+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Zefan Cai",
            "Haoyi Qiu",
            "Haozhe Zhao",
            "Ke Wan",
            "Jiachen Li",
            "Jiuxiang Gu",
            "Wen Xiao",
            "Nanyun Peng",
            "Junjie Hu"
        ],
        "tldr": "This paper introduces VideoBiasEval, a framework for analyzing social biases in video diffusion models, and demonstrates how alignment tuning exacerbates these biases, emphasizing the need for bias-aware evaluation and mitigation strategies.",
        "tldr_zh": "本文介绍了VideoBiasEval，一个用于分析视频扩散模型中社会偏见的框架，并展示了对齐调整如何加剧这些偏见，强调了在整个对齐过程中进行偏见感知评估和缓解策略的必要性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling",
        "summary": "Masked Autoregressive (MAR) models promise better efficiency in visual\ngeneration than autoregressive (AR) models for the ability of parallel\ngeneration, yet their acceleration potential remains constrained by the\nmodeling complexity of spatially correlated visual tokens in a single step. To\naddress this limitation, we introduce Generation then Reconstruction (GtR), a\ntraining-free hierarchical sampling strategy that decomposes generation into\ntwo stages: structure generation establishing global semantic scaffolding,\nfollowed by detail reconstruction efficiently completing remaining tokens.\nAssuming that it is more difficult to create an image from scratch than to\ncomplement images based on a basic image framework, GtR is designed to achieve\nacceleration by computing the reconstruction stage quickly while maintaining\nthe generation quality by computing the generation stage slowly. Moreover,\nobserving that tokens on the details of an image often carry more semantic\ninformation than tokens in the salient regions, we further propose\nFrequency-Weighted Token Selection (FTS) to offer more computation budget to\ntokens on image details, which are localized based on the energy of high\nfrequency information. Extensive experiments on ImageNet class-conditional and\ntext-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining\ncomparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),\nsubstantially outperforming existing acceleration methods across various model\nscales and generation tasks. Our codes will be released in\nhttps://github.com/feihongyan1/GtR.",
        "url": "http://arxiv.org/abs/2510.17171v1",
        "published_date": "2025-10-20T05:22:10+00:00",
        "updated_date": "2025-10-20T05:22:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feihong Yan",
            "Peiru Wang",
            "Yao Zhu",
            "Kaiyu Pang",
            "Qingyan Wei",
            "Huiqi Li",
            "Linfeng Zhang"
        ],
        "tldr": "The paper introduces a two-stage sampling strategy, Generation then Reconstruction (GtR), for accelerating Masked Autoregressive (MAR) models in image generation, achieving significant speedups while maintaining comparable quality by prioritizing detail reconstruction and using frequency-weighted token selection.",
        "tldr_zh": "该论文提出了一种两阶段采样策略，即生成后重建（GtR），用于加速图像生成中的掩蔽自回归（MAR）模型。通过优先考虑细节重建和使用频率加权Token选择，在保持相当质量的同时，实现了显著的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "One-step Diffusion Models with Bregman Density Ratio Matching",
        "summary": "Diffusion and flow models achieve high generative quality but remain\ncomputationally expensive due to slow multi-step sampling. Distillation methods\naccelerate them by training fast student generators, yet most existing\nobjectives lack a unified theoretical foundation. In this work, we propose\nDi-Bregman, a compact framework that formulates diffusion distillation as\nBregman divergence-based density-ratio matching. This convex-analytic view\nconnects several existing objectives through a common lens. Experiments on\nCIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves\nimproved one-step FID over reverse-KL distillation and maintains high visual\nfidelity compared to the teacher model. Our results highlight Bregman\ndensity-ratio matching as a practical and theoretically-grounded route toward\nefficient one-step diffusion generation.",
        "url": "http://arxiv.org/abs/2510.16983v1",
        "published_date": "2025-10-19T20:00:54+00:00",
        "updated_date": "2025-10-19T20:00:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yuanzhi Zhu",
            "Eleftherios Tsonis",
            "Lucas Degeorge",
            "Vicky Kalogeiton"
        ],
        "tldr": "The paper introduces Di-Bregman, a new framework for one-step diffusion model distillation based on Bregman divergence-based density-ratio matching, achieving improved FID and visual fidelity compared to existing methods.",
        "tldr_zh": "该论文介绍了一种名为 Di-Bregman 的新框架，用于基于 Bregman 散度密度比匹配的单步扩散模型蒸馏，与现有方法相比，实现了更高的 FID 和视觉保真度。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "FineVision: Open Data Is All You Need",
        "summary": "The advancement of vision-language models (VLMs) is hampered by a fragmented\nlandscape of inconsistent and contaminated public datasets. We introduce\nFineVision, a meticulously collected, curated, and unified corpus of 24 million\nsamples - the largest open resource of its kind. We unify more than 200 sources\ninto 185 subsets via a semi-automated, human-in-the-loop pipeline: automation\nperforms bulk ingestion and schema mapping, while reviewers audit mappings and\nspot-check outputs to verify faithful consumption of annotations, appropriate\nformatting and diversity, and safety; issues trigger targeted fixes and\nre-runs. The workflow further applies rigorous de-duplication within and across\nsources and decontamination against 66 public benchmarks. FineVision also\nencompasses agentic/GUI tasks with a unified action space; reviewers validate\nschemas and inspect a sample of trajectories to confirm executable fidelity.\nModels trained on FineVision consistently outperform those trained on existing\nopen mixtures across a broad evaluation suite, underscoring the benefits of\nscale, data hygiene, and balanced automation with human oversight. We release\nthe corpus and curation tools to accelerate data-centric VLM research.",
        "url": "http://arxiv.org/abs/2510.17269v1",
        "published_date": "2025-10-20T07:54:46+00:00",
        "updated_date": "2025-10-20T07:54:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Luis Wiedmann",
            "Orr Zohar",
            "Amir Mahla",
            "Xiaohan Wang",
            "Rui Li",
            "Thibaud Frere",
            "Leandro von Werra",
            "Aritra Roy Gosthipaty",
            "Andrés Marafioti"
        ],
        "tldr": "FineVision is a large, meticulously curated, and unified open dataset of 24 million vision-language samples designed to improve VLM training. Models trained on it outperform those trained on existing open mixtures.",
        "tldr_zh": "FineVision是一个大型、精心策划和统一的开放数据集，包含2400万个视觉-语言样本，旨在改善VLM训练。基于此训练的模型优于基于现有开放混合数据训练的模型。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement",
        "summary": "Diffusion-based methods, leveraging pre-trained large models like Stable\nDiffusion via ControlNet, have achieved remarkable performance in several\nlow-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods\noften sacrifice content fidelity to attain higher perceptual realism. This\nissue is exacerbated in low-light scenarios, where severely degraded\ninformation caused by the darkness limits effective control. We identify two\nprimary causes of fidelity loss: the absence of suitable conditional latent\nmodeling and the lack of bidirectional interaction between the conditional\nlatent and noisy latent in the diffusion process. To address this, we propose a\nnovel optimization strategy for conditioning in pre-trained diffusion models,\nenhancing fidelity while preserving realism and aesthetics. Our method\nintroduces a mechanism to recover spatial details lost during VAE encoding,\ni.e., a latent refinement pipeline incorporating generative priors.\nAdditionally, the refined latent condition interacts dynamically with the noisy\nlatent, leading to improved restoration performance. Our approach is\nplug-and-play, seamlessly integrating into existing diffusion networks to\nprovide more effective control. Extensive experiments demonstrate significant\nfidelity improvements in PTDB methods.",
        "url": "http://arxiv.org/abs/2510.17105v1",
        "published_date": "2025-10-20T02:40:06+00:00",
        "updated_date": "2025-10-20T02:40:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaogang Xu",
            "Jian Wang",
            "Yunfan Lu",
            "Ruihang Chu",
            "Ruixing Wang",
            "Jiafei Wu",
            "Bei Yu",
            "Liang Lin"
        ],
        "tldr": "This paper addresses fidelity loss in pre-trained diffusion-based low-light image enhancement by refining the condition latent and introducing bidirectional interaction, improving restoration performance without sacrificing perceptual realism.",
        "tldr_zh": "该论文通过细化条件潜变量和引入双向交互，解决了预训练扩散模型在低光图像增强中保真度损失的问题，从而在不牺牲感知真实感的前提下，提高了图像恢复性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation",
        "summary": "Articulated objects, such as laptops and drawers, exhibit significant\nchallenges for 3D reconstruction and pose estimation due to their multi-part\ngeometries and variable joint configurations, which introduce structural\ndiversity across different states. To address these challenges, we propose\nKineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object\nShape Reconstruction and Generation, a unified framework for reconstructing\ndiverse articulated instances and pose estimation from single view input.\nSpecifically, we first encode complete geometry (SDFs), joint angles, and part\nsegmentation into a structured latent space via a novel Kinematic-Aware VAE\n(KA-VAE). In addition, we employ two conditional diffusion models: one for\nregressing global pose (SE(3)) and joint parameters, and another for generating\nthe kinematic-aware latent code from partial observations. Finally, we produce\nan iterative optimization module that bidirectionally refines reconstruction\naccuracy and kinematic parameters via Chamfer-distance minimization while\npreserving articulation constraints. Experimental results on synthetic,\nsemi-synthetic, and real-world datasets demonstrate the effectiveness of our\napproach in accurately reconstructing articulated objects and estimating their\nkinematic properties.",
        "url": "http://arxiv.org/abs/2510.17137v1",
        "published_date": "2025-10-20T04:15:40+00:00",
        "updated_date": "2025-10-20T04:15:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "WenBo Xu",
            "Liu Liu",
            "Li Zhang",
            "Ran Zhang",
            "Hao Wu",
            "Dan Guo",
            "Meng Wang"
        ],
        "tldr": "The paper introduces KineDiff3D, a diffusion-based framework for reconstructing and estimating the pose of articulated objects from single view input, leveraging a kinematic-aware VAE and conditional diffusion models.",
        "tldr_zh": "该论文介绍了KineDiff3D，一个基于扩散模型的框架，用于从单视图输入重建和估计铰接对象的姿态，利用了运动学感知的VAE和条件扩散模型。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]