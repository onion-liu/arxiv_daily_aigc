[
    {
        "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model",
        "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.",
        "url": "http://arxiv.org/abs/2512.22096v1",
        "published_date": "2025-12-26T17:52:49+00:00",
        "updated_date": "2025-12-26T17:52:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaofeng Mao",
            "Zhen Li",
            "Chuanhao Li",
            "Xiaojie Xu",
            "Kaining Ying",
            "Tong He",
            "Jiangmiao Pang",
            "Yu Qiao",
            "Kaipeng Zhang"
        ],
        "tldr": "The paper introduces Yume-1.5, a novel framework for generating interactive and explorable worlds from text or image prompts, addressing limitations in parameter size, inference speed, and context handling of existing methods by using context compression, real-time streaming acceleration, and text-controlled event generation.",
        "tldr_zh": "本文介绍了Yume-1.5，一种从文本或图像提示生成交互式和可探索世界的新框架。该框架通过使用上下文压缩、实时流加速和文本控制的事件生成，解决了现有方法在参数大小、推理速度和上下文处理方面的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer",
        "summary": "Recent progress in diffusion models has significantly advanced the field of human image animation. While existing methods can generate temporally consistent results for short or regular motions, significant challenges remain, particularly in generating long-duration videos. Furthermore, the synthesis of fine-grained facial and hand details remains under-explored, limiting the applicability of current approaches in real-world, high-quality applications. To address these limitations, we propose a diffusion transformer (DiT)-based framework which focuses on generating high-fidelity and long-duration human animation videos. First, we design a set of hybrid implicit guidance signals and a sharpness guidance factor, enabling our framework to additionally incorporate detailed facial and hand features as guidance. Next, we incorporate the time-aware position shift fusion module, modify the input format within the DiT backbone, and refer to this mechanism as the Position Shift Adaptive Module, which enables video generation of arbitrary length. Finally, we introduce a novel data augmentation strategy and a skeleton alignment model to reduce the impact of human shape variations across different identities. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving superior performance in both high-fidelity and long-duration human image animation.",
        "url": "http://arxiv.org/abs/2512.21905v1",
        "published_date": "2025-12-26T07:36:48+00:00",
        "updated_date": "2025-12-26T07:36:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shen Zheng",
            "Jiaran Cai",
            "Yuansheng Guan",
            "Shenneng Huang",
            "Xingpei Ma",
            "Junjie Cao",
            "Hanfeng Zhao",
            "Qiang Zhang",
            "Shunsi Zhang",
            "Xiao-Ping Zhang"
        ],
        "tldr": "This paper introduces a Diffusion Transformer (DiT) based framework to generate high-fidelity and long-duration human animation videos, addressing limitations of existing methods in handling long sequences and fine-grained details.",
        "tldr_zh": "本文介绍了一种基于扩散Transformer (DiT) 的框架，用于生成高保真度和长时程的人体动画视频，解决了现有方法在处理长序列和精细细节方面的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation",
        "summary": "Decoder-only autoregressive image generation typically relies on fixed-length tokenization schemes whose token counts grow quadratically with resolution, substantially increasing the computational and memory demands of attention. We present DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into a variable number of patches for efficient image generation. Our work is the first to demonstrate that next-token prediction entropy from a lightweight and unsupervised autoregressive model provides a reliable criterion for merging tokens into larger patches based on information content. DPAR makes minimal modifications to the standard decoder architecture, ensuring compatibility with multimodal generation frameworks and allocating more compute to generation of high-information image regions. Further, we demonstrate that training with dynamically sized patches yields representations that are robust to patch boundaries, allowing DPAR to scale to larger patch sizes at inference. DPAR reduces token count by 1.81x and 2.06x on Imagenet 256 and 384 generation resolution respectively, leading to a reduction of up to 40% FLOPs in training costs. Further, our method exhibits faster convergence and improves FID by up to 27.1% relative to baseline models.",
        "url": "http://arxiv.org/abs/2512.21867v1",
        "published_date": "2025-12-26T05:03:47+00:00",
        "updated_date": "2025-12-26T05:03:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Divyansh Srivastava",
            "Akshay Mehra",
            "Pranav Maneriker",
            "Debopam Sanyal",
            "Vishnu Raj",
            "Vijay Kamarshi",
            "Fan Du",
            "Joshua Kimball"
        ],
        "tldr": "The paper introduces DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into patches based on information content, reducing computational costs and improving image generation quality.",
        "tldr_zh": "该论文介绍了DPAR，一种新的仅解码器的自回归模型，该模型基于信息内容动态地将图像tokens聚合为patches，从而降低了计算成本并提高了图像生成质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fast Inference of Visual Autoregressive Model with Adjacency-Adaptive Dynamical Draft Trees",
        "summary": "Autoregressive (AR) image models achieve diffusion-level quality but suffer from sequential inference, requiring approximately 2,000 steps for a 576x576 image. Speculative decoding with draft trees accelerates LLMs yet underperforms on visual AR models due to spatially varying token prediction difficulty. We identify a key obstacle in applying speculative decoding to visual AR models: inconsistent acceptance rates across draft trees due to varying prediction difficulties in different image regions. We propose Adjacency-Adaptive Dynamical Draft Trees (ADT-Tree), an adjacency-adaptive dynamic draft tree that dynamically adjusts draft tree depth and width by leveraging adjacent token states and prior acceptance rates. ADT-Tree initializes via horizontal adjacency, then refines depth/width via bisectional adaptation, yielding deeper trees in simple regions and wider trees in complex ones. The empirical evaluations on MS-COCO 2017 and PartiPrompts demonstrate that ADT-Tree achieves speedups of 3.13xand 3.05x, respectively. Moreover, it integrates seamlessly with relaxed sampling methods such as LANTERN, enabling further acceleration. Code is available at https://github.com/Haodong-Lei-Ray/ADT-Tree.",
        "url": "http://arxiv.org/abs/2512.21857v1",
        "published_date": "2025-12-26T04:45:49+00:00",
        "updated_date": "2025-12-26T04:45:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haodong Lei",
            "Hongsong Wang",
            "Xin Geng",
            "Liang Wang",
            "Pan Zhou"
        ],
        "tldr": "The paper introduces Adjacency-Adaptive Dynamical Draft Trees (ADT-Tree), a method to accelerate the inference speed of visual autoregressive models by dynamically adjusting draft tree depth and width based on token prediction difficulty, achieving significant speedups on image generation tasks.",
        "tldr_zh": "本文介绍了邻接自适应动态草稿树（ADT-Tree），该方法通过根据token预测的难度动态调整草稿树的深度和宽度，来加速视觉自回归模型的推理速度，并在图像生成任务上实现了显著的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation",
        "summary": "Parameter-Efficient Fine-Tuning of Diffusion Transformers (DiTs) for diverse, multi-conditional tasks often suffers from task interference when using monolithic adapters like LoRA. The Mixture of Low-rank Experts (MoLE) architecture offers a modular solution, but its potential is usually limited by routing policies that operate at a token level. Such local routing can conflict with the global nature of user instructions, leading to artifacts like spatial fragmentation and semantic drift in complex image generation tasks. To address these limitations, we introduce InstructMoLE, a novel framework that employs an Instruction-Guided Mixture of Low-Rank Experts. Instead of per-token routing, InstructMoLE utilizes a global routing signal, Instruction-Guided Routing (IGR), derived from the user's comprehensive instruction. This ensures that a single, coherently chosen expert council is applied uniformly across all input tokens, preserving the global semantics and structural integrity of the generation process. To complement this, we introduce an output-space orthogonality loss, which promotes expert functional diversity and mitigates representational collapse. Extensive experiments demonstrate that InstructMoLE significantly outperforms existing LoRA adapters and MoLE variants across challenging multi-conditional generation benchmarks. Our work presents a robust and generalizable framework for instruction-driven fine-tuning of generative models, enabling superior compositional control and fidelity to user intent.",
        "url": "http://arxiv.org/abs/2512.21788v1",
        "published_date": "2025-12-25T21:37:12+00:00",
        "updated_date": "2025-12-25T21:37:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jinqi Xiao",
            "Qing Yan",
            "Liming Jiang",
            "Zichuan Liu",
            "Hao Kang",
            "Shen Sang",
            "Tiancheng Zhi",
            "Jing Liu",
            "Cheng Yang",
            "Xin Lu",
            "Bo Yuan"
        ],
        "tldr": "InstructMoLE introduces an instruction-guided approach for mixture of experts in diffusion transformers, improving multi-conditional image generation by using global routing based on user instructions and promoting expert diversity with an orthogonality loss.",
        "tldr_zh": "InstructMoLE 提出了一种指令引导的专家混合方法，用于扩散 Transformer，通过使用基于用户指令的全局路由和正交性损失来提高专家多样性，从而改进多条件图像生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Inference-based GAN Video Generation",
        "summary": "Video generation has seen remarkable progresses thanks to advancements in generative deep learning. Generated videos should not only display coherent and continuous movement but also meaningful movement in successions of scenes. Generating models such as Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs) and more recently Diffusion Networks have been used for generating short video sequences, usually of up to 16 frames. In this paper, we first propose a new type of video generator by enabling adversarial-based unconditional video generators with a variational encoder, akin to a VAE-GAN hybrid structure, in order to enable the generation process with inference capabilities. The proposed model, as in other video deep learning-based processing frameworks, incorporates two processing branches, one for content and another for movement. However, existing models struggle with the temporal scaling of the generated videos. In classical approaches when aiming to increase the generated video length, the resulting video quality degrades, particularly when considering generating significantly long sequences. To overcome this limitation, our research study extends the initially proposed VAE-GAN video generation model by employing a novel, memory-efficient approach to generate long videos composed of hundreds or thousands of frames ensuring their temporal continuity, consistency and dynamics. Our approach leverages a Markov chain framework with a recall mechanism, with each state representing a VAE-GAN short-length video generator. This setup allows for the sequential connection of generated video sub-sequences, enabling temporal dependencies, resulting in meaningful long video sequences.",
        "url": "http://arxiv.org/abs/2512.21776v1",
        "published_date": "2025-12-25T20:14:38+00:00",
        "updated_date": "2025-12-25T20:14:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jingbo Yang",
            "Adrian G. Bors"
        ],
        "tldr": "This paper introduces a VAE-GAN hybrid video generation model using a Markov chain framework with a recall mechanism to generate long, temporally consistent videos, addressing the limitations of existing models in scaling video length.",
        "tldr_zh": "本文介绍了一种VAE-GAN混合视频生成模型，该模型使用带有召回机制的马尔科夫链框架来生成长时间且时间一致的视频，从而解决了现有模型在扩展视频长度方面的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]