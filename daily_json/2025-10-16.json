[
    {
        "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching",
        "summary": "Next-generation multimodal foundation models capable of any-to-any\ncross-modal generation and multi-turn interaction will serve as core components\nof artificial general intelligence systems, playing a pivotal role in\nhuman-machine interaction. However, most existing multimodal models remain\nconstrained by autoregressive architectures, whose inherent limitations prevent\na balanced integration of understanding and generation capabilities. Although\nhybrid and decoupling strategies have been explored to address these tasks\nwithin unified frameworks separately, their redundant, non-integrated designs\nlimit their applicability to broader scenarios, such as cross-modal\nretrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal\nfoundation model that achieves unified modeling through discrete flow\nparadigms. By leveraging metric-induced probability paths and kinetic optimal\nvelocities, NExT-OMNI natively supports any-to-any understanding and generation\nwith enhanced response efficiency, while enabling broader application scenarios\nthrough concise unified representations rather than task-decoupled designs.\nTrained on large-scale interleaved text, image, video, and audio data,\nNExT-OMNI delivers competitive performance on multimodal generation and\nunderstanding benchmarks, while outperforming prior unified models in\nmulti-turn multimodal interaction and cross-modal retrieval, highlighting its\narchitectural advantages as a next-generation multimodal foundation model. To\nadvance further research, we release training details, data protocols, and\nopen-source both the code and model checkpoints.",
        "url": "http://arxiv.org/abs/2510.13721v1",
        "published_date": "2025-10-15T16:25:18+00:00",
        "updated_date": "2025-10-15T16:25:18+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Run Luo",
            "Xiaobo Xia",
            "Lu Wang",
            "Longze Chen",
            "Renke Shan",
            "Jing Luo",
            "Min Yang",
            "Tat-Seng Chua"
        ],
        "tldr": "The paper introduces NExT-OMNI, an open-source omnimodal foundation model using discrete flow matching for any-to-any cross-modal generation and understanding, claiming superior performance in multi-turn interaction and cross-modal retrieval compared to existing unified models.",
        "tldr_zh": "该论文介绍了NExT-OMNI，一个开源的通用模态基础模型，使用离散流匹配实现任意模态之间的生成和理解。论文声称，与现有的统一模型相比，该模型在多轮交互和跨模态检索方面表现更佳。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
        "summary": "We propose FlashWorld, a generative model that produces 3D scenes from a\nsingle image or text prompt in seconds, 10~100$\\times$ faster than previous\nworks while possessing superior rendering quality. Our approach shifts from the\nconventional multi-view-oriented (MV-oriented) paradigm, which generates\nmulti-view images for subsequent 3D reconstruction, to a 3D-oriented approach\nwhere the model directly produces 3D Gaussian representations during multi-view\ngeneration. While ensuring 3D consistency, 3D-oriented method typically suffers\npoor visual quality. FlashWorld includes a dual-mode pre-training phase\nfollowed by a cross-mode post-training phase, effectively integrating the\nstrengths of both paradigms. Specifically, leveraging the prior from a video\ndiffusion model, we first pre-train a dual-mode multi-view diffusion model,\nwhich jointly supports MV-oriented and 3D-oriented generation modes. To bridge\nthe quality gap in 3D-oriented generation, we further propose a cross-mode\npost-training distillation by matching distribution from consistent 3D-oriented\nmode to high-quality MV-oriented mode. This not only enhances visual quality\nwhile maintaining 3D consistency, but also reduces the required denoising steps\nfor inference. Also, we propose a strategy to leverage massive single-view\nimages and text prompts during this process to enhance the model's\ngeneralization to out-of-distribution inputs. Extensive experiments demonstrate\nthe superiority and efficiency of our method.",
        "url": "http://arxiv.org/abs/2510.13678v1",
        "published_date": "2025-10-15T15:35:48+00:00",
        "updated_date": "2025-10-15T15:35:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyang Li",
            "Tengfei Wang",
            "Zixiao Gu",
            "Shengchuan Zhang",
            "Chunchao Guo",
            "Liujuan Cao"
        ],
        "tldr": "FlashWorld proposes a novel approach to 3D scene generation that's significantly faster and maintains superior rendering quality by directly generating 3D Gaussians and using a dual-mode training strategy.",
        "tldr_zh": "FlashWorld 提出了一种新颖的 3D 场景生成方法，通过直接生成 3D 高斯函数并采用双模训练策略，显著提高了速度并保持了卓越的渲染质量。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas",
        "summary": "Masked autoregressive models (MAR) have recently emerged as a powerful\nparadigm for image and video generation, combining the flexibility of masked\nmodeling with the potential of continuous tokenizer. However, video MAR models\nsuffer from two major limitations: the slow-start problem, caused by the lack\nof a structured global prior at early sampling stages, and error accumulation\nacross the autoregression in both spatial and temporal dimensions. In this\nwork, we propose CanvasMAR, a novel video MAR model that mitigates these issues\nby introducing a canvas mechanism--a blurred, global prediction of the next\nframe, used as the starting point for masked generation. The canvas provides\nglobal structure early in sampling, enabling faster and more coherent frame\nsynthesis. Furthermore, we introduce compositional classifier-free guidance\nthat jointly enlarges spatial (canvas) and temporal conditioning, and employ\nnoise-based canvas augmentation to enhance robustness. Experiments on the BAIR\nand Kinetics-600 benchmarks demonstrate that CanvasMAR produces high-quality\nvideos with fewer autoregressive steps. Our approach achieves remarkable\nperformance among autoregressive models on Kinetics-600 dataset and rivals\ndiffusion-based methods.",
        "url": "http://arxiv.org/abs/2510.13669v1",
        "published_date": "2025-10-15T15:29:09+00:00",
        "updated_date": "2025-10-15T15:29:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Zian Li",
            "Muhan Zhang"
        ],
        "tldr": "CanvasMAR addresses the slow-start and error accumulation issues in Masked Autoregressive video generation by introducing a canvas mechanism for global prior and compositional classifier-free guidance, achieving state-of-the-art results among autoregressive models.",
        "tldr_zh": "CanvasMAR通过引入画布机制和组合式无分类器引导，解决了掩码自回归视频生成中的慢启动和误差累积问题，在自回归模型中取得了最先进的效果。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "End-to-End Multi-Modal Diffusion Mamba",
        "summary": "Current end-to-end multi-modal models utilize different encoders and decoders\nto process input and output information. This separation hinders the joint\nrepresentation learning of various modalities. To unify multi-modal processing,\nwe propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDM\nutilizes a Mamba-based multi-step selection diffusion model to progressively\ngenerate and refine modality-specific information through a unified variational\nautoencoder for both encoding and decoding. This innovative approach allows MDM\nto achieve superior performance when processing high-dimensional data,\nparticularly in generating high-resolution images and extended text sequences\nsimultaneously. Our evaluations in areas such as image generation, image\ncaptioning, visual question answering, text comprehension, and reasoning tasks\ndemonstrate that MDM significantly outperforms existing end-to-end models\n(MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTA\nmodels like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM's\neffectiveness in unifying multi-modal processes while maintaining computational\nefficiency, establishing a new direction for end-to-end multi-modal\narchitectures.",
        "url": "http://arxiv.org/abs/2510.13253v1",
        "published_date": "2025-10-15T08:03:50+00:00",
        "updated_date": "2025-10-15T08:03:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chunhao Lu",
            "Qiang Lu",
            "Meichen Dong",
            "Jake Luo"
        ],
        "tldr": "The paper introduces Multi-modal Diffusion Mamba (MDM), a novel architecture employing a Mamba-based diffusion model within a unified variational autoencoder for efficient and superior end-to-end multi-modal processing, outperforming existing models in various generation and reasoning tasks.",
        "tldr_zh": "该论文提出了多模态扩散 Mamba (MDM)，一种新颖的架构，采用基于 Mamba 的扩散模型在统一的变分自动编码器中进行高效且卓越的端到端多模态处理，在各种生成和推理任务中优于现有模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning",
        "summary": "Video generation models nowadays are capable of generating visually realistic\nvideos, but often fail to adhere to physical laws, limiting their ability to\ngenerate physically plausible videos and serve as ''world models''. To address\nthis issue, we propose PhysMaster, which captures physical knowledge as a\nrepresentation for guiding video generation models to enhance their\nphysics-awareness. Specifically, PhysMaster is based on the image-to-video task\nwhere the model is expected to predict physically plausible dynamics from the\ninput image. Since the input image provides physical priors like relative\npositions and potential interactions of objects in the scenario, we devise\nPhysEncoder to encode physical information from it as an extra condition to\ninject physical knowledge into the video generation process. The lack of proper\nsupervision on the model's physical performance beyond mere appearance\nmotivates PhysEncoder to apply reinforcement learning with human feedback to\nphysical representation learning, which leverages feedback from generation\nmodels to optimize physical representations with Direct Preference Optimization\n(DPO) in an end-to-end manner. PhysMaster provides a feasible solution for\nimproving physics-awareness of PhysEncoder and thus of video generation,\nproving its ability on a simple proxy task and generalizability to wide-ranging\nphysical scenarios. This implies that our PhysMaster, which unifies solutions\nfor various physical processes via representation learning in the reinforcement\nlearning paradigm, can act as a generic and plug-in solution for physics-aware\nvideo generation and broader applications.",
        "url": "http://arxiv.org/abs/2510.13809v1",
        "published_date": "2025-10-15T17:59:59+00:00",
        "updated_date": "2025-10-15T17:59:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sihui Ji",
            "Xi Chen",
            "Xin Tao",
            "Pengfei Wan",
            "Hengshuang Zhao"
        ],
        "tldr": "The paper introduces PhysMaster, a reinforcement learning-based approach that learns physical representations to improve physics-awareness in video generation models, leveraging human feedback and direct preference optimization.",
        "tldr_zh": "该论文介绍了PhysMaster，一种基于强化学习的方法，通过学习物理表征来提高视频生成模型中的物理感知能力，利用人类反馈和直接偏好优化。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
        "summary": "Unified multimodal models aim to jointly enable visual understanding and\ngeneration, yet current benchmarks rarely examine their true integration.\nExisting evaluations either treat the two abilities in isolation or overlook\ntasks that inherently couple them. To address this gap, we present Uni-MMMU, a\ncomprehensive and discipline-aware benchmark that systematically unfolds the\nbidirectional synergy between generation and understanding across eight\nreasoning-centric domains, including science, coding, mathematics, and puzzles.\nEach task is bidirectionally coupled, demanding models to (i) leverage\nconceptual understanding to guide precise visual synthesis, or (ii) utilize\ngeneration as a cognitive scaffold for analytical reasoning. Uni-MMMU\nincorporates verifiable intermediate reasoning steps, unique ground truths, and\na reproducible scoring protocol for both textual and visual outputs. Through\nextensive evaluation of state-of-the-art unified, generation-only, and\nunderstanding-only models, we reveal substantial performance disparities and\ncross-modal dependencies, offering new insights into when and how these\nabilities reinforce one another, and establishing a reliable foundation for\nadvancing unified models.",
        "url": "http://arxiv.org/abs/2510.13759v1",
        "published_date": "2025-10-15T17:10:35+00:00",
        "updated_date": "2025-10-15T17:10:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kai Zou",
            "Ziqi Huang",
            "Yuhao Dong",
            "Shulin Tian",
            "Dian Zheng",
            "Hongbo Liu",
            "Jingwen He",
            "Bin Liu",
            "Yu Qiao",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces Uni-MMMU, a new benchmark designed to evaluate the bidirectional synergy between visual understanding and generation in multimodal models across various reasoning-centric domains.",
        "tldr_zh": "该论文介绍了Uni-MMMU，一个新基准，旨在评估多模态模型在多个以推理为中心的领域中视觉理解和生成之间的双向协同作用。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator",
        "summary": "The rapid progress of large, pretrained models for both visual content\ngeneration and 3D reconstruction opens up new possibilities for text-to-3D\ngeneration. Intuitively, one could obtain a formidable 3D scene generator if\none were able to combine the power of a modern latent text-to-video model as\n\"generator\" with the geometric abilities of a recent (feedforward) 3D\nreconstruction system as \"decoder\". We introduce VIST3A, a general framework\nthat does just that, addressing two main challenges. First, the two components\nmust be joined in a way that preserves the rich knowledge encoded in their\nweights. We revisit model stitching, i.e., we identify the layer in the 3D\ndecoder that best matches the latent representation produced by the\ntext-to-video generator and stitch the two parts together. That operation\nrequires only a small dataset and no labels. Second, the text-to-video\ngenerator must be aligned with the stitched 3D decoder, to ensure that the\ngenerated latents are decodable into consistent, perceptually convincing 3D\nscene geometry. To that end, we adapt direct reward finetuning, a popular\ntechnique for human preference alignment. We evaluate the proposed VIST3A\napproach with different video generators and 3D reconstruction models. All\ntested pairings markedly improve over prior text-to-3D models that output\nGaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also\nenables high-quality text-to-pointmap generation.",
        "url": "http://arxiv.org/abs/2510.13454v1",
        "published_date": "2025-10-15T11:55:08+00:00",
        "updated_date": "2025-10-15T11:55:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyojun Go",
            "Dominik Narnhofer",
            "Goutam Bhat",
            "Prune Truong",
            "Federico Tombari",
            "Konrad Schindler"
        ],
        "tldr": "The paper introduces VIST3A, a framework for text-to-3D generation that stitches a text-to-video generator with a 3D reconstruction network, using model stitching and direct reward finetuning for improved 3D scene generation.",
        "tldr_zh": "该论文介绍了 VIST3A，一个文本到 3D 生成框架，它将文本到视频生成器与 3D 重建网络拼接在一起，使用模型拼接和直接奖励微调来改进 3D 场景生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Ultra High-Resolution Image Inpainting with Patch-Based Content Consistency Adapter",
        "summary": "In this work, we present Patch-Adapter, an effective framework for\nhigh-resolution text-guided image inpainting. Unlike existing methods limited\nto lower resolutions, our approach achieves 4K+ resolution while maintaining\nprecise content consistency and prompt alignment, two critical challenges in\nimage inpainting that intensify with increasing resolution and texture\ncomplexity. Patch-Adapter leverages a two-stage adapter architecture to scale\nthe diffusion model's resolution from 1K to 4K+ without requiring structural\noverhauls: (1) Dual Context Adapter learns coherence between masked and\nunmasked regions at reduced resolutions to establish global structural\nconsistency; and (2) Reference Patch Adapter implements a patch-level attention\nmechanism for full-resolution inpainting, preserving local detail fidelity\nthrough adaptive feature fusion. This dual-stage architecture uniquely\naddresses the scalability gap in high-resolution inpainting by decoupling\nglobal semantics from localized refinement. Experiments demonstrate that\nPatch-Adapter not only resolves artifacts common in large-scale inpainting but\nalso achieves state-of-the-art performance on the OpenImages and\nPhoto-Concept-Bucket datasets, outperforming existing methods in both\nperceptual quality and text-prompt adherence.",
        "url": "http://arxiv.org/abs/2510.13419v1",
        "published_date": "2025-10-15T11:18:24+00:00",
        "updated_date": "2025-10-15T11:18:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianhui Zhang",
            "Sheng Cheng",
            "Qirui Sun",
            "Jia Liu",
            "Wang Luyang",
            "Chaoyu Feng",
            "Chen Fang",
            "Lei Lei",
            "Jue Wang",
            "Shuaicheng Liu"
        ],
        "tldr": "The paper introduces Patch-Adapter, a two-stage adapter architecture for high-resolution (4K+) text-guided image inpainting, achieving state-of-the-art performance by maintaining content consistency and prompt alignment.",
        "tldr_zh": "本文介绍了Patch-Adapter，一种用于高分辨率（4K+）文本引导图像修复的两阶段适配器架构，通过保持内容一致性和提示对齐，实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for Text-to-Image Generation",
        "summary": "Reinforcement learning (RL) has garnered increasing attention in\ntext-to-image (T2I) generation. However, most existing RL approaches are\ntailored to either diffusion models or autoregressive models, overlooking an\nimportant alternative: masked generative models. In this work, we propose\nMask-GRPO, the first method to incorporate Group Relative Policy Optimization\n(GRPO)-based RL into this overlooked paradigm. Our core insight is to redefine\nthe transition probability, which is different from current approaches, and\nformulate the unmasking process as a multi-step decision-making problem. To\nfurther enhance our method, we explore several useful strategies, including\nremoving the KL constraint, applying the reduction strategy, and filtering out\nlow-quality samples. Using Mask-GRPO, we improve a base model, Show-o, with\nsubstantial improvements on standard T2I benchmarks and preference alignment,\noutperforming existing state-of-the-art approaches. The code is available on\nhttps://github.com/xingzhejun/Mask-GRPO",
        "url": "http://arxiv.org/abs/2510.13418v1",
        "published_date": "2025-10-15T11:18:12+00:00",
        "updated_date": "2025-10-15T11:18:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifu Luo",
            "Xinhao Hu",
            "Keyu Fan",
            "Haoyuan Sun",
            "Zeyu Chen",
            "Bo Xia",
            "Tiantian Zhang",
            "Yongzhe Chang",
            "Xueqian Wang"
        ],
        "tldr": "The paper introduces Mask-GRPO, a novel reinforcement learning approach integrated with masked generative models for text-to-image generation, achieving state-of-the-art performance by redefining transition probability and employing various enhancement strategies.",
        "tldr_zh": "该论文介绍了Mask-GRPO，一种新的强化学习方法，它与掩码生成模型相结合用于文本到图像的生成。通过重新定义转移概率并采用各种增强策略，该方法实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation",
        "summary": "Outdoor 3D semantic scene generation produces realistic and semantically rich\nenvironments for applications such as urban simulation and autonomous driving.\nHowever, advances in this direction are constrained by the absence of publicly\navailable, well-annotated datasets. We introduce SketchSem3D, the first\nlarge-scale benchmark for generating 3D outdoor semantic scenes from abstract\nfreehand sketches and pseudo-labeled annotations of satellite images.\nSketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-based\nKITTI-360 (containing LiDAR voxels along with their corresponding sketches and\nannotated satellite images), to enable standardized, rigorous, and diverse\nevaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) that\nsignificantly enhances spatial coherence in outdoor 3D scene generation.\nCymbaDiff imposes structured spatial ordering, explicitly captures cylindrical\ncontinuity and vertical hierarchy, and preserves both physical neighborhood\nrelationships and global context within the generated scenes. Extensive\nexperiments on SketchSem3D demonstrate that CymbaDiff achieves superior\nsemantic consistency, spatial realism, and cross-dataset generalization. The\ncode and dataset will be available at\nhttps://github.com/Lillian-research-hub/CymbaDiff",
        "url": "http://arxiv.org/abs/2510.13245v1",
        "published_date": "2025-10-15T07:47:00+00:00",
        "updated_date": "2025-10-15T07:47:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Li Liang",
            "Bo Miao",
            "Xinyu Wang",
            "Naveed Akhtar",
            "Jordan Vice",
            "Ajmal Mian"
        ],
        "tldr": "The paper introduces SketchSem3D, a new large-scale benchmark for 3D outdoor semantic scene generation from sketches, and CymbaDiff, a novel diffusion model that enhances spatial coherence in generated scenes.",
        "tldr_zh": "该论文介绍了 SketchSem3D，一个用于从草图生成 3D 户外语义场景的大规模基准数据集，以及 CymbaDiff，一种新型扩散模型，可增强生成场景中的空间连贯性。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar Propagation",
        "summary": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality.",
        "url": "http://arxiv.org/abs/2510.13084v1",
        "published_date": "2025-10-15T01:55:32+00:00",
        "updated_date": "2025-10-15T01:55:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yi Zuo",
            "Zitao Wang",
            "Lingling Li",
            "Xu Liu",
            "Fang Liu",
            "Licheng Jiao"
        ],
        "tldr": "Edit-Your-Interest is a lightweight, text-driven video editing method that uses a spatio-temporal feature memory and feature propagation to improve efficiency and visual fidelity compared to existing diffusion model approaches.",
        "tldr_zh": "Edit-Your-Interest 是一种轻量级的文本驱动视频编辑方法，它使用时空特征记忆和特征传播来提高效率和视觉保真度，优于现有的扩散模型方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Counting Hallucinations in Diffusion Models",
        "summary": "Diffusion probabilistic models (DPMs) have demonstrated remarkable progress\nin generative tasks, such as image and video synthesis. However, they still\noften produce hallucinated samples (hallucinations) that conflict with\nreal-world knowledge, such as generating an implausible duplicate cup floating\nbeside another cup. Despite their prevalence, the lack of feasible\nmethodologies for systematically quantifying such hallucinations hinders\nprogress in addressing this challenge and obscures potential pathways for\ndesigning next-generation generative models under factual constraints. In this\nwork, we bridge this gap by focusing on a specific form of hallucination, which\nwe term counting hallucination, referring to the generation of an incorrect\nnumber of instances or structured objects, such as a hand image with six\nfingers, despite such patterns being absent from the training data. To this\nend, we construct a dataset suite CountHalluSet, with well-defined counting\ncriteria, comprising ToyShape, SimObject, and RealHand. Using these datasets,\nwe develop a standardized evaluation protocol for quantifying counting\nhallucinations, and systematically examine how different sampling conditions in\nDPMs, including solver type, ODE solver order, sampling steps, and initial\nnoise, affect counting hallucination levels. Furthermore, we analyze their\ncorrelation with common evaluation metrics such as FID, revealing that this\nwidely used image quality metric fails to capture counting hallucinations\nconsistently. This work aims to take the first step toward systematically\nquantifying hallucinations in diffusion models and offer new insights into the\ninvestigation of hallucination phenomena in image generation.",
        "url": "http://arxiv.org/abs/2510.13080v1",
        "published_date": "2025-10-15T01:48:04+00:00",
        "updated_date": "2025-10-15T01:48:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuai Fu",
            "Jian Zhou",
            "Qi Chen",
            "Huang Jing",
            "Huy Anh Nguyen",
            "Xiaohan Liu",
            "Zhixiong Zeng",
            "Lin Ma",
            "Quanshi Zhang",
            "Qi Wu"
        ],
        "tldr": "The paper introduces a new benchmark and evaluation protocol, CountHalluSet, to systematically quantify counting hallucinations in diffusion models, revealing limitations of FID in capturing such errors.",
        "tldr_zh": "该论文引入了一个新的基准和评估协议 CountHalluSet，用于系统地量化扩散模型中的计数幻觉，揭示了 FID 在捕捉此类错误方面的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SeqBench: Benchmarking Sequential Narrative Generation in Text-to-Video Models",
        "summary": "Text-to-video (T2V) generation models have made significant progress in\ncreating visually appealing videos. However, they struggle with generating\ncoherent sequential narratives that require logical progression through\nmultiple events. Existing T2V benchmarks primarily focus on visual quality\nmetrics but fail to evaluate narrative coherence over extended sequences. To\nbridge this gap, we present SeqBench, a comprehensive benchmark for evaluating\nsequential narrative coherence in T2V generation. SeqBench includes a carefully\ndesigned dataset of 320 prompts spanning various narrative complexities, with\n2,560 human-annotated videos generated from 8 state-of-the-art T2V models.\nAdditionally, we design a Dynamic Temporal Graphs (DTG)-based automatic\nevaluation metric, which can efficiently capture long-range dependencies and\ntemporal ordering while maintaining computational efficiency. Our DTG-based\nmetric demonstrates a strong correlation with human annotations. Through\nsystematic evaluation using SeqBench, we reveal critical limitations in current\nT2V models: failure to maintain consistent object states across multi-action\nsequences, physically implausible results in multi-object scenarios, and\ndifficulties in preserving realistic timing and ordering relationships between\nsequential actions. SeqBench provides the first systematic framework for\nevaluating narrative coherence in T2V generation and offers concrete insights\nfor improving sequential reasoning capabilities in future models. Please refer\nto https://videobench.github.io/SeqBench.github.io/ for more details.",
        "url": "http://arxiv.org/abs/2510.13042v1",
        "published_date": "2025-10-14T23:40:57+00:00",
        "updated_date": "2025-10-14T23:40:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhengxu Tang",
            "Zizheng Wang",
            "Luning Wang",
            "Zitao Shuai",
            "Chenhao Zhang",
            "Siyu Qian",
            "Yirui Wu",
            "Bohao Wang",
            "Haosong Rao",
            "Zhenyu Yang",
            "Chenwei Wu"
        ],
        "tldr": "The paper introduces SeqBench, a new benchmark for evaluating the narrative coherence of text-to-video generation models, along with a DTG-based metric that correlates well with human annotations, exposing current models' limitations in sequential reasoning.",
        "tldr_zh": "该论文介绍了一个新的基准测试 SeqBench，用于评估文本到视频生成模型的叙事连贯性，以及一种基于 DTG 的指标，该指标与人类注释高度相关，揭示了当前模型在顺序推理方面的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 10,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]