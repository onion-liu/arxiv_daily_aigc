[
    {
        "title": "Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation",
        "summary": "We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)\ncapable of image understanding and generation tasks. Unlike existing multimodal\ndiffsion language models such as MMaDa and Muddit which only support simple\nimage-level understanding tasks and low-resolution image generation, Lavida-O\nexhibits many new capabilities such as object grounding, image-editing, and\nhigh-resolution (1024px) image synthesis. It is also the first unified MDM that\nuses its understanding capabilities to improve image generation and editing\nresults through planning and iterative self-reflection. To allow effective and\nefficient training and sampling, Lavida-O ntroduces many novel techniques such\nas Elastic Mixture-of-Transformer architecture, universal text conditioning,\nand stratified sampling. \\ours~achieves state-of-the-art performance on a wide\nrange of benchmarks such as RefCOCO object grounding, GenEval text-to-image\ngeneration, and ImgEdit image editing, outperforming existing autoregressive\nand continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while\noffering considerable speedup at inference.",
        "url": "http://arxiv.org/abs/2509.19244v1",
        "published_date": "2025-09-23T17:05:46+00:00",
        "updated_date": "2025-09-23T17:05:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shufan Li",
            "Jiuxiang Gu",
            "Kangning Liu",
            "Zhe Lin",
            "Zijun Wei",
            "Aditya Grover",
            "Jason Kuen"
        ],
        "tldr": "Lavida-O is a novel unified masked diffusion model (MDM) that achieves state-of-the-art performance in image understanding and generation tasks like object grounding, image editing, and high-resolution image synthesis, offering significant speedups compared to existing models.",
        "tldr_zh": "Lavida-O 是一种新颖的统一掩码扩散模型 (MDM)，在图像理解和生成任务（如对象定位、图像编辑和高分辨率图像合成）方面实现了最先进的性能，与现有模型相比，显着提高了速度。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation",
        "summary": "Unified multimodal models have recently attracted considerable attention for\ntheir remarkable abilities in jointly understanding and generating diverse\ncontent. However, as contexts integrate increasingly numerous interleaved\nmultimodal tokens, the iterative processes of diffusion denoising and\nautoregressive decoding impose significant computational overhead. To address\nthis, we propose Hyper-Bagel, a unified acceleration framework designed to\nsimultaneously speed up both multimodal understanding and generation tasks. Our\napproach uses a divide-and-conquer strategy, employing speculative decoding for\nnext-token prediction and a multi-stage distillation process for diffusion\ndenoising. The framework delivers substantial performance gains, achieving over\na 2x speedup in multimodal understanding. For generative tasks, our resulting\nlossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a\n22x speedup in image editing, all while preserving the high-quality output of\nthe original model. We further develop a highly efficient 1-NFE model that\nenables near real-time interactive editing and generation. By combining\nadvanced adversarial distillation with human feedback learning, this model\nachieves ultimate cost-effectiveness and responsiveness, making complex\nmultimodal interactions seamless and instantaneous.",
        "url": "http://arxiv.org/abs/2509.18824v1",
        "published_date": "2025-09-23T09:12:46+00:00",
        "updated_date": "2025-09-23T09:12:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanzuo Lu",
            "Xin Xia",
            "Manlin Zhang",
            "Huafeng Kuang",
            "Jianbin Zheng",
            "Yuxi Ren",
            "Xuefeng Xiao"
        ],
        "tldr": "The paper introduces Hyper-Bagel, a unified acceleration framework that significantly speeds up both multimodal understanding and generation tasks through speculative decoding and multi-stage distillation, achieving impressive speedups in text-to-image generation and image editing.",
        "tldr_zh": "该论文介绍了 Hyper-Bagel，一个统一的加速框架，通过推测性解码和多阶段蒸馏，显著加速了多模态理解和生成任务，并在文本到图像生成和图像编辑方面实现了令人印象深刻的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching",
        "summary": "Conditional generative modeling aims to learn a conditional data distribution\nfrom samples containing data-condition pairs. For this, diffusion and\nflow-based methods have attained compelling results. These methods use a\nlearned (flow) model to transport an initial standard Gaussian noise that\nignores the condition to the conditional data distribution. The model is hence\nrequired to learn both mass transport and conditional injection. To ease the\ndemand on the model, we propose Condition-Aware Reparameterization for Flow\nMatching (CAR-Flow) -- a lightweight, learned shift that conditions the source,\nthe target, or both distributions. By relocating these distributions, CAR-Flow\nshortens the probability path the model must learn, leading to faster training\nin practice. On low-dimensional synthetic data, we visualize and quantify the\neffects of CAR. On higher-dimensional natural image data (ImageNet-256),\nequipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while\nintroducing less than 0.6% additional parameters.",
        "url": "http://arxiv.org/abs/2509.19300v1",
        "published_date": "2025-09-23T17:59:31+00:00",
        "updated_date": "2025-09-23T17:59:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Chen",
            "Pengsheng Guo",
            "Liangchen Song",
            "Jiasen Lu",
            "Rui Qian",
            "Xinze Wang",
            "Tsu-Jui Fu",
            "Wei Liu",
            "Yinfei Yang",
            "Alex Schwing"
        ],
        "tldr": "The paper introduces CAR-Flow, a condition-aware reparameterization technique for flow matching that shifts source and target distributions to ease the learning process for conditional generative models, resulting in faster training and improved performance on image generation tasks.",
        "tldr_zh": "该论文介绍了CAR-Flow，一种条件感知重参数化技术，用于流匹配，通过移动源和目标分布来简化条件生成模型的学习过程，从而加快训练速度并提高图像生成任务的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps",
        "summary": "Despite steady progress in layout-to-image generation, current methods still\nstruggle with layouts containing significant overlap between bounding boxes. We\nidentify two primary challenges: (1) large overlapping regions and (2)\noverlapping instances with minimal semantic distinction. Through both\nqualitative examples and quantitative analysis, we demonstrate how these\nfactors degrade generation quality. To systematically assess this issue, we\nintroduce OverLayScore, a novel metric that quantifies the complexity of\noverlapping bounding boxes. Our analysis reveals that existing benchmarks are\nbiased toward simpler cases with low OverLayScore values, limiting their\neffectiveness in evaluating model performance under more challenging\nconditions. To bridge this gap, we present OverLayBench, a new benchmark\nfeaturing high-quality annotations and a balanced distribution across different\nlevels of OverLayScore. As an initial step toward improving performance on\ncomplex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a\ncurated amodal mask dataset. Together, our contributions lay the groundwork for\nmore robust layout-to-image generation under realistic and challenging\nscenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.",
        "url": "http://arxiv.org/abs/2509.19282v1",
        "published_date": "2025-09-23T17:50:00+00:00",
        "updated_date": "2025-09-23T17:50:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bingnan Li",
            "Chen-Yu Wang",
            "Haiyang Xu",
            "Xiang Zhang",
            "Ethan Armand",
            "Divyansh Srivastava",
            "Xiaojun Shan",
            "Zeyuan Chen",
            "Jianwen Xie",
            "Zhuowen Tu"
        ],
        "tldr": "This paper introduces OverLayBench, a new benchmark and OverLayScore metric to specifically evaluate layout-to-image generation models' performance on scenes with dense overlapping bounding boxes, a known weakness of current methods. They also propose a fine-tuned model, CreatiLayout-AM, as a starting point for improvement.",
        "tldr_zh": "该论文介绍了OverLayBench，一个新的基准测试和OverLayScore指标，专门用于评估布局到图像生成模型在具有密集重叠边界框的场景中的性能，这是当前方法的一个已知弱点。他们还提出了一个微调模型CreatiLayout-AM，作为改进的起点。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models",
        "summary": "Autonomous vehicles (AVs) are expected to revolutionize transportation by\nimproving efficiency and safety. Their success relies on 3D vision systems that\neffectively sense the environment and detect traffic agents. Among sensors AVs\nuse to create a comprehensive view of surroundings, LiDAR provides\nhigh-resolution depth data enabling accurate object detection, safe navigation,\nand collision avoidance. However, collecting real-world LiDAR data is\ntime-consuming and often affected by noise and sparsity due to adverse weather\nor sensor limitations. This work applies a denoising diffusion probabilistic\nmodel (DDPM), enhanced with novel noise scheduling and time-step embedding\ntechniques to generate high-quality synthetic data for augmentation, thereby\nimproving performance across a range of computer vision tasks, particularly in\nAV perception. These modifications impact the denoising process and the model's\ntemporal awareness, allowing it to produce more realistic point clouds based on\nthe projection. The proposed method was extensively evaluated under various\nconfigurations using the IAMCV and KITTI-360 datasets, with four performance\nmetrics compared against state-of-the-art (SOTA) methods. The results\ndemonstrate the model's superior performance over most existing baselines and\nits effectiveness in mitigating the effects of noisy and sparse LiDAR data,\nproducing diverse point clouds with rich spatial relationships and structural\ndetail.",
        "url": "http://arxiv.org/abs/2509.18917v1",
        "published_date": "2025-09-23T12:35:07+00:00",
        "updated_date": "2025-09-23T12:35:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Amirhesam Aghanouri",
            "Cristina Olaverri-Monreal"
        ],
        "tldr": "This paper presents a DDPM-based approach for generating high-quality synthetic LiDAR point cloud data to address the challenges of noise and sparsity in real-world data, demonstrating improved performance in AV perception tasks.",
        "tldr_zh": "本文提出了一种基于DDPM的方法，用于生成高质量的合成LiDAR点云数据，以解决真实世界数据中的噪声和稀疏性问题，并在AV感知任务中表现出改进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping",
        "summary": "Fusing cross-category objects to a single coherent object has gained\nincreasing attention in text-to-image (T2I) generation due to its broad\napplications in virtual reality, digital media, film, and gaming. However,\nexisting methods often produce biased, visually chaotic, or semantically\ninconsistent results due to overlapping artifacts and poor integration.\nMoreover, progress in this field has been limited by the absence of a\ncomprehensive benchmark dataset. To address these problems, we propose\n\\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective\napproach comprising two key components: (1) Group-wise Embedding Swapping,\nwhich fuses semantic attributes from different concepts through feature\nmanipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism\nguided by a balance evaluation score to ensure coherent synthesis.\nAdditionally, we introduce \\textbf{Cross-category Object Fusion (COF)}, a\nlarge-scale, hierarchically structured dataset built upon ImageNet-1K and\nWordNet. COF includes 95 superclasses, each with 10 subclasses, enabling\n451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap\noutperforms state-of-the-art compositional T2I methods, including GPT-Image-1\nusing simple and complex prompts.",
        "url": "http://arxiv.org/abs/2509.18699v1",
        "published_date": "2025-09-23T06:32:14+00:00",
        "updated_date": "2025-09-23T06:32:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zedong Zhang",
            "Ying Tai",
            "Jianjun Qian",
            "Jian Yang",
            "Jun Li"
        ],
        "tldr": "The paper introduces AGSwap, a method for fusing cross-category objects in text-to-image generation using group-wise embedding swapping and adaptive group updating, along with a new large-scale dataset, COF, for benchmarking.",
        "tldr_zh": "该论文介绍了AGSwap，一种通过组嵌入交换和自适应组更新在文本到图像生成中融合跨类别对象的方法，以及一个新的大规模数据集COF，用于基准测试。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "One-shot Embroidery Customization via Contrastive LoRA Modulation",
        "summary": "Diffusion models have significantly advanced image manipulation techniques,\nand their ability to generate photorealistic images is beginning to transform\nretail workflows, particularly in presale visualization. Beyond artistic style\ntransfer, the capability to perform fine-grained visual feature transfer is\nbecoming increasingly important. Embroidery is a textile art form characterized\nby intricate interplay of diverse stitch patterns and material properties,\nwhich poses unique challenges for existing style transfer methods. To explore\nthe customization for such fine-grained features, we propose a novel\ncontrastive learning framework that disentangles fine-grained style and content\nfeatures with a single reference image, building on the classic concept of\nimage analogy. We first construct an image pair to define the target style, and\nthen adopt a similarity metric based on the decoupled representations of\npretrained diffusion models for style-content separation. Subsequently, we\npropose a two-stage contrastive LoRA modulation technique to capture\nfine-grained style features. In the first stage, we iteratively update the\nwhole LoRA and the selected style blocks to initially separate style from\ncontent. In the second stage, we design a contrastive learning strategy to\nfurther decouple style and content through self-knowledge distillation.\nFinally, we build an inference pipeline to handle image or text inputs with\nonly the style blocks. To evaluate our method on fine-grained style transfer,\nwe build a benchmark for embroidery customization. Our approach surpasses prior\nmethods on this task and further demonstrates strong generalization to three\nadditional domains: artistic style transfer, sketch colorization, and\nappearance transfer.",
        "url": "http://arxiv.org/abs/2509.18948v1",
        "published_date": "2025-09-23T12:58:15+00:00",
        "updated_date": "2025-09-23T12:58:15+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Jun Ma",
            "Qian He",
            "Gaofeng He",
            "Huang Chen",
            "Chen Liu",
            "Xiaogang Jin",
            "Huamin Wang"
        ],
        "tldr": "This paper introduces a contrastive learning framework with LoRA modulation for fine-grained style transfer, specifically applied to embroidery customization with diffusion models, showing strong generalization to other domains.",
        "tldr_zh": "本文介绍了一种基于对比学习框架和LoRA调制的细粒度风格迁移方法，专门应用于扩散模型的刺绣定制，并展示了对其他领域的强大泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Towards Application Aligned Synthetic Surgical Image Synthesis",
        "summary": "The scarcity of annotated surgical data poses a significant challenge for\ndeveloping deep learning systems in computer-assisted interventions. While\ndiffusion models can synthesize realistic images, they often suffer from data\nmemorization, resulting in inconsistent or non-diverse samples that may fail to\nimprove, or even harm, downstream performance. We introduce \\emph{Surgical\nApplication-Aligned Diffusion} (SAADi), a new framework that aligns diffusion\nmodels with samples preferred by downstream models. Our method constructs pairs\nof \\emph{preferred} and \\emph{non-preferred} synthetic images and employs\nlightweight fine-tuning of diffusion models to align the image generation\nprocess with downstream objectives explicitly. Experiments on three surgical\ndatasets demonstrate consistent gains of $7$--$9\\%$ in classification and\n$2$--$10\\%$ in segmentation tasks, with the considerable improvements observed\nfor underrepresented classes. Iterative refinement of synthetic samples further\nboosts performance by $4$--$10\\%$. Unlike baseline approaches, our method\novercomes sample degradation and establishes task-aware alignment as a key\nprinciple for mitigating data scarcity and advancing surgical vision\napplications.",
        "url": "http://arxiv.org/abs/2509.18796v1",
        "published_date": "2025-09-23T08:40:40+00:00",
        "updated_date": "2025-09-23T08:40:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Danush Kumar Venkatesh",
            "Stefanie Speidel"
        ],
        "tldr": "The paper introduces Surgical Application-Aligned Diffusion (SAADi), a method to fine-tune diffusion models to generate synthetic surgical images preferred by downstream tasks, leading to improved performance in classification and segmentation, especially for underrepresented classes.",
        "tldr_zh": "该论文介绍了手术应用对齐扩散 (SAADi)，一种微调扩散模型以生成下游任务偏好的合成手术图像的方法，从而提高了分类和分割的性能，特别是对于代表性不足的类别。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Prompt-Guided Dual Latent Steering for Inversion Problems",
        "summary": "Inverting corrupted images into the latent space of diffusion models is\nchallenging. Current methods, which encode an image into a single latent\nvector, struggle to balance structural fidelity with semantic accuracy, leading\nto reconstructions with semantic drift, such as blurred details or incorrect\nattributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering\n(PDLS), a novel, training-free framework built upon Rectified Flow models for\ntheir stable inversion paths. PDLS decomposes the inversion process into two\ncomplementary streams: a structural path to preserve source integrity and a\nsemantic path guided by a prompt. We formulate this dual guidance as an optimal\ncontrol problem and derive a closed-form solution via a Linear Quadratic\nRegulator (LQR). This controller dynamically steers the generative trajectory\nat each step, preventing semantic drift while ensuring the preservation of fine\ndetail without costly, per-image optimization. Extensive experiments on FFHQ-1K\nand ImageNet-1K under various inversion tasks, including Gaussian deblurring,\nmotion deblurring, super-resolution and freeform inpainting, demonstrate that\nPDLS produces reconstructions that are both more faithful to the original image\nand better aligned with the semantic information than single-latent baselines.",
        "url": "http://arxiv.org/abs/2509.18619v1",
        "published_date": "2025-09-23T04:11:06+00:00",
        "updated_date": "2025-09-23T04:11:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yichen Wu",
            "Xu Liu",
            "Chenxuan Zhao",
            "Xinyu Wu"
        ],
        "tldr": "The paper introduces Prompt-Guided Dual Latent Steering (PDLS), a training-free framework leveraging Rectified Flow models for improved image inversion, balancing structural fidelity and semantic accuracy using a dual-stream approach guided by optimal control.",
        "tldr_zh": "该论文介绍了提示引导的双重潜在引导（PDLS），一个无需训练的框架，利用修正流模型改进图像反演，通过最优控制引导的双流方法来平衡结构保真度和语义准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GeoRemover: Removing Objects and Their Causal Visual Artifacts",
        "summary": "Towards intelligent image editing, object removal should eliminate both the\ntarget object and its causal visual artifacts, such as shadows and reflections.\nHowever, existing image appearance-based methods either follow strictly\nmask-aligned training and fail to remove these causal effects which are not\nexplicitly masked, or adopt loosely mask-aligned strategies that lack\ncontrollability and may unintentionally over-erase other objects. We identify\nthat these limitations stem from ignoring the causal relationship between an\nobject's geometry presence and its visual effects. To address this limitation,\nwe propose a geometry-aware two-stage framework that decouples object removal\ninto (1) geometry removal and (2) appearance rendering. In the first stage, we\nremove the object directly from the geometry (e.g., depth) using strictly\nmask-aligned supervision, enabling structure-aware editing with strong\ngeometric constraints. In the second stage, we render a photorealistic RGB\nimage conditioned on the updated geometry, where causal visual effects are\nconsidered implicitly as a result of the modified 3D geometry. To guide\nlearning in the geometry removal stage, we introduce a preference-driven\nobjective based on positive and negative sample pairs, encouraging the model to\nremove objects as well as their causal visual artifacts while avoiding new\nstructural insertions. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance in removing both objects and their\nassociated artifacts on two popular benchmarks. The code is available at\nhttps://github.com/buxiangzhiren/GeoRemover.",
        "url": "http://arxiv.org/abs/2509.18538v1",
        "published_date": "2025-09-23T02:04:19+00:00",
        "updated_date": "2025-09-23T02:04:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixin Zhu",
            "Haoxiang Li",
            "Xuelu Feng",
            "He Wu",
            "Chunming Qiao",
            "Junsong Yuan"
        ],
        "tldr": "The paper introduces GeoRemover, a geometry-aware framework for removing objects and their causal visual artifacts (shadows, reflections) from images, using a two-stage process of geometry removal followed by appearance rendering based on the updated geometry.",
        "tldr_zh": "该论文介绍了GeoRemover，一个几何感知框架，用于从图像中移除物体及其因果视觉伪影（阴影、反射），使用两阶段过程：几何移除，然后是基于更新后的几何体的外观渲染。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]