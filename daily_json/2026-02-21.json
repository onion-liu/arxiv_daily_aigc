[
    {
        "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment",
        "summary": "Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.",
        "url": "http://arxiv.org/abs/2602.17599v1",
        "published_date": "2026-02-19T18:23:58+00:00",
        "updated_date": "2026-02-19T18:23:58+00:00",
        "categories": [
            "cs.CV",
            "cs.MM",
            "cs.SD"
        ],
        "authors": [
            "Ivan Rinaldi",
            "Matteo Mendula",
            "Nicola Fanelli",
            "Florence Levé",
            "Matteo Testi",
            "Giovanna Castellano",
            "Gennaro Vessio"
        ],
        "tldr": "The paper introduces ArtToMus, a framework for direct artwork-to-music generation using a new dataset ArtSound, addressing limitations of existing image-to-music systems that rely on image-to-text conversion.",
        "tldr_zh": "该论文介绍了ArtToMus，一个用于直接将艺术作品转化为音乐的框架，它使用了一个新的数据集ArtSound，解决了现有依赖图像到文本转换的图像到音乐系统的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]