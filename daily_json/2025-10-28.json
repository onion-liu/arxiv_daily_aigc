[
    {
        "title": "FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time",
        "summary": "This paper proposes FreeFuse, a novel training-free approach for\nmulti-subject text-to-image generation through automatic fusion of multiple\nsubject LoRAs. In contrast to existing methods that either focus on\npre-inference LoRA weight merging or rely on segmentation models and complex\ntechniques like noise blending to isolate LoRA outputs, our key insight is that\ncontext-aware dynamic subject masks can be automatically derived from\ncross-attention layer weights. Mathematical analysis shows that directly\napplying these masks to LoRA outputs during inference well approximates the\ncase where the subject LoRA is integrated into the diffusion model and used\nindividually for the masked region. FreeFuse demonstrates superior practicality\nand efficiency as it requires no additional training, no modification to LoRAs,\nno auxiliary models, and no user-defined prompt templates or region\nspecifications. Alternatively, it only requires users to provide the LoRA\nactivation words for seamless integration into standard workflows. Extensive\nexperiments validate that FreeFuse outperforms existing approaches in both\ngeneration quality and usability under the multi-subject generation tasks. The\nproject page is at https://future-item.github.io/FreeFuse/",
        "url": "http://arxiv.org/abs/2510.23515v1",
        "published_date": "2025-10-27T16:54:08+00:00",
        "updated_date": "2025-10-27T16:54:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaoli Liu",
            "Yao-Xiang Ding",
            "Kun Zhou"
        ],
        "tldr": "FreeFuse introduces a training-free method for multi-subject text-to-image generation by automatically fusing multiple subject LoRAs using context-aware masks derived from cross-attention weights, achieving superior performance and usability.",
        "tldr_zh": "FreeFuse 提出了一种无需训练的方法，通过使用从交叉注意力权重导出的上下文感知掩码自动融合多个主体 LoRA，从而实现多主体文本到图像的生成，并实现了卓越的性能和可用性。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling",
        "summary": "Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/.",
        "url": "http://arxiv.org/abs/2510.23605v1",
        "published_date": "2025-10-27T17:59:51+00:00",
        "updated_date": "2025-10-27T17:59:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.GR",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Shuhong Zheng",
            "Ashkan Mirzaei",
            "Igor Gilitschenski"
        ],
        "tldr": "The paper introduces TIRE, a novel method for subject-driven 3D/4D generation that uses video tracking and inpainting to improve identity preservation, surpassing existing state-of-the-art approaches.",
        "tldr_zh": "该论文介绍了一种名为TIRE的新型主体驱动3D/4D生成方法，该方法利用视频跟踪和修复提高身份保持，并优于现有最先进的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "FARMER: Flow AutoRegressive Transformer over Pixels",
        "summary": "Directly modeling the explicit likelihood of the raw data distribution is key\ntopic in the machine learning area, which achieves the scaling successes in\nLarge Language Models by autoregressive modeling. However, continuous AR\nmodeling over visual pixel data suffer from extremely long sequences and\nhigh-dimensional spaces. In this paper, we present FARMER, a novel end-to-end\ngenerative framework that unifies Normalizing Flows (NF) and Autoregressive\n(AR) models for tractable likelihood estimation and high-quality image\nsynthesis directly from raw pixels. FARMER employs an invertible autoregressive\nflow to transform images into latent sequences, whose distribution is modeled\nimplicitly by an autoregressive model. To address the redundancy and complexity\nin pixel-level modeling, we propose a self-supervised dimension reduction\nscheme that partitions NF latent channels into informative and redundant\ngroups, enabling more effective and efficient AR modeling. Furthermore, we\ndesign a one-step distillation scheme to significantly accelerate inference\nspeed and introduce a resampling-based classifier-free guidance algorithm to\nboost image generation quality. Extensive experiments demonstrate that FARMER\nachieves competitive performance compared to existing pixel-based generative\nmodels while providing exact likelihoods and scalable training.",
        "url": "http://arxiv.org/abs/2510.23588v1",
        "published_date": "2025-10-27T17:54:08+00:00",
        "updated_date": "2025-10-27T17:54:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guangting Zheng",
            "Qinyu Zhao",
            "Tao Yang",
            "Fei Xiao",
            "Zhijie Lin",
            "Jie Wu",
            "Jiajun Deng",
            "Yanyong Zhang",
            "Rui Zhu"
        ],
        "tldr": "FARMER is a novel generative framework unifying Normalizing Flows and Autoregressive models for high-quality image synthesis directly from raw pixels, using dimension reduction and distillation for efficiency.",
        "tldr_zh": "FARMER 是一种新颖的生成框架，它统一了归一化流和自回归模型，可直接从原始像素进行高质量图像合成，并使用降维和蒸馏来提高效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence",
        "summary": "The scope of neural code intelligence is rapidly expanding beyond text-based\nsource code to encompass the rich visual outputs that programs generate. This\nvisual dimension is critical for advanced applications like flexible content\ngeneration and precise, program-driven editing of visualizations. However,\nprogress has been impeded by the scarcity of high-quality multimodal code data,\na bottleneck stemming from challenges in synthesis and quality assessment. To\naddress these challenges, we make contributions from both a data and modeling\nperspective. We first introduce a complete synthesis toolkit that leverages\nreciprocal synergies between data modalities to efficiently produce a\nlarge-scale, high-quality corpus spanning from standard charts to complex\ninteractive web UIs and code-driven animations. Leveraging this toolkit, we\nconstruct JanusCode-800K, the largest multimodal code corpus to date. This\npowers the training of our models, JanusCoder and JanusCoderV, which establish\na visual-programmatic interface for generating code from textual instructions,\nvisual inputs, or a combination of both. Our unified model is a departure from\nexisting approaches that build specialized models for isolated tasks. Extensive\nexperiments on both text-centric and vision-centric coding tasks demonstrate\nthe superior performance of the JanusCoder series, with our 7B to 14B scale\nmodels approaching or even exceeding the performance of commercial models.\nFurthermore, extensive analysis provides key insights into harmonizing\nprogrammatic logic with its visual expression. Our code and checkpoints will\nare available at https://github.com/InternLM/JanusCoder.",
        "url": "http://arxiv.org/abs/2510.23538v1",
        "published_date": "2025-10-27T17:13:49+00:00",
        "updated_date": "2025-10-27T17:13:49+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.SE"
        ],
        "authors": [
            "Qiushi Sun",
            "Jingyang Gong",
            "Yang Liu",
            "Qiaosheng Chen",
            "Lei Li",
            "Kai Chen",
            "Qipeng Guo",
            "Ben Kao",
            "Fei Yuan"
        ],
        "tldr": "JanusCoder introduces a multimodal code corpus (JanusCode-800K) and models (JanusCoder, JanusCoderV) for generating code from text, visuals, or both, demonstrating strong performance in visual-programmatic interface tasks.",
        "tldr_zh": "JanusCoder 提出了一个多模态代码语料库 (JanusCode-800K) 和模型 (JanusCoder, JanusCoderV)，用于从文本、视觉或两者生成代码，并在视觉编程接口任务中表现出强大的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Adaptive Stochastic Coefficients for Accelerating Diffusion Sampling",
        "summary": "Diffusion-based generative processes, formulated as differential equation\nsolving, frequently balance computational speed with sample quality. Our\ntheoretical investigation of ODE- and SDE-based solvers reveals complementary\nweaknesses: ODE solvers accumulate irreducible gradient error along\ndeterministic trajectories, while SDE methods suffer from amplified\ndiscretization errors when the step budget is limited. Building upon this\ninsight, we introduce AdaSDE, a novel single-step SDE solver that aims to unify\nthe efficiency of ODEs with the error resilience of SDEs. Specifically, we\nintroduce a single per-step learnable coefficient, estimated via lightweight\ndistillation, which dynamically regulates the error correction strength to\naccelerate diffusion sampling. Notably, our framework can be integrated with\nexisting solvers to enhance their capabilities. Extensive experiments\ndemonstrate state-of-the-art performance: at 5 NFE, AdaSDE achieves FID scores\nof 4.18 on CIFAR-10, 8.05 on FFHQ and 6.96 on LSUN Bedroom. Codes are available\nin https://github.com/WLU-wry02/AdaSDE.",
        "url": "http://arxiv.org/abs/2510.23285v1",
        "published_date": "2025-10-27T12:53:48+00:00",
        "updated_date": "2025-10-27T12:53:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruoyu Wang",
            "Beier Zhu",
            "Junzhi Li",
            "Liangyu Yuan",
            "Chi Zhang"
        ],
        "tldr": "The paper introduces AdaSDE, a novel single-step SDE solver for diffusion models that balances computational speed and sample quality by using a learnable coefficient to dynamically regulate error correction strength. It achieves state-of-the-art FID scores at low NFEs.",
        "tldr_zh": "该论文介绍了一种新的单步SDE求解器AdaSDE，用于扩散模型，它通过使用可学习的系数来动态调节误差校正强度，从而平衡计算速度和样本质量。 它以低NFE实现了最先进的FID分数。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Nested AutoRegressive Models",
        "summary": "AutoRegressive (AR) models have demonstrated competitive performance in image\ngeneration, achieving results comparable to those of diffusion models. However,\ntheir token-by-token image generation mechanism remains computationally\nintensive and existing solutions such as VAR often lead to limited sample\ndiversity. In this work, we propose a Nested AutoRegressive~(NestAR) model,\nwhich proposes nested AutoRegressive architectures in generating images. NestAR\ndesigns multi-scale modules in a hierarchical order. These different scaled\nmodules are constructed in an AR architecture, where one larger-scale module is\nconditioned on outputs from its previous smaller-scale module. Within each\nmodule, NestAR uses another AR structure to generate ``patches'' of tokens. The\nproposed nested AR architecture reduces the overall complexity from\n$\\mathcal{O}(n)$ to $\\mathcal{O}(\\log n)$ in generating $n$ image tokens, as\nwell as increases image diversities. NestAR further incorporates flow matching\nloss to use continuous tokens, and develops objectives to coordinate these\nmulti-scale modules in model training. NestAR achieves competitive image\ngeneration performance while significantly lowering computational cost.",
        "url": "http://arxiv.org/abs/2510.23028v1",
        "published_date": "2025-10-27T05:49:02+00:00",
        "updated_date": "2025-10-27T05:49:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hongyu Wu",
            "Xuhui Fan",
            "Zhangkai Wu",
            "Longbing Cao"
        ],
        "tldr": "The paper introduces NestAR, a nested autoregressive model for image generation that reduces computational complexity and increases image diversity by using multi-scale modules in a hierarchical AR architecture and incorporating flow matching loss.",
        "tldr_zh": "该论文介绍了一种名为 NestAR 的嵌套自回归模型，用于图像生成。该模型通过分层 AR 架构中使用多尺度模块并结合流匹配损失，降低了计算复杂度并增加了图像多样性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoMo: Compositional Motion Customization for Text-to-Video Generation",
        "summary": "While recent text-to-video models excel at generating diverse scenes, they\nstruggle with precise motion control, particularly for complex, multi-subject\nmotions. Although methods for single-motion customization have been developed\nto address this gap, they fail in compositional scenarios due to two primary\nchallenges: motion-appearance entanglement and ineffective multi-motion\nblending. This paper introduces CoMo, a novel framework for\n$\\textbf{compositional motion customization}$ in text-to-video generation,\nenabling the synthesis of multiple, distinct motions within a single video.\nCoMo addresses these issues through a two-phase approach. First, in the\nsingle-motion learning phase, a static-dynamic decoupled tuning paradigm\ndisentangles motion from appearance to learn a motion-specific module. Second,\nin the multi-motion composition phase, a plug-and-play divide-and-merge\nstrategy composes these learned motions without additional training by\nspatially isolating their influence during the denoising process. To facilitate\nresearch in this new domain, we also introduce a new benchmark and a novel\nevaluation metric designed to assess multi-motion fidelity and blending.\nExtensive experiments demonstrate that CoMo achieves state-of-the-art\nperformance, significantly advancing the capabilities of controllable video\ngeneration. Our project page is at https://como6.github.io/.",
        "url": "http://arxiv.org/abs/2510.23007v1",
        "published_date": "2025-10-27T04:57:09+00:00",
        "updated_date": "2025-10-27T04:57:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Youcan Xu",
            "Zhen Wang",
            "Jiaxin Shi",
            "Kexin Li",
            "Feifei Shao",
            "Jun Xiao",
            "Yi Yang",
            "Jun Yu",
            "Long Chen"
        ],
        "tldr": "The paper introduces CoMo, a framework for compositional motion customization in text-to-video generation that enables the synthesis of multiple, distinct motions within a single video using a static-dynamic decoupled tuning paradigm and a divide-and-merge strategy. It also provides a new benchmark and evaluation metric.",
        "tldr_zh": "本文提出了一种名为CoMo的框架，用于文本到视频生成中的组合运动定制，通过静态-动态解耦调整范式和分治合并策略，实现在单个视频中合成多个不同的运动。同时，还提供了一个新的基准和评估指标。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SceneDecorator: Towards Scene-Oriented Story Generation with Scene Planning and Scene Consistency",
        "summary": "Recent text-to-image models have revolutionized image generation, but they\nstill struggle with maintaining concept consistency across generated images.\nWhile existing works focus on character consistency, they often overlook the\ncrucial role of scenes in storytelling, which restricts their creativity in\npractice. This paper introduces scene-oriented story generation, addressing two\nkey challenges: (i) scene planning, where current methods fail to ensure\nscene-level narrative coherence by relying solely on text descriptions, and\n(ii) scene consistency, which remains largely unexplored in terms of\nmaintaining scene consistency across multiple stories. We propose\nSceneDecorator, a training-free framework that employs VLM-Guided Scene\nPlanning to ensure narrative coherence across different scenes in a\n``global-to-local'' manner, and Long-Term Scene-Sharing Attention to maintain\nlong-term scene consistency and subject diversity across generated stories.\nExtensive experiments demonstrate the superior performance of SceneDecorator,\nhighlighting its potential to unleash creativity in the fields of arts, films,\nand games.",
        "url": "http://arxiv.org/abs/2510.22994v1",
        "published_date": "2025-10-27T04:19:22+00:00",
        "updated_date": "2025-10-27T04:19:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Quanjian Song",
            "Donghao Zhou",
            "Jingyu Lin",
            "Fei Shen",
            "Jiaze Wang",
            "Xiaowei Hu",
            "Cunjian Chen",
            "Pheng-Ann Heng"
        ],
        "tldr": "The paper introduces SceneDecorator, a training-free framework for scene-oriented story generation that addresses scene planning and scene consistency issues in text-to-image models, leading to improved narrative coherence and subject diversity.",
        "tldr_zh": "本文介绍了SceneDecorator，一个用于场景导向故事生成的免训练框架，解决了文本到图像模型中场景规划和场景一致性的问题，从而提高了叙事连贯性和主题多样性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method",
        "summary": "Driving scene generation is a critical domain for autonomous driving,\nenabling downstream applications, including perception and planning evaluation.\nOccupancy-centric methods have recently achieved state-of-the-art results by\noffering consistent conditioning across frames and modalities; however, their\nperformance heavily depends on annotated occupancy data, which still remains\nscarce. To overcome this limitation, we curate Nuplan-Occ, the largest semantic\noccupancy dataset to date, constructed from the widely used Nuplan benchmark.\nIts scale and diversity facilitate not only large-scale generative modeling but\nalso autonomous driving downstream applications. Based on this dataset, we\ndevelop a unified framework that jointly synthesizes high-quality semantic\noccupancy, multi-view videos, and LiDAR point clouds. Our approach incorporates\na spatio-temporal disentangled architecture to support high-fidelity spatial\nexpansion and temporal forecasting of 4D dynamic occupancy. To bridge modal\ngaps, we further propose two novel techniques: a Gaussian splatting-based\nsparse point map rendering strategy that enhances multi-view video generation,\nand a sensor-aware embedding strategy that explicitly models LiDAR sensor\nproperties for realistic multi-LiDAR simulation. Extensive experiments\ndemonstrate that our method achieves superior generation fidelity and\nscalability compared to existing approaches, and validates its practical value\nin downstream tasks. Repo:\nhttps://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2",
        "url": "http://arxiv.org/abs/2510.22973v1",
        "published_date": "2025-10-27T03:52:45+00:00",
        "updated_date": "2025-10-27T03:52:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bohan Li",
            "Xin Jin",
            "Hu Zhu",
            "Hongsi Liu",
            "Ruikai Li",
            "Jiazhe Guo",
            "Kaiwen Cai",
            "Chao Ma",
            "Yueming Jin",
            "Hao Zhao",
            "Xiaokang Yang",
            "Wenjun Zeng"
        ],
        "tldr": "The paper introduces Nuplan-Occ, a large-scale semantic occupancy dataset for autonomous driving, and a unified framework for generating high-quality semantic occupancy, multi-view videos, and LiDAR point clouds, outperforming existing methods.",
        "tldr_zh": "该论文介绍了 Nuplan-Occ，一个用于自动驾驶的大规模语义占据数据集，以及一个用于生成高质量语义占据、多视角视频和激光雷达点云的统一框架，优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VALA: Learning Latent Anchors for Training-Free and Temporally Consistent",
        "summary": "Recent advances in training-free video editing have enabled lightweight and\nprecise cross-frame generation by leveraging pre-trained text-to-image\ndiffusion models. However, existing methods often rely on heuristic frame\nselection to maintain temporal consistency during DDIM inversion, which\nintroduces manual bias and reduces the scalability of end-to-end inference. In\nthis paper, we propose~\\textbf{VALA} (\\textbf{V}ariational \\textbf{A}lignment\nfor \\textbf{L}atent \\textbf{A}nchors), a variational alignment module that\nadaptively selects key frames and compresses their latent features into\nsemantic anchors for consistent video editing. To learn meaningful assignments,\nVALA propose a variational framework with a contrastive learning objective.\nTherefore, it can transform cross-frame latent representations into compressed\nlatent anchors that preserve both content and temporal coherence. Our method\ncan be fully integrated into training-free text-to-image based video editing\nmodels. Extensive experiments on real-world video editing benchmarks show that\nVALA achieves state-of-the-art performance in inversion fidelity, editing\nquality, and temporal consistency, while offering improved efficiency over\nprior methods.",
        "url": "http://arxiv.org/abs/2510.22970v1",
        "published_date": "2025-10-27T03:44:11+00:00",
        "updated_date": "2025-10-27T03:44:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhangkai Wu",
            "Xuhui Fan",
            "Zhongyuan Xie",
            "Kaize Shi",
            "Longbing Cao"
        ],
        "tldr": "The paper introduces VALA, a variational alignment module for training-free video editing that adaptively selects key frames and compresses latent features into semantic anchors, improving temporal consistency. It claims state-of-the-art performance in inversion fidelity, editing quality, and temporal consistency.",
        "tldr_zh": "该论文提出了VALA，一种用于免训练视频编辑的变分对齐模块，可以自适应地选择关键帧并将潜在特征压缩为语义锚点，从而提高时间一致性。该方法声称在反演保真度、编辑质量和时间一致性方面达到了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FAME: Fairness-aware Attention-modulated Video Editing",
        "summary": "Training-free video editing (VE) models tend to fall back on gender\nstereotypes when rendering profession-related prompts. We propose \\textbf{FAME}\nfor \\textit{Fairness-aware Attention-modulated Video Editing} that mitigates\nprofession-related gender biases while preserving prompt alignment and temporal\nconsistency for coherent VE. We derive fairness embeddings from existing\nminority representations by softly injecting debiasing tokens into the text\nencoder. Simultaneously, FAME integrates fairness modulation into both temporal\nself attention and prompt-to-region cross attention to mitigate the motion\ncorruption and temporal inconsistency caused by directly introducing fairness\ncues. For temporal self attention, FAME introduces a region constrained\nattention mask combined with time decay weighting, which enhances intra-region\ncoherence while suppressing irrelevant inter-region interactions. For cross\nattention, it reweights tokens to region matching scores by incorporating\nfairness sensitive similarity masks derived from debiasing prompt embeddings.\nTogether, these modulations keep fairness-sensitive semantics tied to the right\nvisual regions and prevent temporal drift across frames. Extensive experiments\non new VE fairness-oriented benchmark \\textit{FairVE} demonstrate that FAME\nachieves stronger fairness alignment and semantic fidelity, surpassing existing\nVE baselines.",
        "url": "http://arxiv.org/abs/2510.22960v1",
        "published_date": "2025-10-27T03:34:15+00:00",
        "updated_date": "2025-10-27T03:34:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhangkai Wu",
            "Xuhui Fan",
            "Zhongyuan Xie",
            "Kaize Shi",
            "Zhidong Li",
            "Longbing Cao"
        ],
        "tldr": "The paper introduces FAME, a training-free video editing method that reduces gender bias in profession-related prompts by modulating attention mechanisms and incorporating fairness embeddings, resulting in improved fairness and semantic quality. They also introduce a new benchmark FairVE.",
        "tldr_zh": "该论文介绍了 FAME，一种无需训练的视频编辑方法，通过调节注意力机制和结合公平性嵌入来减少职业相关提示中的性别偏见，从而提高公平性和语义质量。 他们还引入了一个新的基准测试 FairVE。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation",
        "summary": "Unified multimodal models have recently shown remarkable gains in both\ncapability and versatility, yet most leading systems are still trained from\nscratch and require substantial computational resources. In this paper, we show\nthat competitive performance can be obtained far more efficiently by\nstrategically fusing publicly available models specialized for either\ngeneration or understanding. Our key design is to retain the original blocks\nwhile additionally interleaving multimodal self-attention blocks throughout the\nnetworks. This double fusion mechanism (1) effectively enables rich multi-modal\nfusion while largely preserving the original strengths of the base models, and\n(2) catalyzes synergistic fusion of high-level semantic representations from\nthe understanding encoder with low-level spatial signals from the generation\nencoder. By training with only ~ 35B tokens, this approach achieves strong\nresults across multiple benchmarks: 0.91 on GenEval for compositional\ntext-to-image generation, 82.16 on DPG-Bench for complex text-to-image\ngeneration, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By\nfully releasing the entire suite of code, model weights, and datasets, we hope\nto support future research on unified multimodal modeling.",
        "url": "http://arxiv.org/abs/2510.22946v1",
        "published_date": "2025-10-27T02:59:57+00:00",
        "updated_date": "2025-10-27T02:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeyu Wang",
            "Zilong Chen",
            "Chenhui Gou",
            "Feng Li",
            "Chaorui Deng",
            "Deyao Zhu",
            "Kunchang Li",
            "Weihao Yu",
            "Haoqin Tu",
            "Haoqi Fan",
            "Cihang Xie"
        ],
        "tldr": "The paper introduces LightBagel, a lightweight framework that fuses pre-trained unimodal models for unified multimodal understanding and generation using interleaved multimodal self-attention, achieving strong results on text-to-image and image editing benchmarks with limited training.",
        "tldr_zh": "该论文介绍了LightBagel，一个轻量级的框架，通过交错的多模态自注意力融合预训练的单模态模型，实现统一的多模态理解和生成，并在文本到图像和图像编辑基准测试中取得了良好的结果，同时训练量有限。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Task-Agnostic Fusion of Time Series and Imagery for Earth Observation",
        "summary": "We propose a task-agnostic framework for multimodal fusion of time series and\nsingle timestamp images, enabling cross-modal generation and robust downstream\nperformance. Our approach explores deterministic and learned strategies for\ntime series quantization and then leverages a masked correlation learning\nobjective, aligning discrete image and time series tokens in a unified\nrepresentation space. Instantiated in the Earth observation domain, the\npretrained model generates consistent global temperature profiles from\nsatellite imagery and is validated through counterfactual experiments. Across\ndownstream tasks, our task-agnostic pretraining outperforms task-specific\nfusion by 6\\% in R$^2$ and 2\\% in RMSE on average, and exceeds baseline methods\nby 50\\% in R$^2$ and 12\\% in RMSE. Finally, we analyze gradient sensitivity\nacross modalities, providing insights into model robustness. Code, data, and\nweights will be released under a permissive license.",
        "url": "http://arxiv.org/abs/2510.23118v1",
        "published_date": "2025-10-27T08:38:52+00:00",
        "updated_date": "2025-10-27T08:38:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gianfranco Basile",
            "Johannes Jakubik",
            "Benedikt Blumenstiel",
            "Thomas Brunschwiler",
            "Juan Bernabe Moreno"
        ],
        "tldr": "The paper proposes a task-agnostic multimodal fusion framework for Earth observation, combining time series and single-timestamp imagery using masked correlation learning, demonstrating improved performance in downstream tasks and cross-modal generation of temperature profiles.",
        "tldr_zh": "该论文提出了一种任务无关的多模态融合框架，用于地球观测，结合了时间序列和单时间戳图像，使用掩蔽相关学习，在下游任务中表现出改进的性能，并可以进行温度曲线的跨模态生成。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Residual Diffusion Bridge Model for Image Restoration",
        "summary": "Diffusion bridge models establish probabilistic paths between arbitrary\npaired distributions and exhibit great potential for universal image\nrestoration. Most existing methods merely treat them as simple variants of\nstochastic interpolants, lacking a unified analytical perspective. Besides,\nthey indiscriminately reconstruct images through global noise injection and\nremoval, inevitably distorting undegraded regions due to imperfect\nreconstruction. To address these challenges, we propose the Residual Diffusion\nBridge Model (RDBM). Specifically, we theoretically reformulate the stochastic\ndifferential equations of generalized diffusion bridge and derive the\nanalytical formulas of its forward and reverse processes. Crucially, we\nleverage the residuals from given distributions to modulate the noise injection\nand removal, enabling adaptive restoration of degraded regions while preserving\nintact others. Moreover, we unravel the fundamental mathematical essence of\nexisting bridge models, all of which are special cases of RDBM and empirically\ndemonstrate the optimality of our proposed models. Extensive experiments are\nconducted to demonstrate the state-of-the-art performance of our method both\nqualitatively and quantitatively across diverse image restoration tasks. Code\nis publicly available at https://github.com/MiliLab/RDBM.",
        "url": "http://arxiv.org/abs/2510.23116v1",
        "published_date": "2025-10-27T08:35:49+00:00",
        "updated_date": "2025-10-27T08:35:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hebaixu Wang",
            "Jing Zhang",
            "Haoyang Chen",
            "Haonan Guo",
            "Di Wang",
            "Jiayi Ma",
            "Bo Du"
        ],
        "tldr": "This paper introduces the Residual Diffusion Bridge Model (RDBM) for image restoration, reformulating diffusion bridges with a focus on residual-based noise modulation to improve reconstruction while preserving intact image regions. The authors claim state-of-the-art performance across diverse image restoration tasks.",
        "tldr_zh": "本文提出了用于图像修复的残差扩散桥模型(RDBM)，通过重新构建扩散桥，重点关注基于残差的噪声调制，以提高重建效果同时保留完整的图像区域。作者声称该方法在各种图像修复任务中表现出最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]