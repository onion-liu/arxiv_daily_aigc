[
    {
        "title": "ReDiF: Reinforced Distillation for Few Step Diffusion",
        "summary": "Distillation addresses the slow sampling problem in diffusion models by creating models with smaller size or fewer steps that approximate the behavior of high-step teachers. In this work, we propose a reinforcement learning based distillation framework for diffusion models. Instead of relying on fixed reconstruction or consistency losses, we treat the distillation process as a policy optimization problem, where the student is trained using a reward signal derived from alignment with the teacher's outputs. This RL driven approach dynamically guides the student to explore multiple denoising paths, allowing it to take longer, optimized steps toward high-probability regions of the data distribution, rather than relying on incremental refinements. Our framework utilizes the inherent ability of diffusion models to handle larger steps and effectively manage the generative process. Experimental results show that our method achieves superior performance with significantly fewer inference steps and computational resources compared to existing distillation techniques. Additionally, the framework is model agnostic, applicable to any type of diffusion models with suitable reward functions, providing a general optimization paradigm for efficient diffusion learning.",
        "url": "http://arxiv.org/abs/2512.22802v1",
        "published_date": "2025-12-28T06:27:24+00:00",
        "updated_date": "2025-12-28T06:27:24+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Amirhossein Tighkhorshid",
            "Zahra Dehghanian",
            "Gholamali Aminian",
            "Chengchun Shi",
            "Hamid R. Rabiee"
        ],
        "tldr": "The paper proposes a reinforcement learning based distillation framework (ReDiF) for diffusion models, enabling faster sampling with fewer steps by training the student model to align with the teacher's outputs through a reward signal, achieving superior performance and computational efficiency compared to existing methods.",
        "tldr_zh": "该论文提出了一种基于强化学习的扩散模型蒸馏框架（ReDiF），通过训练学生模型以通过奖励信号与教师模型的输出对齐，从而能够以更少的步骤进行更快的采样，与现有的方法相比，实现了卓越的性能和计算效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation",
        "summary": "This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.",
        "url": "http://arxiv.org/abs/2512.22905v1",
        "published_date": "2025-12-28T12:25:43+00:00",
        "updated_date": "2025-12-28T12:25:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kai Liu",
            "Jungang Li",
            "Yuchong Sun",
            "Shengqiong Wu",
            "Jianzhang Gao",
            "Daoan Zhang",
            "Wei Zhang",
            "Sheng Jin",
            "Sicheng Yu",
            "Geng Zhan",
            "Jiayi Ji",
            "Fan Zhou",
            "Liang Zheng",
            "Shuicheng Yan",
            "Hao Fei",
            "Tat-Seng Chua"
        ],
        "tldr": "JavisGPT is a unified multimodal LLM for joint audio-video comprehension and generation, trained in three stages with a newly created instruction dataset, and demonstrates superior performance on related benchmarks.",
        "tldr_zh": "JavisGPT 是一个统一的多模态LLM，用于联合音频视频理解和生成。它采用了三阶段训练，并使用了一个新创建的指令数据集，在相关基准测试中表现优异。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning",
        "summary": "Human-object interaction (HOI) video generation has garnered increasing attention due to its promising applications in digital humans, e-commerce, advertising, and robotics imitation learning. However, existing methods face two critical limitations: (1) a lack of effective mechanisms to inject multi-view information of the object into the model, leading to poor cross-view consistency, and (2) heavy reliance on fine-grained hand mesh annotations for modeling interaction occlusions. To address these challenges, we introduce ByteLoom, a Diffusion Transformer (DiT)-based framework that generates realistic HOI videos with geometrically consistent object illustration, using simplified human conditioning and 3D object inputs. We first propose an RCM-cache mechanism that leverages Relative Coordinate Maps (RCM) as a universal representation to maintain object's geometry consistency and precisely control 6-DoF object transformations in the meantime. To compensate HOI dataset scarcity and leverage existing datasets, we further design a training curriculum that enhances model capabilities in a progressive style and relaxes the demand of hand mesh. Extensive experiments demonstrate that our method faithfully preserves human identity and the object's multi-view geometry, while maintaining smooth motion and object manipulation.",
        "url": "http://arxiv.org/abs/2512.22854v1",
        "published_date": "2025-12-28T09:38:36+00:00",
        "updated_date": "2025-12-28T09:38:36+00:00",
        "categories": [
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "authors": [
            "Bangya Liu",
            "Xinyu Gong",
            "Zelin Zhao",
            "Ziyang Song",
            "Yulei Lu",
            "Suhui Wu",
            "Jun Zhang",
            "Suman Banerjee",
            "Hao Zhang"
        ],
        "tldr": "ByteLoom is a Diffusion Transformer framework for generating geometrically consistent human-object interaction videos using relative coordinate maps and a progressive curriculum learning approach to address limitations in existing HOI video generation methods.",
        "tldr_zh": "ByteLoom是一个基于扩散Transformer的框架，用于生成几何一致的人-物交互视频，它使用相对坐标图和渐进式课程学习方法，以解决现有HOI视频生成方法的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Parallel Diffusion Solver via Residual Dirichlet Policy Optimization",
        "summary": "Diffusion models (DMs) have achieved state-of-the-art generative performance but suffer from high sampling latency due to their sequential denoising nature. Existing solver-based acceleration methods often face significant image quality degradation under a low-latency budget, primarily due to accumulated truncation errors arising from the inability to capture high-curvature trajectory segments. In this paper, we propose the Ensemble Parallel Direction solver (dubbed as EPD-Solver), a novel ODE solver that mitigates these errors by incorporating multiple parallel gradient evaluations in each step. Motivated by the geometric insight that sampling trajectories are largely confined to a low-dimensional manifold, EPD-Solver leverages the Mean Value Theorem for vector-valued functions to approximate the integral solution more accurately. Importantly, since the additional gradient computations are independent, they can be fully parallelized, preserving low-latency sampling nature. We introduce a two-stage optimization framework. Initially, EPD-Solver optimizes a small set of learnable parameters via a distillation-based approach. We further propose a parameter-efficient Reinforcement Learning (RL) fine-tuning scheme that reformulates the solver as a stochastic Dirichlet policy. Unlike traditional methods that fine-tune the massive backbone, our RL approach operates strictly within the low-dimensional solver space, effectively mitigating reward hacking while enhancing performance in complex text-to-image (T2I) generation tasks. In addition, our method is flexible and can serve as a plugin (EPD-Plugin) to improve existing ODE samplers.",
        "url": "http://arxiv.org/abs/2512.22796v1",
        "published_date": "2025-12-28T05:48:55+00:00",
        "updated_date": "2025-12-28T05:48:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruoyu Wang",
            "Ziyu Li",
            "Beier Zhu",
            "Liangyu Yuan",
            "Hanwang Zhang",
            "Xun Yang",
            "Xiaojun Chang",
            "Chi Zhang"
        ],
        "tldr": "This paper introduces a novel, parallelizable ODE solver (EPD-Solver) for diffusion models that reduces sampling latency by incorporating multiple parallel gradient evaluations and addresses truncation errors using a reinforcement learning fine-tuning scheme, demonstrating improved performance in text-to-image generation.",
        "tldr_zh": "该论文介绍了一种新颖的可并行ODE求解器(EPD-Solver)，用于扩散模型，通过结合多个并行梯度评估来减少采样延迟，并使用强化学习微调方案解决截断误差，证明了在文本到图像生成方面的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RealCamo: Boosting Real Camouflage Synthesis with Layout Controls and Textual-Visual Guidance",
        "summary": "Camouflaged image generation (CIG) has recently emerged as an efficient alternative for acquiring high-quality training data for camouflaged object detection (COD). However, existing CIG methods still suffer from a substantial gap to real camouflaged imagery: generated images either lack sufficient camouflage due to weak visual similarity, or exhibit cluttered backgrounds that are semantically inconsistent with foreground targets. To address these limitations, we propose ReamCamo, a unified out-painting based framework for realistic camouflaged image generation. ReamCamo explicitly introduces additional layout controls to regulate global image structure, thereby improving semantic coherence between foreground objects and generated backgrounds. Moreover, we construct a multi-modal textual-visual condition by combining a unified fine-grained textual task description with texture-oriented background retrieval, which jointly guides the generation process to enhance visual fidelity and realism. To quantitatively assess camouflage quality, we further introduce a background-foreground distribution divergence metric that measures the effectiveness of camouflage in generated images. Extensive experiments and visualizations demonstrate the effectiveness of our proposed framework.",
        "url": "http://arxiv.org/abs/2512.22974v1",
        "published_date": "2025-12-28T15:37:56+00:00",
        "updated_date": "2025-12-28T15:37:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chunyuan Chen",
            "Yunuo Cai",
            "Shujuan Li",
            "Weiyun Liang",
            "Bin Wang",
            "Jing Xu"
        ],
        "tldr": "The paper introduces ReamCamo, a framework for generating realistic camouflaged images by using layout controls and textual-visual guidance to improve semantic coherence and visual fidelity, addressing limitations in existing camouflaged image generation methods.",
        "tldr_zh": "该论文介绍了 ReamCamo，一个用于生成逼真伪装图像的框架，它通过使用布局控制和文本-视觉指导来提高语义一致性和视觉逼真度，从而解决现有伪装图像生成方法的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]