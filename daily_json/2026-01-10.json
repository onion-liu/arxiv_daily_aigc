[
    {
        "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
        "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
        "url": "http://arxiv.org/abs/2601.05241v1",
        "published_date": "2026-01-08T18:59:22+00:00",
        "updated_date": "2026-01-08T18:59:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Boyang Wang",
            "Haoran Zhang",
            "Shujie Zhang",
            "Jinkun Hao",
            "Mingda Jia",
            "Qi Lv",
            "Yucheng Mao",
            "Zhaoyang Lyu",
            "Jia Zeng",
            "Xudong Xu",
            "Jiangmiao Pang"
        ],
        "tldr": "The paper introduces RoboVIP, a method to augment robot manipulation data using a multi-view video generation pipeline with visual identity prompting to guide image diffusion models, leading to improved policy learning.",
        "tldr_zh": "该论文介绍了RoboVIP，一种通过视觉身份提示引导图像扩散模型的多视角视频生成管道，用于扩充机器人操作数据，从而改进策略学习。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Plenoptic Video Generation",
        "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
        "url": "http://arxiv.org/abs/2601.05239v1",
        "published_date": "2026-01-08T18:58:32+00:00",
        "updated_date": "2026-01-08T18:58:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiao Fu",
            "Shitao Tang",
            "Min Shi",
            "Xian Liu",
            "Jinwei Gu",
            "Ming-Yu Liu",
            "Dahua Lin",
            "Chen-Hsuan Lin"
        ],
        "tldr": "The paper introduces PlenopticDreamer, a framework for multi-view consistent video re-rendering using an autoregressive, camera-guided video generation approach, achieving state-of-the-art results on benchmark datasets.",
        "tldr_zh": "该论文介绍了PlenopticDreamer，一个用于多视角一致视频重渲染的框架，通过使用自回归的、相机引导的视频生成方法，在基准数据集上实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching",
        "summary": "Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.",
        "url": "http://arxiv.org/abs/2601.05212v1",
        "published_date": "2026-01-08T18:36:29+00:00",
        "updated_date": "2026-01-08T18:36:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Danilo Danese",
            "Angela Lombardi",
            "Matteo Attimonelli",
            "Giuseppe Fasano",
            "Tommaso Di Noia"
        ],
        "tldr": "The paper introduces FlowLet, a flow matching-based conditional generative framework that synthesizes age-conditioned 3D brain MRIs in the wavelet domain, improving Brain Age Prediction for underrepresented age groups.",
        "tldr_zh": "本文介绍FlowLet，一个基于流匹配的条件生成框架，在小波域中合成年龄相关的3D脑部MRI，从而提高代表性不足的年龄组的脑年龄预测能力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]