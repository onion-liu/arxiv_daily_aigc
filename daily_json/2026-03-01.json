[
    {
        "title": "Mode Seeking meets Mean Seeking for Fast Long Video Generation",
        "summary": "Scaling video generation from seconds to minutes faces a critical bottleneck: while short-video data is abundant and high-fidelity, coherent long-form data is scarce and limited to narrow domains. To address this, we propose a training paradigm where Mode Seeking meets Mean Seeking, decoupling local fidelity from long-term coherence based on a unified representation via a Decoupled Diffusion Transformer. Our approach utilizes a global Flow Matching head trained via supervised learning on long videos to capture narrative structure, while simultaneously employing a local Distribution Matching head that aligns sliding windows to a frozen short-video teacher via a mode-seeking reverse-KL divergence. This strategy enables the synthesis of minute-scale videos that learns long-range coherence and motions from limited long videos via supervised flow matching, while inheriting local realism by aligning every sliding-window segment of the student to a frozen short-video teacher, resulting in a few-step fast long video generator. Evaluations show that our method effectively closes the fidelity-horizon gap by jointly improving local sharpness, motion and long-range consistency. Project website: https://primecai.github.io/mmm/.",
        "url": "http://arxiv.org/abs/2602.24289v1",
        "published_date": "2026-02-27T18:59:02+00:00",
        "updated_date": "2026-02-27T18:59:02+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Shengqu Cai",
            "Weili Nie",
            "Chao Liu",
            "Julius Berner",
            "Lvmin Zhang",
            "Nanye Ma",
            "Hansheng Chen",
            "Maneesh Agrawala",
            "Leonidas Guibas",
            "Gordon Wetzstein",
            "Arash Vahdat"
        ],
        "tldr": "This paper introduces a new training paradigm called Mode Seeking meets Mean Seeking (MMM) for generating long videos by decoupling local fidelity from long-term coherence using a Decoupled Diffusion Transformer, allowing for fast, minute-scale video generation.",
        "tldr_zh": "本文提出了一种名为“寻模与寻均相遇”(MMM)的训练范式，通过解耦局部保真度和长期连贯性，并使用解耦扩散Transformer生成长视频，从而实现快速的分钟级视频生成。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    }
]