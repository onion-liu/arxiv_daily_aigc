[
    {
        "title": "WoW: Towards a World omniscient World model Through Embodied Interaction",
        "summary": "Humans develop an understanding of intuitive physics through active\ninteraction with the world. This approach is in stark contrast to current video\nmodels, such as Sora, which rely on passive observation and therefore struggle\nwith grasping physical causality. This observation leads to our central\nhypothesis: authentic physical intuition of the world model must be grounded in\nextensive, causally rich interactions with the real world. To test this\nhypothesis, we present WoW, a 14-billion-parameter generative world model\ntrained on 2 million robot interaction trajectories. Our findings reveal that\nthe model's understanding of physics is a probabilistic distribution of\nplausible outcomes, leading to stochastic instabilities and physical\nhallucinations. Furthermore, we demonstrate that this emergent capability can\nbe actively constrained toward physical realism by SOPHIA, where\nvision-language model agents evaluate the DiT-generated output and guide its\nrefinement by iteratively evolving the language instructions. In addition, a\nco-trained Inverse Dynamics Model translates these refined plans into\nexecutable robotic actions, thus closing the imagination-to-action loop. We\nestablish WoWBench, a new benchmark focused on physical consistency and causal\nreasoning in video, where WoW achieves state-of-the-art performance in both\nhuman and autonomous evaluation, demonstrating strong ability in physical\ncausality, collision dynamics, and object permanence. Our work provides\nsystematic evidence that large-scale, real-world interaction is a cornerstone\nfor developing physical intuition in AI. Models, data, and benchmarks will be\nopen-sourced.",
        "url": "http://arxiv.org/abs/2509.22642v1",
        "published_date": "2025-09-26T17:59:07+00:00",
        "updated_date": "2025-09-26T17:59:07+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Xiaowei Chi",
            "Peidong Jia",
            "Chun-Kai Fan",
            "Xiaozhu Ju",
            "Weishi Mi",
            "Kevin Zhang",
            "Zhiyuan Qin",
            "Wanxin Tian",
            "Kuangzhi Ge",
            "Hao Li",
            "Zezhong Qian",
            "Anthony Chen",
            "Qiang Zhou",
            "Yueru Jia",
            "Jiaming Liu",
            "Yong Dai",
            "Qingpo Wuwu",
            "Chengyu Bai",
            "Yu-Kai Wang",
            "Ying Li",
            "Lizhang Chen",
            "Yong Bao",
            "Zhiyuan Jiang",
            "Jiacheng Zhu",
            "Kai Tang",
            "Ruichuan An",
            "Yulin Luo",
            "Qiuxuan Feng",
            "Siyuan Zhou",
            "Chi-min Chan",
            "Chengkai Hou",
            "Wei Xue",
            "Sirui Han",
            "Yike Guo",
            "Shanghang Zhang",
            "Jian Tang"
        ],
        "tldr": "The paper introduces WoW, a generative world model trained on robot interaction trajectories, demonstrating improved physical intuition and causal reasoning in video generation through interaction with the real world and iterative refinement using vision-language models.",
        "tldr_zh": "该论文介绍了WoW，一个基于机器人交互轨迹训练的生成世界模型，通过与现实世界的交互以及使用视觉-语言模型进行迭代优化，展示了在视频生成中改进的物理直觉和因果推理能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Scale-Wise VAR is Secretly Discrete Diffusion",
        "summary": "Autoregressive (AR) transformers have emerged as a powerful paradigm for\nvisual generation, largely due to their scalability, computational efficiency\nand unified architecture with language and vision. Among them, next scale\nprediction Visual Autoregressive Generation (VAR) has recently demonstrated\nremarkable performance, even surpassing diffusion-based models. In this work,\nwe revisit VAR and uncover a theoretical insight: when equipped with a\nMarkovian attention mask, VAR is mathematically equivalent to a discrete\ndiffusion. We term this reinterpretation as Scalable Visual Refinement with\nDiscrete Diffusion (SRDD), establishing a principled bridge between AR\ntransformers and diffusion models. Leveraging this new perspective, we show how\none can directly import the advantages of diffusion such as iterative\nrefinement and reduce architectural inefficiencies into VAR, yielding faster\nconvergence, lower inference cost, and improved zero-shot reconstruction.\nAcross multiple datasets, we show that the diffusion based perspective of VAR\nleads to consistent gains in efficiency and generation.",
        "url": "http://arxiv.org/abs/2509.22636v1",
        "published_date": "2025-09-26T17:58:04+00:00",
        "updated_date": "2025-09-26T17:58:04+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Amandeep Kumar",
            "Nithin Gopalakrishnan Nair",
            "Vishal M. Patel"
        ],
        "tldr": "This paper reveals that scale-wise VAR transformers with Markovian attention are mathematically equivalent to discrete diffusion models, enabling performance improvements by importing diffusion techniques into VAR.",
        "tldr_zh": "本文揭示了具有马尔可夫注意力机制的 scale-wise VAR 变换器在数学上等价于离散扩散模型，通过将扩散技术引入 VAR 来提高性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "LongLive: Real-time Interactive Long Video Generation",
        "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
        "url": "http://arxiv.org/abs/2509.22622v1",
        "published_date": "2025-09-26T17:48:24+00:00",
        "updated_date": "2025-09-26T17:48:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuai Yang",
            "Wei Huang",
            "Ruihang Chu",
            "Yicheng Xiao",
            "Yuyang Zhao",
            "Xianbang Wang",
            "Muyang Li",
            "Enze Xie",
            "Yingcong Chen",
            "Yao Lu",
            "Song Han",
            "Yukang Chen"
        ],
        "tldr": "LongLive is a real-time, interactive long video generation framework using a frame-level autoregressive model with KV-recaching, streaming long tuning, and frame-level attention sinks to address efficiency and quality challenges.",
        "tldr_zh": "LongLive是一个实时的，交互式的长视频生成框架，它采用帧级别的自回归模型，结合KV-recaching，流式长时训练和帧级别的注意力汇聚机制，以解决效率和质量方面的挑战。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "X-Streamer: Unified Human World Modeling with Audiovisual Interaction",
        "summary": "We introduce X-Streamer, an end-to-end multimodal human world modeling\nframework for building digital human agents capable of infinite interactions\nacross text, speech, and video within a single unified architecture. Starting\nfrom a single portrait, X-Streamer enables real-time, open-ended video calls\ndriven by streaming multimodal inputs. At its core is a Thinker-Actor\ndual-transformer architecture that unifies multimodal understanding and\ngeneration, turning a static portrait into persistent and intelligent\naudiovisual interactions. The Thinker module perceives and reasons over\nstreaming user inputs, while its hidden states are translated by the Actor into\nsynchronized multimodal streams in real time. Concretely, the Thinker leverages\na pretrained large language-speech model, while the Actor employs a chunk-wise\nautoregressive diffusion model that cross-attends to the Thinker's hidden\nstates to produce time-aligned multimodal responses with interleaved discrete\ntext and audio tokens and continuous video latents. To ensure long-horizon\nstability, we design inter- and intra-chunk attentions with time-aligned\nmultimodal positional embeddings for fine-grained cross-modality alignment and\ncontext retention, further reinforced by chunk-wise diffusion forcing and\nglobal identity referencing. X-Streamer runs in real time on two A100 GPUs,\nsustaining hours-long consistent video chat experiences from arbitrary\nportraits and paving the way toward unified world modeling of interactive\ndigital humans.",
        "url": "http://arxiv.org/abs/2509.21574v1",
        "published_date": "2025-09-25T20:53:27+00:00",
        "updated_date": "2025-09-25T20:53:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "You Xie",
            "Tianpei Gu",
            "Zenan Li",
            "Chenxu Zhang",
            "Guoxian Song",
            "Xiaochen Zhao",
            "Chao Liang",
            "Jianwen Jiang",
            "Hongyi Xu",
            "Linjie Luo"
        ],
        "tldr": "X-Streamer is a multimodal framework enabling real-time, open-ended video calls with digital human agents from a single portrait, using a dual-transformer architecture for unified understanding and generation across text, speech, and video.",
        "tldr_zh": "X-Streamer是一个多模态框架，可以通过单个肖像实现与数字人代理的实时、开放式视频通话，使用双转换器架构来实现跨文本、语音和视频的统一理解和生成。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Group Critical-token Policy Optimization for Autoregressive Image Generation",
        "summary": "Recent studies have extended Reinforcement Learning with Verifiable Rewards\n(RLVR) to autoregressive (AR) visual generation and achieved promising\nprogress. However, existing methods typically apply uniform optimization across\nall image tokens, while the varying contributions of different image tokens for\nRLVR's training remain unexplored. In fact, the key obstacle lies in how to\nidentify more critical image tokens during AR generation and implement\neffective token-wise optimization for them. To tackle this challenge, we\npropose $\\textbf{G}$roup $\\textbf{C}$ritical-token $\\textbf{P}$olicy\n$\\textbf{O}$ptimization ($\\textbf{GCPO}$), which facilitates effective policy\noptimization on critical tokens. We identify the critical tokens in RLVR-based\nAR generation from three perspectives, specifically: $\\textbf{(1)}$ Causal\ndependency: early tokens fundamentally determine the later tokens and final\nimage effect due to unidirectional dependency; $\\textbf{(2)}$ Entropy-induced\nspatial structure: tokens with high entropy gradients correspond to image\nstructure and bridges distinct visual regions; $\\textbf{(3)}$ RLVR-focused\ntoken diversity: tokens with low visual similarity across a group of sampled\nimages contribute to richer token-level diversity. For these identified\ncritical tokens, we further introduce a dynamic token-wise advantage weight to\nencourage exploration, based on confidence divergence between the policy model\nand reference model. By leveraging 30\\% of the image tokens, GCPO achieves\nbetter performance than GRPO with full tokens. Extensive experiments on\nmultiple text-to-image benchmarks for both AR models and unified multimodal\nmodels demonstrate the effectiveness of GCPO for AR visual generation.",
        "url": "http://arxiv.org/abs/2509.22485v1",
        "published_date": "2025-09-26T15:33:18+00:00",
        "updated_date": "2025-09-26T15:33:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guohui Zhang",
            "Hu Yu",
            "Xiaoxiao Ma",
            "JingHao Zhang",
            "Yaning Pan",
            "Mingde Yao",
            "Jie Xiao",
            "Linjiang Huang",
            "Feng Zhao"
        ],
        "tldr": "This paper introduces Group Critical-token Policy Optimization (GCPO) for autoregressive image generation, which identifies and optimizes critical tokens based on causal dependency, entropy, and token diversity, achieving improved performance with only 30% of the tokens.",
        "tldr_zh": "本文介绍了用于自回归图像生成的组关键令牌策略优化（GCPO），该方法基于因果依赖、熵和令牌多样性来识别和优化关键令牌，仅使用 30% 的令牌即可实现更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.5
    },
    {
        "title": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformer",
        "summary": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality.",
        "url": "http://arxiv.org/abs/2509.22323v1",
        "published_date": "2025-09-26T13:20:52+00:00",
        "updated_date": "2025-09-26T13:20:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wangbo Zhao",
            "Yizeng Han",
            "Zhiwei Tang",
            "Jiasheng Tang",
            "Pengfei Zhou",
            "Kai Wang",
            "Bohan Zhuang",
            "Zhangyang Wang",
            "Fan Wang",
            "Yang You"
        ],
        "tldr": "RAPID^3 introduces a training-free framework for accelerating Diffusion Transformer sampling by using reinforced learning to dynamically adjust step size, caching, and attention mechanisms, achieving nearly 3x speedup with competitive generation quality.",
        "tldr_zh": "RAPID^3 提出了一个无需训练的框架，通过使用强化学习来动态调整步长、缓存和注意力机制，加速扩散Transformer的采样过程，在保持竞争力的生成质量的同时，实现了近3倍的速度提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models",
        "summary": "While diffusion models have made remarkable progress in image generation,\ntheir outputs can still appear unrealistic and lack fine details, especially\nwhen using fewer number of neural function evaluations (NFEs) or lower guidance\nscales. To address this issue, we propose a novel momentum-based sampling\ntechnique, termed history-guided sampling (HiGS), which enhances quality and\nefficiency of diffusion sampling by integrating recent model predictions into\neach inference step. Specifically, HiGS leverages the difference between the\ncurrent prediction and a weighted average of past predictions to steer the\nsampling process toward more realistic outputs with better details and\nstructure. Our approach introduces practically no additional computation and\nintegrates seamlessly into existing diffusion frameworks, requiring neither\nextra training nor fine-tuning. Extensive experiments show that HiGS\nconsistently improves image quality across diverse models and architectures and\nunder varying sampling budgets and guidance scales. Moreover, using a\npretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for\nunguided ImageNet generation at 256$\\times$256 with only 30 sampling steps\n(instead of the standard 250). We thus present HiGS as a plug-and-play\nenhancement to standard diffusion sampling that enables faster generation with\nhigher fidelity.",
        "url": "http://arxiv.org/abs/2509.22300v1",
        "published_date": "2025-09-26T13:01:10+00:00",
        "updated_date": "2025-09-26T13:01:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Seyedmorteza Sadat",
            "Farnood Salehi",
            "Romann M. Weber"
        ],
        "tldr": "The paper introduces HiGS, a plug-and-play sampling technique for diffusion models that uses a history-guided approach to improve image quality and generation speed without requiring additional training or fine-tuning. It can achieve state-of-the-art FID scores with fewer sampling steps.",
        "tldr_zh": "该论文介绍了一种名为HiGS的即插即用采样技术，用于扩散模型，该技术使用历史引导方法来提高图像质量和生成速度，而无需额外的训练或微调。它能够以更少的采样步骤实现最先进的FID分数。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniMapGen: A Generative Framework for Large-Scale Map Construction from Multi-modal Data",
        "summary": "Large-scale map construction is foundational for critical applications such\nas autonomous driving and navigation systems. Traditional large-scale map\nconstruction approaches mainly rely on costly and inefficient special data\ncollection vehicles and labor-intensive annotation processes. While existing\nsatellite-based methods have demonstrated promising potential in enhancing the\nefficiency and coverage of map construction, they exhibit two major\nlimitations: (1) inherent drawbacks of satellite data (e.g., occlusions,\noutdatedness) and (2) inefficient vectorization from perception-based methods,\nresulting in discontinuous and rough roads that require extensive\npost-processing. This paper presents a novel generative framework, UniMapGen,\nfor large-scale map construction, offering three key innovations: (1)\nrepresenting lane lines as \\textbf{discrete sequence} and establishing an\niterative strategy to generate more complete and smooth map vectors than\ntraditional perception-based methods. (2) proposing a flexible architecture\nthat supports \\textbf{multi-modal} inputs, enabling dynamic selection among\nBEV, PV, and text prompt, to overcome the drawbacks of satellite data. (3)\ndeveloping a \\textbf{state update} strategy for global continuity and\nconsistency of the constructed large-scale map. UniMapGen achieves\nstate-of-the-art performance on the OpenSatMap dataset. Furthermore, UniMapGen\ncan infer occluded roads and predict roads missing from dataset annotations.\nOur code will be released.",
        "url": "http://arxiv.org/abs/2509.22262v1",
        "published_date": "2025-09-26T12:26:33+00:00",
        "updated_date": "2025-09-26T12:26:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yujian Yuan",
            "Changjie Wu",
            "Xinyuan Chang",
            "Sijin Wang",
            "Hang Zhang",
            "Shiyi Liang",
            "Shuang Zeng",
            "Mu Xu"
        ],
        "tldr": "UniMapGen is a new generative framework for large-scale map construction from multi-modal data, addressing limitations of satellite-based methods by using discrete sequence representation, multi-modal inputs, and state update strategies to generate complete and smooth road maps.",
        "tldr_zh": "UniMapGen是一个新型的生成式框架，用于从多模态数据中构建大规模地图，通过使用离散序列表示、多模态输入和状态更新策略，解决了基于卫星的方法的局限性，从而生成完整且平滑的道路地图。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing",
        "summary": "Text-guided image editing with diffusion models has achieved remarkable\nquality but suffers from prohibitive latency, hindering real-world\napplications. We introduce FlashEdit, a novel framework designed to enable\nhigh-fidelity, real-time image editing. Its efficiency stems from three key\ninnovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses\ncostly iterative processes; (2) a Background Shield (BG-Shield) technique that\nguarantees background preservation by selectively modifying features only\nwithin the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA)\nmechanism that ensures precise, localized edits by suppressing semantic leakage\nto the background. Extensive experiments demonstrate that FlashEdit maintains\nsuperior background consistency and structural integrity, while performing\nedits in under 0.2 seconds, which is an over 150$\\times$ speedup compared to\nprior multi-step methods. Our code will be made publicly available at\nhttps://github.com/JunyiWuCode/FlashEdit.",
        "url": "http://arxiv.org/abs/2509.22244v1",
        "published_date": "2025-09-26T11:59:30+00:00",
        "updated_date": "2025-09-26T11:59:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junyi Wu",
            "Zhiteng Li",
            "Haotong Qin",
            "Xiaohong Liu",
            "Linghe Kong",
            "Yulun Zhang",
            "Xiaokang Yang"
        ],
        "tldr": "FlashEdit introduces a fast, one-step image editing framework using diffusion models, achieving significant speedups while maintaining image quality and background consistency through novel techniques like OSIE, BG-Shield, and SSCA.",
        "tldr_zh": "FlashEdit 提出了一个快速的图像编辑框架，该框架利用扩散模型并通过 OSIE、BG-Shield 和 SSCA 等创新技术实现显著的加速，同时保持图像质量和背景一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DragGANSpace: Latent Space Exploration and Control for GANs",
        "summary": "This work integrates StyleGAN, DragGAN and Principal Component Analysis (PCA)\nto enhance the latent space efficiency and controllability of GAN-generated\nimages. Style-GAN provides a structured latent space, DragGAN enables intuitive\nimage manipulation, and PCA reduces dimensionality and facilitates cross-model\nalignment for more streamlined and interpretable exploration of latent spaces.\nWe apply our techniques to the Animal Faces High Quality (AFHQ) dataset, and\nfind that our approach of integrating PCA-based dimensionality reduction with\nthe Drag-GAN framework for image manipulation retains performance while\nimproving optimization efficiency. Notably, introducing PCA into the latent W+\nlayers of DragGAN can consistently reduce the total optimization time while\nmaintaining good visual quality and even boosting the Structural Similarity\nIndex Measure (SSIM) of the optimized image, particularly in shallower latent\nspaces (W+ layers = 3). We also demonstrate capability for aligning images\ngenerated by two StyleGAN models trained on similar but distinct data domains\n(AFHQ-Dog and AFHQ-Cat), and show that we can control the latent space of these\naligned images to manipulate the images in an intuitive and interpretable\nmanner. Our findings highlight the possibility for efficient and interpretable\nlatent space control for a wide range of image synthesis and editing\napplications.",
        "url": "http://arxiv.org/abs/2509.22169v1",
        "published_date": "2025-09-26T10:30:49+00:00",
        "updated_date": "2025-09-26T10:30:49+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Kirsten Odendaal",
            "Neela Kaushik",
            "Spencer Halverson"
        ],
        "tldr": "This paper introduces DragGANSpace, a method that combines StyleGAN, DragGAN, and PCA to improve the efficiency and controllability of GAN latent spaces, demonstrated on animal faces.",
        "tldr_zh": "本文介绍了 DragGANSpace，一种结合 StyleGAN、DragGAN 和 PCA 的方法，旨在提高 GAN 潜在空间的效率和可控性，并在动物面部数据集上进行了演示。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "REFINE-CONTROL: A Semi-supervised Distillation Method For Conditional Image Generation",
        "summary": "Conditional image generation models have achieved remarkable results by\nleveraging text-based control to generate customized images. However, the high\nresource demands of these models and the scarcity of well-annotated data have\nhindered their deployment on edge devices, leading to enormous costs and\nprivacy concerns, especially when user data is sent to a third party. To\novercome these challenges, we propose Refine-Control, a semi-supervised\ndistillation framework. Specifically, we improve the performance of the student\nmodel by introducing a tri-level knowledge fusion loss to transfer different\nlevels of knowledge. To enhance generalization and alleviate dataset scarcity,\nwe introduce a semi-supervised distillation method utilizing both labeled and\nunlabeled data. Our experiments reveal that Refine-Control achieves significant\nreductions in computational cost and latency, while maintaining high-fidelity\ngeneration capabilities and controllability, as quantified by comparative\nmetrics.",
        "url": "http://arxiv.org/abs/2509.22139v1",
        "published_date": "2025-09-26T09:59:40+00:00",
        "updated_date": "2025-09-26T09:59:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yicheng Jiang",
            "Jin Yuan",
            "Hua Yuan",
            "Yao Zhang",
            "Yong Rui"
        ],
        "tldr": "The paper proposes Refine-Control, a semi-supervised distillation framework for conditional image generation, aiming to reduce computational costs and enhance generalization for deployment on edge devices.",
        "tldr_zh": "该论文提出了Refine-Control，一种半监督蒸馏框架，用于条件图像生成，旨在降低计算成本并增强泛化能力，以便在边缘设备上部署。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Guidance Watermarking for Diffusion Models",
        "summary": "This paper introduces a novel watermarking method for diffusion models. It is\nbased on guiding the diffusion process using the gradient computed from any\noff-the-shelf watermark decoder. The gradient computation encompasses different\nimage augmentations, increasing robustness to attacks against which the decoder\nwas not originally robust, without retraining or fine-tuning. Our method\neffectively convert any \\textit{post-hoc} watermarking scheme into an\nin-generation embedding along the diffusion process. We show that this approach\nis complementary to watermarking techniques modifying the variational\nautoencoder at the end of the diffusion process. We validate the methods on\ndifferent diffusion models and detectors. The watermarking guidance does not\nsignificantly alter the generated image for a given seed and prompt, preserving\nboth the diversity and quality of generation.",
        "url": "http://arxiv.org/abs/2509.22126v1",
        "published_date": "2025-09-26T09:49:44+00:00",
        "updated_date": "2025-09-26T09:49:44+00:00",
        "categories": [
            "cs.CR",
            "cs.CV"
        ],
        "authors": [
            "Enoal Gesny",
            "Eva Giboulot",
            "Teddy Furon",
            "Vivien Chappelier"
        ],
        "tldr": "This paper presents a novel watermarking technique for diffusion models that guides the diffusion process using gradients from existing watermark decoders, enhancing robustness without retraining and preserving generation quality.",
        "tldr_zh": "本文提出了一种新颖的扩散模型水印技术，该技术利用现有水印解码器的梯度来引导扩散过程，从而在无需重新训练的情况下提高鲁棒性并保持生成质量。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Large Material Gaussian Model for Relightable 3D Generation",
        "summary": "The increasing demand for 3D assets across various industries necessitates\nefficient and automated methods for 3D content creation. Leveraging 3D Gaussian\nSplatting, recent large reconstruction models (LRMs) have demonstrated the\nability to efficiently achieve high-quality 3D rendering by integrating\nmultiview diffusion for generation and scalable transformers for\nreconstruction. However, existing models fail to produce the material\nproperties of assets, which is crucial for realistic rendering in diverse\nlighting environments. In this paper, we introduce the Large Material Gaussian\nModel (MGM), a novel framework designed to generate high-quality 3D content\nwith Physically Based Rendering (PBR) materials, ie, albedo, roughness, and\nmetallic properties, rather than merely producing RGB textures with\nuncontrolled light baking. Specifically, we first fine-tune a new multiview\nmaterial diffusion model conditioned on input depth and normal maps. Utilizing\nthe generated multiview PBR images, we explore a Gaussian material\nrepresentation that not only aligns with 2D Gaussian Splatting but also models\neach channel of the PBR materials. The reconstructed point clouds can then be\nrendered to acquire PBR attributes, enabling dynamic relighting by applying\nvarious ambient light maps. Extensive experiments demonstrate that the\nmaterials produced by our method not only exhibit greater visual appeal\ncompared to baseline methods but also enhance material modeling, thereby\nenabling practical downstream rendering applications.",
        "url": "http://arxiv.org/abs/2509.22112v1",
        "published_date": "2025-09-26T09:35:12+00:00",
        "updated_date": "2025-09-26T09:35:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingrui Ye",
            "Lingting Zhu",
            "Runze Zhang",
            "Zeyu Hu",
            "Yingda Yin",
            "Lanjiong Li",
            "Lequan Yu",
            "Qingmin Liao"
        ],
        "tldr": "This paper introduces Large Material Gaussian Model (MGM) to generate 3D assets with PBR materials, enabling relighting capabilities, by fine-tuning a multiview material diffusion model and utilizing a Gaussian material representation.",
        "tldr_zh": "本文介绍了大型材质高斯模型（MGM），它通过微调多视角材质扩散模型并利用高斯材质表示，生成具有PBR材质的3D资产，从而实现重新照明功能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning",
        "summary": "Multi-subject image generation aims to synthesize user-provided subjects in a\nsingle image while preserving subject fidelity, ensuring prompt consistency,\nand aligning with human aesthetic preferences. However, existing methods,\nparticularly those built on the In-Context-Learning paradigm, are limited by\ntheir reliance on simple reconstruction-based objectives, leading to both\nsevere attribute leakage that compromises subject fidelity and failing to align\nwith nuanced human preferences. To address this, we propose MultiCrafter, a\nframework that ensures high-fidelity, preference-aligned generation. First, we\nfind that the root cause of attribute leakage is a significant entanglement of\nattention between different subjects during the generation process. Therefore,\nwe introduce explicit positional supervision to explicitly separate attention\nregions for each subject, effectively mitigating attribute leakage. To enable\nthe model to accurately plan the attention region of different subjects in\ndiverse scenarios, we employ a Mixture-of-Experts architecture to enhance the\nmodel's capacity, allowing different experts to focus on different scenarios.\nFinally, we design a novel online reinforcement learning framework to align the\nmodel with human preferences, featuring a scoring mechanism to accurately\nassess multi-subject fidelity and a more stable training strategy tailored for\nthe MoE architecture. Experiments validate that our framework significantly\nimproves subject fidelity while aligning with human preferences better.",
        "url": "http://arxiv.org/abs/2509.21953v1",
        "published_date": "2025-09-26T06:41:43+00:00",
        "updated_date": "2025-09-26T06:41:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tao Wu",
            "Yibo Jiang",
            "Yehao Lu",
            "Zhizhong Wang",
            "Zeyi Huang",
            "Zequn Qin",
            "Xi Li"
        ],
        "tldr": "MultiCrafter addresses the problem of attribute leakage in multi-subject image generation by introducing spatially disentangled attention with positional supervision and aligning with human preferences using reinforcement learning.",
        "tldr_zh": "MultiCrafter通过引入带位置监督的空间解耦注意力机制，并使用强化学习与人类偏好对齐，解决了多主体图像生成中的属性泄露问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Taming Flow-based I2V Models for Creative Video Editing",
        "summary": "Although image editing techniques have advanced significantly, video editing,\nwhich aims to manipulate videos according to user intent, remains an emerging\nchallenge. Most existing image-conditioned video editing methods either require\ninversion with model-specific design or need extensive optimization, limiting\ntheir capability of leveraging up-to-date image-to-video (I2V) models to\ntransfer the editing capability of image editing models to the video domain. To\nthis end, we propose IF-V2V, an Inversion-Free method that can adapt\noff-the-shelf flow-matching-based I2V models for video editing without\nsignificant computational overhead. To circumvent inversion, we devise Vector\nField Rectification with Sample Deviation to incorporate information from the\nsource video into the denoising process by introducing a deviation term into\nthe denoising vector field. To further ensure consistency with the source video\nin a model-agnostic way, we introduce Structure-and-Motion-Preserving\nInitialization to generate motion-aware temporally correlated noise with\nstructural information embedded. We also present a Deviation Caching mechanism\nto minimize the additional computational cost for denoising vector\nrectification without significantly impacting editing quality. Evaluations\ndemonstrate that our method achieves superior editing quality and consistency\nover existing approaches, offering a lightweight plug-and-play solution to\nrealize visual creativity.",
        "url": "http://arxiv.org/abs/2509.21917v1",
        "published_date": "2025-09-26T05:57:04+00:00",
        "updated_date": "2025-09-26T05:57:04+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Xianghao Kong",
            "Hansheng Chen",
            "Yuwei Guo",
            "Lvmin Zhang",
            "Gordon Wetzstein",
            "Maneesh Agrawala",
            "Anyi Rao"
        ],
        "tldr": "The paper introduces IF-V2V, an inversion-free method for adapting flow-matching-based image-to-video models for creative video editing, improving consistency and reducing computational overhead compared to existing approaches.",
        "tldr_zh": "该论文介绍了IF-V2V，一种无需反演的方法，用于调整基于流匹配的图像到视频模型，以进行创意视频编辑，与现有方法相比，提高了连贯性并降低了计算开销。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image Manipulation",
        "summary": "This paper explores image editing under the joint control of text and drag\ninteractions. While recent advances in text-driven and drag-driven editing have\nachieved remarkable progress, they suffer from complementary limitations:\ntext-driven methods excel in texture manipulation but lack precise spatial\ncontrol, whereas drag-driven approaches primarily modify shape and structure\nwithout fine-grained texture guidance. To address these limitations, we propose\na unified diffusion-based framework for joint drag-text image editing,\nintegrating the strengths of both paradigms. Our framework introduces two key\ninnovations: (1) Point-Cloud Deterministic Drag, which enhances latent-space\nlayout control through 3D feature mapping, and (2) Drag-Text Guided Denoising,\ndynamically balancing the influence of drag and text conditions during\ndenoising. Notably, our model supports flexible editing modes - operating with\ntext-only, drag-only, or combined conditions - while maintaining strong\nperformance in each setting. Extensive quantitative and qualitative experiments\ndemonstrate that our method not only achieves high-fidelity joint editing but\nalso matches or surpasses the performance of specialized text-only or drag-only\napproaches, establishing a versatile and generalizable solution for\ncontrollable image manipulation. Code will be made publicly available to\nreproduce all results presented in this work.",
        "url": "http://arxiv.org/abs/2509.21905v1",
        "published_date": "2025-09-26T05:39:03+00:00",
        "updated_date": "2025-09-26T05:39:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qihang Wang",
            "Yaxiong Wang",
            "Lechao Cheng",
            "Zhun Zhong"
        ],
        "tldr": "The paper introduces TDEdit, a unified diffusion framework for image manipulation controlled by both text prompts and drag interactions, addressing the limitations of text-only and drag-only approaches.",
        "tldr_zh": "该论文介绍了TDEdit，一个统一的扩散框架，用于通过文本提示和拖动交互控制图像编辑，解决了纯文本和纯拖动方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers",
        "summary": "Text-to-video and image-to-video generation have made rapid progress in\nvisual quality, but they remain limited in controlling the precise timing of\nmotion. In contrast, audio provides temporal cues aligned with video motion,\nmaking it a promising condition for temporally controlled video generation.\nHowever, existing audio-to-video (A2V) models struggle with fine-grained\nsynchronization due to indirect conditioning mechanisms or limited temporal\nmodeling capacity. We present Syncphony, which generates 380x640 resolution,\n24fps videos synchronized with diverse audio inputs. Our approach builds upon a\npre-trained video backbone and incorporates two key components to improve\nsynchronization: (1) Motion-aware Loss, which emphasizes learning at\nhigh-motion regions; (2) Audio Sync Guidance, which guides the full model using\na visually aligned off-sync model without audio layers to better exploit audio\ncues at inference while maintaining visual quality. To evaluate\nsynchronization, we propose CycleSync, a video-to-audio-based metric that\nmeasures the amount of motion cues in the generated video to reconstruct the\noriginal audio. Experiments on AVSync15 and The Greatest Hits datasets\ndemonstrate that Syncphony outperforms existing methods in both synchronization\naccuracy and visual quality. Project page is available at:\nhttps://jibin86.github.io/syncphony_project_page",
        "url": "http://arxiv.org/abs/2509.21893v1",
        "published_date": "2025-09-26T05:30:06+00:00",
        "updated_date": "2025-09-26T05:30:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jibin Song",
            "Mingi Kwon",
            "Jaeseok Jeong",
            "Youngjung Uh"
        ],
        "tldr": "Syncphony introduces a novel method for synchronized audio-to-video generation using diffusion transformers, motion-aware loss, and audio sync guidance, outperforming existing methods in both synchronization accuracy and visual quality.",
        "tldr_zh": "Syncphony 提出了一种新的同步音频到视频生成方法，使用扩散Transformer，运动感知损失和音频同步指导，在同步精度和视觉质量方面优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing",
        "summary": "The visual dubbing task aims to generate mouth movements synchronized with\nthe driving audio, which has seen significant progress in recent years.\nHowever, two critical deficiencies hinder their wide application: (1)\nAudio-only driving paradigms inadequately capture speaker-specific lip habits,\nwhich fail to generate lip movements similar to the target avatar; (2)\nConventional blind-inpainting approaches frequently produce visual artifacts\nwhen handling obstructions (e.g., microphones, hands), limiting practical\ndeployment. In this paper, we propose StableDub, a novel and concise framework\nintegrating lip-habit-aware modeling with occlusion-robust synthesis.\nSpecifically, building upon the Stable-Diffusion backbone, we develop a\nlip-habit-modulated mechanism that jointly models phonemic audio-visual\nsynchronization and speaker-specific orofacial dynamics. To achieve plausible\nlip geometries and object appearances under occlusion, we introduce the\nocclusion-aware training strategy by explicitly exposing the occlusion objects\nto the inpainting process. By incorporating the proposed designs, the model\neliminates the necessity for cost-intensive priors in previous methods, thereby\nexhibiting superior training efficiency on the computationally intensive\ndiffusion-based backbone. To further optimize training efficiency from the\nperspective of model architecture, we introduce a hybrid Mamba-Transformer\narchitecture, which demonstrates the enhanced applicability in low-resource\nresearch scenarios. Extensive experimental results demonstrate that StableDub\nachieves superior performance in lip habit resemblance and occlusion\nrobustness. Our method also surpasses other methods in audio-lip sync, video\nquality, and resolution consistency. We expand the applicability of visual\ndubbing methods from comprehensive aspects, and demo videos can be found at\nhttps://stabledub.github.io.",
        "url": "http://arxiv.org/abs/2509.21887v1",
        "published_date": "2025-09-26T05:23:31+00:00",
        "updated_date": "2025-09-26T05:23:31+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Liyang Chen",
            "Tianze Zhou",
            "Xu He",
            "Boshi Tang",
            "Zhiyong Wu",
            "Yang Huang",
            "Yang Wu",
            "Zhongqian Sun",
            "Wei Yang",
            "Helen Meng"
        ],
        "tldr": "StableDub introduces a novel visual dubbing framework using lip-habit-aware modeling and occlusion-robust synthesis within a Stable-Diffusion backbone, achieving state-of-the-art performance and improved training efficiency via a hybrid Mamba-Transformer architecture.",
        "tldr_zh": "StableDub提出了一种新颖的视觉配音框架，该框架在Stable-Diffusion骨干网络中利用了唇部习惯感知建模和遮挡鲁棒合成技术，通过混合Mamba-Transformer架构实现了最先进的性能并提高了训练效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE",
        "summary": "Video-based world models hold significant potential for generating\nhigh-quality embodied manipulation data. However, current video generation\nmethods struggle to achieve stable long-horizon generation: classical\ndiffusion-based approaches often suffer from temporal inconsistency and visual\ndrift over multiple rollouts, while autoregressive methods tend to compromise\non visual detail. To solve this, we introduce LongScape, a hybrid framework\nthat adaptively combines intra-chunk diffusion denoising with inter-chunk\nautoregressive causal generation. Our core innovation is an action-guided,\nvariable-length chunking mechanism that partitions video based on the semantic\ncontext of robotic actions. This ensures each chunk represents a complete,\ncoherent action, enabling the model to flexibly generate diverse dynamics. We\nfurther introduce a Context-aware Mixture-of-Experts (CMoE) framework that\nadaptively activates specialized experts for each chunk during generation,\nguaranteeing high visual quality and seamless chunk transitions. Extensive\nexperimental results demonstrate that our method achieves stable and consistent\nlong-horizon generation over extended rollouts. Our code is available at:\nhttps://github.com/tsinghua-fib-lab/Longscape.",
        "url": "http://arxiv.org/abs/2509.21790v1",
        "published_date": "2025-09-26T02:47:05+00:00",
        "updated_date": "2025-09-26T02:47:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Shang",
            "Lei Jin",
            "Yiding Ma",
            "Xin Zhang",
            "Chen Gao",
            "Wei Wu",
            "Yong Li"
        ],
        "tldr": "LongScape introduces a hybrid video generation framework combining diffusion and autoregressive methods with a context-aware chunking mechanism and a Mixture-of-Experts to achieve stable long-horizon generation in embodied manipulation tasks.",
        "tldr_zh": "LongScape 提出了一种混合视频生成框架，结合了扩散和自回归方法，并采用上下文感知的分块机制和混合专家模型，以在具身操作任务中实现稳定的长程生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models",
        "summary": "Large language models, trained on extensive corpora, successfully unify\ndiverse linguistic tasks within a single generative framework. Inspired by\nthis, recent works like Large Vision Model (LVM) extend this paradigm to vision\nby organizing tasks into sequential visual sentences, where visual prompts\nserve as the context to guide outputs. However, such modeling requires\ntask-specific pre-training across modalities and sources, which is costly and\nlimits scalability to unseen tasks. Given that pre-trained video generation\nmodels inherently capture temporal sequence dependencies, we explore a more\nunified and scalable alternative: can a pre-trained video generation model\nadapt to diverse image and video tasks? To answer this, we propose UniVid, a\nframework that fine-tunes a video diffusion transformer to handle various\nvision tasks without task-specific modifications. Tasks are represented as\nvisual sentences, where the context sequence defines both the task and the\nexpected output modality. We evaluate the generalization of UniVid from two\nperspectives: (1) cross-modal inference with contexts composed of both images\nand videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks\nfrom natural to annotated data, without multi-source pre-training. Despite\nbeing trained solely on natural video data, UniVid generalizes well in both\nsettings. Notably, understanding and generation tasks can easily switch by\nsimply reversing the visual sentence order in this paradigm. These findings\nhighlight the potential of pre-trained video generation models to serve as a\nscalable and unified foundation for vision modeling. Our code will be released\nat https://github.com/CUC-MIPG/UniVid.",
        "url": "http://arxiv.org/abs/2509.21760v1",
        "published_date": "2025-09-26T01:43:40+00:00",
        "updated_date": "2025-09-26T01:43:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lan Chen",
            "Yuchao Gu",
            "Qi Mao"
        ],
        "tldr": "The paper introduces UniVid, a framework that fine-tunes a video diffusion transformer for various vision tasks, unified and scalable with visual sentences, without task-specific pre-training, leveraging pre-trained video generation models.",
        "tldr_zh": "该论文介绍了UniVid，一个通过微调视频扩散变换器来处理各种视觉任务的框架。该框架通过视觉语句实现统一和可扩展性，无需特定于任务的预训练，并利用预训练的视频生成模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction",
        "summary": "High-quality 3D world models are pivotal for embodied intelligence and\nArtificial General Intelligence (AGI), underpinning applications such as AR/VR\ncontent creation and robotic navigation. Despite the established strong\nimaginative priors, current video foundation models lack explicit 3D grounding\ncapabilities, thus being limited in both spatial consistency and their utility\nfor downstream 3D reasoning tasks. In this work, we present FantasyWorld, a\ngeometry-enhanced framework that augments frozen video foundation models with a\ntrainable geometric branch, enabling joint modeling of video latents and an\nimplicit 3D field in a single forward pass. Our approach introduces\ncross-branch supervision, where geometry cues guide video generation and video\npriors regularize 3D prediction, thus yielding consistent and generalizable\n3D-aware video representations. Notably, the resulting latents from the\ngeometric branch can potentially serve as versatile representations for\ndownstream 3D tasks such as novel view synthesis and navigation, without\nrequiring per-scene optimization or fine-tuning. Extensive experiments show\nthat FantasyWorld effectively bridges video imagination and 3D perception,\noutperforming recent geometry-consistent baselines in multi-view coherence and\nstyle consistency. Ablation studies further confirm that these gains stem from\nthe unified backbone and cross-branch information exchange.",
        "url": "http://arxiv.org/abs/2509.21657v1",
        "published_date": "2025-09-25T22:24:23+00:00",
        "updated_date": "2025-09-25T22:24:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yixiang Dai",
            "Fan Jiang",
            "Chiyu Wang",
            "Mu Xu",
            "Yonggang Qi"
        ],
        "tldr": "FantasyWorld introduces a geometry-enhanced framework that integrates video foundation models with a geometric branch to achieve geometry-consistent video and 3D prediction, enabling downstream 3D tasks without per-scene optimization.",
        "tldr_zh": "FantasyWorld 引入了一个几何增强框架，将视频基础模型与几何分支相结合，以实现几何一致的视频和3D预测，从而无需针对每个场景进行优化即可实现下游3D任务。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "What Happens Next? Anticipating Future Motion by Generating Point Trajectories",
        "summary": "We consider the problem of forecasting motion from a single image, i.e.,\npredicting how objects in the world are likely to move, without the ability to\nobserve other parameters such as the object velocities or the forces applied to\nthem. We formulate this task as conditional generation of dense trajectory\ngrids with a model that closely follows the architecture of modern video\ngenerators but outputs motion trajectories instead of pixels. This approach\ncaptures scene-wide dynamics and uncertainty, yielding more accurate and\ndiverse predictions than prior regressors and generators. We extensively\nevaluate our method on simulated data, demonstrate its effectiveness on\ndownstream applications such as robotics, and show promising accuracy on\nreal-world intuitive physics datasets. Although recent state-of-the-art video\ngenerators are often regarded as world models, we show that they struggle with\nforecasting motion from a single image, even in simple physical scenarios such\nas falling blocks or mechanical object interactions, despite fine-tuning on\nsuch data. We show that this limitation arises from the overhead of generating\npixels rather than directly modeling motion.",
        "url": "http://arxiv.org/abs/2509.21592v1",
        "published_date": "2025-09-25T21:03:56+00:00",
        "updated_date": "2025-09-25T21:03:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Gabrijel Boduljak",
            "Laurynas Karazija",
            "Iro Laina",
            "Christian Rupprecht",
            "Andrea Vedaldi"
        ],
        "tldr": "This paper presents a method for forecasting motion from a single image by generating dense trajectory grids, outperforming existing video generators in predicting motion, particularly in simple physical scenarios.",
        "tldr_zh": "本文提出了一种通过生成密集轨迹网格从单个图像预测运动的方法，在预测运动方面优于现有的视频生成器，尤其是在简单的物理场景中。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "No Alignment Needed for Generation: Learning Linearly Separable Representations in Diffusion Models",
        "summary": "Efficient training strategies for large-scale diffusion models have recently\nemphasized the importance of improving discriminative feature representations\nin these models. A central line of work in this direction is representation\nalignment with features obtained from powerful external encoders, which\nimproves the representation quality as assessed through linear probing.\nAlignment-based approaches show promise but depend on large pretrained\nencoders, which are computationally expensive to obtain. In this work, we\npropose an alternative regularization for training, based on promoting the\nLinear SEParability (LSEP) of intermediate layer representations. LSEP\neliminates the need for an auxiliary encoder and representation alignment,\nwhile incorporating linear probing directly into the network's learning\ndynamics rather than treating it as a simple post-hoc evaluation tool. Our\nresults demonstrate substantial improvements in both training efficiency and\ngeneration quality on flow-based transformer architectures such as SiTs,\nachieving an FID of 1.46 on $256 \\times 256$ ImageNet dataset.",
        "url": "http://arxiv.org/abs/2509.21565v1",
        "published_date": "2025-09-25T20:46:48+00:00",
        "updated_date": "2025-09-25T20:46:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Junno Yun",
            "Yaşar Utku Alçalar",
            "Mehmet Akçakaya"
        ],
        "tldr": "This paper proposes a new regularization method (LSEP) for training diffusion models that promotes linear separability of intermediate layer representations, achieving state-of-the-art results on ImageNet without relying on computationally expensive external encoders.",
        "tldr_zh": "该论文提出了一种新的扩散模型训练正则化方法 (LSEP)，旨在提高中间层表示的线性可分性，无需依赖计算密集型外部编码器，并在 ImageNet 上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DistillKac: Few-Step Image Generation via Damped Wave Equations",
        "summary": "We present DistillKac, a fast image generator that uses the damped wave\nequation and its stochastic Kac representation to move probability mass at\nfinite speed. In contrast to diffusion models whose reverse time velocities can\nbecome stiff and implicitly allow unbounded propagation speed, Kac dynamics\nenforce finite speed transport and yield globally bounded kinetic energy.\nBuilding on this structure, we introduce classifier-free guidance in velocity\nspace that preserves square integrability under mild conditions. We then\npropose endpoint only distillation that trains a student to match a frozen\nteacher over long intervals. We prove a stability result that promotes\nsupervision at the endpoints to closeness along the entire path. Experiments\ndemonstrate DistillKac delivers high quality samples with very few function\nevaluations while retaining the numerical stability benefits of finite speed\nprobability flows.",
        "url": "http://arxiv.org/abs/2509.21513v1",
        "published_date": "2025-09-25T20:04:41+00:00",
        "updated_date": "2025-09-25T20:04:41+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "math.PR",
            "stat.ML"
        ],
        "authors": [
            "Weiqiao Han",
            "Chenlin Meng",
            "Christopher D. Manning",
            "Stefano Ermon"
        ],
        "tldr": "DistillKac introduces a fast image generator using damped wave equations and Kac representation, achieving high-quality samples with few function evaluations by enforcing finite speed probability flows and endpoint distillation.",
        "tldr_zh": "DistillKac提出了一种快速图像生成器，它使用阻尼波动方程和Kac表示，通过强制有限速度概率流动和端点蒸馏，以少量的函数评估实现高质量的样本。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Are Hallucinations Bad Estimations?",
        "summary": "We formalize hallucinations in generative models as failures to link an\nestimate to any plausible cause. Under this interpretation, we show that even\nloss-minimizing optimal estimators still hallucinate. We confirm this with a\ngeneral high probability lower bound on hallucinate rate for generic data\ndistributions. This reframes hallucination as structural misalignment between\nloss minimization and human-acceptable outputs, and hence estimation errors\ninduced by miscalibration. Experiments on coin aggregation, open-ended QA, and\ntext-to-image support our theory.",
        "url": "http://arxiv.org/abs/2509.21473v1",
        "published_date": "2025-09-25T19:39:09+00:00",
        "updated_date": "2025-09-25T19:39:09+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Hude Liu",
            "Jerry Yao-Chieh Hu",
            "Jennifer Yuntong Zhang",
            "Zhao Song",
            "Han Liu"
        ],
        "tldr": "This paper reframes hallucinations in generative models as estimation errors caused by structural misalignment, showing that even optimal estimators can hallucinate and providing a theoretical lower bound on hallucination rates across various tasks.",
        "tldr_zh": "这篇论文将生成模型中的幻觉重新定义为由结构性错位引起的估计误差，表明即使是最优的估计器也会产生幻觉，并为各种任务的幻觉率提供了理论下限。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bézier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation",
        "summary": "Training robust learning algorithms across different medical imaging\nmodalities is challenging due to the large domain gap. Unsupervised domain\nadaptation (UDA) mitigates this problem by using annotated images from the\nsource domain and unlabeled images from the target domain to train the deep\nmodels. Existing approaches often rely on GAN-based style transfer, but these\nmethods struggle to capture cross-domain mappings in regions with high\nvariability. In this paper, we propose a unified framework, B\\'ezier Meets\nDiffusion, for cross-domain image generation. First, we introduce a\nB\\'ezier-curve-based style transfer strategy that effectively reduces the\ndomain gap between source and target domains. The transferred source images\nenable the training of a more robust segmentation model across domains.\nThereafter, using pseudo-labels generated by this segmentation model on the\ntarget domain, we train a conditional diffusion model (CDM) to synthesize\nhigh-quality, labeled target-domain images. To mitigate the impact of noisy\npseudo-labels, we further develop an uncertainty-guided score matching method\nthat improves the robustness of CDM training. Extensive experiments on public\ndatasets demonstrate that our approach generates realistic labeled images,\nsignificantly augmenting the target domain and improving segmentation\nperformance.",
        "url": "http://arxiv.org/abs/2509.22476v1",
        "published_date": "2025-09-26T15:23:17+00:00",
        "updated_date": "2025-09-26T15:23:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Li",
            "Meilong Xu",
            "Xiaoling Hu",
            "Weimin Lyu",
            "Chao Chen"
        ],
        "tldr": "This paper introduces a novel unsupervised domain adaptation framework, Bézier Meets Diffusion, for medical image segmentation, leveraging Bézier curves for style transfer and a conditional diffusion model for generating high-quality labeled target-domain images.",
        "tldr_zh": "本文提出了一种新颖的无监督域适应框架Bézier Meets Diffusion，用于医学图像分割，利用Bézier 曲线进行风格迁移，并使用条件扩散模型生成高质量的带标签的目标域图像。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning",
        "summary": "The ability of robots to interpret human instructions and execute\nmanipulation tasks necessitates the availability of task-relevant tabletop\nscenes for training. However, traditional methods for creating these scenes\nrely on time-consuming manual layout design or purely randomized layouts, which\nare limited in terms of plausibility or alignment with the tasks. In this\npaper, we formulate a novel task, namely task-oriented tabletop scene\ngeneration, which poses significant challenges due to the substantial gap\nbetween high-level task instructions and the tabletop scenes. To support\nresearch on such a challenging task, we introduce MesaTask-10K, a large-scale\ndataset comprising approximately 10,700 synthetic tabletop scenes with manually\ncrafted layouts that ensure realistic layouts and intricate inter-object\nrelations. To bridge the gap between tasks and scenes, we propose a Spatial\nReasoning Chain that decomposes the generation process into object inference,\nspatial interrelation reasoning, and scene graph construction for the final 3D\nlayout. We present MesaTask, an LLM-based framework that utilizes this\nreasoning chain and is further enhanced with DPO algorithms to generate\nphysically plausible tabletop scenes that align well with given task\ndescriptions. Exhaustive experiments demonstrate the superior performance of\nMesaTask compared to baselines in generating task-conforming tabletop scenes\nwith realistic layouts. Project page is at https://mesatask.github.io/",
        "url": "http://arxiv.org/abs/2509.22281v1",
        "published_date": "2025-09-26T12:46:00+00:00",
        "updated_date": "2025-09-26T12:46:00+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Jinkun Hao",
            "Naifu Liang",
            "Zhen Luo",
            "Xudong Xu",
            "Weipeng Zhong",
            "Ran Yi",
            "Yichen Jin",
            "Zhaoyang Lyu",
            "Feng Zheng",
            "Lizhuang Ma",
            "Jiangmiao Pang"
        ],
        "tldr": "The paper introduces MesaTask, a new task and dataset (MesaTask-10K) for task-oriented tabletop scene generation, along with a Spatial Reasoning Chain framework (MesaTask) leveraging LLMs and DPO to generate realistic and task-conforming scenes. They show superior performance compared to baselines.",
        "tldr_zh": "该论文介绍了MesaTask，一个新的任务和数据集 (MesaTask-10K)，用于任务导向的桌面场景生成，以及一个利用LLMs和DPO的空间推理链框架 (MesaTask)，用于生成realistic且符合任务的场景。实验表明，与基线相比，该方法表现出卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]