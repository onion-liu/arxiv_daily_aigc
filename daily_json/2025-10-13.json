[
    {
        "title": "VLM-Guided Adaptive Negative Prompting for Creative Generation",
        "summary": "Creative generation is the synthesis of new, surprising, and valuable samples\nthat reflect user intent yet cannot be envisioned in advance. This task aims to\nextend human imagination, enabling the discovery of visual concepts that exist\nin the unexplored spaces between familiar domains. While text-to-image\ndiffusion models excel at rendering photorealistic scenes that faithfully match\nuser prompts, they still struggle to generate genuinely novel content. Existing\napproaches to enhance generative creativity either rely on interpolation of\nimage features, which restricts exploration to predefined categories, or\nrequire time-intensive procedures such as embedding optimization or model\nfine-tuning. We propose VLM-Guided Adaptive Negative-Prompting, a\ntraining-free, inference-time method that promotes creative image generation\nwhile preserving the validity of the generated object. Our approach utilizes a\nvision-language model (VLM) that analyzes intermediate outputs of the\ngeneration process and adaptively steers it away from conventional visual\nconcepts, encouraging the emergence of novel and surprising outputs. We\nevaluate creativity through both novelty and validity, using statistical\nmetrics in the CLIP embedding space. Through extensive experiments, we show\nconsistent gains in creative novelty with negligible computational overhead.\nMoreover, unlike existing methods that primarily generate single objects, our\napproach extends to complex scenarios, such as generating coherent sets of\ncreative objects and preserving creativity within elaborate compositional\nprompts. Our method integrates seamlessly into existing diffusion pipelines,\noffering a practical route to producing creative outputs that venture beyond\nthe constraints of textual descriptions.",
        "url": "http://arxiv.org/abs/2510.10715v1",
        "published_date": "2025-10-12T17:34:59+00:00",
        "updated_date": "2025-10-12T17:34:59+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Shelly Golan",
            "Yotam Nitzan",
            "Zongze Wu",
            "Or Patashnik"
        ],
        "tldr": "This paper introduces a training-free, inference-time method called VLM-Guided Adaptive Negative-Prompting to improve creative image generation by steering diffusion models away from conventional concepts using VLMs.",
        "tldr_zh": "本文介绍了一种名为VLM引导的自适应负提示的免训练推理方法，通过使用VLMs引导扩散模型远离传统概念，从而改进创意图像生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniFlow: A Unified Pixel Flow Tokenizer for Visual Understanding and Generation",
        "summary": "Tokenizer is a crucial component for both visual understanding and\ngeneration. To advance toward the ultimate goal of universal modeling, recent\nresearch has focused on developing a unified tokenizer. However, existing\ntokenizers face a significant performance trade-off between understanding and\ngeneration, stemming from the inherent conflict between high-level semantic\nabstraction and low-level pixel reconstruction. To tackle this challenge, we\npropose a generic and unified tokenizer, namely UniFlow, by flexibly adapting\nany visual encoder with a concise reconstruction decoder. Specifically, we\nintroduce layer-wise adaptive self-distillation applied to the well-pretrained\nvisual encoders, which enables UniFlow to simultaneously inherit the strong\nsemantic features for visual understanding and flexibly adapt to model\nfine-grained details for visual generation. Moreover, we propose a lightweight\npatch-wise pixel flow decoder, which efficiently achieves high-fidelity pixel\nreconstruction by modeling a conditional flow from the noisy state back to the\npatch-wise pixel domain. By leveraging the semantic features as visual\nconditions for the decoder, we effectively alleviate the training conflicts\nbetween understanding and generation. Furthermore, the patch-wise learning\nstrategy simplifies the data distribution, thereby improving training\nefficiency. Extensive experiments across 13 challenging benchmarks spanning 7\nwidely studied visual understanding and generation tasks demonstrate that\nUniFlow achieves a win-win outcome. For instance, our 7B UniFlow-XL not only\nsurpasses the 14B TokenFlow-XL by 7.75% on average understanding benchmarks,\nbut also achieves competitive results in both visual reconstruction and\ngeneration, surpassing UniTok by 0.15 in rFID and 0.09 in gFID (without\nguidance), respectively.",
        "url": "http://arxiv.org/abs/2510.10575v1",
        "published_date": "2025-10-12T12:50:23+00:00",
        "updated_date": "2025-10-12T12:50:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhengrong Yue",
            "Haiyu Zhang",
            "Xiangyu Zeng",
            "Boyu Chen",
            "Chenting Wang",
            "Shaobin Zhuang",
            "Lu Dong",
            "KunPeng Du",
            "Yi Wang",
            "Limin Wang",
            "Yali Wang"
        ],
        "tldr": "The paper introduces UniFlow, a unified pixel flow tokenizer, that aims to bridge the performance gap between visual understanding and generation by using layer-wise adaptive self-distillation and a patch-wise pixel flow decoder. Experimental results demonstrate improved performance on both understanding and generation tasks.",
        "tldr_zh": "该论文介绍了一种统一的像素流分词器 UniFlow，旨在通过使用分层自适应自蒸馏和分块像素流解码器，弥合视觉理解和生成之间的性能差距。实验结果表明，在理解和生成任务上的性能均有所提高。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices",
        "summary": "Cross-attention transformers and other multimodal vision-language models\nexcel at grounding and generation; however, their extensive, full-precision\nbackbones make it challenging to deploy them on edge devices. Memory-augmented\narchitectures enhance the utilization of past context; however, most works\nrarely pair them with aggressive edge-oriented quantization. We introduce\nBitMar, a quantized multimodal transformer that proposes an external human-like\nepisodic memory for effective image-text generation on hardware with limited\nresources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and\none for vision (DiNOv2-based), to create compact embeddings that are combined\nand used to query a fixed-size key-value episodic memory. During vector\nretrieval, the BitNet decoder applies per-layer conditioning, which increases\nthe contextual relevance of generated content. The decoder also employs\nattention sinks with a sliding-window mechanism to process long or streaming\ninputs under tight memory budgets. The combination of per-layer conditioning\nand sliding-window attention achieves a strong quality-speed trade-off,\ndelivering competitive captioning and multimodal understanding at low latency\nwith a small model footprint. These characteristics make BitMar well-suited for\nedge deployment.",
        "url": "http://arxiv.org/abs/2510.10560v1",
        "published_date": "2025-10-12T11:59:41+00:00",
        "updated_date": "2025-10-12T11:59:41+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "68T50",
            "I.2.7"
        ],
        "authors": [
            "Euhid Aman",
            "Esteban Carlin",
            "Hsing-Kuo Pao",
            "Giovanni Beltrame",
            "Ghaluh Indah Permata Sari",
            "Yie-Tarng Chen"
        ],
        "tldr": "BitMar is a quantized multimodal transformer using low-bit encoders and episodic memory for efficient image-text generation on edge devices, achieving a strong quality-speed trade-off.",
        "tldr_zh": "BitMar 是一种量化的多模态 Transformer，使用低比特编码器和情景记忆，以在边缘设备上实现高效的图像-文本生成，并在质量和速度之间实现了良好平衡。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Head-wise Adaptive Rotary Positional Encoding for Fine-Grained Image Generation",
        "summary": "Transformers rely on explicit positional encoding to model structure in data.\nWhile Rotary Position Embedding (RoPE) excels in 1D domains, its application to\nimage generation reveals significant limitations such as fine-grained spatial\nrelation modeling, color cues, and object counting. This paper identifies key\nlimitations of standard multi-dimensional RoPE-rigid frequency allocation,\naxis-wise independence, and uniform head treatment-in capturing the complex\nstructural biases required for fine-grained image generation. We propose\nHARoPE, a head-wise adaptive extension that inserts a learnable linear\ntransformation parameterized via singular value decomposition (SVD) before the\nrotary mapping. This lightweight modification enables dynamic frequency\nreallocation, semantic alignment of rotary planes, and head-specific positional\nreceptive fields while rigorously preserving RoPE's relative-position property.\nExtensive experiments on class-conditional ImageNet and text-to-image\ngeneration (Flux and MMDiT) demonstrate that HARoPE consistently improves\nperformance over strong RoPE baselines and other extensions. The method serves\nas an effective drop-in replacement, offering a principled and adaptable\nsolution for enhancing positional awareness in transformer-based image\ngenerative models.",
        "url": "http://arxiv.org/abs/2510.10489v1",
        "published_date": "2025-10-12T07:46:28+00:00",
        "updated_date": "2025-10-12T07:46:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaye Li",
            "Baoyou Chen",
            "Hui Li",
            "Zilong Dong",
            "Jingdong Wang",
            "Siyu Zhu"
        ],
        "tldr": "The paper introduces HARoPE, a head-wise adaptive extension of Rotary Position Embedding (RoPE), addressing limitations in fine-grained image generation with transformers. HARoPE enhances performance by improving spatial relation modeling and positional awareness while preserving RoPE's valuable properties.",
        "tldr_zh": "该论文介绍了HARoPE，一种Rotary Position Embedding (RoPE) 的头部自适应扩展，旨在解决transformer在细粒度图像生成中的局限性。HARoPE通过改善空间关系建模和位置感知，同时保留RoPE的宝贵性质，来提高性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]