[
    {
        "title": "GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models",
        "summary": "The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our code is available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.",
        "url": "http://arxiv.org/abs/2512.15560v1",
        "published_date": "2025-12-17T16:09:43+00:00",
        "updated_date": "2025-12-17T16:09:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bozhou Li",
            "Sihan Yang",
            "Yushuo Guan",
            "Ruichuan An",
            "Xinlong Chen",
            "Yang Shi",
            "Pengfei Wan",
            "Wentao Zhang",
            "Yuanxing zhang"
        ],
        "tldr": "The paper introduces GRAN-TED, a new text encoder with a novel training paradigm, along with TED-6K, a text-only benchmark, to improve text-conditional diffusion models for image and video generation.",
        "tldr_zh": "该论文介绍了 GRAN-TED，一种具有新型训练范式的文本编码器，以及 TED-6K，一个纯文本基准，旨在改进用于图像和视频生成的文本条件扩散模型。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation",
        "summary": "We introduce TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation designed to enable fair, reproducible comparison across methods. While current state-of-the-art systems rely on closed data or compute-heavy models, TalkVerse offers 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours. These are curated from over 60k hours of video via a transparent pipeline that includes scene-cut detection, aesthetic assessment, strict audio-visual synchronization checks, and comprehensive annotations including 2D skeletons and structured visual/audio-style captions. Leveraging TalkVerse, we present a reproducible 5B DiT baseline built on Wan2.2-5B. By utilizing a video VAE with a high downsampling ratio and a sliding window mechanism with motion-frame context, our model achieves minute-long generation with low drift. It delivers comparable lip-sync and visual quality to the 14B Wan-S2V model but with 10$\\times$ lower inference cost. To enhance storytelling in long videos, we integrate an MLLM director to rewrite prompts based on audio and visual cues. Furthermore, our model supports zero-shot video dubbing via controlled latent noise injection. We open-source the dataset, training recipes, and 5B checkpoints to lower barriers for research in audio-driven human video generation. Project Page: https://zhenzhiwang.github.io/talkverse/",
        "url": "http://arxiv.org/abs/2512.14938v1",
        "published_date": "2025-12-16T22:01:08+00:00",
        "updated_date": "2025-12-16T22:01:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM",
            "cs.SD"
        ],
        "authors": [
            "Zhenzhi Wang",
            "Jian Wang",
            "Ke Ma",
            "Dahua Lin",
            "Bing Zhou"
        ],
        "tldr": "TalkVerse introduces a large-scale, open audio-driven video dataset (2.3M clips, 6.3k hours) and a 5B diffusion transformer model capable of generating minute-long talking videos with comparable quality to larger models and supports zero-shot video dubbing.",
        "tldr_zh": "TalkVerse 介绍了一个大规模的开放音频驱动视频数据集（230万个片段，6300小时），以及一个5B扩散transformer模型，该模型能够生成分钟级别的说话视频，质量与更大的模型相当，并支持零样本视频配音。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
        "summary": "The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.",
        "url": "http://arxiv.org/abs/2512.14699v1",
        "published_date": "2025-12-16T18:59:59+00:00",
        "updated_date": "2025-12-16T18:59:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sihui Ji",
            "Xi Chen",
            "Shuai Yang",
            "Xin Tao",
            "Pengfei Wan",
            "Hengshuang Zhao"
        ],
        "tldr": "The paper introduces MemFlow, a novel memory management approach for streaming video generation that dynamically updates and selectively activates memory tokens based on text prompts to improve long-context consistency and efficiency.",
        "tldr_zh": "该论文介绍了MemFlow，一种用于流式视频生成的新型内存管理方法，它根据文本提示动态更新和选择性激活内存令牌，以提高长上下文一致性和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning",
        "summary": "We propose \\textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning $15$ high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.",
        "url": "http://arxiv.org/abs/2512.15635v1",
        "published_date": "2025-12-17T17:47:18+00:00",
        "updated_date": "2025-12-17T17:47:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuanhang Li",
            "Yiren Song",
            "Junzhe Bai",
            "Xinran Liang",
            "Hu Yang",
            "Libiao Jin",
            "Qi Mao"
        ],
        "tldr": "IC-Effect is a DiT-based framework for few-shot video VFX editing that utilizes in-context learning and spatiotemporal sparse tokenization to achieve high-quality, controllable, and temporally consistent effects while preserving the background. They also release a paired VFX editing dataset.",
        "tldr_zh": "IC-Effect是一个基于DiT的框架，用于少样本视频VFX编辑。它利用上下文学习和时空稀疏标记化，在保持背景的同时，实现高质量、可控且时间一致的特效。他们还发布了一个配对的VFX编辑数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition",
        "summary": "Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose \\textbf{Qwen-Image-Layered}, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling \\textbf{inherent editability}, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on \\href{https://github.com/QwenLM/Qwen-Image-Layered}{https://github.com/QwenLM/Qwen-Image-Layered}",
        "url": "http://arxiv.org/abs/2512.15603v1",
        "published_date": "2025-12-17T17:12:42+00:00",
        "updated_date": "2025-12-17T17:12:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shengming Yin",
            "Zekai Zhang",
            "Zecheng Tang",
            "Kaiyuan Gao",
            "Xiao Xu",
            "Kun Yan",
            "Jiahao Li",
            "Yilei Chen",
            "Yuxiang Chen",
            "Heung-Yeung Shum",
            "Lionel M. Ni",
            "Jingren Zhou",
            "Junyang Lin",
            "Chenfei Wu"
        ],
        "tldr": "The paper introduces Qwen-Image-Layered, a diffusion model that decomposes RGB images into semantically disentangled RGBA layers for inherently editable image manipulation, addressing consistency issues in existing image editing methods.",
        "tldr_zh": "该论文介绍了Qwen-Image-Layered，这是一种扩散模型，可以将RGB图像分解为语义上解耦的RGBA层，以实现固有的可编辑图像操作，从而解决现有图像编辑方法中的一致性问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting",
        "summary": "Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency. We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, \"Off The Grid\" distribution. Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches. This module is trained end-to-end with a 3D reconstruction backbone using self-supervised learning. Our resulting pose-free model generates photorealistic scenes in seconds, achieving state-of-the-art novel view synthesis for feed-forward models. It outperforms competitors while using far fewer primitives, demonstrating a more accurate and efficient allocation that captures fine details and reduces artifacts. Moreover, we observe that by learning to render 3D Gaussians, our 3D reconstruction backbone improves camera pose estimation, suggesting opportunities to train these foundational models without labels.",
        "url": "http://arxiv.org/abs/2512.15508v1",
        "published_date": "2025-12-17T14:59:21+00:00",
        "updated_date": "2025-12-17T14:59:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Arthur Moreau",
            "Richard Shaw",
            "Michal Nazarczuk",
            "Jisu Shin",
            "Thomas Tanay",
            "Zhensong Zhang",
            "Songcen Xu",
            "Eduardo Pérez-Pellitero"
        ],
        "tldr": "This paper introduces a novel feed-forward 3D Gaussian Splatting architecture that adaptively places Gaussian primitives at a sub-pixel level, achieving state-of-the-art novel view synthesis with fewer primitives and improved pose estimation.",
        "tldr_zh": "本文介绍了一种新的前馈3D高斯溅射架构，该架构以亚像素级别自适应地放置高斯基元，从而以更少的基元和改进的姿态估计实现了最先进的新视角合成。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Expand and Prune: Maximizing Trajectory Diversity for Effective GRPO in Generative Models",
        "summary": "Group Relative Policy Optimization (GRPO) is a powerful technique for aligning generative models, but its effectiveness is bottlenecked by the conflict between large group sizes and prohibitive computational costs. In this work, we investigate the trade-off through empirical studies, yielding two key observations. First, we discover the reward clustering phenomenon in which many trajectories collapse toward the group-mean reward, offering limited optimization value. Second, we design a heuristic strategy named Optimal Variance Filtering (OVF), and verify that a high-variance subset of trajectories, selected by OVF can outperform the larger, unfiltered group. However, this static, post-sampling OVF approach still necessitates critical computational overhead, as it performs unnecessary sampling for trajectories that are ultimately discarded. To resolve this, we propose Pro-GRPO (Proactive GRPO), a novel dynamic framework that integrates latent feature-based trajectory pruning into the sampling process. Through the early termination of reward-clustered trajectories, Pro-GRPO reduces computational overhead. Leveraging its efficiency, Pro-GRPO employs an \"Expand-and-Prune\" strategy. This strategy first expands the size of initial sampling group to maximize trajectory diversity, then it applies multi-step OVF to the latents, avoiding prohibitive computational costs. Extensive experiments on both diffusion-based and flow-based models demonstrate the generality and effectiveness of our Pro-GRPO framework.",
        "url": "http://arxiv.org/abs/2512.15347v1",
        "published_date": "2025-12-17T11:44:34+00:00",
        "updated_date": "2025-12-17T11:44:34+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Shiran Ge",
            "Chenyi Huang",
            "Yuang Ai",
            "Qihang Fan",
            "Huaibo Huang",
            "Ran He"
        ],
        "tldr": "This paper introduces Pro-GRPO, a method to improve the efficiency of Group Relative Policy Optimization (GRPO) in generative models by dynamically pruning trajectories and maximizing trajectory diversity through an \"Expand-and-Prune\" strategy.",
        "tldr_zh": "本文介绍了一种名为Pro-GRPO的方法，通过动态剪枝轨迹并利用“扩展-剪枝”策略来最大化轨迹多样性，从而提高生成模型中组相对策略优化（GRPO）的效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SynthSeg-Agents: Multi-Agent Synthetic Data Generation for Zero-Shot Weakly Supervised Semantic Segmentation",
        "summary": "Weakly Supervised Semantic Segmentation (WSSS) with image level labels aims to produce pixel level predictions without requiring dense annotations. While recent approaches have leveraged generative models to augment existing data, they remain dependent on real world training samples. In this paper, we introduce a novel direction, Zero Shot Weakly Supervised Semantic Segmentation (ZSWSSS), and propose SynthSeg Agents, a multi agent framework driven by Large Language Models (LLMs) to generate synthetic training data entirely without real images. SynthSeg Agents comprises two key modules, a Self Refine Prompt Agent and an Image Generation Agent. The Self Refine Prompt Agent autonomously crafts diverse and semantically rich image prompts via iterative refinement, memory mechanisms, and prompt space exploration, guided by CLIP based similarity and nearest neighbor diversity filtering. These prompts are then passed to the Image Generation Agent, which leverages Vision Language Models (VLMs) to synthesize candidate images. A frozen CLIP scoring model is employed to select high quality samples, and a ViT based classifier is further trained to relabel the entire synthetic dataset with improved semantic precision. Our framework produces high quality training data without any real image supervision. Experiments on PASCAL VOC 2012 and COCO 2014 show that SynthSeg Agents achieves competitive performance without using real training images. This highlights the potential of LLM driven agents in enabling cost efficient and scalable semantic segmentation.",
        "url": "http://arxiv.org/abs/2512.15310v1",
        "published_date": "2025-12-17T10:58:38+00:00",
        "updated_date": "2025-12-17T10:58:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wangyu Wu",
            "Zhenhong Chen",
            "Xiaowei Huang",
            "Fei Ma",
            "Jimin Xiao"
        ],
        "tldr": "The paper introduces SynthSeg-Agents, a multi-agent LLM-driven framework for zero-shot weakly supervised semantic segmentation that generates synthetic training data without relying on real images, achieving competitive performance on benchmark datasets.",
        "tldr_zh": "该论文介绍了SynthSeg-Agents，一个多智能体LLM驱动的零样本弱监督语义分割框架，无需真实图像即可生成合成训练数据，并在基准数据集上取得了有竞争力的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Borrowing from anything: A generalizable framework for reference-guided instance editing",
        "summary": "Reference-guided instance editing is fundamentally limited by semantic entanglement, where a reference's intrinsic appearance is intertwined with its extrinsic attributes. The key challenge lies in disentangling what information should be borrowed from the reference, and determining how to apply it appropriately to the target. To tackle this challenge, we propose GENIE, a Generalizable Instance Editing framework capable of achieving explicit disentanglement. GENIE first corrects spatial misalignments with a Spatial Alignment Module (SAM). Then, an Adaptive Residual Scaling Module (ARSM) learns what to borrow by amplifying salient intrinsic cues while suppressing extrinsic attributes, while a Progressive Attention Fusion (PAF) mechanism learns how to render this appearance onto the target, preserving its structure. Extensive experiments on the challenging AnyInsertion dataset demonstrate that GENIE achieves state-of-the-art fidelity and robustness, setting a new standard for disentanglement-based instance editing.",
        "url": "http://arxiv.org/abs/2512.15138v1",
        "published_date": "2025-12-17T06:59:29+00:00",
        "updated_date": "2025-12-17T06:59:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shengxiao Zhou",
            "Chenghua Li",
            "Jianhao Huang",
            "Qinghao Hu",
            "Yifan Zhang"
        ],
        "tldr": "The paper introduces GENIE, a framework for reference-guided instance editing that disentangles appearance and attributes using spatial alignment, adaptive residual scaling, and progressive attention fusion, achieving state-of-the-art results on the AnyInsertion dataset.",
        "tldr_zh": "该论文介绍了GENIE，一个参考引导的实例编辑框架，它通过空间对齐、自适应残差缩放和渐进式注意力融合来解耦外观和属性，并在AnyInsertion数据集上实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3DProxyImg: Controllable 3D-Aware Animation Synthesis from Single Image via 2D-3D Aligned Proxy Embedding",
        "summary": "3D animation is central to modern visual media, yet traditional production pipelines remain labor-intensive, expertise-demanding, and computationally expensive. Recent AIGC-based approaches partially automate asset creation and rigging, but they either inherit the heavy costs of full 3D pipelines or rely on video-synthesis paradigms that sacrifice 3D controllability and interactivity. We focus on single-image 3D animation generation and argue that progress is fundamentally constrained by a trade-off between rendering quality and 3D control.\n  To address this limitation, we propose a lightweight 3D animation framework that decouples geometric control from appearance synthesis. The core idea is a 2D-3D aligned proxy representation that uses a coarse 3D estimate as a structural carrier, while delegating high-fidelity appearance and view synthesis to learned image-space generative priors. This proxy formulation enables 3D-aware motion control and interaction comparable to classical pipelines, without requiring accurate geometry or expensive optimization, and naturally extends to coherent background animation. Extensive experiments demonstrate that our method achieves efficient animation generation on low-power platforms and outperforms video-based 3D animation generation in identity preservation, geometric and textural consistency, and the level of precise, interactive control it offers to users.",
        "url": "http://arxiv.org/abs/2512.15126v1",
        "published_date": "2025-12-17T06:38:07+00:00",
        "updated_date": "2025-12-17T06:38:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yupeng Zhu",
            "Xiongzhen Zhang",
            "Ye Chen",
            "Bingbing Ni"
        ],
        "tldr": "This paper introduces a novel 3D animation framework, 3DProxyImg, that uses a 2D-3D aligned proxy embedding to enable controllable 3D-aware animation synthesis from a single image, achieving efficiency and improved control compared to existing methods.",
        "tldr_zh": "本文提出了一种新颖的3D动画框架3DProxyImg，它使用2D-3D对齐的代理嵌入，从单张图像实现可控的3D感知动画合成，与现有方法相比，实现了效率和控制方面的提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
        "summary": "Creating new visual concepts often requires connecting distinct ideas through their most relevant shared attributes -- their vibe. We introduce Vibe Blending, a novel task for generating coherent and meaningful hybrids that reveals these shared attributes between images. Achieving such blends is challenging for current methods, which struggle to identify and traverse nonlinear paths linking distant concepts in latent space. We propose Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesics in feature spaces like CLIP, enabling smooth and semantically consistent transitions between concepts. To evaluate creative quality, we design a cognitively inspired framework combining human judgments, LLM reasoning, and a geometric path-based difficulty score. We find that Vibe Space produces blends that humans consistently rate as more creative and coherent than current methods.",
        "url": "http://arxiv.org/abs/2512.14884v1",
        "published_date": "2025-12-16T20:03:23+00:00",
        "updated_date": "2025-12-16T20:03:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huzheng Yang",
            "Katherine Xu",
            "Andrew Lu",
            "Michael D. Grossberg",
            "Yutong Bai",
            "Jianbo Shi"
        ],
        "tldr": "The paper introduces \"Vibe Space,\" a method for generating creative and coherent image blends by traversing nonlinear paths in latent space, outperforming existing techniques in human evaluations.",
        "tldr_zh": "该论文介绍了\"Vibe Space\"，这是一种通过在潜在空间中穿越非线性路径来生成创意和连贯图像融合的方法，在人类评估中优于现有技术。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spherical Leech Quantization for Visual Tokenization and Generation",
        "summary": "Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization ($Λ_{24}$-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.",
        "url": "http://arxiv.org/abs/2512.14697v1",
        "published_date": "2025-12-16T18:59:57+00:00",
        "updated_date": "2025-12-16T18:59:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "eess.SP"
        ],
        "authors": [
            "Yue Zhao",
            "Hanwen Jiang",
            "Zhenlin Xu",
            "Chutong Yang",
            "Ehsan Adeli",
            "Philipp Krähenbühl"
        ],
        "tldr": "This paper introduces Spherical Leech Quantization ($Λ_{24}$-SQ), a Leech lattice-based quantization method, for improved image tokenization, compression, and generation, achieving better reconstruction quality than existing methods.",
        "tldr_zh": "该论文介绍了一种基于Leech格的球形Leech量化($Λ_{24}$-SQ)方法，用于改进图像标记化、压缩和生成，并实现了比现有方法更好的重建质量。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Native and Compact Structured Latents for 3D Generation",
        "summary": "Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.",
        "url": "http://arxiv.org/abs/2512.14692v1",
        "published_date": "2025-12-16T18:58:28+00:00",
        "updated_date": "2025-12-16T18:58:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jianfeng Xiang",
            "Xiaoxue Chen",
            "Sicheng Xu",
            "Ruicheng Wang",
            "Zelong Lv",
            "Yu Deng",
            "Hongyuan Zhu",
            "Yue Dong",
            "Hao Zhao",
            "Nicholas Jing Yuan",
            "Jiaolong Yang"
        ],
        "tldr": "This paper introduces O-Voxel, a new sparse voxel structure for 3D generation that captures complex topologies and detailed appearance, and demonstrates its effectiveness through large-scale flow-matching models.",
        "tldr_zh": "本文介绍了一种新的稀疏体素结构O-Voxel，用于3D生成，它可以捕获复杂的拓扑结构和详细的外观，并通过大规模的流匹配模型展示了其有效性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    }
]