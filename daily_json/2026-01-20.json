[
    {
        "title": "S2DiT: Sandwich Diffusion Transformer for Mobile Streaming Video Generation",
        "summary": "Diffusion Transformers (DiTs) have recently improved video generation quality. However, their heavy computational cost makes real-time or on-device generation infeasible. In this work, we introduce S2DiT, a Streaming Sandwich Diffusion Transformer designed for efficient, high-fidelity, and streaming video generation on mobile hardware. S2DiT generates more tokens but maintains efficiency with novel efficient attentions: a mixture of LinConv Hybrid Attention (LCHA) and Stride Self-Attention (SSA). Based on this, we uncover the sandwich design via a budget-aware dynamic programming search, achieving superior quality and efficiency. We further propose a 2-in-1 distillation framework that transfers the capacity of large teacher models (e.g., Wan 2.2-14B) to the compact few-step sandwich model. Together, S2DiT achieves quality on par with state-of-the-art server video models, while streaming at over 10 FPS on an iPhone.",
        "url": "http://arxiv.org/abs/2601.12719v1",
        "published_date": "2026-01-19T04:48:21+00:00",
        "updated_date": "2026-01-19T04:48:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lin Zhao",
            "Yushu Wu",
            "Aleksei Lebedev",
            "Dishani Lahiri",
            "Meng Dong",
            "Arpit Sahni",
            "Michael Vasilkovsky",
            "Hao Chen",
            "Ju Hu",
            "Aliaksandr Siarohin",
            "Sergey Tulyakov",
            "Yanzhi Wang",
            "Anil Kag",
            "Yanyu Li"
        ],
        "tldr": "S2DiT is a novel diffusion transformer architecture designed for efficient, high-fidelity, and streaming video generation on mobile devices, achieving state-of-the-art quality with over 10 FPS on an iPhone.",
        "tldr_zh": "S2DiT 是一种新颖的扩散 Transformer 架构，专为在移动设备上实现高效、高保真和流式视频生成而设计，在 iPhone 上以超过 10 FPS 的速度实现了最先进的质量。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "ICo3D: An Interactive Conversational 3D Virtual Human",
        "summary": "This work presents Interactive Conversational 3D Virtual Human (ICo3D), a method for generating an interactive, conversational, and photorealistic 3D human avatar. Based on multi-view captures of a subject, we create an animatable 3D face model and a dynamic 3D body model, both rendered by splatting Gaussian primitives. Once merged together, they represent a lifelike virtual human avatar suitable for real-time user interactions. We equip our avatar with an LLM for conversational ability. During conversation, the audio speech of the avatar is used as a driving signal to animate the face model, enabling precise synchronization. We describe improvements to our dynamic Gaussian models that enhance photorealism: SWinGS++ for body reconstruction and HeadGaS++ for face reconstruction, and provide as well a solution to merge the separate face and body models without artifacts. We also present a demo of the complete system, showcasing several use cases of real-time conversation with the 3D avatar. Our approach offers a fully integrated virtual avatar experience, supporting both oral and written form interactions in immersive environments. ICo3D is applicable to a wide range of fields, including gaming, virtual assistance, and personalized education, among others. Project page: https://ico3d.github.io/",
        "url": "http://arxiv.org/abs/2601.13148v1",
        "published_date": "2026-01-19T15:30:08+00:00",
        "updated_date": "2026-01-19T15:30:08+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Richard Shaw",
            "Youngkyoon Jang",
            "Athanasios Papaioannou",
            "Arthur Moreau",
            "Helisa Dhamo",
            "Zhensong Zhang",
            "Eduardo Pérez-Pellitero"
        ],
        "tldr": "The paper introduces ICo3D, a method for creating interactive, conversational, and photorealistic 3D human avatars using multi-view capture, Gaussian primitives, and LLMs, with applications in various immersive environments.",
        "tldr_zh": "该论文介绍了 ICo3D，一种使用多视角捕获、高斯基元和大型语言模型创建交互式、对话式和逼真 3D 人体化身的方法，可应用于各种沉浸式环境。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PhaseMark: A Post-hoc, Optimization-Free Watermarking of AI-generated Images in the Latent Frequency Domain",
        "summary": "The proliferation of hyper-realistic images from Latent Diffusion Models (LDMs) demands robust watermarking, yet existing post-hoc methods are prohibitively slow due to iterative optimization or inversion processes. We introduce PhaseMark, a single-shot, optimization-free framework that directly modulates the phase in the VAE latent frequency domain. This approach makes PhaseMark thousands of times faster than optimization-based techniques while achieving state-of-the-art resilience against severe attacks, including regeneration, without degrading image quality. We analyze four modulation variants, revealing a clear performance-quality trade-off. PhaseMark demonstrates a new paradigm where efficient, resilient watermarking is achieved by exploiting intrinsic latent properties.",
        "url": "http://arxiv.org/abs/2601.13128v1",
        "published_date": "2026-01-19T15:13:23+00:00",
        "updated_date": "2026-01-19T15:13:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sung Ju Lee",
            "Nam Ik Cho"
        ],
        "tldr": "PhaseMark is a novel, fast, and resilient post-hoc watermarking technique for AI-generated images that operates in the latent frequency domain, avoiding iterative optimization.",
        "tldr_zh": "PhaseMark是一种新颖、快速且具有弹性的AI生成图像水印后处理技术，它在潜在频率域中操作，避免了迭代优化。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Exploring Talking Head Models With Adjacent Frame Prior for Speech-Preserving Facial Expression Manipulation",
        "summary": "Speech-Preserving Facial Expression Manipulation (SPFEM) is an innovative technique aimed at altering facial expressions in images and videos while retaining the original mouth movements. Despite advancements, SPFEM still struggles with accurate lip synchronization due to the complex interplay between facial expressions and mouth shapes. Capitalizing on the advanced capabilities of audio-driven talking head generation (AD-THG) models in synthesizing precise lip movements, our research introduces a novel integration of these models with SPFEM. We present a new framework, Talking Head Facial Expression Manipulation (THFEM), which utilizes AD-THG models to generate frames with accurately synchronized lip movements from audio inputs and SPFEM-altered images. However, increasing the number of frames generated by AD-THG models tends to compromise the realism and expression fidelity of the images. To counter this, we develop an adjacent frame learning strategy that finetunes AD-THG models to predict sequences of consecutive frames. This strategy enables the models to incorporate information from neighboring frames, significantly improving image quality during testing. Our extensive experimental evaluations demonstrate that this framework effectively preserves mouth shapes during expression manipulations, highlighting the substantial benefits of integrating AD-THG with SPFEM.",
        "url": "http://arxiv.org/abs/2601.12876v1",
        "published_date": "2026-01-19T09:31:24+00:00",
        "updated_date": "2026-01-19T09:31:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenxuan Lu",
            "Zhihua Xu",
            "Zhijing Yang",
            "Feng Gao",
            "Yongyi Lu",
            "Keze Wang",
            "Tianshui Chen"
        ],
        "tldr": "This paper introduces THFEM, a framework that integrates audio-driven talking head generation (AD-THG) models with speech-preserving facial expression manipulation (SPFEM), utilizing an adjacent frame learning strategy to improve image quality and lip synchronization during facial expression manipulation.",
        "tldr_zh": "本文介绍了一种名为THFEM的框架，该框架将音频驱动的说话头生成（AD-THG）模型与保留语音的面部表情操纵（SPFEM）相结合，并利用相邻帧学习策略来提高面部表情操纵期间的图像质量和唇部同步效果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fusion-Restoration Image Processing Algorithm to Improve the High-Temperature Deformation Measurement",
        "summary": "In the deformation measurement of high-temperature structures, image degradation caused by thermal radiation and random errors introduced by heat haze restrict the accuracy and effectiveness of deformation measurement. To suppress thermal radiation and heat haze using fusion-restoration image processing methods, thereby improving the accuracy and effectiveness of DIC in the measurement of high-temperature deformation. For image degradation caused by thermal radiation, based on the image layered representation, the image is decomposed into positive and negative channels for parallel processing, and then optimized for quality by multi-exposure image fusion. To counteract the high-frequency, random errors introduced by heat haze, we adopt the FSIM as the objective function to guide the iterative optimization of model parameters, and the grayscale average algorithm is applied to equalize anomalous gray values, thereby reducing measurement error. The proposed multi-exposure image fusion algorithm effectively suppresses image degradation caused by complex illumination conditions, boosting the effective computation area from 26% to 50% for under-exposed images and from 32% to 40% for over-exposed images without degrading measurement accuracy in the experiment. Meanwhile, the image restoration combined with the grayscale average algorithm reduces static thermal deformation measurement errors. The error in ε_xx is reduced by 85.3%, while the errors in ε_yy and γ_xy are reduced by 36.0% and 36.4%, respectively. We present image processing methods to suppress the interference of thermal radiation and heat haze in high-temperature deformation measurement using DIC. The experimental results verify that the proposed method can effectively improve image quality, reduce deformation measurement errors, and has potential application value in thermal deformation measurement.",
        "url": "http://arxiv.org/abs/2601.12682v1",
        "published_date": "2026-01-19T02:55:16+00:00",
        "updated_date": "2026-01-19T02:55:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Banglei Guan",
            "Dongcai Tan",
            "Jing Tao",
            "Ang Su",
            "Yang Shang",
            "Qifeng Yu"
        ],
        "tldr": "This paper proposes an image processing algorithm combining fusion and restoration techniques to improve deformation measurement accuracy in high-temperature environments by mitigating thermal radiation and heat haze effects, achieving significant error reduction in experimental settings.",
        "tldr_zh": "该论文提出了一种融合与恢复相结合的图像处理算法，通过减轻热辐射和热雾的影响，提高高温环境下变形测量的精度，并在实验环境中实现了显著的误差降低。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 4
    },
    {
        "title": "Deep Feature Deformation Weights",
        "summary": "Handle-based mesh deformation has been a long-standing paradigm in computer graphics, enabling intuitive shape edits from sparse controls. Classic techniques offer precise and rapid deformation control. However, they solve an optimization problem with constraints defined by control handle placement, requiring a user to know apriori the ideal distribution of handles on the shape to accomplish the desired edit. The mapping from handle set to deformation behavior is often unintuitive and, importantly, non-semantic. Modern data-driven methods, on the other hand, leverage a data prior to obtain semantic edits, but are slow and imprecise. We propose a technique that fuses the semantic prior of data with the precise control and speed of traditional frameworks. Our approach is surprisingly simple yet effective: deep feature proximity makes for smooth and semantic deformation weights, with no need for additional regularization. The weights can be computed in real-time for any surface point, whereas prior methods require optimization for new handles. Moreover, the semantic prior from deep features enables co-deformation of semantic parts. We introduce an improved feature distillation pipeline, barycentric feature distillation, which efficiently uses the visual signal from shape renders to minimize distillation cost. This allows our weights to be computed for high resolution meshes in under a minute, in contrast to potentially hours for both classical and neural methods. We preserve and extend properties of classical methods through feature space constraints and locality weighting. Our field representation allows for automatic detection of semantic symmetries, which we use to produce symmetry-preserving deformations. We show a proof-of-concept application which can produce deformations for meshes up to 1 million faces in real-time on a consumer-grade machine.",
        "url": "http://arxiv.org/abs/2601.12527v1",
        "published_date": "2026-01-18T18:23:03+00:00",
        "updated_date": "2026-01-18T18:23:03+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Richard Liu",
            "Itai Lang",
            "Rana Hanocka"
        ],
        "tldr": "This paper presents a method for real-time mesh deformation that combines the speed of classic handle-based techniques with the semantic awareness of modern deep learning approaches, using deep feature proximity for smooth and semantic deformation weights and a new barycentric feature distillation pipeline.",
        "tldr_zh": "本文提出了一种实时网格变形方法，结合了经典基于句柄技术的速度和现代深度学习方法的语义感知能力，使用深度特征邻近性来实现平滑的语义变形权重，并提出了一种新的重心特征蒸馏管道。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 3
    }
]