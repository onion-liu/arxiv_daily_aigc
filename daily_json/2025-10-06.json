[
    {
        "title": "Scaling Sequence-to-Sequence Generative Neural Rendering",
        "summary": "We present Kaleido, a family of generative models designed for\nphotorealistic, unified object- and scene-level neural rendering. Kaleido\noperates on the principle that 3D can be regarded as a specialised sub-domain\nof video, expressed purely as a sequence-to-sequence image synthesis task.\nThrough a systemic study of scaling sequence-to-sequence generative neural\nrendering, we introduce key architectural innovations that enable our model to:\ni) perform generative view synthesis without explicit 3D representations; ii)\ngenerate any number of 6-DoF target views conditioned on any number of\nreference views via a masked autoregressive framework; and iii) seamlessly\nunify 3D and video modelling within a single decoder-only rectified flow\ntransformer. Within this unified framework, Kaleido leverages large-scale video\ndata for pre-training, which significantly improves spatial consistency and\nreduces reliance on scarce, camera-labelled 3D datasets -- all without any\narchitectural modifications. Kaleido sets a new state-of-the-art on a range of\nview synthesis benchmarks. Its zero-shot performance substantially outperforms\nother generative methods in few-view settings, and, for the first time, matches\nthe quality of per-scene optimisation methods in many-view settings.",
        "url": "http://arxiv.org/abs/2510.04236v1",
        "published_date": "2025-10-05T15:03:31+00:00",
        "updated_date": "2025-10-05T15:03:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shikun Liu",
            "Kam Woh Ng",
            "Wonbong Jang",
            "Jiadong Guo",
            "Junlin Han",
            "Haozhe Liu",
            "Yiannis Douratsos",
            "Juan C. Pérez",
            "Zijian Zhou",
            "Chi Phung",
            "Tao Xiang",
            "Juan-Manuel Pérez-Rúa"
        ],
        "tldr": "The paper presents Kaleido, a sequence-to-sequence generative neural rendering model that unifies 3D and video modeling for photorealistic view synthesis, achieving state-of-the-art results and leveraging large-scale video pre-training to improve spatial consistency.",
        "tldr_zh": "该论文提出了 Kaleido，一种序列到序列的生成神经渲染模型，它统一了 3D 和视频建模以实现逼真的视图合成，取得了最先进的结果，并利用大规模视频预训练来提高空间一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering",
        "summary": "Autoregressive (AR) models have shown great promise in image generation, yet\nthey face a fundamental inefficiency stemming from their core component: a\nvast, unstructured vocabulary of visual tokens. This conventional approach\ntreats tokens as a flat vocabulary, disregarding the intrinsic structure of the\ntoken embedding space where proximity often correlates with semantic\nsimilarity. This oversight results in a highly complex prediction task, which\nhinders training efficiency and limits final generation quality. To resolve\nthis, we propose Manifold-Aligned Semantic Clustering (MASC), a principled\nframework that constructs a hierarchical semantic tree directly from the\ncodebook's intrinsic structure. MASC employs a novel geometry-aware distance\nmetric and a density-driven agglomerative construction to model the underlying\nmanifold of the token embeddings. By transforming the flat, high-dimensional\nprediction task into a structured, hierarchical one, MASC introduces a\nbeneficial inductive bias that significantly simplifies the learning problem\nfor the AR model. MASC is designed as a plug-and-play module, and our extensive\nexperiments validate its effectiveness: it accelerates training by up to 57%\nand significantly improves generation quality, reducing the FID of LlamaGen-XL\nfrom 2.87 to 2.58. MASC elevates existing AR frameworks to be highly\ncompetitive with state-of-the-art methods, establishing that structuring the\nprediction space is as crucial as architectural innovation for scalable\ngenerative modeling.",
        "url": "http://arxiv.org/abs/2510.04220v1",
        "published_date": "2025-10-05T14:23:51+00:00",
        "updated_date": "2025-10-05T14:23:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Lixuan He",
            "Shikang Zheng",
            "Linfeng Zhang"
        ],
        "tldr": "The paper introduces MASC, a method that structures the vocabulary of visual tokens in autoregressive image generation models by building a hierarchical semantic tree, leading to faster training and improved generation quality.",
        "tldr_zh": "该论文介绍了一种名为MASC的方法，通过构建分层语义树来组织自回归图像生成模型中的视觉token词汇表，从而加快训练速度并提高生成质量。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation",
        "summary": "Recent advances in large generative models have significantly advanced image\nediting and in-context image generation, yet a critical gap remains in ensuring\nphysical consistency, where edited objects must remain coherent. This\ncapability is especially vital for world simulation related tasks. In this\npaper, we present ChronoEdit, a framework that reframes image editing as a\nvideo generation problem. First, ChronoEdit treats the input and edited images\nas the first and last frames of a video, allowing it to leverage large\npretrained video generative models that capture not only object appearance but\nalso the implicit physics of motion and interaction through learned temporal\nconsistency. Second, ChronoEdit introduces a temporal reasoning stage that\nexplicitly performs editing at inference time. Under this setting, the target\nframe is jointly denoised with reasoning tokens to imagine a plausible editing\ntrajectory that constrains the solution space to physically viable\ntransformations. The reasoning tokens are then dropped after a few steps to\navoid the high computational cost of rendering a full video. To validate\nChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for\ncontexts that require physical consistency, and demonstrate that ChronoEdit\nsurpasses state-of-the-art baselines in both visual fidelity and physical\nplausibility. Code and models for both the 14B and 2B variants of ChronoEdit\nwill be released on the project page:\nhttps://research.nvidia.com/labs/toronto-ai/chronoedit",
        "url": "http://arxiv.org/abs/2510.04290v1",
        "published_date": "2025-10-05T17:02:01+00:00",
        "updated_date": "2025-10-05T17:02:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jay Zhangjie Wu",
            "Xuanchi Ren",
            "Tianchang Shen",
            "Tianshi Cao",
            "Kai He",
            "Yifan Lu",
            "Ruiyuan Gao",
            "Enze Xie",
            "Shiyi Lan",
            "Jose M. Alvarez",
            "Jun Gao",
            "Sanja Fidler",
            "Zian Wang",
            "Huan Ling"
        ],
        "tldr": "ChronoEdit reframes image editing as a video generation problem using temporal reasoning and pre-trained video models to ensure physical consistency during editing, outperforming existing methods on a new benchmark.",
        "tldr_zh": "ChronoEdit将图像编辑重新定义为视频生成问题，通过时间推理和预训练的视频模型来确保编辑过程中的物理一致性，并在新基准测试中表现优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge",
        "summary": "While text-to-image (T2I) models can synthesize high-quality images, their\nperformance degrades significantly when prompted with novel or\nout-of-distribution (OOD) entities due to inherent knowledge cutoffs. We\nintroduce World-To-Image, a novel framework that bridges this gap by empowering\nT2I generation with agent-driven world knowledge. We design an agent that\ndynamically searches the web to retrieve images for concepts unknown to the\nbase model. This information is then used to perform multimodal prompt\noptimization, steering powerful generative backbones toward an accurate\nsynthesis. Critically, our evaluation goes beyond traditional metrics,\nutilizing modern assessments like LLMGrader and ImageReward to measure true\nsemantic fidelity. Our experiments show that World-To-Image substantially\noutperforms state-of-the-art methods in both semantic alignment and visual\naesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated\nNICE benchmark. Our framework achieves these results with high efficiency in\nless than three iterations, paving the way for T2I systems that can better\nreflect the ever-changing real world. Our demo code is available\nhere\\footnote{https://github.com/mhson-kyle/World-To-Image}.",
        "url": "http://arxiv.org/abs/2510.04201v1",
        "published_date": "2025-10-05T13:35:30+00:00",
        "updated_date": "2025-10-05T13:35:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Moo Hyun Son",
            "Jintaek Oh",
            "Sun Bin Mun",
            "Jaechul Roh",
            "Sehyun Choi"
        ],
        "tldr": "The paper introduces World-To-Image, a framework that enhances text-to-image generation by using an agent to search the web for relevant images of novel concepts, leading to improved semantic fidelity and visual aesthetics even for out-of-distribution prompts.",
        "tldr_zh": "该论文介绍了一种名为World-To-Image的框架，它通过使用代理在网络上搜索新概念的相关图像来增强文本到图像的生成，从而即使对于分布外的提示也能提高语义保真度和视觉美感。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks",
        "summary": "Recent advances in image editing have shifted from manual pixel manipulation\nto employing deep learning methods like stable diffusion models, which now\nleverage cross-attention mechanisms for text-driven control. This transition\nhas simplified the editing process but also introduced variability in results,\nsuch as inconsistent hair color changes. Our research aims to enhance the\nprecision and reliability of prompt-to-prompt image editing frameworks by\nexploring and optimizing hyperparameters. We present a comprehensive study of\nthe \"word swap\" method, develop an \"attention re-weight method\" for better\nadaptability, and propose the \"CL P2P\" framework to address existing\nlimitations like cycle inconsistency. This work contributes to understanding\nand improving the interaction between hyperparameter settings and the\narchitectural choices of neural network models, specifically their attention\nmechanisms, which significantly influence the composition and quality of the\ngenerated images.",
        "url": "http://arxiv.org/abs/2510.04034v1",
        "published_date": "2025-10-05T04:56:07+00:00",
        "updated_date": "2025-10-05T04:56:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Linn Bieske",
            "Carla Lorente"
        ],
        "tldr": "This paper explores hyperparameter optimization and proposes new mechanisms, including an attention re-weight method and a novel framework (CL P2P), to enhance the precision and reliability of prompt-to-prompt image editing via cross-attention.",
        "tldr_zh": "本文通过探索超参数优化并提出新的机制，包括注意力重加权方法和新的框架（CL P2P），以提高通过交叉注意力实现的prompt-to-prompt图像编辑的精度和可靠性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generating Human Motion Videos using a Cascaded Text-to-Video Framework",
        "summary": "Human video generation is becoming an increasingly important task with broad\napplications in graphics, entertainment, and embodied AI. Despite the rapid\nprogress of video diffusion models (VDMs), their use for general-purpose human\nvideo generation remains underexplored, with most works constrained to\nimage-to-video setups or narrow domains like dance videos. In this work, we\npropose CAMEO, a cascaded framework for general human motion video generation.\nIt seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,\nmitigating suboptimal factors that may arise in this process across both\ntraining and inference through carefully designed components. Specifically, we\nanalyze and prepare both textual prompts and visual conditions to effectively\ntrain the VDM, ensuring robust alignment between motion descriptions,\nconditioning signals, and the generated videos. Furthermore, we introduce a\ncamera-aware conditioning module that connects the two stages, automatically\nselecting viewpoints aligned with the input text to enhance coherence and\nreduce manual intervention. We demonstrate the effectiveness of our approach on\nboth the MovieGen benchmark and a newly introduced benchmark tailored to the\nT2M-VDM combination, while highlighting its versatility across diverse use\ncases.",
        "url": "http://arxiv.org/abs/2510.03909v1",
        "published_date": "2025-10-04T19:16:28+00:00",
        "updated_date": "2025-10-04T19:16:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyelin Nam",
            "Hyojun Go",
            "Byeongjun Park",
            "Byung-Hoon Kim",
            "Hyungjin Chung"
        ],
        "tldr": "The paper introduces CAMEO, a cascaded framework for generating human motion videos from text, addressing limitations in existing video diffusion models by bridging Text-to-Motion models and conditional VDMs with camera-aware conditioning.",
        "tldr_zh": "本文介绍了 CAMEO，一个用于从文本生成人体运动视频的级联框架。它通过将文本到运动模型和条件视频扩散模型与相机感知调节相结合，解决了现有视频扩散模型的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]