[
    {
        "title": "Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models",
        "summary": "Decomposing complex data into factorized representations can reveal reusable components and enable synthesizing new samples via component recombination. We investigate this in the context of diffusion-based models that learn factorized latent spaces without factor-level supervision. In images, factors can capture background, illumination, and object attributes; in robotic videos, they can capture reusable motion components. To improve both latent factor discovery and quality of compositional generation, we introduce an adversarial training signal via a discriminator trained to distinguish between single-source samples and those generated by recombining factors across sources. By optimizing the generator to fool this discriminator, we encourage physical and semantic consistency in the resulting recombinations. Our method outperforms implementations of prior baselines on CelebA-HQ, Virtual KITTI, CLEVR, and Falcor3D, achieving lower FID scores and better disentanglement as measured by MIG and MCC. Furthermore, we demonstrate a novel application to robotic video trajectories: by recombining learned action components, we generate diverse sequences that significantly increase state-space coverage for exploration on the LIBERO benchmark.",
        "url": "http://arxiv.org/abs/2601.22057v1",
        "published_date": "2026-01-29T17:57:06+00:00",
        "updated_date": "2026-01-29T17:57:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Archer Wang",
            "Emile Anand",
            "Yilun Du",
            "Marin Soljačić"
        ],
        "tldr": "This paper introduces an unsupervised method for decomposing data into factorized representations using diffusion models, enhanced by a discriminator-driven adversarial training signal, enabling high-quality compositional generation and outperforming existing baselines on image and robotic video datasets.",
        "tldr_zh": "该论文提出了一种无监督方法，使用扩散模型将数据分解为因子化表示，并通过判别器驱动的对抗训练信号进行增强，从而实现高质量的组合生成，并在图像和机器人视频数据集上优于现有基线。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-Shot Video Restoration and Enhancement with Assistance of Video Diffusion Models",
        "summary": "Although diffusion-based zero-shot image restoration and enhancement methods have achieved great success, applying them to video restoration or enhancement will lead to severe temporal flickering. In this paper, we propose the first framework that utilizes the rapidly-developed video diffusion model to assist the image-based method in maintaining more temporal consistency for zero-shot video restoration and enhancement. We propose homologous latents fusion, heterogenous latents fusion, and a COT-based fusion ratio strategy to utilize both homologous and heterogenous text-to-video diffusion models to complement the image method. Moreover, we propose temporal-strengthening post-processing to utilize the image-to-video diffusion model to further improve temporal consistency. Our method is training-free and can be applied to any diffusion-based image restoration and enhancement methods. Experimental results demonstrate the superiority of the proposed method.",
        "url": "http://arxiv.org/abs/2601.21922v1",
        "published_date": "2026-01-29T16:14:07+00:00",
        "updated_date": "2026-01-29T16:14:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cong Cao",
            "Huanjing Yue",
            "Shangbin Xie",
            "Xin Liu",
            "Jingyu Yang"
        ],
        "tldr": "This paper presents a novel, training-free framework for zero-shot video restoration and enhancement that leverages video diffusion models to improve temporal consistency in image-based methods, using techniques like homologous/heterogenous latent fusion and temporal-strengthening post-processing.",
        "tldr_zh": "本文提出了一种新颖的、无需训练的零样本视频修复和增强框架，该框架利用视频扩散模型来提高基于图像的方法中的时间一致性，使用了同源/异源潜在融合和时间强化后处理等技术。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Past- and Future-Informed KV Cache Policy with Salience Estimation in Autoregressive Video Diffusion",
        "summary": "Video generation is pivotal to digital media creation, and recent advances in autoregressive video generation have markedly enhanced the efficiency of real-time video synthesis. However, existing approaches generally rely on heuristic KV Cache policies, which ignore differences in token importance in long-term video generation. This leads to the loss of critical spatiotemporal information and the accumulation of redundant, invalid cache, thereby degrading video generation quality and efficiency. To address this limitation, we first observe that token contributions to video generation are highly time-heterogeneous and accordingly propose a novel Past- and Future-Informed KV Cache Policy (PaFu-KV). Specifically, PaFu-KV introduces a lightweight Salience Estimation Head distilled from a bidirectional teacher to estimate salience scores, allowing the KV cache to retain informative tokens while discarding less relevant ones. This policy yields a better quality-efficiency trade-off by shrinking KV cache capacity and reducing memory footprint at inference time. Extensive experiments on benchmarks demonstrate that our method preserves high-fidelity video generation quality while enables accelerated inference, thereby enabling more efficient long-horizon video generation. Our code will be released upon paper acceptance.",
        "url": "http://arxiv.org/abs/2601.21896v1",
        "published_date": "2026-01-29T15:55:29+00:00",
        "updated_date": "2026-01-29T15:55:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanmo Chen",
            "Chenghao Xu",
            "Xu Yang",
            "Xuan Chen",
            "Cheng Deng"
        ],
        "tldr": "This paper introduces a novel KV cache policy for autoregressive video diffusion models that utilizes a salience estimation head to retain important tokens and discard less relevant ones, improving efficiency and quality.",
        "tldr_zh": "该论文提出了一种新的自回归视频扩散模型的KV缓存策略，该策略使用显著性估计头来保留重要token并丢弃不太相关的token，从而提高效率和质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Classifier-Free Guidance of Flow Matching via Manifold Projection",
        "summary": "Classifier-free guidance (CFG) is a widely used technique for controllable generation in diffusion and flow-based models. Despite its empirical success, CFG relies on a heuristic linear extrapolation that is often sensitive to the guidance scale. In this work, we provide a principled interpretation of CFG through the lens of optimization. We demonstrate that the velocity field in flow matching corresponds to the gradient of a sequence of smoothed distance functions, which guides latent variables toward the scaled target image set. This perspective reveals that the standard CFG formulation is an approximation of this gradient, where the prediction gap, the discrepancy between conditional and unconditional outputs, governs guidance sensitivity. Leveraging this insight, we reformulate the CFG sampling as a homotopy optimization with a manifold constraint. This formulation necessitates a manifold projection step, which we implement via an incremental gradient descent scheme during sampling. To improve computational efficiency and stability, we further enhance this iterative process with Anderson Acceleration without requiring additional model evaluations. Our proposed methods are training-free and consistently refine generation fidelity, prompt alignment, and robustness to the guidance scale. We validate their effectiveness across diverse benchmarks, demonstrating significant improvements on large-scale models such as DiT-XL-2-256, Flux, and Stable Diffusion 3.5.",
        "url": "http://arxiv.org/abs/2601.21892v1",
        "published_date": "2026-01-29T15:49:31+00:00",
        "updated_date": "2026-01-29T15:49:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jian-Feng Cai",
            "Haixia Liu",
            "Zhengyi Su",
            "Chao Wang"
        ],
        "tldr": "This paper proposes a manifold projection-based classifier-free guidance method for flow matching models that improves generation fidelity, prompt alignment, and robustness to the guidance scale without requiring additional training.",
        "tldr_zh": "本文提出了一种基于流形投影的无分类器指导方法，用于流匹配模型，该方法无需额外训练即可提高生成保真度、提示对齐和对指导尺度的鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bi-Anchor Interpolation Solver for Accelerating Generative Modeling",
        "summary": "Flow Matching (FM) models have emerged as a leading paradigm for high-fidelity synthesis. However, their reliance on iterative Ordinary Differential Equation (ODE) solving creates a significant latency bottleneck. Existing solutions face a dichotomy: training-free solvers suffer from significant performance degradation at low Neural Function Evaluations (NFEs), while training-based one- or few-steps generation methods incur prohibitive training costs and lack plug-and-play versatility. To bridge this gap, we propose the Bi-Anchor Interpolation Solver (BA-solver). BA-solver retains the versatility of standard training-free solvers while achieving significant acceleration by introducing a lightweight SideNet (1-2% backbone size) alongside the frozen backbone. Specifically, our method is founded on two synergistic components: \\textbf{1) Bidirectional Temporal Perception}, where the SideNet learns to approximate both future and historical velocities without retraining the heavy backbone; and 2) Bi-Anchor Velocity Integration, which utilizes the SideNet with two anchor velocities to efficiently approximate intermediate velocities for batched high-order integration. By utilizing the backbone to establish high-precision ``anchors'' and the SideNet to densify the trajectory, BA-solver enables large interval sizes with minimized error. Empirical results on ImageNet-256^2 demonstrate that BA-solver achieves generation quality comparable to 100+ NFEs Euler solver in just 10 NFEs and maintains high fidelity in as few as 5 NFEs, incurring negligible training costs. Furthermore, BA-solver ensures seamless integration with existing generative pipelines, facilitating downstream tasks such as image editing.",
        "url": "http://arxiv.org/abs/2601.21542v1",
        "published_date": "2026-01-29T10:59:36+00:00",
        "updated_date": "2026-01-29T10:59:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hongxu Chen",
            "Hongxiang Li",
            "Zhen Wang",
            "Long Chen"
        ],
        "tldr": "The paper introduces Bi-Anchor Interpolation Solver (BA-solver), a lightweight method that accelerates Flow Matching generative models by using a small SideNet to approximate velocities for high-order integration, achieving similar quality to 100+ NFEs Euler solver in just 10 NFEs.",
        "tldr_zh": "该论文介绍了一种名为Bi-Anchor Interpolation Solver (BA-solver) 的轻量级方法，通过使用小型SideNet来近似速度以进行高阶积分，从而加速Flow Matching生成模型，在仅10个NFEs中实现了与100+ NFEs Euler solver相似的质量。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing",
        "summary": "Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2601.21498v1",
        "published_date": "2026-01-29T10:15:55+00:00",
        "updated_date": "2026-01-29T10:15:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Thanh-Nhan Vo",
            "Trong-Thuan Nguyen",
            "Tam V. Nguyen",
            "Minh-Triet Tran"
        ],
        "tldr": "SimGraph is a unified scene graph-based framework for image generation and editing, integrating token-based generation and diffusion-based editing for improved control and consistency.",
        "tldr_zh": "SimGraph是一个统一的基于场景图的图像生成和编辑框架，集成了基于token的生成和基于扩散的编辑，以实现更好的控制和一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Revisiting Diffusion Model Predictions Through Dimensionality",
        "summary": "Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise ($\\varepsilon$) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which $\\varepsilon$-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance.",
        "url": "http://arxiv.org/abs/2601.21419v1",
        "published_date": "2026-01-29T08:56:55+00:00",
        "updated_date": "2026-01-29T08:56:55+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Qing Jin",
            "Chaoyang Wang"
        ],
        "tldr": "This paper provides a theoretical framework explaining why direct data prediction is superior in high-dimensional diffusion models, and introduces k-Diff, a method for learning the optimal prediction target directly from data, demonstrating improved generative performance.",
        "tldr_zh": "该论文提供了一个理论框架，解释了为什么直接数据预测在高维扩散模型中更有效，并提出了k-Diff方法，可以直接从数据中学习最佳预测目标，从而提高生成性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation",
        "summary": "Unified Multimodal Models (UMMs) integrate both visual understanding and generation within a single framework. Their ultimate aspiration is to create a cycle where understanding and generation mutually reinforce each other. While recent post-training methods have successfully leveraged understanding to enhance generation, the reverse direction of utilizing generation to improve understanding remains largely unexplored. In this work, we propose UniMRG (Unified Multi-Representation Generation), a simple yet effective architecture-agnostic post-training method. UniMRG enhances the understanding capabilities of UMMs by incorporating auxiliary generation tasks. Specifically, we train UMMs to generate multiple intrinsic representations of input images, namely pixel (reconstruction), depth (geometry), and segmentation (structure), alongside standard visual understanding objectives. By synthesizing these diverse representations, UMMs capture complementary information regarding appearance, spatial relations, and structural layout. Consequently, UMMs develop a deeper and more comprehensive understanding of visual inputs. Extensive experiments across diverse UMM architectures demonstrate that our method notably enhances fine-grained perception, reduces hallucinations, and improves spatial understanding, while simultaneously boosting generation capabilities.",
        "url": "http://arxiv.org/abs/2601.21406v1",
        "published_date": "2026-01-29T08:42:25+00:00",
        "updated_date": "2026-01-29T08:42:25+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zihan Su",
            "Hongyang Wei",
            "Kangrui Cen",
            "Yong Wang",
            "Guanhua Chen",
            "Chun Yuan",
            "Xiangxiang Chu"
        ],
        "tldr": "The paper introduces UniMRG, a post-training method that enhances unified multimodal model understanding by having them generate multiple image representations (pixel, depth, segmentation) alongside standard understanding tasks.",
        "tldr_zh": "该论文介绍了UniMRG，一种后训练方法，通过让统一多模态模型在执行标准理解任务的同时生成多个图像表示（像素、深度、分割），从而增强其理解能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models",
        "summary": "Recent advances in generative foundational models, often termed \"world models,\" have propelled interest in applying them to critical tasks like robotic planning and autonomous system training. For reliable deployment, these models must exhibit high physical fidelity, accurately simulating real-world dynamics. Existing physics-based video benchmarks, however, suffer from entanglement, where a single test simultaneously evaluates multiple physical laws and concepts, fundamentally limiting their diagnostic capability. We introduce WorldBench, a novel video-based benchmark specifically designed for concept-specific, disentangled evaluation, allowing us to rigorously isolate and assess understanding of a single physical concept or law at a time. To make WorldBench comprehensive, we design benchmarks at two different levels: 1) an evaluation of intuitive physical understanding with concepts such as object permanence or scale/perspective, and 2) an evaluation of low-level physical constants and material properties such as friction coefficients or fluid viscosity. When SOTA video-based world models are evaluated on WorldBench, we find specific patterns of failure in particular physics concepts, with all tested models lacking the physical consistency required to generate reliable real-world interactions. Through its concept-specific evaluation, WorldBench offers a more nuanced and scalable framework for rigorously evaluating the physical reasoning capabilities of video generation and world models, paving the way for more robust and generalizable world-model-driven learning.",
        "url": "http://arxiv.org/abs/2601.21282v1",
        "published_date": "2026-01-29T05:31:02+00:00",
        "updated_date": "2026-01-29T05:31:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rishi Upadhyay",
            "Howard Zhang",
            "Jim Solomon",
            "Ayush Agrawal",
            "Pranay Boreddy",
            "Shruti Satya Narayana",
            "Yunhao Ba",
            "Alex Wong",
            "Celso M de Melo",
            "Achuta Kadambi"
        ],
        "tldr": "The paper introduces WorldBench, a new video-based benchmark for evaluating the physical reasoning capabilities of world models, designed for disentangled assessment of specific physical concepts.",
        "tldr_zh": "该论文介绍了一个名为 WorldBench 的新视频基准，用于评估世界模型的物理推理能力，该基准旨在对特定物理概念进行解耦评估。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought",
        "summary": "Multimodal models for text-to-image generation have achieved strong visual fidelity, yet they remain brittle under compositional structural constraints-notably generative numeracy, attribute binding, and part-level relations. To address these challenges, we propose Shape-of-Thought (SoT), a visual CoT framework that enables progressive shape assembly via coherent 2D projections without external engines at inference time. SoT trains a unified multimodal autoregressive model to generate interleaved textual plans and rendered intermediate states, helping the model capture shape-assembly logic without producing explicit geometric representations. To support this paradigm, we introduce SoT-26K, a large-scale dataset of grounded assembly traces derived from part-based CAD hierarchies, and T2S-CompBench, a benchmark for evaluating structural integrity and trace faithfulness. Fine-tuning on SoT-26K achieves 88.4% on component numeracy and 84.8% on structural topology, outperforming text-only baselines by around 20%. SoT establishes a new paradigm for transparent, process-supervised compositional generation. The code is available at https://anonymous.4open.science/r/16FE/. The SoT-26K dataset will be released upon acceptance.",
        "url": "http://arxiv.org/abs/2601.21081v1",
        "published_date": "2026-01-28T22:07:17+00:00",
        "updated_date": "2026-01-28T22:07:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Huo",
            "Siyu Zhang",
            "Kun Zeng",
            "Haoyue Liu",
            "Owen Lee",
            "Junlin Chen",
            "Yuquan Lu",
            "Yifu Guo",
            "Yaodong Liang",
            "Xiaoying Tang"
        ],
        "tldr": "The paper introduces Shape-of-Thought (SoT), a visual CoT framework for improved compositional text-to-image generation, addressing challenges in generative numeracy, attribute binding, and part-level relations. It includes a new dataset, SoT-26K, and benchmark, T2S-CompBench, demonstrating significant improvements over text-only baselines.",
        "tldr_zh": "该论文介绍了一种名为Shape-of-Thought (SoT) 的视觉CoT框架，旨在改进组合文本到图像的生成，解决了生成数字、属性绑定和零件级关系中的挑战。它包括一个新的数据集SoT-26K和一个基准测试T2S-CompBench，表明与仅文本的基线相比有了显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning",
        "summary": "Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.",
        "url": "http://arxiv.org/abs/2601.21037v1",
        "published_date": "2026-01-28T20:57:55+00:00",
        "updated_date": "2026-01-28T20:57:55+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Chengzu Li",
            "Zanyi Wang",
            "Jiaang Li",
            "Yi Xu",
            "Han Zhou",
            "Huanyu Zhang",
            "Ruichuan An",
            "Dengyang Jiang",
            "Zhaochong An",
            "Ivan Vulić",
            "Serge Belongie",
            "Anna Korhonen"
        ],
        "tldr": "This paper explores using video generation models for visual reasoning, demonstrating robust zero-shot generalization and the importance of visual context and test-time scaling in tasks like maze navigation and Tangram puzzles.",
        "tldr_zh": "本文探索了使用视频生成模型进行视觉推理，展示了在迷宫导航和七巧板拼图等任务中的强大零样本泛化能力，以及视觉上下文和测试时缩放的重要性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ChartE$^{3}$: A Comprehensive Benchmark for End-to-End Chart Editing",
        "summary": "Charts are a fundamental visualization format for structured data analysis. Enabling end-to-end chart editing according to user intent is of great practical value, yet remains challenging due to the need for both fine-grained control and global structural consistency. Most existing approaches adopt pipeline-based designs, where natural language or code serves as an intermediate representation, limiting their ability to faithfully execute complex edits. We introduce ChartE$^{3}$, an End-to-End Chart Editing benchmark that directly evaluates models without relying on intermediate natural language programs or code-level supervision. ChartE$^{3}$ focuses on two complementary editing dimensions: local editing, which involves fine-grained appearance changes such as font or color adjustments, and global editing, which requires holistic, data-centric transformations including data filtering and trend line addition. ChartE$^{3}$ contains over 1,200 high-quality samples constructed via a well-designed data pipeline with human curation. Each sample is provided as a triplet of a chart image, its underlying code, and a multimodal editing instruction, enabling evaluation from both objective and subjective perspectives. Extensive benchmarking of state-of-the-art multimodal large language models reveals substantial performance gaps, particularly on global editing tasks, highlighting critical limitations in current end-to-end chart editing capabilities.",
        "url": "http://arxiv.org/abs/2601.21694v1",
        "published_date": "2026-01-29T13:29:27+00:00",
        "updated_date": "2026-01-29T13:29:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuo Li",
            "Jiajun Sun",
            "Zhekai Wang",
            "Xiaoran Fan",
            "Hui Li",
            "Dingwen Yang",
            "Zhiheng Xi",
            "Yijun Wang",
            "Zifei Shan",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "tldr": "The paper introduces ChartE$^{3}$, a benchmark for end-to-end chart editing, highlighting the limitations of current multimodal LLMs in performing complex edits and providing a dataset for future research.",
        "tldr_zh": "该论文介绍了ChartE$^{3}$，一个用于端到端图表编辑的基准测试，强调了当前多模态LLM在执行复杂编辑方面的局限性，并为未来的研究提供了一个数据集。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "HERS: Hidden-Pattern Expert Learning for Risk-Specific Vehicle Damage Adaptation in Diffusion Models",
        "summary": "Recent advances in text-to-image (T2I) diffusion models have enabled increasingly realistic synthesis of vehicle damage, raising concerns about their reliability in automated insurance workflows. The ability to generate crash-like imagery challenges the boundary between authentic and synthetic data, introducing new risks of misuse in fraud or claim manipulation. To address these issues, we propose HERS (Hidden-Pattern Expert Learning for Risk-Specific Damage Adaptation), a framework designed to improve fidelity, controllability, and domain alignment of diffusion-generated damage images. HERS fine-tunes a base diffusion model via domain-specific expert adaptation without requiring manual annotation. Using self-supervised image-text pairs automatically generated by a large language model and T2I pipeline, HERS models each damage category, such as dents, scratches, broken lights, or cracked paint, as a separate expert. These experts are later integrated into a unified multi-damage model that balances specialization with generalization. We evaluate HERS across four diffusion backbones and observe consistent improvements: plus 5.5 percent in text faithfulness and plus 2.3 percent in human preference ratings compared to baselines. Beyond image fidelity, we discuss implications for fraud detection, auditability, and safe deployment of generative models in high-stakes domains. Our findings highlight both the opportunities and risks of domain-specific diffusion, underscoring the importance of trustworthy generation in safety-critical applications such as auto insurance.",
        "url": "http://arxiv.org/abs/2601.21517v1",
        "published_date": "2026-01-29T10:30:07+00:00",
        "updated_date": "2026-01-29T10:30:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Teerapong Panboonyuen"
        ],
        "tldr": "The paper introduces HERS, a framework that fine-tunes diffusion models for generating realistic vehicle damage images for auto insurance, addressing concerns about misuse and improving fidelity and controllability without manual annotation.",
        "tldr_zh": "该论文介绍了HERS，一个用于微调扩散模型的框架，可以生成逼真的车辆损伤图像，用于汽车保险。该框架旨在解决滥用问题，并在无需手动标注的情况下提高图像的保真度和可控性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]