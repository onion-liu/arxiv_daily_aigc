[
    {
        "title": "RealDPO: Real or Not Real, that is the Preference",
        "summary": "Video generative models have recently achieved notable advancements in\nsynthesis quality. However, generating complex motions remains a critical\nchallenge, as existing models often struggle to produce natural, smooth, and\ncontextually consistent movements. This gap between generated and real-world\nmotions limits their practical applicability. To address this issue, we\nintroduce RealDPO, a novel alignment paradigm that leverages real-world data as\npositive samples for preference learning, enabling more accurate motion\nsynthesis. Unlike traditional supervised fine-tuning (SFT), which offers\nlimited corrective feedback, RealDPO employs Direct Preference Optimization\n(DPO) with a tailored loss function to enhance motion realism. By contrasting\nreal-world videos with erroneous model outputs, RealDPO enables iterative\nself-correction, progressively refining motion quality. To support\npost-training in complex motion synthesis, we propose RealAction-5K, a curated\ndataset of high-quality videos capturing human daily activities with rich and\nprecise motion details. Extensive experiments demonstrate that RealDPO\nsignificantly improves video quality, text alignment, and motion realism\ncompared to state-of-the-art models and existing preference optimization\ntechniques.",
        "url": "http://arxiv.org/abs/2510.14955v1",
        "published_date": "2025-10-16T17:58:25+00:00",
        "updated_date": "2025-10-16T17:58:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Guo Cheng",
            "Danni Yang",
            "Ziqi Huang",
            "Jianlou Si",
            "Chenyang Si",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces RealDPO, a novel alignment paradigm using real-world video data for preference learning to improve the realism of motion in video generation, along with a new dataset, RealAction-5K.",
        "tldr_zh": "该论文介绍了RealDPO，一种利用真实世界视频数据进行偏好学习的新对齐范式，以提高视频生成中动作的真实感，并提出了一个新的数据集RealAction-5K。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
        "summary": "Despite the rapid progress of instruction-based image editing, its extension\nto video remains underexplored, primarily due to the prohibitive cost and\ncomplexity of constructing large-scale paired video editing datasets. To\naddress this challenge, we introduce a low-cost pretraining strategy for\ninstruction-based video editing that leverages in-context learning from\nunpaired video clips. We show that pretraining a foundation video generation\nmodel with this strategy endows it with general editing capabilities, such as\nadding, replacing, or deleting operations, according to input editing\ninstructions. The pretrained model can then be efficiently refined with a small\namount of high-quality paired editing data. Built upon HunyuanVideoT2V, our\nframework first pretrains on approximately 1M real video clips to learn basic\nediting concepts, and subsequently fine-tunes on fewer than 150k curated\nediting pairs to extend more editing tasks and improve the editing quality.\nComparative experiments show that our method surpasses existing\ninstruction-based video editing approaches in both instruction alignment and\nvisual fidelity, achieving a 12\\% improvement in editing instruction following\nand a 15\\% improvement in editing quality.",
        "url": "http://arxiv.org/abs/2510.14648v1",
        "published_date": "2025-10-16T13:02:11+00:00",
        "updated_date": "2025-10-16T13:02:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinyao Liao",
            "Xianfang Zeng",
            "Ziye Song",
            "Zhoujie Fu",
            "Gang Yu",
            "Guosheng Lin"
        ],
        "tldr": "This paper introduces a pretraining strategy for instruction-based video editing using in-context learning from unpaired video clips, followed by fine-tuning on a smaller paired dataset, achieving improvements in instruction alignment and visual fidelity.",
        "tldr_zh": "该论文提出了一种基于非配对视频片段的上下文学习指令式视频编辑预训练策略，然后在较小的配对数据集上进行微调，从而在指令对齐和视觉保真度方面取得了改进。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing",
        "summary": "We present an inference-time diffusion sampling method to perform multi-view\nconsistent image editing using pre-trained 2D image editing models. These\nmodels can independently produce high-quality edits for each image in a set of\nmulti-view images of a 3D scene or object, but they do not maintain consistency\nacross views. Existing approaches typically address this by optimizing over\nexplicit 3D representations, but they suffer from a lengthy optimization\nprocess and instability under sparse view settings. We propose an implicit 3D\nregularization approach by constraining the generated 2D image sequences to\nadhere to a pre-trained multi-view image distribution. This is achieved through\ncoupled diffusion sampling, a simple diffusion sampling technique that\nconcurrently samples two trajectories from both a multi-view image distribution\nand a 2D edited image distribution, using a coupling term to enforce the\nmulti-view consistency among the generated images. We validate the\neffectiveness and generality of this framework on three distinct multi-view\nimage editing tasks, demonstrating its applicability across various model\narchitectures and highlighting its potential as a general solution for\nmulti-view consistent editing.",
        "url": "http://arxiv.org/abs/2510.14981v1",
        "published_date": "2025-10-16T17:59:59+00:00",
        "updated_date": "2025-10-16T17:59:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hadi Alzayer",
            "Yunzhi Zhang",
            "Chen Geng",
            "Jia-Bin Huang",
            "Jiajun Wu"
        ],
        "tldr": "The paper presents a training-free method for multi-view consistent image editing using coupled diffusion sampling, enforcing consistency by adhering to a pre-trained multi-view image distribution during the editing process. It avoids 3D optimization, offering potentially faster and more stable results.",
        "tldr_zh": "该论文提出了一种无需训练的多视图一致图像编辑方法，通过耦合扩散采样，利用预训练的多视图图像分布来强制一致性，避免了3D优化，可能提供更快更稳定的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Learning an Image Editing Model without Image Editing Pairs",
        "summary": "Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
        "url": "http://arxiv.org/abs/2510.14978v1",
        "published_date": "2025-10-16T17:59:57+00:00",
        "updated_date": "2025-10-16T17:59:57+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nupur Kumari",
            "Sheng-Yu Wang",
            "Nanxuan Zhao",
            "Yotam Nitzan",
            "Yuheng Li",
            "Krishna Kumar Singh",
            "Richard Zhang",
            "Eli Shechtman",
            "Jun-Yan Zhu",
            "Xun Huang"
        ],
        "tldr": "This paper introduces a novel training paradigm for image editing models that eliminates the need for paired data by leveraging VLMs for feedback and DMD for visual fidelity, achieving comparable performance to supervised methods.",
        "tldr_zh": "本文介绍了一种新颖的图像编辑模型训练范式，该范式通过利用视觉语言模型进行反馈和分布匹配损失 (DMD) 以保证视觉保真度，从而消除了对成对数据的需求，并且实现了与监督方法相当的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
        "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.",
        "url": "http://arxiv.org/abs/2510.14975v1",
        "published_date": "2025-10-16T17:59:54+00:00",
        "updated_date": "2025-10-16T17:59:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hengyuan Xu",
            "Wei Cheng",
            "Peng Xing",
            "Yixiao Fang",
            "Shuhan Wu",
            "Rui Wang",
            "Xianfang Zeng",
            "Daxin Jiang",
            "Gang Yu",
            "Xingjun Ma",
            "Yu-Gang Jiang"
        ],
        "tldr": "The paper introduces WithAnyone, a diffusion-based model and a new paired dataset to address the copy-paste problem in identity-consistent image generation, improving controllability by balancing identity fidelity with diversity.",
        "tldr_zh": "该论文介绍了 WithAnyone，一个基于扩散模型的模型和一个新的配对数据集，用于解决身份一致图像生成中的复制粘贴问题，通过平衡身份保真度和多样性来提高可控性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
        "summary": "Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard $\\ell_2$ flow matching loss. By simply\nmimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.",
        "url": "http://arxiv.org/abs/2510.14974v1",
        "published_date": "2025-10-16T17:59:51+00:00",
        "updated_date": "2025-10-16T17:59:51+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Hansheng Chen",
            "Kai Zhang",
            "Hao Tan",
            "Leonidas Guibas",
            "Gordon Wetzstein",
            "Sai Bi"
        ],
        "tldr": "The paper introduces pi-Flow, a policy-based flow model for few-step image generation that uses imitation distillation to improve quality and diversity compared to existing methods, achieving state-of-the-art results on ImageNet.",
        "tldr_zh": "这篇论文介绍了pi-Flow，一种基于策略的流动模型，用于少量步骤图像生成。它使用模仿蒸馏来提高质量和多样性，相比现有方法，在ImageNet上取得了最好的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning",
        "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/",
        "url": "http://arxiv.org/abs/2510.14958v1",
        "published_date": "2025-10-16T17:58:58+00:00",
        "updated_date": "2025-10-16T17:58:58+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Weikang Shi",
            "Aldrich Yu",
            "Rongyao Fang",
            "Houxing Ren",
            "Ke Wang",
            "Aojun Zhou",
            "Changyao Tian",
            "Xinyu Fu",
            "Yuxuan Hu",
            "Zimu Lu",
            "Linjiang Huang",
            "Si Liu",
            "Rui Liu",
            "Hongsheng Li"
        ],
        "tldr": "The paper introduces MathCanvas, a framework for enhancing LMMs with visual chain-of-thought reasoning for mathematical problems, including a novel pre-training and fine-tuning strategy along with new datasets and a benchmark for evaluation; the results show significant improvements over existing approaches.",
        "tldr_zh": "本文介绍了MathCanvas，一个用于增强大型多模态模型(LMMs)在数学问题中视觉链式思考推理能力的框架，包括一种新颖的预训练和微调策略，以及用于评估的新数据集和一个基准；结果表明，该方法相比现有方法有了显著的改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression",
        "summary": "Whole-body multi-modal human motion generation poses two primary challenges:\ncreating an effective motion generation mechanism and integrating various\nmodalities, such as text, speech, and music, into a cohesive framework. Unlike\nprevious methods that usually employ discrete masked modeling or autoregressive\nmodeling, we develop a continuous masked autoregressive motion transformer,\nwhere a causal attention is performed considering the sequential nature within\nthe human motion. Within this transformer, we introduce a gated linear\nattention and an RMSNorm module, which drive the transformer to pay attention\nto the key actions and suppress the instability caused by either the abnormal\nmovements or the heterogeneous distributions within multi-modalities. To\nfurther enhance both the motion generation and the multimodal generalization,\nwe employ the DiT structure to diffuse the conditions from the transformer\ntowards the targets. To fuse different modalities, AdaLN and cross-attention\nare leveraged to inject the text, speech, and music signals. Experimental\nresults demonstrate that our framework outperforms previous methods across all\nmodalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.",
        "url": "http://arxiv.org/abs/2510.14954v1",
        "published_date": "2025-10-16T17:57:53+00:00",
        "updated_date": "2025-10-16T17:57:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhe Li",
            "Weihao Yuan",
            "Weichao Shen",
            "Siyu Zhu",
            "Zilong Dong",
            "Chang Xu"
        ],
        "tldr": "The paper introduces OmniMotion, a novel continuous masked autoregressive motion transformer for multimodal human motion generation (text, speech, music), outperforming existing methods in various motion synthesis tasks.",
        "tldr_zh": "该论文提出了OmniMotion，一种新型的连续掩码自回归运动Transformer，用于多模态人体运动生成（文本、语音、音乐），在各种运动合成任务中优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation",
        "summary": "We present 3DScenePrompt, a framework that generates the next video chunk\nfrom arbitrary-length input while enabling precise camera control and\npreserving scene consistency. Unlike methods conditioned on a single image or a\nshort clip, we employ dual spatio-temporal conditioning that reformulates\ncontext-view referencing across the input video. Our approach conditions on\nboth temporally adjacent frames for motion continuity and spatially adjacent\ncontent for scene consistency. However, when generating beyond temporal\nboundaries, directly using spatially adjacent frames would incorrectly preserve\ndynamic elements from the past. We address this by introducing a 3D scene\nmemory that represents exclusively the static geometry extracted from the\nentire input video. To construct this memory, we leverage dynamic SLAM with our\nnewly introduced dynamic masking strategy that explicitly separates static\nscene geometry from moving elements. The static scene representation can then\nbe projected to any target viewpoint, providing geometrically consistent warped\nviews that serve as strong 3D spatial prompts while allowing dynamic regions to\nevolve naturally from temporal context. This enables our model to maintain\nlong-range spatial coherence and precise camera control without sacrificing\ncomputational efficiency or motion realism. Extensive experiments demonstrate\nthat our framework significantly outperforms existing methods in scene\nconsistency, camera controllability, and generation quality. Project page :\nhttps://cvlab-kaist.github.io/3DScenePrompt/",
        "url": "http://arxiv.org/abs/2510.14945v1",
        "published_date": "2025-10-16T17:55:25+00:00",
        "updated_date": "2025-10-16T17:55:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "JoungBin Lee",
            "Jaewoo Jung",
            "Jisang Han",
            "Takuya Narihira",
            "Kazumi Fukuda",
            "Junyoung Seo",
            "Sunghwan Hong",
            "Yuki Mitsufuji",
            "Seungryong Kim"
        ],
        "tldr": "This paper introduces 3DScenePrompt, a framework for generating scene-consistent, camera-controllable videos from arbitrary-length input by using a 3D scene memory constructed with dynamic SLAM and masking.",
        "tldr_zh": "本文介绍了 3DScenePrompt，一个通过使用动态 SLAM 和掩蔽构建的 3D 场景记忆，从任意长度的输入生成场景一致、相机可控视频的框架。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention",
        "summary": "Text-to-image generation with visual autoregressive~(VAR) models has recently\nachieved impressive advances in generation fidelity and inference efficiency.\nWhile control mechanisms have been explored for diffusion models, enabling\nprecise and flexible control within VAR paradigm remains underexplored. To\nbridge this critical gap, in this paper, we introduce ScaleWeaver, a novel\nframework designed to achieve high-fidelity, controllable generation upon\nadvanced VAR models through parameter-efficient fine-tuning. The core module in\nScaleWeaver is the improved MMDiT block with the proposed Reference Attention\nmodule, which efficiently and effectively incorporates conditional information.\nDifferent from MM Attention, the proposed Reference Attention module discards\nthe unnecessary attention from image$\\rightarrow$condition, reducing\ncomputational cost while stabilizing control injection. Besides, it\nstrategically emphasizes parameter reuse, leveraging the capability of the VAR\nbackbone itself with a few introduced parameters to process control\ninformation, and equipping a zero-initialized linear projection to ensure that\ncontrol signals are incorporated effectively without disrupting the generative\ncapability of the base model. Extensive experiments show that ScaleWeaver\ndelivers high-quality generation and precise control while attaining superior\nefficiency over diffusion-based methods, making ScaleWeaver a practical and\neffective solution for controllable text-to-image generation within the visual\nautoregressive paradigm. Code and models will be released.",
        "url": "http://arxiv.org/abs/2510.14882v1",
        "published_date": "2025-10-16T17:00:59+00:00",
        "updated_date": "2025-10-16T17:00:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Keli Liu",
            "Zhendong Wang",
            "Wengang Zhou",
            "Shaodong Xu",
            "Ruixiao Dong",
            "Houqiang Li"
        ],
        "tldr": "ScaleWeaver introduces a parameter-efficient fine-tuning framework for controllable text-to-image generation using visual autoregressive models, achieving high fidelity and efficiency through a novel Reference Attention mechanism.",
        "tldr_zh": "ScaleWeaver 提出了一种参数高效的微调框架，用于使用视觉自回归模型的可控文本到图像生成，通过一种新的参考注意力机制实现了高保真度和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
        "summary": "Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration.",
        "url": "http://arxiv.org/abs/2510.14847v1",
        "published_date": "2025-10-16T16:19:13+00:00",
        "updated_date": "2025-10-16T16:19:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Meiqi Wu",
            "Jiashu Zhu",
            "Xiaokun Feng",
            "Chubin Chen",
            "Chen Zhu",
            "Bingze Song",
            "Fangyuan Mao",
            "Jiahong Wu",
            "Xiangxiang Chu",
            "Kaiqi Huang"
        ],
        "tldr": "The paper introduces ImagerySearch, an adaptive test-time search strategy for video generation that dynamically adjusts the inference search space and reward function to enhance performance in imaginative scenarios with long-distance semantic relationships. They also provide a new benchmark, LDT-Bench, for evaluating such scenarios.",
        "tldr_zh": "该论文提出了 ImagerySearch，一种自适应的测试时搜索策略，用于视频生成，通过动态调整推理搜索空间和奖励函数，提升在具有长距离语义关系的想象场景中的性能。他们还提供了一个新的基准测试 LDT-Bench，用于评估此类场景。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FraQAT: Quantization Aware Training with Fractional bits",
        "summary": "State-of-the-art (SOTA) generative models have demonstrated impressive\ncapabilities in image synthesis or text generation, often with a large capacity\nmodel. However, these large models cannot be deployed on smartphones due to the\nlimited availability of on-board memory and computations. Quantization methods\nlower the precision of the model parameters, allowing for efficient\ncomputations, \\eg, in \\INT{8}. Although aggressive quantization addresses\nefficiency and memory constraints, preserving the quality of the model remains\na challenge. To retain quality in previous aggressive quantization, we propose\na new fractional bits quantization (\\short) approach. The novelty is a simple\nyet effective idea: we progressively reduce the model's precision from 32 to 4\nbits per parameter, and exploit the fractional bits during optimization to\nmaintain high generation quality. We show that the \\short{} yields improved\nquality on a variety of diffusion models, including SD3.5-Medium, Sana,\n\\pixart, and FLUX.1-schnell, while achieving $4-7\\%$ lower FiD than standard\nQAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the\nQualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP).",
        "url": "http://arxiv.org/abs/2510.14823v1",
        "published_date": "2025-10-16T16:01:08+00:00",
        "updated_date": "2025-10-16T16:01:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Luca Morreale",
            "Alberto Gil C. P. Ramos",
            "Malcolm Chadwick",
            "Mehid Noroozi",
            "Ruchika Chavhan",
            "Abhinav Mehrotra",
            "Sourav Bhattacharya"
        ],
        "tldr": "This paper introduces a fractional bits quantization (FraQAT) approach for compressing large generative models, progressively reducing precision from 32 to 4 bits, enabling deployment on devices like smartphones while maintaining generation quality, achieving lower FID scores than standard QAT on diffusion models.",
        "tldr_zh": "本文提出了一种分数比特量化（FraQAT）方法，用于压缩大型生成模型，逐步将精度从32比特降低到4比特，从而能够在智能手机等设备上进行部署，同时保持生成质量，在扩散模型上实现了比标准QAT更低的FID分数。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Adapting Self-Supervised Representations as a Latent Space for Efficient Generation",
        "summary": "We introduce Representation Tokenizer (RepTok), a generative modeling\nframework that represents an image using a single continuous latent token\nobtained from self-supervised vision transformers. Building on a pre-trained\nSSL encoder, we fine-tune only the semantic token embedding and pair it with a\ngenerative decoder trained jointly using a standard flow matching objective.\nThis adaptation enriches the token with low-level, reconstruction-relevant\ndetails, enabling faithful image reconstruction. To preserve the favorable\ngeometry of the original SSL space, we add a cosine-similarity loss that\nregularizes the adapted token, ensuring the latent space remains smooth and\nsuitable for generation. Our single-token formulation resolves spatial\nredundancies of 2D latent spaces and significantly reduces training costs.\nDespite its simplicity and efficiency, RepTok achieves competitive results on\nclass-conditional ImageNet generation and naturally extends to text-to-image\nsynthesis, reaching competitive zero-shot performance on MS-COCO under\nextremely limited training budgets. Our findings highlight the potential of\nfine-tuned SSL representations as compact and effective latent spaces for\nefficient generative modeling.",
        "url": "http://arxiv.org/abs/2510.14630v1",
        "published_date": "2025-10-16T12:43:03+00:00",
        "updated_date": "2025-10-16T12:43:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ming Gui",
            "Johannes Schusterbauer",
            "Timy Phan",
            "Felix Krause",
            "Josh Susskind",
            "Miguel Angel Bautista",
            "Björn Ommer"
        ],
        "tldr": "The paper introduces Representation Tokenizer (RepTok), a method for efficient image generation by fine-tuning self-supervised learning representations into a single latent token, achieving competitive results with low training costs.",
        "tldr_zh": "该论文介绍了Representation Tokenizer (RepTok)，一种通过将自监督学习表示微调为单个潜在标记来实现高效图像生成的方法，以低训练成本实现了具有竞争力的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding",
        "summary": "Video generation has recently made striking visual progress, but maintaining\ncoherent object motion and interactions remains difficult. We trace two\npractical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)\noften collapse to too few effective tokens after encoding, weakening guidance;\nand (ii) optimizing for appearance and motion in a single head can favor\ntexture over temporal consistency. We present STANCE, an image-to-video\nframework that addresses both issues with two simple components. First, we\nintroduce Instance Cues -- a pixel-aligned control signal that turns sparse,\nuser-editable hints into a dense 2.5D (camera-relative) motion field by\naveraging per-instance flow and augmenting with monocular depth over the\ninstance mask. This reduces depth ambiguity compared to 2D arrow inputs while\nremaining easy to use. Second, we preserve the salience of these cues in token\nspace with Dense RoPE, which tags a small set of motion tokens (anchored on the\nfirst frame) with spatial-addressable rotary embeddings. Paired with joint RGB\n\\(+\\) auxiliary-map prediction (segmentation or depth), our model anchors\nstructure while RGB handles appearance, stabilizing optimization and improving\ntemporal coherence without requiring per-frame trajectory scripts.",
        "url": "http://arxiv.org/abs/2510.14588v1",
        "published_date": "2025-10-16T11:50:38+00:00",
        "updated_date": "2025-10-16T11:50:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhifei Chen",
            "Tianshuo Xu",
            "Leyi Wu",
            "Luozhou Wang",
            "Dongyu Yan",
            "Zihan You",
            "Wenting Luo",
            "Guo Zhang",
            "Yingcong Chen"
        ],
        "tldr": "STANCE introduces Instance Cues and Dense RoPE to improve motion coherence in image-to-video generation by enhancing motion guidance and stabilizing optimization.",
        "tldr_zh": "STANCE提出了Instance Cues和Dense RoPE来改善图像到视频生成中的运动连贯性，通过增强运动引导和稳定优化。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Consistent text-to-image generation via scene de-contextualization",
        "summary": "Consistent text-to-image (T2I) generation seeks to produce\nidentity-preserving images of the same subject across diverse scenes, yet it\noften fails due to a phenomenon called identity (ID) shift. Previous methods\nhave tackled this issue, but typically rely on the unrealistic assumption of\nknowing all target scenes in advance. This paper reveals that a key source of\nID shift is the native correlation between subject and scene context, called\nscene contextualization, which arises naturally as T2I models fit the training\ndistribution of vast natural images. We formally prove the near-universality of\nthis scene-ID correlation and derive theoretical bounds on its strength. On\nthis basis, we propose a novel, efficient, training-free prompt embedding\nediting approach, called Scene De-Contextualization (SDeC), that imposes an\ninversion process of T2I's built-in scene contextualization. Specifically, it\nidentifies and suppresses the latent scene-ID correlation within the ID\nprompt's embedding by quantifying the SVD directional stability to adaptively\nre-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene\nuse (one scene per prompt) without requiring prior access to all target scenes.\nThis makes it a highly flexible and general solution well-suited to real-world\napplications where such prior knowledge is often unavailable or varies over\ntime. Experiments demonstrate that SDeC significantly enhances identity\npreservation while maintaining scene diversity.",
        "url": "http://arxiv.org/abs/2510.14553v1",
        "published_date": "2025-10-16T10:54:49+00:00",
        "updated_date": "2025-10-16T10:54:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Song Tang",
            "Peihao Gong",
            "Kunyu Li",
            "Kai Guo",
            "Boyu Wang",
            "Mao Ye",
            "Jianwei Zhang",
            "Xiatian Zhu"
        ],
        "tldr": "The paper introduces Scene De-Contextualization (SDeC), a training-free prompt editing approach to improve identity preservation in text-to-image generation by suppressing scene-ID correlation without requiring prior knowledge of target scenes.",
        "tldr_zh": "该论文介绍了一种名为场景去上下文(SDeC)的无需训练的 prompt 编辑方法，通过抑制场景-ID 相关性来提高文本到图像生成中的身份保留，而无需事先了解目标场景。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models",
        "summary": "In text-to-image generation, different initial noises induce distinct\ndenoising paths with a pretrained Stable Diffusion (SD) model. While this\npattern could output diverse images, some of them may fail to align well with\nthe prompt. Existing methods alleviate this issue either by altering the\ndenoising dynamics or by drawing multiple noises and conducting post-selection.\nIn this paper, we attribute the misalignment to a training-inference mismatch:\nduring training, prompt-conditioned noises lie in a prompt-specific subset of\nthe latent space, whereas at inference the noise is drawn from a\nprompt-agnostic Gaussian prior. To close this gap, we propose a noise projector\nthat applies text-conditioned refinement to the initial noise before denoising.\nConditioned on the prompt embedding, it maps the noise to a prompt-aware\ncounterpart that better matches the distribution observed during SD training,\nwithout modifying the SD model. Our framework consists of these steps: we first\nsample some noises and obtain token-level feedback for their corresponding\nimages from a vision-language model (VLM), then distill these signals into a\nreward model, and finally optimize the noise projector via a quasi-direct\npreference optimization. Our design has two benefits: (i) it requires no\nreference images or handcrafted priors, and (ii) it incurs small inference\ncost, replacing multi-sample selection with a single forward pass. Extensive\nexperiments further show that our prompt-aware noise projection improves\ntext-image alignment across diverse prompts.",
        "url": "http://arxiv.org/abs/2510.14526v1",
        "published_date": "2025-10-16T10:14:34+00:00",
        "updated_date": "2025-10-16T10:14:34+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yunze Tong",
            "Didi Zhu",
            "Zijing Hu",
            "Jinluan Yang",
            "Ziyu Zhao"
        ],
        "tldr": "This paper introduces a noise projection technique to improve text-to-image alignment in diffusion models by refining the initial noise based on the text prompt, addressing the training-inference mismatch between prompt-conditioned and prompt-agnostic noise distributions.",
        "tldr_zh": "本文提出了一种噪声投影技术，通过基于文本提示细化初始噪声，来改善扩散模型中的文本到图像对齐，解决了提示条件下的噪声分布和提示无关的噪声分布之间训练-推理不匹配的问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation",
        "summary": "Recent progress in text-to-image (T2I) generative models has led to\nsignificant improvements in generating high-quality images aligned with text\nprompts. However, these models still struggle with prompts involving multiple\nobjects, often resulting in object neglect or object mixing. Through extensive\nstudies, we identify four problematic scenarios, Similar Shapes, Similar\nTextures, Dissimilar Background Biases, and Many Objects, where inter-object\nrelationships frequently lead to such failures. Motivated by two key\nobservations about CLIP embeddings, we propose DOS (Directional Object\nSeparation), a method that modifies three types of CLIP text embeddings before\npassing them into text-to-image models. Experimental results show that DOS\nconsistently improves the success rate of multi-object image generation and\nreduces object mixing. In human evaluations, DOS significantly outperforms four\ncompeting methods, receiving 26.24%-43.04% more votes across four benchmarks.\nThese results highlight DOS as a practical and effective solution for improving\nmulti-object image generation.",
        "url": "http://arxiv.org/abs/2510.14376v1",
        "published_date": "2025-10-16T07:17:23+00:00",
        "updated_date": "2025-10-16T07:17:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dongnam Byun",
            "Jungwon Park",
            "Jumgmin Ko",
            "Changin Choi",
            "Wonjong Rhee"
        ],
        "tldr": "The paper introduces Directional Object Separation (DOS), a method to improve multi-object image generation in text-to-image models by modifying CLIP text embeddings to address object neglect and mixing issues. DOS outperforms existing methods in human evaluations.",
        "tldr_zh": "该论文介绍了方向对象分离（DOS），这是一种通过修改CLIP文本嵌入来改进文本到图像模型中的多对象图像生成的方法，以解决对象忽略和混合问题。在人工评估中，DOS优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning",
        "summary": "While advanced methods like VACE and Phantom have advanced video generation\nfor specific subjects in diverse scenarios, they struggle with multi-human\nidentity preservation in dynamic interactions, where consistent identities\nacross multiple characters are critical. To address this, we propose\nIdentity-GRPO, a human feedback-driven optimization pipeline for refining\nmulti-human identity-preserving video generation. First, we construct a video\nreward model trained on a large-scale preference dataset containing\nhuman-annotated and synthetic distortion data, with pairwise annotations\nfocused on maintaining human consistency throughout the video. We then employ a\nGRPO variant tailored for multi-human consistency, which greatly enhances both\nVACE and Phantom. Through extensive ablation studies, we evaluate the impact of\nannotation quality and design choices on policy optimization. Experiments show\nthat Identity-GRPO achieves up to 18.9% improvement in human consistency\nmetrics over baseline methods, offering actionable insights for aligning\nreinforcement learning with personalized video generation.",
        "url": "http://arxiv.org/abs/2510.14256v1",
        "published_date": "2025-10-16T03:13:56+00:00",
        "updated_date": "2025-10-16T03:13:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangyu Meng",
            "Zixian Zhang",
            "Zhenghao Zhang",
            "Junchao Liao",
            "Long Qin",
            "Weizhi Wang"
        ],
        "tldr": "The paper introduces Identity-GRPO, a reinforcement learning based pipeline for improving multi-human identity preservation in video generation, showing significant improvements over existing methods like VACE and Phantom.",
        "tldr_zh": "本文介绍了一种基于强化学习的流程Identity-GRPO，用于提高视频生成中多个人物身份的保持能力，并在VACE和Phantom等现有方法上显示出显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization",
        "summary": "Recent advances in image-to-video (I2V) generation have achieved remarkable\nprogress in synthesizing high-quality, temporally coherent videos from static\nimages. Among all the applications of I2V, human-centric video generation\nincludes a large portion. However, existing I2V models encounter difficulties\nin maintaining identity consistency between the input human image and the\ngenerated video, especially when the person in the video exhibits significant\nexpression changes and movements. This issue becomes critical when the human\nface occupies merely a small fraction of the image. Since humans are highly\nsensitive to identity variations, this poses a critical yet under-explored\nchallenge in I2V generation. In this paper, we propose Identity-Preserving\nReward-guided Optimization (IPRO), a novel video diffusion framework based on\nreinforcement learning to enhance identity preservation. Instead of introducing\nauxiliary modules or altering model architectures, our approach introduces a\ndirect and effective tuning algorithm that optimizes diffusion models using a\nface identity scorer. To improve performance and accelerate convergence, our\nmethod backpropagates the reward signal through the last steps of the sampling\nchain, enabling richer gradient feedback. We also propose a novel facial\nscoring mechanism that treats faces in ground-truth videos as facial feature\npools, providing multi-angle facial information to enhance generalization. A\nKL-divergence regularization is further incorporated to stabilize training and\nprevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V\nmodel and our in-house I2V model demonstrate the effectiveness of our method.\nOur project and code are available at\n\\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.",
        "url": "http://arxiv.org/abs/2510.14255v1",
        "published_date": "2025-10-16T03:13:47+00:00",
        "updated_date": "2025-10-16T03:13:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liao Shen",
            "Wentao Jiang",
            "Yiran Zhu",
            "Tiezheng Ge",
            "Zhiguo Cao",
            "Bo Zheng"
        ],
        "tldr": "The paper introduces IPRO, a reinforcement learning-based approach for improving identity preservation in image-to-video generation, using a reward-guided optimization strategy and a novel facial scoring mechanism.",
        "tldr_zh": "该论文介绍了IPRO，一种基于强化学习的方法，通过奖励引导的优化策略和新颖的面部评分机制，来提高图像到视频生成中的身份保持。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Terra: Explorable Native 3D World Model with Point Latents",
        "summary": "World models have garnered increasing attention for comprehensive modeling of\nthe real world. However, most existing methods still rely on pixel-aligned\nrepresentations as the basis for world evolution, neglecting the inherent 3D\nnature of the physical world. This could undermine the 3D consistency and\ndiminish the modeling efficiency of world models. In this paper, we present\nTerra, a native 3D world model that represents and generates explorable\nenvironments in an intrinsic 3D latent space. Specifically, we propose a novel\npoint-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into\na latent point representation, which is subsequently decoded as 3D Gaussian\nprimitives to jointly model geometry and appearance. We then introduce a sparse\npoint flow matching network (SPFlow) for generating the latent point\nrepresentation, which simultaneously denoises the positions and features of the\npoint latents. Our Terra enables exact multi-view consistency with native 3D\nrepresentation and architecture, and supports flexible rendering from any\nviewpoint with only a single generation process. Furthermore, Terra achieves\nexplorable world modeling through progressive generation in the point latent\nspace. We conduct extensive experiments on the challenging indoor scenes from\nScanNet v2. Terra achieves state-of-the-art performance in both reconstruction\nand generation with high 3D consistency.",
        "url": "http://arxiv.org/abs/2510.14977v1",
        "published_date": "2025-10-16T17:59:56+00:00",
        "updated_date": "2025-10-16T17:59:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yuanhui Huang",
            "Weiliang Chen",
            "Wenzhao Zheng",
            "Xin Tao",
            "Pengfei Wan",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "tldr": "The paper introduces Terra, a native 3D world model using a point-to-Gaussian VAE and sparse point flow matching for generating explorable 3D environments with 3D consistency and state-of-the-art performance on indoor scene reconstruction and generation.",
        "tldr_zh": "该论文介绍了一种名为Terra的原生3D世界模型，它使用点到高斯变分自编码器和稀疏点流匹配，用于生成具有3D一致性的可探索3D环境，并在室内场景重建和生成方面取得了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions",
        "summary": "Hand-object interaction (HOI) is fundamental for humans to express intent.\nExisting HOI generation research is predominantly confined to fixed grasping\npatterns, where control is tied to physical priors such as force closure or\ngeneric intent instructions, even when expressed through elaborate language.\nSuch an overly general conditioning imposes a strong inductive bias for stable\ngrasps, thus failing to capture the diversity of daily HOI. To address these\nlimitations, we introduce Free-Form HOI Generation, which aims to generate\ncontrollable, diverse, and physically plausible HOI conditioned on fine-grained\nintent, extending HOI from grasping to free-form interactions, like pushing,\npoking, and rotating. To support this task, we construct WildO2, an in-the-wild\ndiverse 3D HOI dataset, which includes diverse HOI derived from internet\nvideos. Specifically, it contains 4.4k unique interactions across 92 intents\nand 610 object categories, each with detailed semantic annotations. Building on\nthis dataset, we propose TOUCH, a three-stage framework centered on a\nmulti-level diffusion model that facilitates fine-grained semantic control to\ngenerate versatile hand poses beyond grasping priors. This process leverages\nexplicit contact modeling for conditioning and is subsequently refined with\ncontact consistency and physical constraints to ensure realism. Comprehensive\nexperiments demonstrate our method's ability to generate controllable, diverse,\nand physically plausible hand interactions representative of daily activities.\nThe project page is $\\href{https://guangyid.github.io/hoi123touch}{here}$.",
        "url": "http://arxiv.org/abs/2510.14874v1",
        "published_date": "2025-10-16T16:52:58+00:00",
        "updated_date": "2025-10-16T16:52:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guangyi Han",
            "Wei Zhai",
            "Yuhang Yang",
            "Yang Cao",
            "Zheng-Jun Zha"
        ],
        "tldr": "This paper introduces TOUCH, a framework for generating diverse and controllable hand-object interactions (HOI) beyond simple grasping, using a new dataset (WildO2) and a multi-level diffusion model. It enables generating more complex interactions such as pushing and poking.",
        "tldr_zh": "本文介绍了TOUCH，一个用于生成多样化和可控的手部-物体交互（HOI）的框架，超越了简单的抓取。该框架使用了一个新的数据集（WildO2）和一个多层次的扩散模型，能够生成更复杂的交互，例如推动和戳刺。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multi-modal video data-pipelines for machine learning with minimal human supervision",
        "summary": "The real-world is inherently multi-modal at its core. Our tools observe and\ntake snapshots of it, in digital form, such as videos or sounds, however much\nof it is lost. Similarly for actions and information passing between humans,\nlanguages are used as a written form of communication. Traditionally, Machine\nLearning models have been unimodal (i.e. rgb -> semantic or text ->\nsentiment_class). Recent trends go towards bi-modality, where images and text\nare learned together, however, in order to truly understand the world, we need\nto integrate all these independent modalities. In this work we try to combine\nas many visual modalities as we can using little to no human supervision. In\norder to do this, we use pre-trained experts and procedural combinations\nbetween them on top of raw videos using a fully autonomous data-pipeline, which\nwe also open-source. We then make use of PHG-MAE, a model specifically designed\nto leverage multi-modal data. We show that this model which was efficiently\ndistilled into a low-parameter (<1M) can have competitive results compared to\nmodels of ~300M parameters. We deploy this model and analyze the use-case of\nreal-time semantic segmentation from handheld devices or webcams on commodity\nhardware. Finally, we deploy other off-the-shelf models using the same\nframework, such as DPT for near real-time depth estimation.",
        "url": "http://arxiv.org/abs/2510.14862v1",
        "published_date": "2025-10-16T16:36:29+00:00",
        "updated_date": "2025-10-16T16:36:29+00:00",
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "authors": [
            "Mihai-Cristian Pîrvu",
            "Marius Leordeanu"
        ],
        "tldr": "This paper presents an autonomous data pipeline for combining multiple visual modalities from videos using pre-trained experts and a multi-modal model (PHG-MAE), achieving competitive semantic segmentation and depth estimation results on commodity hardware with a low-parameter model.",
        "tldr_zh": "该论文提出了一个自主数据管道，利用预训练专家和多模态模型（PHG-MAE）组合视频中的多种视觉模态，在商品硬件上以低参数模型实现了具有竞争力的语义分割和深度估计结果。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
        "summary": "Recent research on motion generation has shown significant progress in\ngenerating semantically aligned motion with singular semantics. However, when\nemploying these models to create composite sequences containing multiple\nsemantically generated motion clips, they often struggle to preserve the\ncontinuity of motion dynamics at the transition boundaries between clips,\nresulting in awkward transitions and abrupt artifacts. To address these\nchallenges, we present Compositional Phase Diffusion, which leverages the\nSemantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module\n(TPDM) to progressively incorporate semantic guidance and phase details from\nadjacent motion clips into the diffusion process. Specifically, SPDM and TPDM\noperate within the latent motion frequency domain established by the\npre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them\nto learn semantically important and transition-aware phase information from\nvariable-length motion clips during training. Experimental results demonstrate\nthe competitive performance of our proposed framework in generating\ncompositional motion sequences that align semantically with the input\nconditions, while preserving phase transitional continuity between preceding\nand succeeding motion clips. Additionally, motion inbetweening task is made\npossible by keeping the phase parameter of the input motion sequences fixed\nthroughout the diffusion process, showcasing the potential for extending the\nproposed framework to accommodate various application scenarios. Codes are\navailable at https://github.com/asdryau/TransPhase.",
        "url": "http://arxiv.org/abs/2510.14427v1",
        "published_date": "2025-10-16T08:28:46+00:00",
        "updated_date": "2025-10-16T08:28:46+00:00",
        "categories": [
            "cs.MM",
            "cs.CV"
        ],
        "authors": [
            "Ho Yin Au",
            "Jie Chen",
            "Junkun Jiang",
            "Jingyu Xiang"
        ],
        "tldr": "The paper introduces Compositional Phase Diffusion, a method using Semantic and Transitional Phase Diffusion Modules within a latent motion frequency domain to generate continuous composite motion sequences and enable motion inbetweening.",
        "tldr_zh": "该论文介绍了组合相位扩散 (Compositional Phase Diffusion)，一种利用潜在运动频率域中的语义和过渡相位扩散模块生成连续复合运动序列并实现运动插值的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]