[
    {
        "title": "Distribution Matching Variational AutoEncoder",
        "summary": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \\textbf{Distribution-Matching VAE} (\\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.",
        "url": "http://arxiv.org/abs/2512.07778v1",
        "published_date": "2025-12-08T17:59:47+00:00",
        "updated_date": "2025-12-08T17:59:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sen Ye",
            "Jianning Pei",
            "Mengde Xu",
            "Shuyang Gu",
            "Chunyu Wang",
            "Liwei Wang",
            "Han Hu"
        ],
        "tldr": "The paper introduces Distribution-Matching VAE (DMVAE), which explicitly aligns the encoder's latent distribution with arbitrary reference distributions, showing that SSL-derived distributions are beneficial for image generation through VAEs, achieving a gFID of 3.2 on ImageNet with 64 epochs.",
        "tldr_zh": "该论文介绍了一种分布匹配变分自编码器 (DMVAE)，它将编码器的潜在分布与任意参考分布明确对齐，结果表明，SSL 衍生的分布有利于通过 VAE 进行图像生成，仅使用 64 个 epoch 在 ImageNet 上实现了 gFID 3.2。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation",
        "summary": "Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.",
        "url": "http://arxiv.org/abs/2512.07747v1",
        "published_date": "2025-12-08T17:34:15+00:00",
        "updated_date": "2025-12-08T17:34:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shihao Zhao",
            "Yitong Chen",
            "Zeyinzi Jiang",
            "Bojia Zi",
            "Shaozhe Hao",
            "Yu Liu",
            "Chaojie Mao",
            "Kwan-Yee K. Wong"
        ],
        "tldr": "The paper introduces Unison, a low-cost, two-stage framework for unified multimodal understanding and generation that automatically parses user intentions and meta-information for various tasks, achieving strong performance with limited resources.",
        "tldr_zh": "该论文介绍了一种名为Unison的低成本两阶段框架，用于统一多模态理解和生成。该框架能够自动解析用户意图和元信息，适用于各种任务，并在有限的资源下实现了强大的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LongCat-Image Technical Report",
        "summary": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.",
        "url": "http://arxiv.org/abs/2512.07584v1",
        "published_date": "2025-12-08T14:26:40+00:00",
        "updated_date": "2025-12-08T14:26:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Meituan LongCat Team",
            "Hanghang Ma",
            "Haoxian Tan",
            "Jiale Huang",
            "Junqiang Wu",
            "Jun-Yan He",
            "Lishuai Gao",
            "Songlin Xiao",
            "Xiaoming Wei",
            "Xiaoqi Ma",
            "Xunliang Cai",
            "Yayong Guan",
            "Jie Hu"
        ],
        "tldr": "LongCat-Image is a new open-source bilingual image generation model with strong Chinese character rendering, photorealism, and deployment efficiency, claiming SOTA results and providing a comprehensive open-source ecosystem.",
        "tldr_zh": "LongCat-Image是一个新的开源双语图像生成模型，具有强大的中文文字渲染能力、逼真的写实效果和高效的部署性能。它声称达到了SOTA水平，并提供了一个全面的开源生态系统。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer",
        "summary": "Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.",
        "url": "http://arxiv.org/abs/2512.07500v1",
        "published_date": "2025-12-08T12:34:03+00:00",
        "updated_date": "2025-12-08T12:34:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Penghui Liu",
            "Jiangshan Wang",
            "Yutong Shen",
            "Shanhui Mo",
            "Chenyang Qi",
            "Yue Ma"
        ],
        "tldr": "The paper introduces MultiMotion, a framework addressing multi-object video motion transfer in Diffusion Transformers (DiT) using Mask-aware Attention Motion Flow (AMF) and a Rectified Predictor-Corrector (RectPC) solver, along with a new benchmark dataset.",
        "tldr_zh": "本文介绍了 MultiMotion，一个使用 Mask-aware Attention Motion Flow (AMF) 和 Rectified Predictor-Corrector (RectPC) 求解器来解决 Diffusion Transformer (DiT) 中多对象视频运动迁移问题的框架，并提出了一个新的基准数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Video Editing with Temporal Reasoner",
        "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.",
        "url": "http://arxiv.org/abs/2512.07469v1",
        "published_date": "2025-12-08T11:50:18+00:00",
        "updated_date": "2025-12-08T11:50:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangpeng Yang",
            "Ji Xie",
            "Yiyuan Yang",
            "Yan Huang",
            "Min Xu",
            "Qiang Wu"
        ],
        "tldr": "The paper introduces VideoCoF, a novel approach for unified video editing that uses a 'see, reason, then edit' procedure with reasoning tokens for precise instruction-to-region alignment and fine-grained editing without masks.",
        "tldr_zh": "该论文介绍了一种名为 VideoCoF 的新型统一视频编辑方法，它使用“看、推理、然后编辑”的流程，通过推理 tokens 实现精确的指令到区域对齐和精细的编辑，无需使用 masks。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting",
        "summary": "Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.",
        "url": "http://arxiv.org/abs/2512.07345v1",
        "published_date": "2025-12-08T09:33:39+00:00",
        "updated_date": "2025-12-08T09:33:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shilong Jin",
            "Haoran Duan",
            "Litao Hua",
            "Wentao Huang",
            "Yuan Zhou"
        ],
        "tldr": "The paper introduces TD-Attn, a novel framework to debias T2I diffusion models, enhancing multi-view consistency in 3D tasks by addressing prior view biases using 3D-aware attention and hierarchical attention modulation.",
        "tldr_zh": "该论文提出了TD-Attn，一种新的框架，通过使用3D感知注意力和分层注意力调制来解决先验视图偏差，从而消除T2I扩散模型的偏差，增强3D任务中的多视图一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation",
        "summary": "Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \\textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \\href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.",
        "url": "http://arxiv.org/abs/2512.07328v1",
        "published_date": "2025-12-08T09:12:18+00:00",
        "updated_date": "2025-12-08T09:12:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ziyang Mai",
            "Yu-Wing Tai"
        ],
        "tldr": "The paper introduces ContextAnyone, a context-aware diffusion framework for generating character-consistent videos from text and a single reference image, addressing the challenge of maintaining consistent character identities across scenes in text-to-video generation.",
        "tldr_zh": "该论文介绍了 ContextAnyone，一个上下文感知的扩散框架，用于从文本和单个参考图像生成角色一致的视频，解决了文本到视频生成中保持跨场景角色身份一致性的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Camera Positional Encoding for Controlled Video Generation",
        "summary": "Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.",
        "url": "http://arxiv.org/abs/2512.07237v1",
        "published_date": "2025-12-08T07:34:01+00:00",
        "updated_date": "2025-12-08T07:34:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cheng Zhang",
            "Boying Li",
            "Meng Wei",
            "Yan-Pei Cao",
            "Camilo Cruz Gambardella",
            "Dinh Phung",
            "Jianfei Cai"
        ],
        "tldr": "The paper introduces a Unified Camera Positional Encoding (UCPE) method for camera-controlled video generation in Transformers, enabling better control over camera parameters like pose, intrinsics, and lens distortion, evaluated on a new large video dataset.",
        "tldr_zh": "该论文介绍了一种统一的相机位置编码（UCPE）方法，用于Transformer中的相机控制视频生成，能够更好地控制相机参数，如姿势、内参和镜头畸变，并在一个新的大型视频数据集上进行了评估。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Understanding Diffusion Models via Code Execution",
        "summary": "Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.",
        "url": "http://arxiv.org/abs/2512.07201v1",
        "published_date": "2025-12-08T06:25:07+00:00",
        "updated_date": "2025-12-08T06:25:07+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Cheng Yu"
        ],
        "tldr": "This paper presents a concise, 300-line implementation of diffusion models to bridge the gap between theory and practice, offering an implementation-first understanding for researchers. It aims to provide a clear understanding of how diffusion models operate in code.",
        "tldr_zh": "该论文提出了一个简洁的、300行的扩散模型实现，旨在弥合理论和实践之间的差距，为研究人员提供一个以实现为先的理解。它旨在清晰地展示扩散模型如何在代码中运行。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Generating Storytelling Images with Rich Chains-of-Reasoning",
        "summary": "An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.",
        "url": "http://arxiv.org/abs/2512.07198v1",
        "published_date": "2025-12-08T06:18:44+00:00",
        "updated_date": "2025-12-08T06:18:44+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Xiujie Song",
            "Qi Jia",
            "Shota Watanabe",
            "Xiaoyi Pang",
            "Ruijie Chen",
            "Mengyue Wu",
            "Kenny Q. Zhu"
        ],
        "tldr": "This paper introduces the task of Storytelling Image Generation, proposing a two-stage pipeline (StorytellingPainter) using LLMs and T2I models, along with a dedicated evaluation framework and Mini-Storyteller models to bridge the performance gap between open-source and proprietary LLMs.",
        "tldr_zh": "该论文介绍了故事图像生成任务，提出了一个两阶段的流水线（StorytellingPainter），结合了LLMs和T2I模型。同时，为了缩小开源LLMs和专有LLMs之间的性能差距，论文还设计了一个专门的评估框架和Mini-Storyteller模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Human Geometry Distribution for 3D Animation Generation",
        "summary": "Generating realistic human geometry animations remains a challenging task, as it requires modeling natural clothing dynamics with fine-grained geometric details under limited data. To address these challenges, we propose two novel designs. First, we propose a compact distribution-based latent representation that enables efficient and high-quality geometry generation. We improve upon previous work by establishing a more uniform mapping between SMPL and avatar geometries. Second, we introduce a generative animation model that fully exploits the diversity of limited motion data. We focus on short-term transitions while maintaining long-term consistency through an identity-conditioned design. These two designs formulate our method as a two-stage framework: the first stage learns a latent space, while the second learns to generate animations within this latent space. We conducted experiments on both our latent space and animation model. We demonstrate that our latent space produces high-fidelity human geometry surpassing previous methods ($90\\%$ lower Chamfer Dist.). The animation model synthesizes diverse animations with detailed and natural dynamics ($2.2 \\times$ higher user study score), achieving the best results across all evaluation metrics.",
        "url": "http://arxiv.org/abs/2512.07459v1",
        "published_date": "2025-12-08T11:35:16+00:00",
        "updated_date": "2025-12-08T11:35:16+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Xiangjun Tang",
            "Biao Zhang",
            "Peter Wonka"
        ],
        "tldr": "This paper introduces a two-stage framework for generating realistic 3D human geometry animations by using a compact, distribution-based latent representation and a generative animation model that prioritizes short-term transitions and long-term consistency within limited motion data.",
        "tldr_zh": "本文介绍了一个两阶段框架，它通过使用紧凑的、基于分布的潜在表示和生成式动画模型，来生成逼真的3D人体几何动画。该模型优先考虑短期过渡和长期的在有限运动数据下的一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation",
        "summary": "Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA",
        "url": "http://arxiv.org/abs/2512.07628v1",
        "published_date": "2025-12-08T15:17:01+00:00",
        "updated_date": "2025-12-08T15:17:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiqi Li",
            "Wenhuan Li",
            "Tengfei Wang",
            "Zhenwei Wang",
            "Junta Wu",
            "Haoyuan Wang",
            "Yunhan Yang",
            "Zehuan Huang",
            "Yang Li",
            "Peidong Liu",
            "Chunchao Guo"
        ],
        "tldr": "The paper introduces MoCA, a compositional 3D generative model that uses importance-based component routing and compression to enable scalable 3D asset generation with a high number of components. It outperforms existing methods in compositional object and scene generation.",
        "tldr_zh": "该论文介绍了MoCA，一种组合式3D生成模型，它使用基于重要性的组件路由和压缩，从而能够生成具有大量组件的可扩展3D资产。它在组合对象和场景生成方面优于现有方法。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]