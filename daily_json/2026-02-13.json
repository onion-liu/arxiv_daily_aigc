[
    {
        "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
        "summary": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.",
        "url": "http://arxiv.org/abs/2602.12205v1",
        "published_date": "2026-02-12T17:44:24+00:00",
        "updated_date": "2026-02-12T17:44:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dianyi Wang",
            "Ruihang Li",
            "Feng Han",
            "Chaofan Ma",
            "Wei Song",
            "Siyuan Wang",
            "Yibin Wang",
            "Yi Xin",
            "Hongjian Liu",
            "Zhixiong Zhang",
            "Shengyuan Ding",
            "Tianhang Wang",
            "Zhenglin Cheng",
            "Tao Lin",
            "Cheng Jin",
            "Kaicheng Yu",
            "Jingjing Chen",
            "Wenjie Wang",
            "Zhongyu Wei",
            "Jiaqi Wang"
        ],
        "tldr": "DeepGen 1.0 is a lightweight (5B) unified multimodal model for image generation and editing that achieves state-of-the-art performance with a novel training strategy and architecture (SCB) despite using fewer parameters and training data than competing models. The model, training code, weights, and datasets are open-sourced.",
        "tldr_zh": "DeepGen 1.0是一个轻量级（5B）统一多模态模型，用于图像生成和编辑，通过一种新颖的训练策略和架构（SCB），在参数和训练数据少于竞争模型的情况下，实现了最先进的性能。该模型、训练代码、权重和数据集已开源。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation",
        "summary": "Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.",
        "url": "http://arxiv.org/abs/2602.12160v1",
        "published_date": "2026-02-12T16:41:52+00:00",
        "updated_date": "2026-02-12T16:41:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xu Guo",
            "Fulong Ye",
            "Qichao Sun",
            "Liyang Chen",
            "Bingchuan Li",
            "Pengze Zhang",
            "Jiawei Liu",
            "Songtao Zhao",
            "Qian He",
            "Xiangwang Hou"
        ],
        "tldr": "DreamID-Omni is a unified framework for controllable human-centric audio-video generation, addressing challenges in identity and timbre control by using a Symmetric Conditional Diffusion Transformer and a Dual-Level Disentanglement strategy.",
        "tldr_zh": "DreamID-Omni是一个用于可控的以人为中心的音频-视频生成的统一框架，通过使用对称条件扩散转换器和双层解耦策略，解决了身份和音色控制方面的挑战。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "LUVE : Latent-Cascaded Ultra-High-Resolution Video Generation with Dual Frequency Experts",
        "summary": "Recent advances in video diffusion models have significantly improved visual quality, yet ultra-high-resolution (UHR) video generation remains a formidable challenge due to the compounded difficulties of motion modeling, semantic planning, and detail synthesis. To address these limitations, we propose \\textbf{LUVE}, a \\textbf{L}atent-cascaded \\textbf{U}HR \\textbf{V}ideo generation framework built upon dual frequency \\textbf{E}xperts. LUVE employs a three-stage architecture comprising low-resolution motion generation for motion-consistent latent synthesis, video latent upsampling that performs resolution upsampling directly in the latent space to mitigate memory and computational overhead, and high-resolution content refinement that integrates low-frequency and high-frequency experts to jointly enhance semantic coherence and fine-grained detail generation. Extensive experiments demonstrate that our LUVE achieves superior photorealism and content fidelity in UHR video generation, and comprehensive ablation studies further validate the effectiveness of each component. The project is available at \\href{https://unicornanrocinu.github.io/LUVE_web/}{https://github.io/LUVE/}.",
        "url": "http://arxiv.org/abs/2602.11564v1",
        "published_date": "2026-02-12T04:35:16+00:00",
        "updated_date": "2026-02-12T04:35:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Zhao",
            "Jiawei Chen",
            "Hongyu Li",
            "Zhuoliang Kang",
            "Shilin Lu",
            "Xiaoming Wei",
            "Kai Zhang",
            "Jian Yang",
            "Ying Tai"
        ],
        "tldr": "The paper introduces LUVE, a latent-cascaded framework using dual frequency experts for ultra-high-resolution video generation, achieving superior photorealism and content fidelity.",
        "tldr_zh": "该论文介绍了LUVE，一个使用双频专家进行超高分辨率视频生成的潜在级联框架，实现了卓越的照片真实感和内容保真度。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "GENIUS: Generative Fluid Intelligence Evaluation Suite",
        "summary": "Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess $\\textit{Crystallized Intelligence}$, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks $\\textit{Generative Fluid Intelligence (GFI)}$: the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce $\\textbf{GENIUS}$ ($\\textbf{GEN}$ Fluid $\\textbf{I}$ntelligence Eval$\\textbf{U}$ation $\\textbf{S}$uite). We formalize $\\textit{GFI}$ as a synthesis of three primitives. These include $\\textit{Inducing Implicit Patterns}$ (e.g., inferring personalized visual preferences), $\\textit{Executing Ad-hoc Constraints}$ (e.g., visualizing abstract metaphors), and $\\textit{Adapting to Contextual Knowledge}$ (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, $\\textbf{GENIUS}$ establishes a rigorous standard for $\\textit{GFI}$, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: $\\href{https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$.",
        "url": "http://arxiv.org/abs/2602.11144v1",
        "published_date": "2026-02-11T18:55:54+00:00",
        "updated_date": "2026-02-11T18:55:54+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ruichuan An",
            "Sihan Yang",
            "Ziyu Guo",
            "Wei Dai",
            "Zijun Shen",
            "Haodong Li",
            "Renrui Zhang",
            "Xinyu Wei",
            "Guopeng Li",
            "Wenshan Wu",
            "Wentao Zhang"
        ],
        "tldr": "The paper introduces GENIUS, a new benchmark for evaluating Generative Fluid Intelligence (GFI) in Unified Multimodal Models, and identifies limitations in current models regarding context comprehension, proposing an attention intervention strategy.",
        "tldr_zh": "该论文介绍了GENIUS，一个新的用于评估统一多模态模型中生成式流体智能（GFI）的基准，并指出当前模型在上下文理解方面的局限性，提出了一种注意力干预策略。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching",
        "summary": "We propose UniDFlow, a unified discrete flow-matching framework for multimodal understanding, generation, and editing. It decouples understanding and generation via task-specific low-rank adapters, avoiding objective interference and representation entanglement, while a novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlpw achieves SOTA performance across eight benchmarks and exhibits strong zero-shot generalization to tasks including inpainting, in-context image generation, reference-based editing, and compositional generation, despite no explicit task-specific training.",
        "url": "http://arxiv.org/abs/2602.12221v1",
        "published_date": "2026-02-12T17:59:08+00:00",
        "updated_date": "2026-02-12T17:59:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Onkar Susladkar",
            "Tushar Prakash",
            "Gayatri Deshmukh",
            "Kiet A. Nguyen",
            "Jiaxun Zhang",
            "Adheesh Juvekar",
            "Tianshu Bao",
            "Lin Chai",
            "Sparsh Mittal",
            "Inderjit S Dhillon",
            "Ismini Lourentzou"
        ],
        "tldr": "UniDFlow is a new discrete flow-matching framework for multimodal tasks like generation and editing, achieving state-of-the-art results across multiple benchmarks and demonstrating strong zero-shot generalization.",
        "tldr_zh": "UniDFlow 是一种新的离散流匹配框架，用于多模态任务（如生成和编辑），在多个基准测试中实现了最先进的结果，并展示了强大的零样本泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FAIL: Flow Matching Adversarial Imitation Learning for Image Generation",
        "summary": "Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial training without explicit rewards or pairwise comparisons. We derive two algorithms: FAIL-PD exploits differentiable ODE solvers for low-variance pathwise gradients, while FAIL-PG provides a black-box alternative for discrete or computationally constrained settings. Fine-tuning FLUX with only 13,000 demonstrations from Nano Banana pro, FAIL achieves competitive performance on prompt following and aesthetic benchmarks. Furthermore, the framework generalizes effectively to discrete image and video generation, and functions as a robust regularizer to mitigate reward hacking in reward-based optimization. Code and data are available at https://github.com/HansPolo113/FAIL.",
        "url": "http://arxiv.org/abs/2602.12155v1",
        "published_date": "2026-02-12T16:36:33+00:00",
        "updated_date": "2026-02-12T16:36:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yeyao Ma",
            "Chen Li",
            "Xiaosong Zhang",
            "Han Hu",
            "Weidi Xie"
        ],
        "tldr": "The paper introduces Flow Matching Adversarial Imitation Learning (FAIL), a novel adversarial imitation learning approach for fine-tuning flow matching models in image and video generation, demonstrating competitive performance without explicit rewards or pairwise comparisons.",
        "tldr_zh": "该论文介绍了一种新的对抗模仿学习方法，即流匹配对抗模仿学习 (FAIL)，用于微调图像和视频生成中的流匹配模型，无需显式奖励或成对比较即可实现有竞争力的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PosterOmni: Generalized Artistic Poster Creation via Task Distillation and Unified Reward Feedback",
        "summary": "Image-to-poster generation is a high-demand task requiring not only local adjustments but also high-level design understanding. Models must generate text, layout, style, and visual elements while preserving semantic fidelity and aesthetic coherence. The process spans two regimes: local editing, where ID-driven generation, rescaling, filling, and extending must preserve concrete visual entities; and global creation, where layout- and style-driven tasks rely on understanding abstract design concepts. These intertwined demands make image-to-poster a multi-dimensional process coupling entity-preserving editing with concept-driven creation under image-prompt control. To address these challenges, we propose PosterOmni, a generalized artistic poster creation framework that unlocks the potential of a base edit model for multi-task image-to-poster generation. PosterOmni integrates the two regimes, namely local editing and global creation, within a single system through an efficient data-distillation-reward pipeline: (i) constructing multi-scenario image-to-poster datasets covering six task types across entity-based and concept-based creation; (ii) distilling knowledge between local and global experts for supervised fine-tuning; and (iii) applying unified PosterOmni Reward Feedback to jointly align visual entity-preserving and aesthetic preference across all tasks. Additionally, we establish PosterOmni-Bench, a unified benchmark for evaluating both local editing and global creation. Extensive experiments show that PosterOmni significantly enhances reference adherence, global composition quality, and aesthetic harmony, outperforming all open-source baselines and even surpassing several proprietary systems.",
        "url": "http://arxiv.org/abs/2602.12127v1",
        "published_date": "2026-02-12T16:16:38+00:00",
        "updated_date": "2026-02-12T16:16:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sixiang Chen",
            "Jianyu Lai",
            "Jialin Gao",
            "Hengyu Shi",
            "Zhongying Liu",
            "Tian Ye",
            "Junfeng Luo",
            "Xiaoming Wei",
            "Lei Zhu"
        ],
        "tldr": "The paper introduces PosterOmni, a framework for generalized artistic poster creation that combines local image editing with global design creation through a data-distillation-reward pipeline, achieving state-of-the-art results.",
        "tldr_zh": "该论文提出了 PosterOmni，一个用于广义艺术海报创作的框架，通过数据蒸馏奖励管道结合了局部图像编辑和全局设计创作，取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation",
        "summary": "While diffusion models have shown exceptional capabilities in aesthetic image synthesis, they often struggle with complex spatial understanding and reasoning. Existing approaches resort to Multimodal Large Language Models (MLLMs) to enhance this capability. However, they either incur high computational costs through joint training or suffer from spatial information loss when relying solely on textual prompts. To alleviate these limitations, we propose a Spatial Chain-of-Thought (SCoT) framework, a plug-and-play approach that effectively bridges the reasoning capabilities of MLLMs with the generative power of diffusion models. Specifically, we first enhance the diffusion model's layout awareness by training it on an interleaved text-coordinate instruction format. We then leverage state-of-the-art MLLMs as planners to generate comprehensive layout plans, transferring their spatial planning capabilities directly to the generation process. Extensive experiments demonstrate that our method achieves state-of-the-art performance on image generation benchmarks and significantly outperforms baselines on complex reasoning tasks, while also showing strong efficacy in image editing scenarios.",
        "url": "http://arxiv.org/abs/2602.11980v1",
        "published_date": "2026-02-12T14:12:14+00:00",
        "updated_date": "2026-02-12T14:12:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wei Chen",
            "Yancheng Long",
            "Mingqiao Liu",
            "Haojie Ding",
            "Yankai Yang",
            "Hongyang Wei",
            "Yi-Fan Zhang",
            "Bin Wen",
            "Fan Yang",
            "Tingting Gao",
            "Han Li",
            "Long Chen"
        ],
        "tldr": "This paper introduces Spatial Chain-of-Thought (SCoT), a plug-and-play framework that combines MLLM planning with diffusion model generation to improve spatial reasoning in image generation and editing, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了空间思维链（SCoT），这是一种即插即用的框架，它将MLLM规划与扩散模型生成相结合，以提高图像生成和编辑中的空间推理能力，达到了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiffPlace: Street View Generation via Place-Controllable Diffusion Model Enhancing Place Recognition",
        "summary": "Generative models have advanced significantly in realistic image synthesis, with diffusion models excelling in quality and stability. Recent multi-view diffusion models improve 3D-aware street view generation, but they struggle to produce place-aware and background-consistent urban scenes from text, BEV maps, and object bounding boxes. This limits their effectiveness in generating realistic samples for place recognition tasks. To address these challenges, we propose DiffPlace, a novel framework that introduces a place-ID controller to enable place-controllable multi-view image generation. The place-ID controller employs linear projection, perceiver transformer, and contrastive learning to map place-ID embeddings into a fixed CLIP space, allowing the model to synthesize images with consistent background buildings while flexibly modifying foreground objects and weather conditions. Extensive experiments, including quantitative comparisons and augmented training evaluations, demonstrate that DiffPlace outperforms existing methods in both generation quality and training support for visual place recognition. Our results highlight the potential of generative models in enhancing scene-level and place-aware synthesis, providing a valuable approach for improving place recognition in autonomous driving",
        "url": "http://arxiv.org/abs/2602.11875v1",
        "published_date": "2026-02-12T12:26:09+00:00",
        "updated_date": "2026-02-12T12:26:09+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Ji Li",
            "Zhiwei Li",
            "Shihao Li",
            "Zhenjiang Yu",
            "Boyang Wang",
            "Haiou Liu"
        ],
        "tldr": "DiffPlace introduces a place-ID controller for multi-view diffusion models, enabling place-controllable street view generation with consistent backgrounds, improving performance in place recognition tasks.",
        "tldr_zh": "DiffPlace 引入了一种地点ID控制器用于多视角扩散模型，实现了地点可控的街景生成，能够保持背景一致性，并提高了地点识别任务的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Semantically Conditioned Diffusion Models for Cerebral DSA Synthesis",
        "summary": "Digital subtraction angiography (DSA) plays a central role in the diagnosis and treatment of cerebrovascular disease, yet its invasive nature and high acquisition cost severely limit large-scale data collection and public data sharing. Therefore, we developed a semantically conditioned latent diffusion model (LDM) that synthesizes arterial-phase cerebral DSA frames under explicit control of anatomical circulation (anterior vs.\\ posterior) and canonical C-arm positions. We curated a large single-centre DSA dataset of 99,349 frames and trained a conditional LDM using text embeddings that encoded anatomy and acquisition geometry. To assess clinical realism, four medical experts, including two neuroradiologists, one neurosurgeon, and one internal medicine expert, systematically rated 400 synthetic DSA images using a 5-grade Likert scale for evaluating proximal large, medium, and small peripheral vessels. The generated images achieved image-wise overall Likert scores ranging from 3.1 to 3.3, with high inter-rater reliability (ICC(2,k) = 0.80--0.87). Distributional similarity to real DSA frames was supported by a low median Fréchet inception distance (FID) of 15.27. Our results indicate that semantically controlled LDMs can produce realistic synthetic DSAs suitable for downstream algorithm development, research, and training.",
        "url": "http://arxiv.org/abs/2602.11703v1",
        "published_date": "2026-02-12T08:31:00+00:00",
        "updated_date": "2026-02-12T08:31:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qiwen Xu",
            "David Rügamer",
            "Holger Wenz",
            "Johann Fontana",
            "Nora Meggyeshazi",
            "Andreas Bender",
            "Máté E. Maros"
        ],
        "tldr": "This paper introduces a semantically conditioned latent diffusion model for synthesizing realistic cerebral DSA images, using text embeddings to control anatomy and acquisition geometry, and achieving high realism scores from medical experts.",
        "tldr_zh": "本文介绍了一种语义条件潜在扩散模型，用于合成逼真的脑血管DSA图像。该模型使用文本嵌入来控制解剖结构和采集几何，并获得了医学专家的高度逼真度评分。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EmoSpace: Fine-Grained Emotion Prototype Learning for Immersive Affective Content Generation",
        "summary": "Emotion is important for creating compelling virtual reality (VR) content. Although some generative methods have been applied to lower the barrier to creating emotionally rich content, they fail to capture the nuanced emotional semantics and the fine-grained control essential for immersive experiences. To address these limitations, we introduce EmoSpace, a novel framework for emotion-aware content generation that learns dynamic, interpretable emotion prototypes through vision-language alignment. We employ a hierarchical emotion representation with rich learnable prototypes that evolve during training, enabling fine-grained emotional control without requiring explicit emotion labels. We develop a controllable generation pipeline featuring multi-prototype guidance, temporal blending, and attention reweighting that supports diverse applications, including emotional image outpainting, stylized generation, and emotional panorama generation for VR environments. Our experiments demonstrate the superior performance of EmoSpace over existing methods in both qualitative and quantitative evaluations. Additionally, we present a comprehensive user study investigating how VR environments affect emotional perception compared to desktop settings. Our work facilitates immersive visual content generation with fine-grained emotion control and supports applications like therapy, education, storytelling, artistic creation, and cultural preservation. Code and models will be made publicly available.",
        "url": "http://arxiv.org/abs/2602.11658v1",
        "published_date": "2026-02-12T07:23:41+00:00",
        "updated_date": "2026-02-12T07:23:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bingyuan Wang",
            "Xingbei Chen",
            "Zongyang Qiu",
            "Linping Yuan",
            "Zeyu Wang"
        ],
        "tldr": "EmoSpace introduces a framework for emotion-aware content generation, learning emotion prototypes through vision-language alignment, offering fine-grained emotion control for applications like VR content creation.",
        "tldr_zh": "EmoSpace 提出了一种情感感知内容生成框架，通过视觉-语言对齐学习情感原型，为VR内容创建等应用提供细粒度的情感控制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Latent Forcing: Reordering the Diffusion Trajectory for Pixel-Space Image Generation",
        "summary": "Latent diffusion models excel at generating high-quality images but lose the benefits of end-to-end modeling. They discard information during image encoding, require a separately trained decoder, and model an auxiliary distribution to the raw data. In this paper, we propose Latent Forcing, a simple modification to existing architectures that achieves the efficiency of latent diffusion while operating on raw natural images. Our approach orders the denoising trajectory by jointly processing latents and pixels with separately tuned noise schedules. This allows the latents to act as a scratchpad for intermediate computation before high-frequency pixel features are generated. We find that the order of conditioning signals is critical, and we analyze this to explain differences between REPA distillation in the tokenizer and the diffusion model, conditional versus unconditional generation, and how tokenizer reconstruction quality relates to diffusability. Applied to ImageNet, Latent Forcing achieves a new state-of-the-art for diffusion transformer-based pixel generation at our compute scale.",
        "url": "http://arxiv.org/abs/2602.11401v1",
        "published_date": "2026-02-11T22:09:58+00:00",
        "updated_date": "2026-02-11T22:09:58+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Alan Baade",
            "Eric Ryan Chan",
            "Kyle Sargent",
            "Changan Chen",
            "Justin Johnson",
            "Ehsan Adeli",
            "Li Fei-Fei"
        ],
        "tldr": "Latent Forcing introduces a novel approach to pixel-space image generation within diffusion models by reordering the denoising trajectory, achieving state-of-the-art results on ImageNet with improved efficiency.",
        "tldr_zh": "Latent Forcing 提出了一种新的扩散模型像素空间图像生成方法，通过重新排序去噪轨迹，在 ImageNet 上实现了最先进的结果，并提高了效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FastFlow: Accelerating The Generative Flow Matching Models with Bandit Inference",
        "summary": "Flow-matching models deliver state-of-the-art fidelity in image and video generation, but the inherent sequential denoising process renders them slower. Existing acceleration methods like distillation, trajectory truncation, and consistency approaches are static, require retraining, and often fail to generalize across tasks. We propose FastFlow, a plug-and-play adaptive inference framework that accelerates generation in flow matching models. FastFlow identifies denoising steps that produce only minor adjustments to the denoising path and approximates them without using the full neural network models used for velocity predictions. The approximation utilizes finite-difference velocity estimates from prior predictions to efficiently extrapolate future states, enabling faster advancements along the denoising path at zero compute cost. This enables skipping computation at intermediary steps. We model the decision of how many steps to safely skip before requiring a full model computation as a multi-armed bandit problem. The bandit learns the optimal skips to balance speed with performance. FastFlow integrates seamlessly with existing pipelines and generalizes across image generation, video generation, and editing tasks. Experiments demonstrate a speedup of over 2.6x while maintaining high-quality outputs. The source code for this work can be found at https://github.com/Div290/FastFlow.",
        "url": "http://arxiv.org/abs/2602.11105v1",
        "published_date": "2026-02-11T18:21:11+00:00",
        "updated_date": "2026-02-11T18:21:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Divya Jyoti Bajpai",
            "Dhruv Bhardwaj",
            "Soumya Roy",
            "Tejas Duseja",
            "Harsh Agarwal",
            "Aashay Sandansing",
            "Manjesh Kumar Hanawal"
        ],
        "tldr": "FastFlow is a plug-and-play framework for accelerating flow-matching generative models by adaptively skipping denoising steps based on a multi-armed bandit approach, achieving significant speedups while maintaining output quality.",
        "tldr_zh": "FastFlow是一个即插即用的框架，通过基于多臂老虎机算法自适应地跳过去噪步骤，加速flow-matching生成模型，在保持输出质量的同时实现了显著的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EO-VAE: Towards A Multi-sensor Tokenizer for Earth Observation Data",
        "summary": "State-of-the-art generative image and video models rely heavily on tokenizers that compress high-dimensional inputs into more efficient latent representations. While this paradigm has revolutionized RGB generation, Earth observation (EO) data presents unique challenges due to diverse sensor specifications and variable spectral channels. We propose EO-VAE, a multi-sensor variational autoencoder designed to serve as a foundational tokenizer for the EO domain. Unlike prior approaches that train separate tokenizers for each modality, EO-VAE utilizes a single model to encode and reconstruct flexible channel combinations via dynamic hypernetworks. Our experiments on the TerraMesh dataset demonstrate that EO-VAE achieves superior reconstruction fidelity compared to the TerraMind tokenizers, establishing a robust baseline for latent generative modeling in remote sensing.",
        "url": "http://arxiv.org/abs/2602.12177v1",
        "published_date": "2026-02-12T17:09:14+00:00",
        "updated_date": "2026-02-12T17:09:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nils Lehmann",
            "Yi Wang",
            "Zhitong Xiong",
            "Xiaoxiang Zhu"
        ],
        "tldr": "The paper introduces EO-VAE, a multi-sensor variational autoencoder for Earth Observation data, utilizing dynamic hypernetworks to encode and reconstruct flexible channel combinations, outperforming existing methods in reconstruction fidelity.",
        "tldr_zh": "该论文介绍了EO-VAE，一个用于地球观测数据的多传感器变分自编码器，它使用动态超网络来编码和重构灵活的通道组合，在重建保真度方面优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "AssetFormer: Modular 3D Assets Generation with Autoregressive Transformer",
        "summary": "The digital industry demands high-quality, diverse modular 3D assets, especially for user-generated content~(UGC). In this work, we introduce AssetFormer, an autoregressive Transformer-based model designed to generate modular 3D assets from textual descriptions. Our pilot study leverages real-world modular assets collected from online platforms. AssetFormer tackles the challenge of creating assets composed of primitives that adhere to constrained design parameters for various applications. By innovatively adapting module sequencing and decoding techniques inspired by language models, our approach enhances asset generation quality through autoregressive modeling. Initial results indicate the effectiveness of AssetFormer in streamlining asset creation for professional development and UGC scenarios. This work presents a flexible framework extendable to various types of modular 3D assets, contributing to the broader field of 3D content generation. The code is available at https://github.com/Advocate99/AssetFormer.",
        "url": "http://arxiv.org/abs/2602.12100v1",
        "published_date": "2026-02-12T15:55:21+00:00",
        "updated_date": "2026-02-12T15:55:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lingting Zhu",
            "Shengju Qian",
            "Haidi Fan",
            "Jiayu Dong",
            "Zhenchao Jin",
            "Siwei Zhou",
            "Gen Dong",
            "Xin Wang",
            "Lequan Yu"
        ],
        "tldr": "AssetFormer is an autoregressive Transformer-based model that generates modular 3D assets from textual descriptions, leveraging real-world data for applications in professional development and UGC. It adapts module sequencing and decoding techniques to generate high-quality 3D assets.",
        "tldr_zh": "AssetFormer是一个基于自回归 Transformer 的模型，它根据文本描述生成模块化 3D 资产，并利用真实世界的数据用于专业开发和 UGC 应用。它通过改进模块排序和解码技术来生成高质量的 3D 资产。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LLM-Driven 3D Scene Generation of Agricultural Simulation Environments",
        "summary": "Procedural generation techniques in 3D rendering engines have revolutionized the creation of complex environments, reducing reliance on manual design. Recent approaches using Large Language Models (LLMs) for 3D scene generation show promise but often lack domain-specific reasoning, verification mechanisms, and modular design. These limitations lead to reduced control and poor scalability. This paper investigates the use of LLMs to generate agricultural synthetic simulation environments from natural language prompts, specifically to address the limitations of lacking domain-specific reasoning, verification mechanisms, and modular design. A modular multi-LLM pipeline was developed, integrating 3D asset retrieval, domain knowledge injection, and code generation for the Unreal rendering engine using its API. This results in a 3D environment with realistic planting layouts and environmental context, all based on the input prompt and the domain knowledge. To enhance accuracy and scalability, the system employs a hybrid strategy combining LLM optimization techniques such as few-shot prompting, Retrieval-Augmented Generation (RAG), finetuning, and validation. Unlike monolithic models, the modular architecture enables structured data handling, intermediate verification, and flexible expansion. The system was evaluated using structured prompts and semantic accuracy metrics. A user study assessed realism and familiarity against real-world images, while an expert comparison demonstrated significant time savings over manual scene design. The results confirm the effectiveness of multi-LLM pipelines in automating domain-specific 3D scene generation with improved reliability and precision. Future work will explore expanding the asset hierarchy, incorporating real-time generation, and adapting the pipeline to other simulation domains beyond agriculture.",
        "url": "http://arxiv.org/abs/2602.11706v1",
        "published_date": "2026-02-12T08:33:01+00:00",
        "updated_date": "2026-02-12T08:33:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Arafa Yoncalik",
            "Wouter Jansen",
            "Nico Huebel",
            "Mohammad Hasan Rahmani",
            "Jan Steckel"
        ],
        "tldr": "This paper presents a modular multi-LLM pipeline for generating realistic 3D agricultural simulation environments from natural language prompts, addressing limitations of existing LLM-based scene generation methods with domain-specific reasoning and verification.",
        "tldr_zh": "本文提出了一种模块化的多LLM管线，用于从自然语言提示生成逼真的3D农业模拟环境，通过特定领域的推理和验证，解决了现有基于LLM的场景生成方法的局限性。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "OMEGA-Avatar: One-shot Modeling of 360° Gaussian Avatars",
        "summary": "Creating high-fidelity, animatable 3D avatars from a single image remains a formidable challenge. We identified three desirable attributes of avatar generation: 1) the method should be feed-forward, 2) model a 360° full-head, and 3) should be animation-ready. However, current work addresses only two of the three points simultaneously. To address these limitations, we propose OMEGA-Avatar, the first feed-forward framework that simultaneously generates a generalizable, 360°-complete, and animatable 3D Gaussian head from a single image. Starting from a feed-forward and animatable framework, we address the 360° full-head avatar generation problem with two novel components. First, to overcome poor hair modeling in full-head avatar generation, we introduce a semantic-aware mesh deformation module that integrates multi-view normals to optimize a FLAME head with hair while preserving its topology structure. Second, to enable effective feed-forward decoding of full-head features, we propose a multi-view feature splatting module that constructs a shared canonical UV representation from features across multiple views through differentiable bilinear splatting, hierarchical UV mapping, and visibility-aware fusion. This approach preserves both global structural coherence and local high-frequency details across all viewpoints, ensuring 360° consistency without per-instance optimization. Extensive experiments demonstrate that OMEGA-Avatar achieves state-of-the-art performance, significantly outperforming existing baselines in 360° full-head completeness while robustly preserving identity across different viewpoints.",
        "url": "http://arxiv.org/abs/2602.11693v1",
        "published_date": "2026-02-12T08:16:38+00:00",
        "updated_date": "2026-02-12T08:16:38+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Zehao Xia",
            "Yiqun Wang",
            "Zhengda Lu",
            "Kai Liu",
            "Jun Xiao",
            "Peter Wonka"
        ],
        "tldr": "The paper introduces OMEGA-Avatar, a feed-forward framework for generating animatable, 360-degree complete 3D Gaussian head avatars from a single image, addressing limitations of existing methods in hair modeling and feature decoding.",
        "tldr_zh": "该论文介绍了OMEGA-Avatar，一个前馈框架，用于从单张图像生成可动画的、360度完整的3D高斯头部头像，解决了现有方法在头发建模和特征解码方面的局限性。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]