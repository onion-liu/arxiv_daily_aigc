[
    {
        "title": "Emu3.5: Native Multimodal Models are World Learners",
        "summary": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.",
        "url": "http://arxiv.org/abs/2510.26583v1",
        "published_date": "2025-10-30T15:11:16+00:00",
        "updated_date": "2025-10-30T15:11:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yufeng Cui",
            "Honghao Chen",
            "Haoge Deng",
            "Xu Huang",
            "Xinghang Li",
            "Jirong Liu",
            "Yang Liu",
            "Zhuoyan Luo",
            "Jinsheng Wang",
            "Wenxuan Wang",
            "Yueze Wang",
            "Chengyuan Wang",
            "Fan Zhang",
            "Yingli Zhao",
            "Ting Pan",
            "Xianduo Li",
            "Zecheng Hao",
            "Wenxuan Ma",
            "Zhuo Chen",
            "Yulong Ao",
            "Tiejun Huang",
            "Zhongyuan Wang",
            "Xinlong Wang"
        ],
        "tldr": "Emu3.5 is a large-scale multimodal model trained on trillions of tokens from internet videos, capable of native multimodal predictions, long-horizon generation, and demonstrates strong performance in image/video generation and embodied manipulation.",
        "tldr_zh": "Emu3.5是一个大规模多模态模型，在来自互联网视频的数万亿 token 上进行训练，能够进行原生多模态预测、长程生成，并在图像/视频生成和具身操作方面表现出强大的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation",
        "summary": "Recently text-to-video generation has made impressive progress in producing\nshort, high-quality clips, but evaluating long-form outputs remains a major\nchallenge especially when processing complex prompts. Existing benchmarks\nmostly rely on simplified prompts and focus on low-level metrics, overlooking\nfine-grained alignment with prompts and abstract dimensions such as narrative\ncoherence and thematic expression. To address these gaps, we propose\nLoCoT2V-Bench, a benchmark specifically designed for long video generation\n(LVG) under complex input conditions. Based on various real-world videos,\nLoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating\nelements like scene transitions and event dynamics. Moreover, it constructs a\nmulti-dimensional evaluation framework that includes our newly proposed metrics\nsuch as event-level alignment, fine-grained temporal consistency, content\nclarity, and the Human Expectation Realization Degree (HERD) that focuses on\nmore abstract attributes like narrative flow, emotional response, and character\ndevelopment. Using this framework, we conduct a comprehensive evaluation of\nnine representative LVG models, finding that while current methods perform well\non basic visual and temporal aspects, they struggle with inter-event\nconsistency, fine-grained alignment, and high-level thematic adherence, etc.\nOverall, LoCoT2V-Bench provides a comprehensive and reliable platform for\nevaluating long-form complex text-to-video generation and highlights critical\ndirections for future method improvement.",
        "url": "http://arxiv.org/abs/2510.26412v1",
        "published_date": "2025-10-30T12:00:46+00:00",
        "updated_date": "2025-10-30T12:00:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiangqing Zheng",
            "Chengyue Wu",
            "Kehai Chen",
            "Min Zhang"
        ],
        "tldr": "The paper introduces LoCoT2V-Bench, a new benchmark for evaluating long-form text-to-video generation with complex prompts, addressing limitations in existing benchmarks by focusing on fine-grained alignment, narrative coherence, and thematic expression.",
        "tldr_zh": "该论文介绍了LoCoT2V-Bench，一个新的用于评估基于复杂提示的长视频生成模型的基准，通过关注细粒度对齐、叙事连贯性和主题表达，解决了现有基准的局限性。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes",
        "summary": "There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.",
        "url": "http://arxiv.org/abs/2510.26800v1",
        "published_date": "2025-10-30T17:59:51+00:00",
        "updated_date": "2025-10-30T17:59:51+00:00",
        "categories": [
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "authors": [
            "Yukun Huang",
            "Jiwen Yu",
            "Yanning Zhou",
            "Jianan Wang",
            "Xintao Wang",
            "Pengfei Wan",
            "Xihui Liu"
        ],
        "tldr": "The paper introduces OmniX, a framework that uses 2D generative priors for panoramic perception, generation, and completion to create graphics-ready 3D scenes suitable for PBR and simulation.",
        "tldr_zh": "该论文介绍了OmniX，一个利用2D生成先验进行全景感知、生成和补全的框架，旨在创建适用于基于物理渲染（PBR）和仿真的、可用于图形处理的3D场景。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "The Quest for Generalizable Motion Generation: Data, Model, and Evaluation",
        "summary": "Despite recent advances in 3D human motion generation (MoGen) on standard\nbenchmarks, existing models still face a fundamental bottleneck in their\ngeneralization capability. In contrast, adjacent generative fields, most\nnotably video generation (ViGen), have demonstrated remarkable generalization\nin modeling human behaviors, highlighting transferable insights that MoGen can\nleverage. Motivated by this observation, we present a comprehensive framework\nthat systematically transfers knowledge from ViGen to MoGen across three key\npillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a\nlarge-scale dataset comprising 228,000 high-quality motion samples that\nintegrates high-fidelity optical MoCap data with semantically annotated motions\nfrom web videos and synthesized samples generated by state-of-the-art ViGen\nmodels. The dataset includes both text-motion pairs and text-video-motion\ntriplets, substantially expanding semantic diversity. Second, we propose\nViMoGen, a flow-matching-based diffusion transformer that unifies priors from\nMoCap data and ViGen models through gated multimodal conditioning. To enhance\nefficiency, we further develop ViMoGen-light, a distilled variant that\neliminates video generation dependencies while preserving strong\ngeneralization. Finally, we present MBench, a hierarchical benchmark designed\nfor fine-grained evaluation across motion quality, prompt fidelity, and\ngeneralization ability. Extensive experiments show that our framework\nsignificantly outperforms existing approaches in both automatic and human\nevaluations. The code, data, and benchmark will be made publicly available.",
        "url": "http://arxiv.org/abs/2510.26794v1",
        "published_date": "2025-10-30T17:59:27+00:00",
        "updated_date": "2025-10-30T17:59:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jing Lin",
            "Ruisi Wang",
            "Junzhe Lu",
            "Ziqi Huang",
            "Guorui Song",
            "Ailing Zeng",
            "Xian Liu",
            "Chen Wei",
            "Wanqi Yin",
            "Qingping Sun",
            "Zhongang Cai",
            "Lei Yang",
            "Ziwei Liu"
        ],
        "tldr": "This paper introduces ViMoGen, a framework for improving 3D human motion generation by transferring knowledge from video generation, including a large-scale dataset (ViMoGen-228K), a flow-matching-based diffusion transformer, and a hierarchical benchmark (MBench). The framework demonstrates improved generalization and performance compared to existing methods.",
        "tldr_zh": "本文介绍了一个名为ViMoGen的框架，通过从视频生成领域迁移知识来改善3D人体运动生成，包括一个大型数据集(ViMoGen-228K)、一个基于流匹配的扩散Transformer和一个分层基准测试(MBench)。该框架展示了比现有方法更好的泛化能力和性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dynamic VLM-Guided Negative Prompting for Diffusion Models",
        "summary": "We propose a novel approach for dynamic negative prompting in diffusion\nmodels that leverages Vision-Language Models (VLMs) to adaptively generate\nnegative prompts during the denoising process. Unlike traditional Negative\nPrompting methods that use fixed negative prompts, our method generates\nintermediate image predictions at specific denoising steps and queries a VLM to\nproduce contextually appropriate negative prompts. We evaluate our approach on\nvarious benchmark datasets and demonstrate the trade-offs between negative\nguidance strength and text-image alignment.",
        "url": "http://arxiv.org/abs/2510.26052v1",
        "published_date": "2025-10-30T01:10:25+00:00",
        "updated_date": "2025-10-30T01:10:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hoyeon Chang",
            "Seungjin Kim",
            "Yoonseok Choi"
        ],
        "tldr": "This paper introduces a dynamic negative prompting technique for diffusion models, utilizing VLMs to generate context-aware negative prompts during the denoising process, aiming to improve text-image alignment.",
        "tldr_zh": "该论文提出了一种动态负面提示技术，用于扩散模型。该技术利用视觉-语言模型在去噪过程中生成上下文相关的负面提示，旨在提高文本-图像对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing",
        "summary": "Rectified flow models have become a de facto standard in image generation due\nto their stable sampling trajectories and high-fidelity outputs. Despite their\nstrong generative capabilities, they face critical limitations in image editing\ntasks: inaccurate inversion processes for mapping real images back into the\nlatent space, and gradient entanglement issues during editing often result in\noutputs that do not faithfully reflect the target prompt. Recent efforts have\nattempted to directly map source and target distributions via ODE-based\napproaches without inversion; however,these methods still yield suboptimal\nediting quality. In this work, we propose a flow decomposition-and-aggregation\nframework built upon an inversion-free formulation to address these\nlimitations. Specifically, we semantically decompose the target prompt into\nmultiple sub-prompts, compute an independent flow for each, and aggregate them\nto form a unified editing trajectory. While we empirically observe that\ndecomposing the original flow enhances diversity in the target space,\ngenerating semantically aligned outputs still requires consistent guidance\ntoward the full target prompt. To this end, we design a projection and\nsoft-aggregation mechanism for flow, inspired by gradient conflict resolution\nin multi-task learning. This approach adaptively weights the sub-target\nvelocity fields, suppressing semantic redundancy while emphasizing distinct\ndirections, thereby preserving both diversity and consistency in the final\nedited output. Experimental results demonstrate that our method outperforms\nexisting zero-shot editing approaches in terms of semantic fidelity and\nattribute disentanglement. The code is available at\nhttps://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.",
        "url": "http://arxiv.org/abs/2510.25970v1",
        "published_date": "2025-10-29T21:12:58+00:00",
        "updated_date": "2025-10-29T21:12:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sung-Hoon Yoon",
            "Minghan Li",
            "Gaspard Beaudouin",
            "Congcong Wen",
            "Muhammad Rafay Azhar",
            "Mengyu Wang"
        ],
        "tldr": "The paper introduces SplitFlow, an inversion-free text-to-image editing framework that decomposes the target prompt into sub-prompts, computes independent flows for each, and aggregates them using a novel projection and soft-aggregation mechanism to improve semantic fidelity and attribute disentanglement.",
        "tldr_zh": "该论文介绍了SplitFlow，一个无需反演的文本到图像编辑框架，它将目标提示分解为多个子提示，为每个子提示计算独立的 flow，并使用一种新颖的投影和软聚合机制来聚合它们，以提高语义保真度和属性解耦，而无需图像反演。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency",
        "summary": "Current text-to-image generative models are trained on large uncurated\ndatasets to enable diverse generation capabilities. However, this does not\nalign well with user preferences. Recently, reward models have been\nspecifically designed to perform post-hoc selection of generated images and\nalign them to a reward, typically user preference. This discarding of\ninformative data together with the optimizing for a single reward tend to harm\ndiversity, semantic fidelity and efficiency. Instead of this post-processing,\nwe propose to condition the model on multiple reward models during training to\nlet the model learn user preferences directly. We show that this not only\ndramatically improves the visual quality of the generated images but it also\nsignificantly speeds up the training. Our proposed method, called MIRO,\nachieves state-of-the-art performances on the GenEval compositional benchmark\nand user-preference scores (PickAScore, ImageReward, HPSv2).",
        "url": "http://arxiv.org/abs/2510.25897v1",
        "published_date": "2025-10-29T18:59:17+00:00",
        "updated_date": "2025-10-29T18:59:17+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nicolas Dufour",
            "Lucas Degeorge",
            "Arijit Ghosh",
            "Vicky Kalogeiton",
            "David Picard"
        ],
        "tldr": "The paper proposes MIRO, a method for pretraining text-to-image models by conditioning on multiple reward models during training, improving image quality and training efficiency compared to post-hoc reward optimization.",
        "tldr_zh": "该论文提出了MIRO，一种通过在训练期间基于多个奖励模型进行条件预训练的文本到图像模型方法，相比于事后奖励优化，提高了图像质量和训练效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws",
        "summary": "Existing infrared and visible image fusion methods often face the dilemma of\nbalancing modal information. Generative fusion methods reconstruct fused images\nby learning from data distributions, but their generative capabilities remain\nlimited. Moreover, the lack of interpretability in modal information selection\nfurther affects the reliability and consistency of fusion results in complex\nscenarios. This manuscript revisits the essence of generative image fusion\nunder the inspiration of human cognitive laws and proposes a novel infrared and\nvisible image fusion method, termed HCLFuse. First, HCLFuse investigates the\nquantification theory of information mapping in unsupervised fusion networks,\nwhich leads to the design of a multi-scale mask-regulated variational\nbottleneck encoder. This encoder applies posterior probability modeling and\ninformation decomposition to extract accurate and concise low-level modal\ninformation, thereby supporting the generation of high-fidelity structural\ndetails. Furthermore, the probabilistic generative capability of the diffusion\nmodel is integrated with physical laws, forming a time-varying physical\nguidance mechanism that adaptively regulates the generation process at\ndifferent stages, thereby enhancing the ability of the model to perceive the\nintrinsic structure of data and reducing dependence on data quality.\nExperimental results show that the proposed method achieves state-of-the-art\nfusion performance in qualitative and quantitative evaluations across multiple\ndatasets and significantly improves semantic segmentation metrics. This fully\ndemonstrates the advantages of this generative image fusion method, drawing\ninspiration from human cognition, in enhancing structural consistency and\ndetail quality.",
        "url": "http://arxiv.org/abs/2510.26268v1",
        "published_date": "2025-10-30T08:53:13+00:00",
        "updated_date": "2025-10-30T08:53:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lin Guo",
            "Xiaoqing Luo",
            "Wei Xie",
            "Zhancheng Zhang",
            "Hui Li",
            "Rui Wang",
            "Zhenhua Feng",
            "Xiaoning Song"
        ],
        "tldr": "The paper proposes a new infrared and visible image fusion method (HCLFuse) inspired by human cognitive laws, utilizing a multi-scale variational bottleneck encoder and a diffusion model with physical guidance to achieve state-of-the-art fusion performance and improved semantic segmentation metrics.",
        "tldr_zh": "该论文提出了一种新的红外和可见光图像融合方法（HCLFuse），其灵感来自人类认知规律，利用多尺度变分瓶颈编码器和具有物理指导的扩散模型，实现了最先进的融合性能和改进的语义分割指标。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FullPart: Generating each 3D Part at Full Resolution",
        "summary": "Part-based 3D generation holds great potential for various applications.\nPrevious part generators that represent parts using implicit vector-set tokens\noften suffer from insufficient geometric details. Another line of work adopts\nan explicit voxel representation but shares a global voxel grid among all\nparts; this often causes small parts to occupy too few voxels, leading to\ndegraded quality. In this paper, we propose FullPart, a novel framework that\ncombines both implicit and explicit paradigms. It first derives the bounding\nbox layout through an implicit box vector-set diffusion process, a task that\nimplicit diffusion handles effectively since box tokens contain little\ngeometric detail. Then, it generates detailed parts, each within its own fixed\nfull-resolution voxel grid. Instead of sharing a global low-resolution space,\neach part in our method - even small ones - is generated at full resolution,\nenabling the synthesis of intricate details. We further introduce a\ncenter-point encoding strategy to address the misalignment issue when\nexchanging information between parts of different actual sizes, thereby\nmaintaining global coherence. Moreover, to tackle the scarcity of reliable part\ndata, we present PartVerse-XL, the largest human-annotated 3D part dataset to\ndate with 40K objects and 320K parts. Extensive experiments demonstrate that\nFullPart achieves state-of-the-art results in 3D part generation. We will\nrelease all code, data, and model to benefit future research in 3D part\ngeneration.",
        "url": "http://arxiv.org/abs/2510.26140v1",
        "published_date": "2025-10-30T04:51:05+00:00",
        "updated_date": "2025-10-30T04:51:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lihe Ding",
            "Shaocong Dong",
            "Yaokun Li",
            "Chenjian Gao",
            "Xiao Chen",
            "Rui Han",
            "Yihao Kuang",
            "Hong Zhang",
            "Bo Huang",
            "Zhanpeng Huang",
            "Zibin Wang",
            "Dan Xu",
            "Tianfan Xue"
        ],
        "tldr": "The paper introduces FullPart, a novel framework for generating high-resolution 3D parts by combining implicit and explicit representations, and a new large-scale part dataset (PartVerse-XL).",
        "tldr_zh": "该论文介绍了一种名为FullPart的新框架，它通过结合隐式和显式表示来生成高分辨率的3D部件，并创建了一个新的大型部件数据集（PartVerse-XL）。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]