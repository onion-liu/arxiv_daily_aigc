<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv CS.CV Papers (Image/Video Generation) - September 29, 2025</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/framer-motion/10.16.4/framer-motion.dev.js"></script>
    <!-- Example using Font Awesome (replace with your preferred icon library if needed) -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');

        :root {
            /* New Palette: Light, Clean, Futuristic with Teal/Aqua accents */
            --bg-color: #f8fafc; /* Tailwind slate-50 (Very Light Gray) */
            --card-bg-color: #ffffff; /* White */
            --text-color: #1e293b; /* Tailwind slate-800 (Dark Gray-Blue) */
            --text-muted-color: #64748b; /* Tailwind slate-500 (Medium Gray-Blue) */
            --header-color: #0f172a; /* Tailwind slate-900 (Very Dark Blue) */
            --highlight-primary: #14b8a6; /* Tailwind teal-500 */
            --highlight-secondary: #67e8f9; /* Tailwind cyan-300 */
            --border-color: #e2e8f0; /* Tailwind slate-200 (Light Gray) */
            --shadow-color: rgba(15, 23, 42, 0.08); /* Subtle shadow based on slate-900 */
        }

        body {
            background-color: var(--bg-color);
            color: var(--text-color);
            font-family: 'Inter', sans-serif;
            overflow-x: hidden; /* Prevent horizontal scroll */
            line-height: 1.6;
        }

        .bento-grid {
            display: grid;
            gap: 1.5rem; /* Tailwind gap-6 */
            grid-template-columns: 1fr; /* Force single column */
            padding-bottom: 4rem; /* Add padding at the bottom */
        }

        .bento-item {
            /* Apply semi-transparent white background and blur */
            background-color: rgba(255, 255, 255, 0.7); /* White with 70% opacity */
            backdrop-filter: blur(10px); /* Apply blur effect */
            -webkit-backdrop-filter: blur(10px); /* Safari prefix */
            border-radius: 1rem; /* Slightly larger radius */
            padding: 1.75rem; /* Slightly more padding */
            border: 1px solid rgba(226, 232, 240, 0.5); /* Lighter border with transparency */
            box-shadow: 0 4px 12px var(--shadow-color);
            transition: transform 0.3s ease-out, box-shadow 0.3s ease-out, background-color 0.3s ease-out;
            overflow: hidden; /* Ensure content doesn't overflow */
            position: relative; /* For potential pseudo-elements */
        }

        /* Removed ::before pseudo-element for a cleaner look */


        .bento-item:hover {
            transform: translateY(-6px);
            box-shadow: 0 10px 20px var(--shadow-color), 0 4px 8px rgba(15, 23, 42, 0.06); /* Adjusted hover shadow */
        }

        .paper-title {
            font-size: 1.125rem; /* Tailwind text-lg */
            font-weight: 600; /* Tailwind font-semibold */
            color: var(--highlight-primary); /* Use new primary highlight */
            margin-bottom: 0.75rem; /* Tailwind mb-3 */
            line-height: 1.4;
        }

        .paper-summary {
            font-size: 0.875rem; /* Tailwind text-sm */
            color: var(--text-muted-color);
            margin-bottom: 1.25rem; /* Tailwind mb-5 */
            line-height: 1.6;
        }

        .paper-link {
            display: inline-flex; /* Use flex for icon alignment */
            align-items: center;
            font-size: 0.875rem; /* Tailwind text-sm */
            font-weight: 600;
            color: var(--highlight-primary);
            text-decoration: none;
            padding: 0.5rem 1rem; /* Add padding */
            border-radius: 0.5rem; /* Slightly rounder */
            background-color: rgba(20, 184, 166, 0.08); /* Subtle teal background */
            border: 1px solid rgba(20, 184, 166, 0.2);
            transition: background-color 0.3s ease, color 0.3s ease, transform 0.2s ease;
        }

        .paper-link i {
            margin-right: 0.5rem; /* Tailwind mr-2 */
            transition: transform 0.3s ease;
        }

        .paper-link:hover {
            background-color: rgba(20, 184, 166, 0.15);
            color: #0d9488; /* Darker teal on hover */
            transform: translateY(-1px);
        }
        .paper-link:hover i {
             transform: translateX(2px);
        }

        .paper-authors {
            font-size: 0.75rem; /* Tailwind text-xs */
            color: var(--text-muted-color);
            margin-top: 1rem; /* Tailwind mt-4 */
            font-style: italic;
        }

        .header {
            text-align: center;
            margin-bottom: 3rem; /* Tailwind mb-12 */
            padding-top: 3rem; /* Tailwind pt-12 */
        }

        .header h1 {
            font-size: 2.5rem; /* Tailwind text-4xl or 5xl */
            font-weight: 700; /* Tailwind font-bold */
            color: var(--header-color);
            letter-spacing: -0.025em; /* Tailwind tracking-tight */
            margin-bottom: 0.5rem;
            /* Optional: Add a subtle text gradient */
            /* background: linear-gradient(90deg, var(--highlight-primary), var(--highlight-secondary)); */
            /* -webkit-background-clip: text; */
            /* -webkit-text-fill-color: transparent; */
        }

        .header p {
            font-size: 1.125rem; /* Tailwind text-lg */
            color: var(--text-muted-color);
            margin-top: 0.5rem; /* Tailwind mt-2 */
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
        }

        .footer {
            text-align: center;
            color: var(--text-muted-color);
            font-size: 0.875rem; /* Tailwind text-sm */
            padding-top: 2rem;
            padding-bottom: 2rem; /* Tailwind py-8 */
            border-top: 1px solid var(--border-color);
            margin-top: 4rem;
        }

        /* Simple line graphic element (optional) */
        .line-graphic {
            height: 1px; /* Thinner line */
            background: linear-gradient(90deg, rgba(20, 184, 166, 0), var(--highlight-primary), rgba(20, 184, 166, 0));
            opacity: 0.6;
            margin: 1.5rem 0; /* Adjust margin */
        }

        /* Framer Motion requires the script, styles enhance appearance */
        [data-motion-element] {
             /* Base styles for elements animated by Framer Motion */
        }

        .paper-tldr {
            font-size: 0.95rem; /* Slightly bigger than summary */
            color: #475569; /* Changed to Tailwind slate-600 (slightly darker than summary) */
            margin-top: 0.75rem; /* Tailwind mt-3 */
            margin-bottom: 0.75rem; /* Tailwind mb-2 */
            /* font-style: italic; */
            font-weight: bold;
        }

        .paper-rating {
            margin-top: 1rem; /* Tailwind mt-4 */
            margin-bottom: 1rem; /* Tailwind mb-4 */
            color: #f59e0b; /* Tailwind amber-500 */
        }

        .paper-rating i {
            margin-right: 0.125rem; /* Tailwind mr-0.5 */
        }

        /* Apply consistent star color to sub-ratings */
        .paper-sub-ratings .rating-item i {
            color: #f59e0b; /* Match overall rating star color (amber-500) */
            margin-right: 0.125rem; /* Consistent spacing */
        }

    </style>
</head>
<body class="container mx-auto px-4 antialiased">

    <motion.div
        initial="{ opacity: 0, y: -30 }"
        animate="{ opacity: 1, y: 0 }"
        transition="{ duration: 0.6, ease: 'easeOut' }"
        class="header"
        data-motion-element
    >
        <h1>AIGC Daily Papers</h1>
        <p>Daily papers related to Image/Video/Multimodal Generation from cs.CV</p>
        <p>September 29, 2025</p>
        <div class="line-graphic mt-4 mb-8 mx-auto w-1/4"></div> <!-- Added line graphic -->
    </motion.div>

    <div class="bento-grid" id="paper-grid">
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.0, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention</h2>
            <p class="paper-summary">In Diffusion Transformer (DiT) models, particularly for video generation,
attention latency is a major bottleneck due to the long sequence length and the
quadratic complexity. We find that attention weights can be separated into two
parts: a small fraction of large weights with high rank and the remaining
weights with very low rank. This naturally suggests applying sparse
acceleration to the first part and low-rank acceleration to the second. Based
on this finding, we propose SLA (Sparse-Linear Attention), a trainable
attention method that fuses sparse and linear attention to accelerate diffusion
models. SLA classifies attention weights into critical, marginal, and
negligible categories, applying O(N^2) attention to critical weights, O(N)
attention to marginal weights, and skipping negligible ones. SLA combines these
computations into a single GPU kernel and supports both forward and backward
passes. With only a few fine-tuning steps using SLA, DiT models achieve a 20x
reduction in attention computation, resulting in significant acceleration
without loss of generation quality. Experiments show that SLA reduces attention
computation by 95% without degrading end-to-end generation quality,
outperforming baseline methods. In addition, we implement an efficient GPU
kernel for SLA, which yields a 13.7x speedup in attention computation and a
2.2x end-to-end speedup in video generation on Wan2.1-1.3B.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces Sparse-Linear Attention (SLA), a method to accelerate Diffusion Transformer models by selectively applying sparse and linear attention based on the significance of attention weights, achieving significant speedups in video generation without quality loss.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了稀疏线性注意力（SLA），一种通过根据注意力权重的显着性选择性地应用稀疏和线性注意力来加速扩散Transformer模型的方法，从而在视频生成中实现显著加速且不损失质量。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star-half-alt"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(9/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.24006v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Jintao Zhang, Haoxu Wang, Kai Jiang, Shuo Yang, Kaiwen Zheng, Haocheng Xi, Ziteng Wang, Hongzhou Zhu, Min Zhao, Ion Stoica, Joseph E. Gonzalez, Jun Zhu, Jianfei Chen</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.05, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">HunyuanImage 3.0 Technical Report</h2>
            <p class="paper-summary">We present HunyuanImage 3.0, a native multimodal model that unifies
multimodal understanding and generation within an autoregressive framework,
with its image generation module publicly available. The achievement of
HunyuanImage 3.0 relies on several key components, including meticulous data
curation, advanced architecture design, a native Chain-of-Thoughts schema,
progressive model pre-training, aggressive model post-training, and an
efficient infrastructure that enables large-scale training and inference. With
these advancements, we successfully trained a Mixture-of-Experts (MoE) model
comprising over 80 billion parameters in total, with 13 billion parameters
activated per token during inference, making it the largest and most powerful
open-source image generative model to date. We conducted extensive experiments
and the results of automatic and human evaluation of text-image alignment and
visual quality demonstrate that HunyuanImage 3.0 rivals previous
state-of-the-art models. By releasing the code and weights of HunyuanImage 3.0,
we aim to enable the community to explore new ideas with a state-of-the-art
foundation model, fostering a dynamic and vibrant multimodal ecosystem. All
open source assets are publicly available at
https://github.com/Tencent-Hunyuan/HunyuanImage-3.0</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: HunyuanImage 3.0 is a large-scale, open-source multimodal model for image generation trained with an autoregressive framework and MoE architecture, achieving state-of-the-art results and publicly released to foster research.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: HunyuanImage 3.0 是一个大规模的开源多模态图像生成模型，它采用自回归框架和 MoE 架构进行训练，实现了最先进的成果，并且公开发布以促进研究。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(10/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star-half-alt"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(9/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23951v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Siyu Cao, Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, Tiankai Hang, Duojun Huang, Jie Jiang, Zhengkai Jiang, Weijie Kong, Changlin Li, Donghao Li, Junzhe Li, Xin Li, Yang Li, Zhenxi Li, Zhimin Li, Jiaxin Lin, Linus, Lucaz Liu, Shu Liu, Songtao Liu, Yu Liu, Yuhong Liu, Yanxin Long, Fanbin Lu, Qinglin Lu, Yuyang Peng, Yuanbo Peng, Xiangwei Shen, Yixuan Shi, Jiale Tao, Yangyu Tao, Qi Tian, Pengfei Wan, Chunyu Wang, Kai Wang, Lei Wang, Linqing Wang, Lucas Wang, Qixun Wang, Weiyan Wang, Hao Wen, Bing Wu, Jianbing Wu, Yue Wu, Senhao Xie, Fang Yang, Miles Yang, Xiaofeng Yang, Xuan Yang, Zhantao Yang, Jingmiao Yu, Zheng Yuan, Chao Zhang, Jian-Wei Zhang, Peizhen Zhang, Shi-Xue Zhang, Tao Zhang, Weigang Zhang, Yepeng Zhang, Yingfang Zhang, Zihao Zhang, Zijian Zhang, Penghao Zhao, Zhiyuan Zhao, Xuefei Zhe, Jianchen Zhu, Zhao Zhong</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.1, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Uni4D-LLM: A Unified SpatioTemporal-Aware VLM for 4D Understanding and Generation</h2>
            <p class="paper-summary">Vision-language models (VLMs) have demonstrated strong performance in 2D
scene understanding and generation, but extending this unification to the
physical world remains an open challenge. Existing 3D and 4D approaches
typically embed scene geometry into autoregressive model for semantic
understanding and diffusion model for content generation. This paradigm gap
prevents a single model from jointly handling both tasks, especially in dynamic
4D settings where spatiotemporal modeling is critical. We propose Uni4D-LLM,
the first unified VLM framework with spatiotemporal awareness for 4D scene
understanding and generation. Our design is guided by two key insights: 1)
Unification requires a shared representation. We extract semantic features for
understanding and noisy-injected appearance features for generation,
incorporate 4D geometric cues, and fuse them into a spatiotemporal-aware visual
representation through adaptive cross-attention. 2) Unification requires a
shared architecture. Both autoregression and diffusion are built on Transformer
backbones, and this enables integration into a single LLM with task-specific
heads. By aligning visual and linguistic representations, our Uni4D-LLM
produces predictions for both understanding and generation within one
Transformer-based framework. We further apply instruction fine-tuning on
diverse 4D vision-language datasets to improve generalization across tasks.
Extensive experiments on multiple benchmarks demonstrate that Uni4D-LLM
achieves competitive or superior results compared to state-of-the-art models
and offers the first true unification of 4D scene understanding and generation.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: Uni4D-LLM is a novel unified VLM framework for 4D scene understanding and generation, utilizing a shared spatio-temporal-aware representation and a Transformer-based architecture with task-specific heads.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: Uni4D-LLM 是一种新颖的统一 VLM 框架，用于 4D 场景理解和生成，它利用共享的时空感知表示和基于 Transformer 的架构，并具有特定于任务的头部。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star-half-alt"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(9/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23828v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Hanyu Zhou, Gim Hee Lee</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.15000000000000002, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">UniAlignment: Semantic Alignment for Unified Image Generation, Understanding, Manipulation and Perception</h2>
            <p class="paper-summary">The remarkable success of diffusion models in text-to-image generation has
sparked growing interest in expanding their capabilities to a variety of
multi-modal tasks, including image understanding, manipulation, and perception.
These tasks require advanced semantic comprehension across both visual and
textual modalities, especially in scenarios involving complex semantic
instructions. However, existing approaches often rely heavily on
vision-language models (VLMs) or modular designs for semantic guidance, leading
to fragmented architectures and computational inefficiency. To address these
challenges, we propose UniAlignment, a unified multimodal generation framework
within a single diffusion transformer. UniAlignment introduces a dual-stream
diffusion training strategy that incorporates both intrinsic-modal semantic
alignment and cross-modal semantic alignment, thereby enhancing the model's
cross-modal consistency and instruction-following robustness. Additionally, we
present SemGen-Bench, a new benchmark specifically designed to evaluate
multimodal semantic consistency under complex textual instructions. Extensive
experiments across multiple tasks and benchmarks demonstrate that UniAlignment
outperforms existing baselines, underscoring the significant potential of
diffusion models in unified multimodal generation.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces UniAlignment, a unified diffusion-based framework for multimodal generation, understanding, manipulation, and perception, which uses a dual-stream training strategy to enhance cross-modal consistency and instruction-following. They also introduce a new benchmark called SemGen-Bench for evaluating multimodal semantic consistency.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了 UniAlignment，一个用于多模态生成、理解、操作和感知的统一扩散框架，它使用双流训练策略来增强跨模态一致性和指令遵循。他们还引入了一个名为 SemGen-Bench 的新基准，用于评估多模态语义一致性。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(10/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star-half-alt"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(9/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23760v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Xinyang Song, Libin Wang, Weining Wang, Shaozhen Liu, Dandan Zheng, Jingdong Chen, Qi Li, Zhenan Sun</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.2, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Token Painter: Training-Free Text-Guided Image Inpainting via Mask Autoregressive Models</h2>
            <p class="paper-summary">Text-guided image inpainting aims to inpaint masked image regions based on a
textual prompt while preserving the background. Although diffusion-based
methods have become dominant, their property of modeling the entire image in
latent space makes it challenging for the results to align well with prompt
details and maintain a consistent background. To address these issues, we
explore Mask AutoRegressive (MAR) models for this task. MAR naturally supports
image inpainting by generating latent tokens corresponding to mask regions,
enabling better local controllability without altering the background. However,
directly applying MAR to this task makes the inpainting content either ignore
the prompts or be disharmonious with the background context. Through analysis
of the attention maps from the inpainting images, we identify the impact of
background tokens on text tokens during the MAR generation, and leverage this
to design \textbf{Token Painter}, a training-free text-guided image inpainting
method based on MAR. Our approach introduces two key components: (1)
Dual-Stream Encoder Information Fusion (DEIF), which fuses the semantic and
context information from text and background in frequency domain to produce
novel guidance tokens, allowing MAR to generate text-faithful inpainting
content while keeping harmonious with background context. (2) Adaptive Decoder
Attention Score Enhancing (ADAE), which adaptively enhances attention scores on
guidance tokens and inpainting tokens to further enhance the alignment of
prompt details and the content visual quality. Extensive experiments
demonstrate that our training-free method outperforms prior state-of-the-art
methods across almost all metrics and delivers superior visual results. Codes
will be released.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces Token Painter, a training-free text-guided image inpainting method using Mask AutoRegressive models and frequency domain fusion to improve consistency and adherence to text prompts.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了一种名为Token Painter的免训练文本引导图像修复方法，该方法使用掩码自回归模型和频域融合来提高图像修复的一致性和对文本提示的遵循度。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23919v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Longtao Jiang, Mingfei Han, Lei Chen, Yongqiang Yu, Feng Zhao, Xiaojun Chang, Zhihui Li</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.25, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling</h2>
            <p class="paper-summary">Instruction-guided image editing has achieved remarkable progress, yet
current models still face challenges with complex instructions and often
require multiple samples to produce a desired result. Reinforcement Learning
(RL) offers a promising solution, but its adoption in image editing has been
severely hindered by the lack of a high-fidelity, efficient reward signal. In
this work, we present a comprehensive methodology to overcome this barrier,
centered on the development of a state-of-the-art, specialized reward model. We
first introduce EditReward-Bench, a comprehensive benchmark to systematically
evaluate reward models on editing quality. Building on this benchmark, we
develop EditScore, a series of reward models (7B-72B) for evaluating the
quality of instruction-guided image editing. Through meticulous data curation
and filtering, EditScore effectively matches the performance of learning
proprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy
tailored for the generative nature of EditScore, our largest variant even
surpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity
reward model is the key to unlocking online RL for image editing. Our
experiments show that, while even the largest open-source VLMs fail to provide
an effective learning signal, EditScore enables efficient and robust policy
optimization. Applying our framework to a strong base model, OmniGen2, results
in a final model that shows a substantial and consistent performance uplift.
Overall, this work provides the first systematic path from benchmarking to
reward modeling to RL training in image editing, showing that a high-fidelity,
domain-specialized reward model is the key to unlocking the full potential of
RL in this domain.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: This paper introduces EditScore, a novel reward model for instruction-guided image editing, that surpasses existing models, and enables effective online reinforcement learning for improved image editing results.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 本文介绍了EditScore，一种用于指令引导图像编辑的新型奖励模型，该模型超越了现有模型，并实现了有效的在线强化学习，从而改进了图像编辑结果。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23909v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Xin Luo, Jiahao Wang, Chenyuan Wu, Shitao Xiao, Xiyan Jiang, Defu Lian, Jiajun Zhang, Dong Liu, Zheng liu</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.30000000000000004, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Not All Tokens are Guided Equal: Improving Guidance in Visual Autoregressive Models</h2>
            <p class="paper-summary">Autoregressive (AR) models based on next-scale prediction are rapidly
emerging as a powerful tool for image generation, but they face a critical
weakness: information inconsistencies between patches across timesteps
introduced by progressive resolution scaling. These inconsistencies scatter
guidance signals, causing them to drift away from conditioning information and
leaving behind ambiguous, unfaithful features. We tackle this challenge with
Information-Grounding Guidance (IGG), a novel mechanism that anchors guidance
to semantically important regions through attention. By adaptively reinforcing
informative patches during sampling, IGG ensures that guidance and content
remain tightly aligned. Across both class-conditioned and text-to-image
generation tasks, IGG delivers sharper, more coherent, and semantically
grounded images, setting a new benchmark for AR-based methods.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces Information-Grounding Guidance (IGG), a novel attention-based mechanism for autoregressive image generation models that improves coherence and semantic grounding by reinforcing informative patches during sampling, leading to sharper and more faithful images.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了一种名为信息接地引导（IGG）的新型基于注意力的机制，用于自回归图像生成模型，通过在采样过程中强化信息丰富的补丁来提高连贯性和语义接地，从而生成更清晰、更忠实的图像。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23876v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Ky Dan Nguyen, Hoang Lam Tran, Anh-Dung Dinh, Daochang Liu, Weidong Cai, Xiuying Wang, Chang Xu</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.35000000000000003, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">ReLumix: Extending Image Relighting to Video via Video Diffusion Models</h2>
            <p class="paper-summary">Controlling illumination during video post-production is a crucial yet
elusive goal in computational photography. Existing methods often lack
flexibility, restricting users to certain relighting models. This paper
introduces ReLumix, a novel framework that decouples the relighting algorithm
from temporal synthesis, thereby enabling any image relighting technique to be
seamlessly applied to video. Our approach reformulates video relighting into a
simple yet effective two-stage process: (1) an artist relights a single
reference frame using any preferred image-based technique (e.g., Diffusion
Models, physics-based renderers); and (2) a fine-tuned stable video diffusion
(SVD) model seamlessly propagates this target illumination throughout the
sequence. To ensure temporal coherence and prevent artifacts, we introduce a
gated cross-attention mechanism for smooth feature blending and a temporal
bootstrapping strategy that harnesses SVD's powerful motion priors. Although
trained on synthetic data, ReLumix shows competitive generalization to
real-world videos. The method demonstrates significant improvements in visual
fidelity, offering a scalable and versatile solution for dynamic lighting
control.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: ReLumix enables video relighting by decoupling the relighting algorithm from temporal synthesis, allowing any image relighting technique to be applied to video through a fine-tuned stable video diffusion model.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: ReLumix通过分离光照重构算法和时间合成，实现了视频光照重构，允许将任何图像光照重构技术通过微调的稳定视频扩散模型应用于视频。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23769v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Lezhong Wang, Shutong Jin, Ruiqi Cui, Anders Bjorholm Dahl, Jeppe Revall Frisvad, Siavash Bigdeli</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.4, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and Generation</h2>
            <p class="paper-summary">In this work, we present HieraTok, a novel multi-scale Vision Transformer
(ViT)-based tokenizer that overcomes the inherent limitation of modeling
single-scale representations. This is realized through two key designs: (1)
multi-scale downsampling applied to the token map generated by the tokenizer
encoder, producing a sequence of multi-scale tokens, and (2) a scale-causal
attention mechanism that enables the progressive flow of information from
low-resolution global semantic features to high-resolution structural details.
Coupling these designs, HieraTok achieves significant improvements in both
image reconstruction and generation tasks. Under identical settings, the
multi-scale visual tokenizer outperforms its single-scale counterpart by a
27.2\% improvement in rFID ($1.47 \rightarrow 1.07$). When integrated into
downstream generation frameworks, it achieves a $1.38\times$ faster convergence
rate and an 18.9\% boost in gFID ($16.4 \rightarrow 13.3$), which may be
attributed to the smoother and more uniformly distributed latent space.
Furthermore, by scaling up the tokenizer's training, we demonstrate its
potential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To
the best of our knowledge, we are the first to introduce multi-scale ViT-based
tokenizer in image reconstruction and image generation. We hope our findings
and designs advance the ViT-based tokenizers in visual generation tasks.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces HieraTok, a multi-scale Vision Transformer tokenizer for image reconstruction and generation, showing significant improvements in rFID, gFID, and convergence rate compared to single-scale tokenizers. It claims to be the first multi-scale ViT-based tokenizer in this domain.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了HieraTok，一种用于图像重建和生成的基于多尺度视觉Transformer的tokenizer，与单尺度tokenizer相比，在rFID、gFID和收敛速度方面表现出显著改进。论文声称是该领域首个多尺度ViT tokenizer。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23736v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Cong Chen, Ziyuan Huang, Cheng Zou, Muzhi Zhu, Kaixiang Ji, Jiajia Liu, Jingdong Chen, Hao Chen, Chunhua Shen</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.45, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">M3DLayout: A Multi-Source Dataset of 3D Indoor Layouts and Structured Descriptions for 3D Generation</h2>
            <p class="paper-summary">In text-driven 3D scene generation, object layout serves as a crucial
intermediate representation that bridges high-level language instructions with
detailed geometric output. It not only provides a structural blueprint for
ensuring physical plausibility but also supports semantic controllability and
interactive editing. However, the learning capabilities of current 3D indoor
layout generation models are constrained by the limited scale, diversity, and
annotation quality of existing datasets. To address this, we introduce
M3DLayout, a large-scale, multi-source dataset for 3D indoor layout generation.
M3DLayout comprises 15,080 layouts and over 258k object instances, integrating
three distinct sources: real-world scans, professional CAD designs, and
procedurally generated scenes. Each layout is paired with detailed structured
text describing global scene summaries, relational placements of large
furniture, and fine-grained arrangements of smaller items. This diverse and
richly annotated resource enables models to learn complex spatial and semantic
patterns across a wide variety of indoor environments. To assess the potential
of M3DLayout, we establish a benchmark using a text-conditioned diffusion
model. Experimental results demonstrate that our dataset provides a solid
foundation for training layout generation models. Its multi-source composition
enhances diversity, notably through the Inf3DLayout subset which provides rich
small-object information, enabling the generation of more complex and detailed
scenes. We hope that M3DLayout can serve as a valuable resource for advancing
research in text-driven 3D scene synthesis.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces M3DLayout, a large-scale, multi-source dataset of 3D indoor layouts with structured descriptions for text-driven 3D scene generation, aimed at improving the learning capabilities of layout generation models.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了M3DLayout，一个大规模、多来源的3D室内布局数据集，包含结构化描述，用于文本驱动的3D场景生成，旨在提高布局生成模型的学习能力。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23728v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Yiheng Zhang, Zhuojiang Cai, Mingdao Wang, Meitong Guo, Tianxiao Li, Li Lin, Yuwang Wang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.5, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">QuantSparse: Comprehensively Compressing Video Diffusion Transformer with Model Quantization and Attention Sparsification</h2>
            <p class="paper-summary">Diffusion transformers exhibit remarkable video generation capability, yet
their prohibitive computational and memory costs hinder practical deployment.
Model quantization and attention sparsification are two promising directions
for compression, but each alone suffers severe performance degradation under
aggressive compression. Combining them promises compounded efficiency gains,
but naive integration is ineffective. The sparsity-induced information loss
exacerbates quantization noise, leading to amplified attention shifts. To
address this, we propose \textbf{QuantSparse}, a unified framework that
integrates model quantization with attention sparsification. Specifically, we
introduce \textit{Multi-Scale Salient Attention Distillation}, which leverages
both global structural guidance and local salient supervision to mitigate
quantization-induced bias. In addition, we develop \textit{Second-Order Sparse
Attention Reparameterization}, which exploits the temporal stability of
second-order residuals to efficiently recover information lost under sparsity.
Experiments on HunyuanVideo-13B demonstrate that QuantSparse achieves 20.88
PSNR, substantially outperforming the state-of-the-art quantization baseline
Q-VDiT (16.85 PSNR), while simultaneously delivering a \textbf{3.68$\times$}
reduction in storage and \textbf{1.88$\times$} acceleration in end-to-end
inference. Our code will be released in
https://github.com/wlfeng0509/QuantSparse.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces QuantSparse, a method that integrates model quantization and attention sparsification for compressing video diffusion transformers, achieving significant improvements in PSNR, storage reduction, and inference speed compared to existing quantization baselines.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了 QuantSparse，这是一种结合模型量化和注意力稀疏化的方法，用于压缩视频扩散 Transformer。与现有的量化基线相比，该方法在 PSNR、存储减少和推理速度方面取得了显著提升。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23681v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Weilun Feng, Chuanguang Yang, Haotong Qin, Mingqiang Wu, Yuqi Li, Xiangqi Li, Zhulin An, Libo Huang, Yulun Zhang, Michele Magno, Yongjun Xu</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.55, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training</h2>
            <p class="paper-summary">We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models
(LMMs) that achieve state-of-the-art performance with significantly reduced
computational and financial costs. Different from the existing works,
LLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for
building high-quality vision-language models entirely from scratch. The
LLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale
Curated Datasets: We construct an 85M concept-balanced pretraining dataset
LLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 26M instruction
dataset LLaVA-OneVision-1.5-Instruct, collectively encompassing 64B compressed
multimodal tokens. (2) Efficient Training Framework: We develop a complete
end-to-end efficient training framework leveraging an offline parallel data
packing strategy to facilitate the training of LLaVA-OneVision-1.5 within a
$16,000 budget. (3) State-of-the-art Performance: Experimental results
demonstrate that LLaVA-OneVision1.5 yields exceptionally competitive
performance across a broad range of downstream tasks. Specifically,
LLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and
LLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. We
anticipate releasing LLaVA-OneVision-1.5-RL shortly and encourage the community
to await further updates.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: LLaVA-OneVision-1.5 introduces a fully open and efficient framework for training large multimodal models, achieving state-of-the-art performance with significantly reduced costs and outperforming Qwen2.5-VL on many benchmarks.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: LLaVA-OneVision-1.5 提出了一个完全开放且高效的框架，用于训练大型多模态模型，以显著降低的成本实现了最先进的性能，并在多个基准测试中优于 Qwen2.5-VL。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23661v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, Huajie Tan, Chunyuan Li, Jing Yang, Jie Yu, Xiyao Wang, Bin Qin, Yumeng Wang, Zizhen Yan, Ziyong Feng, Ziwei Liu, Bo Li, Jiankang Deng</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.6000000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders</h2>
            <p class="paper-summary">This paper explores a novel lightweight approach LightFair to achieve fair
text-to-image diffusion models (T2I DMs) by addressing the adverse effects of
the text encoder. Most existing methods either couple different parts of the
diffusion model for full-parameter training or rely on auxiliary networks for
correction. They incur heavy training or sampling burden and unsatisfactory
performance. Since T2I DMs consist of multiple components, with the text
encoder being the most fine-tunable and front-end module, this paper focuses on
mitigating bias by fine-tuning text embeddings. To validate feasibility, we
observe that the text encoder's neutral embedding output shows substantial
skewness across image embeddings of various attributes in the CLIP space. More
importantly, the noise prediction network further amplifies this imbalance. To
finetune the text embedding, we propose a collaborative distance-constrained
debiasing strategy that balances embedding distances to improve fairness
without auxiliary references. However, mitigating bias can compromise the
original generation quality. To address this, we introduce a two-stage
text-guided sampling strategy to limit when the debiased text encoder
intervenes. Extensive experiments demonstrate that LightFair is effective and
efficient. Notably, on Stable Diffusion v1.5, our method achieves SOTA
debiasing at just $1/4$ of the training burden, with virtually no increase in
sampling burden. The code is available at https://github.com/boyuh/LightFair.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: LightFair proposes a lightweight and efficient method for debiasing text-to-image diffusion models by fine-tuning the text encoder, achieving state-of-the-art fairness with minimal training overhead and no increase in sampling burden.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: LightFair提出了一种轻量级且高效的方法，通过微调文本编码器来去偏见文本到图像的扩散模型，以最小的训练开销和不增加采样负担实现最先进的公平性。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23639v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Boyu Han, Qianqian Xu, Shilong Bao, Zhiyong Yang, Kangli Zi, Qingming Huang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.65, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">VMDiff: Visual Mixing Diffusion for Limitless Cross-Object Synthesis</h2>
            <p class="paper-summary">Creating novel images by fusing visual cues from multiple sources is a
fundamental yet underexplored problem in image-to-image generation, with broad
applications in artistic creation, virtual reality and visual media. Existing
methods often face two key challenges: coexistent generation, where multiple
objects are simply juxtaposed without true integration, and bias generation,
where one object dominates the output due to semantic imbalance. To address
these issues, we propose Visual Mixing Diffusion (VMDiff), a simple yet
effective diffusion-based framework that synthesizes a single, coherent object
by integrating two input images at both noise and latent levels. Our approach
comprises: (1) a hybrid sampling process that combines guided denoising,
inversion, and spherical interpolation with adjustable parameters to achieve
structure-aware fusion, mitigating coexistent generation; and (2) an efficient
adaptive adjustment module, which introduces a novel similarity-based score to
automatically and adaptively search for optimal parameters, countering semantic
bias. Experiments on a curated benchmark of 780 concept pairs demonstrate that
our method outperforms strong baselines in visual quality, semantic
consistency, and human-rated creativity.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces VMDiff, a diffusion-based framework for synthesizing novel images by integrating visual cues from two input images, addressing challenges of coexistent generation and semantic bias.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了VMDiff，一种基于扩散的框架，通过整合两张输入图像的视觉线索来合成新的图像，解决了共存生成和语义偏差的挑战。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23605v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Zeren Xiong, Yue Yu, Zedong Zhang, Shuo Chen, Jian Yang, Jun Li</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.7000000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization</h2>
            <p class="paper-summary">Diffusion Transformers (DiTs) have recently emerged as a powerful backbone
for image generation, demonstrating superior scalability and performance over
U-Net architectures. However, their practical deployment is hindered by
substantial computational and memory costs. While Quantization-Aware Training
(QAT) has shown promise for U-Nets, its application to DiTs faces unique
challenges, primarily due to the sensitivity and distributional complexity of
activations. In this work, we identify activation quantization as the primary
bottleneck for pushing DiTs to extremely low-bit settings. To address this, we
propose a systematic QAT framework for DiTs, named RobuQ. We start by
establishing a strong ternary weight (W1.58A4) DiT baseline. Building upon
this, we propose RobustQuantizer to achieve robust activation quantization. Our
theoretical analyses show that the Hadamard transform can convert unknown
per-token distributions into per-token normal distributions, providing a strong
foundation for this method. Furthermore, we propose AMPN, the first
Activation-only Mixed-Precision Network pipeline for DiTs. This method applies
ternary weights across the entire network while allocating different activation
precisions to each layer to eliminate information bottlenecks. Through
extensive experiments on unconditional and conditional image generation, our
RobuQ framework achieves state-of-the-art performance for DiT quantization in
sub-4-bit quantization configuration. To the best of our knowledge, RobuQ is
the first achieving stable and competitive image generation on large datasets
like ImageNet-1K with activations quantized to average 2 bits. The code and
models will be available at https://github.com/racoonykc/RobuQ .</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: This paper proposes RobuQ, a novel quantization-aware training framework for Diffusion Transformers (DiTs) that enables stable and competitive image generation with extremely low-bit activation quantization (W1.58A2), achieving state-of-the-art performance on ImageNet-1K.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 本文提出了RobuQ，一种新颖的扩散Transformer (DiT) 量化感知训练框架，能够以极低比特激活量化 (W1.58A2) 实现稳定且具有竞争力的图像生成，并在ImageNet-1K上实现了最先进的性能。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23582v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Kaicheng Yang, Xun Zhang, Haotong Qin, Yucheng Lin, Kaisen Yang, Xianglong Yan, Yulun Zhang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.75, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">FoR-SALE: Frame of Reference-guided Spatial Adjustment in LLM-based Diffusion Editing</h2>
            <p class="paper-summary">Frame of Reference (FoR) is a fundamental concept in spatial reasoning that
humans utilize to comprehend and describe space. With the rapid progress in
Multimodal Language models, the moment has come to integrate this
long-overlooked dimension into these models. In particular, in text-to-image
(T2I) generation, even state-of-the-art models exhibit a significant
performance gap when spatial descriptions are provided from perspectives other
than the camera. To address this limitation, we propose Frame of
Reference-guided Spatial Adjustment in LLM-based Diffusion Editing (FoR-SALE),
an extension of the Self-correcting LLM-controlled Diffusion (SLD) framework
for T2I. For-Sale evaluates the alignment between a given text and an initially
generated image, and refines the image based on the Frame of Reference
specified in the spatial expressions. It employs vision modules to extract the
spatial configuration of the image, while simultaneously mapping the spatial
expression to a corresponding camera perspective. This unified perspective
enables direct evaluation of alignment between language and vision. When
misalignment is detected, the required editing operations are generated and
applied. FoR-SALE applies novel latent-space operations to adjust the facing
direction and depth of the generated images. We evaluate FoR-SALE on two
benchmarks specifically designed to assess spatial understanding with FoR. Our
framework improves the performance of state-of-the-art T2I models by up to 5.3%
using only a single round of correction.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces FoR-SALE, a framework that enhances text-to-image generation by incorporating Frame of Reference (FoR) to improve spatial understanding and address perspective-related errors, demonstrating a performance increase of 5.3% on spatial benchmarks.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了FoR-SALE，一个通过结合参考系（FoR）来增强文本到图像生成的框架，以提高空间理解并解决与视角相关的错误，在空间基准测试中表现提升了5.3%。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23452v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Tanawan Premsri, Parisa Kordjamshidi</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.8, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">SAR-KnowLIP: Towards Multimodal Foundation Models for Remote Sensing</h2>
            <p class="paper-summary">Cross-modal artificial intelligence has garnered widespread attention in
recent years, achieving significant progress in the study of natural images.
However, existing methods are mostly designed for RGB imagery, leaving a
significant gap in modeling synthetic aperture radar (SAR) imagery. SAR, with
its all-day, all-weather imaging capabilities, plays an irreplaceable role in
remote sensing scene understanding. To address this gap, this paper proposes
SAR-KnowLIP, the first universal SAR multimodal foundational model, along with
reusable data and evaluation baselines. Specifically: (1) This work introduces
the critical yet long-overlooked attribute of geographic information into
remote sensing research, constructing SAR-GEOVL-1M (the first large-scale SAR
dataset with complete geographic projection properties), covering multiple
satellite platforms, 120,000 images, and 135 cities. (2) Aligned structured
text is generated through a hierarchical cognitive chain-of-thought (HCoT),
providing more than one million multi-dimensional semantic annotations of
landforms, regional functions, target attributes, and spatial relationships.
(3) We design a Self-Consistent Iterative Optimization mechanism that
continuously enhances cross-modal alignment through a self-supervised closed
loop of contrastive, matching, and reconstruction learning on a transferable
multimodal encoder. (4) A unified evaluation benchmark is established across 11
representative downstream vision and vision-language tasks, with comparisons
against 14 leading foundation models, where SAR-KnowLIP demonstrates leading
performance, particularly in object counting and land-cover classification. We
expect that SAR-KnowLIP's large-scale multimodal data, transferable model
architecture, and comprehensive experimental benchmark will significantly
advance the development of SAR multimodal baseline models.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces SAR-KnowLIP, a novel multimodal foundation model for SAR imagery, featuring a large-scale dataset (SAR-GEOVL-1M) with geographic information, aligned structured text annotations, and a self-consistent iterative optimization mechanism, demonstrating leading performance on various downstream tasks.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了SAR-KnowLIP，一种用于SAR图像的新型多模态基础模型，具有一个大规模数据集(SAR-GEOVL-1M)，其中包含地理信息、对齐的结构化文本注释和一个自一致迭代优化机制，并在各种下游任务上表现出领先性能。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(5/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star-half-alt"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(7/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23927v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Yi Yang, Xiaokun Zhang, Qingchen Fang, Ziqi Ye, Rui Li, Li Liu, Haipeng Wang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.8500000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">MoReact: Generating Reactive Motion from Textual Descriptions</h2>
            <p class="paper-summary">Modeling and generating human reactions poses a significant challenge with
broad applications for computer vision and human-computer interaction. Existing
methods either treat multiple individuals as a single entity, directly
generating interactions, or rely solely on one person's motion to generate the
other's reaction, failing to integrate the rich semantic information that
underpins human interactions. Yet, these methods often fall short in adaptive
responsiveness, i.e., the ability to accurately respond to diverse and dynamic
interaction scenarios. Recognizing this gap, our work introduces an approach
tailored to address the limitations of existing models by focusing on
text-driven human reaction generation. Our model specifically generates
realistic motion sequences for individuals that responding to the other's
actions based on a descriptive text of the interaction scenario. The goal is to
produce motion sequences that not only complement the opponent's movements but
also semantically fit the described interactions. To achieve this, we present
MoReact, a diffusion-based method designed to disentangle the generation of
global trajectories and local motions sequentially. This approach stems from
the observation that generating global trajectories first is crucial for
guiding local motion, ensuring better alignment with given action and text.
Furthermore, we introduce a novel interaction loss to enhance the realism of
generated close interactions. Our experiments, utilizing data adapted from a
two-person motion dataset, demonstrate the efficacy of our approach for this
novel task, which is capable of producing realistic, diverse, and controllable
reactions that not only closely match the movements of the counterpart but also
adhere to the textual guidance. Please find our webpage at
https://xiyan-xu.github.io/MoReactWebPage.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces MoReact, a diffusion-based method for generating realistic human reaction motion sequences from textual descriptions of interaction scenarios, showing improved realism and controllability.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了MoReact，一种基于扩散模型的方法，用于从交互场景的文本描述中生成逼真的人类反应运动序列，展示了改进的真实性和可控性。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star-half-alt"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(7/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23911v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Xiyan Xu, Sirui Xu, Yu-Xiong Wang, Liang-Yan Gui</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.9, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">CrimEdit: Controllable Editing for Counterfactual Object Removal, Insertion, and Movement</h2>
            <p class="paper-summary">Recent works on object removal and insertion have enhanced their performance
by handling object effects such as shadows and reflections, using diffusion
models trained on counterfactual datasets. However, the performance impact of
applying classifier-free guidance to handle object effects across removal and
insertion tasks within a unified model remains largely unexplored. To address
this gap and improve efficiency in composite editing, we propose CrimEdit,
which jointly trains the task embeddings for removal and insertion within a
single model and leverages them in a classifier-free guidance scheme --
enhancing the removal of both objects and their effects, and enabling
controllable synthesis of object effects during insertion. CrimEdit also
extends these two task prompts to be applied to spatially distinct regions,
enabling object movement (repositioning) within a single denoising step. By
employing both guidance techniques, extensive experiments show that CrimEdit
achieves superior object removal, controllable effect insertion, and efficient
object movement without requiring additional training or separate removal and
insertion stages.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: CrimEdit introduces a unified diffusion model with classifier-free guidance that jointly handles object removal, effect insertion, and movement in images, achieving superior performance without additional training.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: CrimEdit 引入了一个统一的扩散模型，该模型具有无分类器指导，可以共同处理图像中的物体移除、效果插入和移动，无需额外训练即可实现卓越的性能。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star-half-alt"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(7/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23708v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Boseong Jeon, Junghyuk Lee, Jimin Park, Kwanyoung Kim, Jingi Jung, Sangwon Lee, Hyunbo Shim</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.9500000000000001, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">FlowLUT: Efficient Image Enhancement via Differentiable LUTs and Iterative Flow Matching</h2>
            <p class="paper-summary">Deep learning-based image enhancement methods face a fundamental trade-off
between computational efficiency and representational capacity. For example,
although a conventional three-dimensional Look-Up Table (3D LUT) can process a
degraded image in real time, it lacks representational flexibility and depends
solely on a fixed prior. To address this problem, we introduce FlowLUT, a novel
end-to-end model that integrates the efficiency of LUTs, multiple priors, and
the parameter-independent characteristic of flow-matched reconstructed images.
Specifically, firstly, the input image is transformed in color space by a
collection of differentiable 3D LUTs (containing a large number of 3D LUTs with
different priors). Subsequently, a lightweight content-aware dynamically
predicts fusion weights, enabling scene-adaptive color correction with
$\mathcal{O}(1)$ complexity. Next, a lightweight fusion prediction network runs
on multiple 3D LUTs, with $\mathcal{O}(1)$ complexity for scene-adaptive color
correction.Furthermore, to address the inherent representation limitations of
LUTs, we design an innovative iterative flow matching method to restore local
structural details and eliminate artifacts. Finally, the entire model is
jointly optimized under a composite loss function enforcing perceptual and
structural fidelity. Extensive experimental results demonstrate the
effectiveness of our method on three benchmarks.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces FlowLUT, a novel image enhancement method that combines the efficiency of 3D LUTs with iterative flow matching to achieve real-time, scene-adaptive color correction and detail restoration, outperforming existing methods on multiple benchmarks. It uses multiple 3D LUTs with a lightweight network to dynamically predict the fusion weights for scene-adaptive color correction.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了一种名为FlowLUT的新型图像增强方法，它结合了3D LUT的效率和迭代流匹配，实现了实时、场景自适应的色彩校正和细节恢复，并在多个基准测试中优于现有方法。 FlowLUT使用多个含有不同先验的3D LUT，并通过轻量级网络动态预测融合权重以实现场景自适应色彩校正。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="far fa-star"></i><i class="far fa-star"></i><i class="far fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(2/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(6/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(4/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2509.23608v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Liubing Hu, Chen Wu, Anrui Wang, Dianjie Lu, Guijuan Zhang, Zhuoran Zheng</p>
            
        </motion.div>
        
    </div>

    <footer class="footer">
        Generated on 2025-10-01 04:29:20 UTC. Powered by <a href="https://github.com/onion-liu" target="_blank">onion-liu</a>.
    </footer>

</body>
</html>